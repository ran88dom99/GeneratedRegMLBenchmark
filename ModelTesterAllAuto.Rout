
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> which.computer<-Sys.info()[['nodename']]
> task.subject<-"14th20hp3cv"
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,52,1,3)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+ })
> if(length(gensTTest)<1) gensTTest<-gensTTesto#inversion[length(inversion):1]
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "algorithm may be broken"
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
[1] "task may be broken"
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm") 
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo", 
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Thu Feb 08 23:22:20 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> for(gend.data in gensTTest){
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+       
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+       
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+       
+       
+       
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+       
+       
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+           
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           if((norming=="asis")&&(trans.y==2)){next}
+           
+           
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           
+           
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+           
+           
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+           
+           
+           
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==c("ALTA","HOPPER"))>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+           
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             write.table( gensTTest[-1],file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)}
+           
+         }
+       }
+     }
+   }
+   
+ }

Attaching package: 'mlr'

The following object is masked from 'package:caret':

    train

Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
replacing previous import 'BBmisc::isFALSE' by 'backports::isFALSE' when loading 'mlr' 
[1] "Fri Feb 09 02:47:44 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 02:47:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.gausspr: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 02:48:02 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=202; interaction.depth=10; shrinkage=0.186; n.minobsinnode=14
[Tune-y] 1: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 2: n.trees=25; interaction.depth=9; shrinkage=0.0449; n.minobsinnode=20
[Tune-y] 2: rmse.test.rmse=1.94; time: 0.1 min
[Tune-x] 3: n.trees=241; interaction.depth=7; shrinkage=0.487; n.minobsinnode=20
[Tune-y] 3: rmse.test.rmse=1.03; time: 0.5 min
[Tune-x] 4: n.trees=30; interaction.depth=9; shrinkage=0.196; n.minobsinnode=6
[Tune-y] 4: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 5: n.trees=353; interaction.depth=5; shrinkage=0.153; n.minobsinnode=23
[Tune-y] 5: rmse.test.rmse=   1; time: 0.6 min
[Tune-x] 6: n.trees=671; interaction.depth=3; shrinkage=0.0584; n.minobsinnode=9
[Tune-y] 6: rmse.test.rmse=   1; time: 0.7 min
[Tune-x] 7: n.trees=206; interaction.depth=7; shrinkage=0.502; n.minobsinnode=20
[Tune-y] 7: rmse.test.rmse=1.03; time: 0.4 min
[Tune-x] 8: n.trees=26; interaction.depth=7; shrinkage=0.582; n.minobsinnode=20
[Tune-y] 8: rmse.test.rmse=1.02; time: 0.1 min
[Tune-x] 9: n.trees=102; interaction.depth=1; shrinkage=0.248; n.minobsinnode=21
[Tune-y] 9: rmse.test.rmse=2.39; time: 0.1 min
[Tune-x] 10: n.trees=18; interaction.depth=9; shrinkage=0.179; n.minobsinnode=7
[Tune-y] 10: rmse.test.rmse=1.05; time: 0.1 min
[Tune-x] 11: n.trees=881; interaction.depth=5; shrinkage=0.436; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=1.03; time: 1.4 min
[Tune-x] 12: n.trees=41; interaction.depth=1; shrinkage=0.401; n.minobsinnode=15
[Tune-y] 12: rmse.test.rmse= 2.4; time: 0.0 min
[Tune-x] 13: n.trees=140; interaction.depth=7; shrinkage=0.463; n.minobsinnode=7
[Tune-y] 13: rmse.test.rmse=1.02; time: 0.3 min
[Tune-x] 14: n.trees=33; interaction.depth=9; shrinkage=0.451; n.minobsinnode=7
[Tune-y] 14: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 15: n.trees=32; interaction.depth=6; shrinkage=0.588; n.minobsinnode=23
[Tune-y] 15: rmse.test.rmse=1.02; time: 0.1 min
[Tune-x] 16: n.trees=28; interaction.depth=7; shrinkage=0.547; n.minobsinnode=6
[Tune-y] 16: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 17: n.trees=319; interaction.depth=5; shrinkage=0.599; n.minobsinnode=9
[Tune-y] 17: rmse.test.rmse=1.02; time: 0.5 min
[Tune-x] 18: n.trees=217; interaction.depth=5; shrinkage=0.125; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=   1; time: 0.3 min
[Tune-x] 19: n.trees=32; interaction.depth=7; shrinkage=0.549; n.minobsinnode=25
[Tune-y] 19: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 20: n.trees=28; interaction.depth=9; shrinkage=0.0584; n.minobsinnode=17
[Tune-y] 20: rmse.test.rmse= 1.5; time: 0.1 min
[Tune] Result: n.trees=671; interaction.depth=3; shrinkage=0.0584; n.minobsinnode=9 : rmse.test.rmse=   1
[1] "Fri Feb 09 02:54:42 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 02:54:47 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.653; lambda=4.13
[Tune-y] 1: rmse.test.rmse=4.26; time: 0.0 min
[Tune-x] 2: alpha=0.309; lambda=0.0609
[Tune-y] 2: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 3: alpha=0.203; lambda=1.81
[Tune-y] 3: rmse.test.rmse=2.86; time: 0.0 min
[Tune-x] 4: alpha=0.0732; lambda=0.728
[Tune-y] 4: rmse.test.rmse=2.46; time: 0.0 min
[Tune-x] 5: alpha=0.691; lambda=0.235
[Tune-y] 5: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 6: alpha=0.811; lambda=0.766
[Tune-y] 6: rmse.test.rmse=2.87; time: 0.0 min
[Tune-x] 7: alpha=0.236; lambda=2.17
[Tune-y] 7: rmse.test.rmse=3.04; time: 0.0 min
[Tune-x] 8: alpha=0.326; lambda=0.00184
[Tune-y] 8: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 9: alpha=0.774; lambda=0.078
[Tune-y] 9: rmse.test.rmse= 2.4; time: 0.0 min
[Tune-x] 10: alpha=0.254; lambda=2.59
[Tune-y] 10: rmse.test.rmse=3.22; time: 0.0 min
[Tune-x] 11: alpha=0.913; lambda=0.0113
[Tune-y] 11: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 12: alpha=0.0959; lambda=0.00732
[Tune-y] 12: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 13: alpha=0.657; lambda=0.389
[Tune-y] 13: rmse.test.rmse=2.51; time: 0.0 min
[Tune-x] 14: alpha=0.837; lambda=0.652
[Tune-y] 14: rmse.test.rmse=2.78; time: 0.0 min
[Tune-x] 15: alpha=0.21; lambda=0.25
[Tune-y] 15: rmse.test.rmse=2.41; time: 0.0 min
[Tune-x] 16: alpha=0.97; lambda=0.689
[Tune-y] 16: rmse.test.rmse=2.89; time: 0.0 min
[Tune-x] 17: alpha=0.505; lambda=0.00168
[Tune-y] 17: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 18: alpha=0.412; lambda=1.21
[Tune-y] 18: rmse.test.rmse=2.85; time: 0.0 min
[Tune-x] 19: alpha=0.124; lambda=2.28
[Tune-y] 19: rmse.test.rmse=2.88; time: 0.0 min
[Tune-x] 20: alpha=0.298; lambda=0.00343
[Tune-y] 20: rmse.test.rmse=2.39; time: 0.0 min
[Tune] Result: alpha=0.298; lambda=0.00343 : rmse.test.rmse=2.39
[1] "Fri Feb 09 02:55:03 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

H2O is not running yet, starting it now...

Note:  In case of errors look at the following log files:
    C:\Users\gvg\AppData\Local\Temp\RtmpclR1Xe/h2o_gvg_started_from_r.out
    C:\Users\gvg\AppData\Local\Temp\RtmpclR1Xe/h2o_gvg_started_from_r.err

java version "1.8.0_25"
Java(TM) SE Runtime Environment (build 1.8.0_25-b18)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)

Starting H2O JVM and connecting: .. Connection successful!

R is connected to the H2O cluster: 
    H2O cluster uptime:         9 seconds 711 milliseconds 
    H2O cluster version:        3.16.0.2 
    H2O cluster version age:    2 months and 9 days  
    H2O cluster name:           H2O_started_from_R_gvg_ymg602 
    H2O cluster total nodes:    1 
    H2O cluster total memory:   0.77 GB 
    H2O cluster total cores:    4 
    H2O cluster allowed cores:  4 
    H2O cluster healthy:        TRUE 
    H2O Connection ip:          localhost 
    H2O Connection port:        54321 
    H2O Connection proxy:       NA 
    H2O Internal Security:      FALSE 
    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 
    R Version:                  R version 3.4.3 (2017-11-30) 

  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   5%  |                                                                              |=======                                                               |  10%  |                                                                              |===========                                                           |  15%  |                                                                              |==============                                                        |  20%  |                                                                              |==================                                                    |  25%  |                                                                              |=========================                                             |  35%  |                                                                              |================================                                      |  46%  |                                                                              |===========================================                           |  61%  |                                                                              |=====================================================                 |  76%  |                                                                              |================================================================      |  91%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 02:56:23 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====                                                                  |   6%  |                                                                              |===========================                                           |  38%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 02:56:38 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 02:56:48 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |====                                                                  |   6%  |                                                                              |========                                                              |  12%  |                                                                              |=============                                                         |  18%  |                                                                              |==================                                                    |  26%  |                                                                              |========================                                              |  34%  |                                                                              |============================                                          |  40%  |                                                                              |===================================                                   |  50%  |                                                                              |=========================================                             |  58%  |                                                                              |==============================================                        |  66%  |                                                                              |====================================================                  |  74%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 02:57:13 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Loading required package: kknn

Attaching package: 'kknn'

The following object is masked from 'package:caret':

    contr.dummy

Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 02:57:41 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
i = 1 (of 10192), d = 37, its = 51
i = 2 (of 10192), d = 37, its = 37
i = 3 (of 10192), d = 37, its = 37
i = 4 (of 10192), d = 6.41223, its = 7
i = 5 (of 10192), d = 4.82125, its = 7
i = 6 (of 10192), d = 37, its = 52
i = 7 (of 10192), d = 37, its = 50
i = 8 (of 10192), d = 37, its = 54
i = 9 (of 10192), d = 37, its = 49
i = 10 (of 10192), d = 37, its = 41
i = 11 (of 10192), d = 37, its = 53
i = 12 (of 10192), d = 37, its = 37
i = 13 (of 10192), d = 37, its = 39
i = 14 (of 10192), d = 15.5789, its = 7
i = 15 (of 10192), d = 37, its = 43
i = 16 (of 10192), d = 37, its = 51
i = 17 (of 10192), d = 37, its = 42
i = 18 (of 10192), d = 37, its = 37
i = 19 (of 10192), d = 5.81574, its = 7
i = 20 (of 10192), d = 37, its = 43
i = 21 (of 10192), d = 12.5053, its = 7
i = 22 (of 10192), d = 37, its = 40
i = 23 (of 10192), d = 37, its = 57
i = 24 (of 10192), d = 2.78408, its = 30
i = 25 (of 10192), d = 37, its = 53
i = 26 (of 10192), d = 37, its = 54
i = 27 (of 10192), d = 37, its = 50
i = 28 (of 10192), d = 37, its = 54
i = 29 (of 10192), d = 37, its = 42
i = 30 (of 10192), d = 37, its = 37
i = 31 (of 10192), d = 37, its = 39
i = 32 (of 10192), d = 37, its = 53
i = 33 (of 10192), d = 37, its = 52
i = 34 (of 10192), d = 37, its = 49
i = 35 (of 10192), d = 37, its = 50
i = 36 (of 10192), d = 37, its = 49
i = 37 (of 10192), d = 37, its = 55
i = 38 (of 10192), d = 37, its = 54
i = 39 (of 10192), d = 37, its = 37
i = 40 (of 10192), d = 37, its = 53
i = 41 (of 10192), d = 37, its = 53
i = 42 (of 10192), d = 37, its = 37
i = 43 (of 10192), d = 37, its = 37
i = 44 (of 10192), d = 37, its = 40
i = 45 (of 10192), d = 37, its = 52
i = 46 (of 10192), d = 37, its = 37
i = 47 (of 10192), d = 3.16006, its = 19
i = 48 (of 10192), d = 37, its = 60
i = 49 (of 10192), d = 37, its = 55
i = 50 (of 10192), d = 13.1141, its = 6
i = 51 (of 10192), d = 37, its = 37
i = 52 (of 10192), d = 37, its = 54
i = 53 (of 10192), d = 37, its = 40
i = 54 (of 10192), d = 37, its = 56
i = 55 (of 10192), d = 37, its = 51
i = 56 (of 10192), d = 37, its = 53
i = 57 (of 10192), d = 37, its = 52
i = 58 (of 10192), d = 7.50812, its = 4
i = 59 (of 10192), d = 37, its = 52
i = 60 (of 10192), d = 37, its = 38
i = 61 (of 10192), d = 8.80575, its = 4
i = 62 (of 10192), d = 37, its = 37
i = 63 (of 10192), d = 37, its = 51
i = 64 (of 10192), d = 1.37029, its = 19
i = 65 (of 10192), d = 37, its = 57
i = 66 (of 10192), d = 6.42317, its = 6
i = 67 (of 10192), d = 37, its = 57
i = 68 (of 10192), d = 2.16444, its = 17
i = 69 (of 10192), d = 7.92147, its = 3
i = 70 (of 10192), d = 37, its = 49
i = 71 (of 10192), d = 8.3456, its = 3
i = 72 (of 10192), d = 37, its = 41
i = 73 (of 10192), d = 37, its = 37
i = 74 (of 10192), d = 28.4025, its = 8
i = 75 (of 10192), d = 37, its = 52
i = 76 (of 10192), d = 37, its = 37
i = 77 (of 10192), d = 37, its = 50
i = 78 (of 10192), d = 6.5809, its = 5
i = 79 (of 10192), d = 4.30432, its = 22
i = 80 (of 10192), d = 12.5617, its = 6
i = 81 (of 10192), d = 37, its = 54
i = 82 (of 10192), d = 37, its = 37
i = 83 (of 10192), d = 37, its = 37
i = 84 (of 10192), d = 37, its = 51
i = 85 (of 10192), d = 37, its = 43
i = 86 (of 10192), d = 37, its = 53
i = 87 (of 10192), d = 37, its = 53
i = 88 (of 10192), d = 37, its = 43
i = 89 (of 10192), d = 13.431, its = 6
i = 90 (of 10192), d = 3.53261, its = 22
i = 91 (of 10192), d = 37, its = 47
i = 92 (of 10192), d = 20.449, its = 8
i = 93 (of 10192), d = 2.88007, its = 18
i = 94 (of 10192), d = 37, its = 37
i = 95 (of 10192), d = 37, its = 52
i = 96 (of 10192), d = 11.5805, its = 5
i = 97 (of 10192), d = 2.63672, its = 17
i = 98 (of 10192), d = 37, its = 53
i = 99 (of 10192), d = 37, its = 56
i = 100 (of 10192), d = 37, its = 37
i = 101 (of 10192), d = 5.50562, its = 6
i = 102 (of 10192), d = 37, its = 50
i = 103 (of 10192), d = 37, its = 43
i = 104 (of 10192), d = 37, its = 37
i = 105 (of 10192), d = 37, its = 39
i = 106 (of 10192), d = 37, its = 55
i = 107 (of 10192), d = 37, its = 37
i = 108 (of 10192), d = 37, its = 55
i = 109 (of 10192), d = 37, its = 51
i = 110 (of 10192), d = 37, its = 52
i = 111 (of 10192), d = 37, its = 47
i = 112 (of 10192), d = 37, its = 52
i = 113 (of 10192), d = 37, its = 37
i = 114 (of 10192), d = 37, its = 40
i = 115 (of 10192), d = 7.60976, its = 4
i = 116 (of 10192), d = 3.08763, its = 20
i = 117 (of 10192), d = 37, its = 54
i = 118 (of 10192), d = 37, its = 37
i = 119 (of 10192), d = 0.5, its = 45
i = 120 (of 10192), d = 4.11236, its = 20
i = 121 (of 10192), d = 37, its = 50
i = 122 (of 10192), d = 1.60801, its = 23
i = 123 (of 10192), d = 6.33677, its = 6
i = 124 (of 10192), d = 37, its = 54
i = 125 (of 10192), d = 4.52834, its = 24
i = 126 (of 10192), d = 37, its = 52
i = 127 (of 10192), d = 37, its = 48
i = 128 (of 10192), d = 37, its = 53
i = 129 (of 10192), d = 37, its = 53
i = 130 (of 10192), d = 37, its = 58
i = 131 (of 10192), d = 37, its = 53
i = 132 (of 10192), d = 9.92153, its = 5
i = 133 (of 10192), d = 3.70213, its = 20
i = 134 (of 10192), d = 37, its = 53
i = 135 (of 10192), d = 37, its = 54
i = 136 (of 10192), d = 37, its = 54
i = 137 (of 10192), d = 37, its = 50
i = 138 (of 10192), d = 37, its = 52
i = 139 (of 10192), d = 37, its = 53
i = 140 (of 10192), d = 4.00064, its = 26
i = 141 (of 10192), d = 1.8913, its = 17
i = 142 (of 10192), d = 37, its = 54
i = 143 (of 10192), d = 8.79958, its = 4
i = 144 (of 10192), d = 37, its = 57
i = 145 (of 10192), d = 5.30263, its = 25
i = 146 (of 10192), d = 30.0895, its = 9
i = 147 (of 10192), d = 6.8805, its = 5
i = 148 (of 10192), d = 37, its = 53
i = 149 (of 10192), d = 2.0889, its = 22
i = 150 (of 10192), d = 37, its = 53
i = 151 (of 10192), d = 37, its = 42
i = 152 (of 10192), d = 37, its = 52
i = 153 (of 10192), d = 6.8767, its = 5
i = 154 (of 10192), d = 37, its = 52
i = 155 (of 10192), d = 37, its = 54
i = 156 (of 10192), d = 37, its = 40
i = 157 (of 10192), d = 37, its = 56
i = 158 (of 10192), d = 1.97224, its = 18
i = 159 (of 10192), d = 27.3865, its = 7
i = 160 (of 10192), d = 37, its = 40
i = 161 (of 10192), d = 37, its = 52
i = 162 (of 10192), d = 37, its = 39
i = 163 (of 10192), d = 37, its = 50
i = 164 (of 10192), d = 5.9021, its = 6
i = 165 (of 10192), d = 37, its = 54
i = 166 (of 10192), d = 37, its = 39
i = 167 (of 10192), d = 15.8774, its = 7
i = 168 (of 10192), d = 19.4458, its = 7
i = 169 (of 10192), d = 37, its = 53
i = 170 (of 10192), d = 3.6898, its = 20
i = 171 (of 10192), d = 37, its = 37
i = 172 (of 10192), d = 37, its = 53
i = 173 (of 10192), d = 37, its = 52
i = 174 (of 10192), d = 37, its = 47
i = 175 (of 10192), d = 37, its = 41
i = 176 (of 10192), d = 37, its = 50
i = 177 (of 10192), d = 37, its = 51
i = 178 (of 10192), d = 37, its = 39
i = 179 (of 10192), d = 37, its = 37
i = 180 (of 10192), d = 37, its = 48
i = 181 (of 10192), d = 37, its = 46
i = 182 (of 10192), d = 7.29626, its = 4
i = 183 (of 10192), d = 37, its = 43
i = 184 (of 10192), d = 37, its = 51
i = 185 (of 10192), d = 37, its = 52
i = 186 (of 10192), d = 37, its = 41
i = 187 (of 10192), d = 37, its = 52
i = 188 (of 10192), d = 37, its = 52
i = 189 (of 10192), d = 37, its = 51
i = 190 (of 10192), d = 8.79419, its = 4
i = 191 (of 10192), d = 2.15025, its = 18
i = 192 (of 10192), d = 37, its = 53
i = 193 (of 10192), d = 37, its = 39
i = 194 (of 10192), d = 2.72595, its = 20
i = 195 (of 10192), d = 37, its = 52
i = 196 (of 10192), d = 6.27438, its = 6
i = 197 (of 10192), d = 37, its = 51
i = 198 (of 10192), d = 37, its = 40
i = 199 (of 10192), d = 37, its = 53
i = 200 (of 10192), d = 37, its = 42
i = 201 (of 10192), d = 37, its = 37
i = 202 (of 10192), d = 3.17977, its = 16
i = 203 (of 10192), d = 8.40048, its = 4
i = 204 (of 10192), d = 37, its = 41
i = 205 (of 10192), d = 5.3053, its = 6
i = 206 (of 10192), d = 37, its = 51
i = 207 (of 10192), d = 37, its = 53
i = 208 (of 10192), d = 37, its = 51
i = 209 (of 10192), d = 37, its = 42
i = 210 (of 10192), d = 37, its = 52
i = 211 (of 10192), d = 2.74554, its = 19
i = 212 (of 10192), d = 37, its = 56
i = 213 (of 10192), d = 37, its = 40
i = 214 (of 10192), d = 4.18871, its = 19
i = 215 (of 10192), d = 37, its = 37
i = 216 (of 10192), d = 37, its = 50
i = 217 (of 10192), d = 37, its = 37
i = 218 (of 10192), d = 3.35134, its = 22
i = 219 (of 10192), d = 37, its = 52
i = 220 (of 10192), d = 37, its = 51
i = 221 (of 10192), d = 37, its = 39
i = 222 (of 10192), d = 37, its = 51
i = 223 (of 10192), d = 37, its = 40
i = 224 (of 10192), d = 37, its = 37
i = 225 (of 10192), d = 37, its = 52
i = 226 (of 10192), d = 7.62401, its = 4
i = 227 (of 10192), d = 37, its = 55
i = 228 (of 10192), d = 37, its = 50
i = 229 (of 10192), d = 37, its = 54
i = 230 (of 10192), d = 37, its = 52
i = 231 (of 10192), d = 37, its = 39
i = 232 (of 10192), d = 37, its = 37
i = 233 (of 10192), d = 37, its = 52
i = 234 (of 10192), d = 37, its = 54
i = 235 (of 10192), d = 37, its = 50
i = 236 (of 10192), d = 3.05943, its = 16
i = 237 (of 10192), d = 37, its = 37
i = 238 (of 10192), d = 37, its = 49
i = 239 (of 10192), d = 37, its = 52
i = 240 (of 10192), d = 37, its = 50
i = 241 (of 10192), d = 37, its = 60
i = 242 (of 10192), d = 37, its = 54
i = 243 (of 10192), d = 37, its = 37
i = 244 (of 10192), d = 37, its = 56
i = 245 (of 10192), d = 4.13844, its = 21
i = 246 (of 10192), d = 37, its = 37
i = 247 (of 10192), d = 4.58791, its = 16
i = 248 (of 10192), d = 37, its = 52
i = 249 (of 10192), d = 4.73027, its = 21
i = 250 (of 10192), d = 37, its = 53
i = 251 (of 10192), d = 37, its = 47
i = 252 (of 10192), d = 6.54234, its = 6
i = 253 (of 10192), d = 3.50187, its = 18
i = 254 (of 10192), d = 37, its = 42
i = 255 (of 10192), d = 37, its = 50
i = 256 (of 10192), d = 37, its = 52
i = 257 (of 10192), d = 37, its = 56
i = 258 (of 10192), d = 37, its = 56
i = 259 (of 10192), d = 37, its = 52
i = 260 (of 10192), d = 37, its = 37
i = 261 (of 10192), d = 7.22707, its = 5
i = 262 (of 10192), d = 37, its = 53
i = 263 (of 10192), d = 37, its = 54
i = 264 (of 10192), d = 37, its = 22
i = 265 (of 10192), d = 37, its = 51
i = 266 (of 10192), d = 37, its = 53
i = 267 (of 10192), d = 18.328, its = 7
i = 268 (of 10192), d = 37, its = 52
i = 269 (of 10192), d = 37, its = 54
i = 270 (of 10192), d = 37, its = 55
i = 271 (of 10192), d = 2.16915, its = 20
i = 272 (of 10192), d = 7.11032, its = 5
i = 273 (of 10192), d = 15.6886, its = 7
i = 274 (of 10192), d = 37, its = 37
i = 275 (of 10192), d = 37, its = 55
i = 276 (of 10192), d = 4.05054, its = 19
i = 277 (of 10192), d = 13.6672, its = 6
i = 278 (of 10192), d = 10.1531, its = 5
i = 279 (of 10192), d = 10.9947, its = 5
i = 280 (of 10192), d = 3.97598, its = 17
i = 281 (of 10192), d = 37, its = 51
i = 282 (of 10192), d = 37, its = 52
i = 283 (of 10192), d = 37, its = 37
i = 284 (of 10192), d = 37, its = 37
i = 285 (of 10192), d = 37, its = 39
i = 286 (of 10192), d = 37, its = 39
i = 287 (of 10192), d = 13.6137, its = 6
i = 288 (of 10192), d = 2.45731, its = 20
i = 289 (of 10192), d = 37, its = 38
i = 290 (of 10192), d = 37, its = 52
i = 291 (of 10192), d = 37, its = 55
i = 292 (of 10192), d = 37, its = 50
i = 293 (of 10192), d = 37, its = 53
i = 294 (of 10192), d = 37, its = 50
i = 295 (of 10192), d = 37, its = 57
i = 296 (of 10192), d = 4.90947, its = 19
i = 297 (of 10192), d = 37, its = 40
i = 298 (of 10192), d = 18.9078, its = 7
i = 299 (of 10192), d = 37, its = 54
i = 300 (of 10192), d = 37, its = 47
i = 301 (of 10192), d = 9.52887, its = 5
i = 302 (of 10192), d = 37, its = 52
i = 303 (of 10192), d = 37, its = 52
i = 304 (of 10192), d = 37, its = 49
i = 305 (of 10192), d = 37, its = 51
i = 306 (of 10192), d = 37, its = 51
i = 307 (of 10192), d = 37, its = 53
i = 308 (of 10192), d = 37, its = 52
i = 309 (of 10192), d = 37, its = 37
i = 310 (of 10192), d = 37, its = 53
i = 311 (of 10192), d = 37, its = 48
i = 312 (of 10192), d = 6.77515, its = 5
i = 313 (of 10192), d = 37, its = 52
i = 314 (of 10192), d = 37, its = 55
i = 315 (of 10192), d = 37, its = 53
i = 316 (of 10192), d = 37, its = 55
i = 317 (of 10192), d = 37, its = 54
i = 318 (of 10192), d = 37, its = 50
i = 319 (of 10192), d = 7.17494, its = 5
i = 320 (of 10192), d = 37, its = 54
i = 321 (of 10192), d = 37, its = 43
i = 322 (of 10192), d = 4.35486, its = 21
i = 323 (of 10192), d = 37, its = 51
i = 324 (of 10192), d = 37, its = 51
i = 325 (of 10192), d = 37, its = 55
i = 326 (of 10192), d = 3.36284, its = 22
i = 327 (of 10192), d = 37, its = 37
i = 328 (of 10192), d = 37, its = 40
i = 329 (of 10192), d = 37, its = 52
i = 330 (of 10192), d = 37, its = 57
i = 331 (of 10192), d = 37, its = 55
i = 332 (of 10192), d = 7.66605, its = 4
i = 333 (of 10192), d = 37, its = 54
i = 334 (of 10192), d = 9.98858, its = 6
i = 335 (of 10192), d = 37, its = 52
i = 336 (of 10192), d = 37, its = 37
i = 337 (of 10192), d = 37, its = 51
i = 338 (of 10192), d = 37, its = 42
i = 339 (of 10192), d = 37, its = 42
i = 340 (of 10192), d = 37, its = 54
i = 341 (of 10192), d = 37, its = 52
i = 342 (of 10192), d = 37, its = 52
i = 343 (of 10192), d = 37, its = 57
i = 344 (of 10192), d = 6.0087, its = 6
i = 345 (of 10192), d = 37, its = 58
i = 346 (of 10192), d = 37, its = 41
i = 347 (of 10192), d = 9.73611, its = 5
i = 348 (of 10192), d = 37, its = 50
i = 349 (of 10192), d = 11.6475, its = 6
i = 350 (of 10192), d = 9.2903, its = 4
i = 351 (of 10192), d = 37, its = 58
i = 352 (of 10192), d = 37, its = 44
i = 353 (of 10192), d = 37, its = 53
i = 354 (of 10192), d = 37, its = 51
i = 355 (of 10192), d = 37, its = 37
i = 356 (of 10192), d = 37, its = 53
i = 357 (of 10192), d = 37, its = 51
i = 358 (of 10192), d = 9.1172, its = 5
i = 359 (of 10192), d = 37, its = 40
i = 360 (of 10192), d = 37, its = 55
i = 361 (of 10192), d = 10.4215, its = 5
i = 362 (of 10192), d = 37, its = 52
i = 363 (of 10192), d = 37, its = 53
i = 364 (of 10192), d = 9.57431, its = 5
i = 365 (of 10192), d = 4.58905, its = 17
i = 366 (of 10192), d = 37, its = 39
i = 367 (of 10192), d = 37, its = 37
i = 368 (of 10192), d = 37, its = 52
i = 369 (of 10192), d = 37, its = 50
i = 370 (of 10192), d = 37, its = 53
i = 371 (of 10192), d = 37, its = 40
i = 372 (of 10192), d = 5.89303, its = 7
i = 373 (of 10192), d = 37, its = 55
i = 374 (of 10192), d = 3.74967, its = 19
i = 375 (of 10192), d = 3.82036, its = 18
i = 376 (of 10192), d = 37, its = 54
i = 377 (of 10192), d = 37, its = 51
i = 378 (of 10192), d = 2.11771, its = 19
i = 379 (of 10192), d = 8.73417, its = 4
i = 380 (of 10192), d = 1.51447, its = 17
i = 381 (of 10192), d = 37, its = 55
i = 382 (of 10192), d = 5.04745, its = 20
i = 383 (of 10192), d = 7.94156, its = 3
i = 384 (of 10192), d = 37, its = 51
i = 385 (of 10192), d = 6.54968, its = 6
i = 386 (of 10192), d = 37, its = 39
i = 387 (of 10192), d = 37, its = 55
i = 388 (of 10192), d = 37, its = 51
i = 389 (of 10192), d = 37, its = 54
i = 390 (of 10192), d = 37, its = 39
i = 391 (of 10192), d = 37, its = 37
i = 392 (of 10192), d = 37, its = 53
i = 393 (of 10192), d = 8.17196, its = 3
i = 394 (of 10192), d = 37, its = 52
i = 395 (of 10192), d = 10.2364, its = 5
i = 396 (of 10192), d = 37, its = 49
i = 397 (of 10192), d = 37, its = 37
i = 398 (of 10192), d = 37, its = 50
i = 399 (of 10192), d = 37, its = 51
i = 400 (of 10192), d = 37, its = 53
i = 401 (of 10192), d = 37, its = 37
i = 402 (of 10192), d = 37, its = 40
i = 403 (of 10192), d = 9.61966, its = 5
i = 404 (of 10192), d = 37, its = 53
i = 405 (of 10192), d = 37, its = 54
i = 406 (of 10192), d = 4.26725, its = 18
i = 407 (of 10192), d = 37, its = 54
i = 408 (of 10192), d = 37, its = 52
i = 409 (of 10192), d = 1.76961, its = 17
i = 410 (of 10192), d = 37, its = 50
i = 411 (of 10192), d = 37, its = 55
i = 412 (of 10192), d = 37, its = 53
i = 413 (of 10192), d = 37, its = 52
i = 414 (of 10192), d = 9.04096, its = 4
i = 415 (of 10192), d = 5.65105, its = 6
i = 416 (of 10192), d = 37, its = 37
i = 417 (of 10192), d = 8.83999, its = 4
i = 418 (of 10192), d = 37, its = 41
i = 419 (of 10192), d = 37, its = 37
i = 420 (of 10192), d = 4.3722, its = 22
i = 421 (of 10192), d = 37, its = 56
i = 422 (of 10192), d = 37, its = 51
i = 423 (of 10192), d = 6.79083, its = 5
i = 424 (of 10192), d = 37, its = 37
i = 425 (of 10192), d = 37, its = 40
i = 426 (of 10192), d = 37, its = 40
i = 427 (of 10192), d = 3.3296, its = 21
i = 428 (of 10192), d = 37, its = 50
i = 429 (of 10192), d = 37, its = 37
i = 430 (of 10192), d = 37, its = 40
i = 431 (of 10192), d = 37, its = 40
i = 432 (of 10192), d = 37, its = 41
i = 433 (of 10192), d = 5.77524, its = 6
i = 434 (of 10192), d = 37, its = 37
i = 435 (of 10192), d = 37, its = 52
i = 436 (of 10192), d = 37, its = 51
i = 437 (of 10192), d = 37, its = 54
i = 438 (of 10192), d = 8.83719, its = 4
i = 439 (of 10192), d = 0.975758, its = 20
i = 440 (of 10192), d = 37, its = 54
i = 441 (of 10192), d = 37, its = 52
i = 442 (of 10192), d = 37, its = 54
i = 443 (of 10192), d = 37, its = 37
i = 444 (of 10192), d = 37, its = 38
i = 445 (of 10192), d = 37, its = 53
i = 446 (of 10192), d = 37, its = 52
i = 447 (of 10192), d = 37, its = 55
i = 448 (of 10192), d = 37, its = 45
i = 449 (of 10192), d = 8.03123, its = 3
i = 450 (of 10192), d = 37, its = 37
i = 451 (of 10192), d = 37, its = 41
i = 452 (of 10192), d = 37, its = 38
i = 453 (of 10192), d = 37, its = 55
i = 454 (of 10192), d = 37, its = 37
i = 455 (of 10192), d = 37, its = 51
i = 456 (of 10192), d = 5.36554, its = 8
i = 457 (of 10192), d = 37, its = 51
i = 458 (of 10192), d = 8.95221, its = 4
i = 459 (of 10192), d = 37, its = 52
i = 460 (of 10192), d = 37, its = 57
i = 461 (of 10192), d = 37, its = 54
i = 462 (of 10192), d = 5.39907, its = 6
i = 463 (of 10192), d = 2.38852, its = 28
i = 464 (of 10192), d = 4.07642, its = 18
i = 465 (of 10192), d = 1.78142, its = 18
i = 466 (of 10192), d = 37, its = 51
i = 467 (of 10192), d = 37, its = 37
i = 468 (of 10192), d = 37, its = 53
i = 469 (of 10192), d = 37, its = 37
i = 470 (of 10192), d = 37, its = 40
i = 471 (of 10192), d = 37, its = 52
i = 472 (of 10192), d = 37, its = 55
i = 473 (of 10192), d = 37, its = 49
i = 474 (of 10192), d = 37, its = 55
i = 475 (of 10192), d = 6.19002, its = 6
i = 476 (of 10192), d = 37, its = 39
i = 477 (of 10192), d = 37, its = 55
i = 478 (of 10192), d = 37, its = 42
i = 479 (of 10192), d = 16.452, its = 9
i = 480 (of 10192), d = 37, its = 49
i = 481 (of 10192), d = 37, its = 53
i = 482 (of 10192), d = 37, its = 56
i = 483 (of 10192), d = 3.63395, its = 25
i = 484 (of 10192), d = 5.79546, its = 6
i = 485 (of 10192), d = 37, its = 57
i = 486 (of 10192), d = 37, its = 37
i = 487 (of 10192), d = 2.99981, its = 19
i = 488 (of 10192), d = 37, its = 41
i = 489 (of 10192), d = 37, its = 53
i = 490 (of 10192), d = 37, its = 51
i = 491 (of 10192), d = 37, its = 51
i = 492 (of 10192), d = 37, its = 55
i = 493 (of 10192), d = 37, its = 47
i = 494 (of 10192), d = 37, its = 53
i = 495 (of 10192), d = 37, its = 37
i = 496 (of 10192), d = 37, its = 52
i = 497 (of 10192), d = 37, its = 54
i = 498 (of 10192), d = 37, its = 52
i = 499 (of 10192), d = 3.64145, its = 17
i = 500 (of 10192), d = 37, its = 52
i = 501 (of 10192), d = 37, its = 55
i = 502 (of 10192), d = 7.84395, its = 3
i = 503 (of 10192), d = 3.76332, its = 16
i = 504 (of 10192), d = 2.04358, its = 19
i = 505 (of 10192), d = 2.65302, its = 18
i = 506 (of 10192), d = 10.4163, its = 5
i = 507 (of 10192), d = 37, its = 52
i = 508 (of 10192), d = 2.0429, its = 17
i = 509 (of 10192), d = 6.92058, its = 5
i = 510 (of 10192), d = 37, its = 56
i = 511 (of 10192), d = 37, its = 51
i = 512 (of 10192), d = 37, its = 42
i = 513 (of 10192), d = 37, its = 47
i = 514 (of 10192), d = 37, its = 53
i = 515 (of 10192), d = 37, its = 52
i = 516 (of 10192), d = 2.38852, its = 28
i = 517 (of 10192), d = 37, its = 53
i = 518 (of 10192), d = 37, its = 48
i = 519 (of 10192), d = 37, its = 48
i = 520 (of 10192), d = 37, its = 52
i = 521 (of 10192), d = 37, its = 45
i = 522 (of 10192), d = 37, its = 37
i = 523 (of 10192), d = 36.4782, its = 9
i = 524 (of 10192), d = 37, its = 37
i = 525 (of 10192), d = 5.91274, its = 6
i = 526 (of 10192), d = 37, its = 53
i = 527 (of 10192), d = 8.97408, its = 4
i = 528 (of 10192), d = 8.13124, its = 3
i = 529 (of 10192), d = 37, its = 52
i = 530 (of 10192), d = 37, its = 57
i = 531 (of 10192), d = 37, its = 52
i = 532 (of 10192), d = 37, its = 54
i = 533 (of 10192), d = 37, its = 53
i = 534 (of 10192), d = 37, its = 37
i = 535 (of 10192), d = 37, its = 51
i = 536 (of 10192), d = 37, its = 39
i = 537 (of 10192), d = 37, its = 52
i = 538 (of 10192), d = 9.47885, its = 5
i = 539 (of 10192), d = 37, its = 48
i = 540 (of 10192), d = 37, its = 40
i = 541 (of 10192), d = 37, its = 40
i = 542 (of 10192), d = 8.36837, its = 3
i = 543 (of 10192), d = 37, its = 56
i = 544 (of 10192), d = 6.0087, its = 6
i = 545 (of 10192), d = 37, its = 57
i = 546 (of 10192), d = 2.79051, its = 19
i = 547 (of 10192), d = 37, its = 52
i = 548 (of 10192), d = 37, its = 50
i = 549 (of 10192), d = 37, its = 49
i = 550 (of 10192), d = 37, its = 38
i = 551 (of 10192), d = 37, its = 40
i = 552 (of 10192), d = 37, its = 49
i = 553 (of 10192), d = 37, its = 43
i = 554 (of 10192), d = 37, its = 41
i = 555 (of 10192), d = 37, its = 54
i = 556 (of 10192), d = 37, its = 56
i = 557 (of 10192), d = 37, its = 55
i = 558 (of 10192), d = 37, its = 37
i = 559 (of 10192), d = 37, its = 53
i = 560 (of 10192), d = 37, its = 40
i = 561 (of 10192), d = 2.54456, its = 24
i = 562 (of 10192), d = 37, its = 52
i = 563 (of 10192), d = 37, its = 50
i = 564 (of 10192), d = 7.5844, its = 4
i = 565 (of 10192), d = 37, its = 49
i = 566 (of 10192), d = 37, its = 52
i = 567 (of 10192), d = 37, its = 55
i = 568 (of 10192), d = 37, its = 50
i = 569 (of 10192), d = 1.62774, its = 21
i = 570 (of 10192), d = 37, its = 48
i = 571 (of 10192), d = 37, its = 51
i = 572 (of 10192), d = 37, its = 51
i = 573 (of 10192), d = 6.59819, its = 5
i = 574 (of 10192), d = 37, its = 52
i = 575 (of 10192), d = 37, its = 50
i = 576 (of 10192), d = 37, its = 53
i = 577 (of 10192), d = 4.46263, its = 20
i = 578 (of 10192), d = 6.83062, its = 5
i = 579 (of 10192), d = 37, its = 53
i = 580 (of 10192), d = 3.44327, its = 18
i = 581 (of 10192), d = 37, its = 52
i = 582 (of 10192), d = 37, its = 39
i = 583 (of 10192), d = 2.16855, its = 17
i = 584 (of 10192), d = 37, its = 37
i = 585 (of 10192), d = 37, its = 39
i = 586 (of 10192), d = 2.58601, its = 19
i = 587 (of 10192), d = 37, its = 37
i = 588 (of 10192), d = 37, its = 56
i = 589 (of 10192), d = 6.79716, its = 5
i = 590 (of 10192), d = 37, its = 37
i = 591 (of 10192), d = 37, its = 51
i = 592 (of 10192), d = 37, its = 37
i = 593 (of 10192), d = 37, its = 53
i = 594 (of 10192), d = 4.40278, its = 29
i = 595 (of 10192), d = 5.79171, its = 7
i = 596 (of 10192), d = 37, its = 53
i = 597 (of 10192), d = 37, its = 53
i = 598 (of 10192), d = 37, its = 43
i = 599 (of 10192), d = 8.97015, its = 4
i = 600 (of 10192), d = 37, its = 52
i = 601 (of 10192), d = 37, its = 53
i = 602 (of 10192), d = 37, its = 55
i = 603 (of 10192), d = 37, its = 53
i = 604 (of 10192), d = 37, its = 37
i = 605 (of 10192), d = 37, its = 40
i = 606 (of 10192), d = 37, its = 55
i = 607 (of 10192), d = 37, its = 48
i = 608 (of 10192), d = 37, its = 52
i = 609 (of 10192), d = 37, its = 51
i = 610 (of 10192), d = 37, its = 53
i = 611 (of 10192), d = 10.7911, its = 5
i = 612 (of 10192), d = 37, its = 48
i = 613 (of 10192), d = 37, its = 54
i = 614 (of 10192), d = 1.59021, its = 18
i = 615 (of 10192), d = 37, its = 52
i = 616 (of 10192), d = 37, its = 55
i = 617 (of 10192), d = 37, its = 52
i = 618 (of 10192), d = 9.12727, its = 5
i = 619 (of 10192), d = 37, its = 41
i = 620 (of 10192), d = 1.97305, its = 20
i = 621 (of 10192), d = 37, its = 37
i = 622 (of 10192), d = 37, its = 41
i = 623 (of 10192), d = 7.87842, its = 3
i = 624 (of 10192), d = 4.54544, its = 19
i = 625 (of 10192), d = 37, its = 37
i = 626 (of 10192), d = 37, its = 45
i = 627 (of 10192), d = 37, its = 52
i = 628 (of 10192), d = 12.6401, its = 6
i = 629 (of 10192), d = 32.4771, its = 18
i = 630 (of 10192), d = 2.18192, its = 17
i = 631 (of 10192), d = 37, its = 39
i = 632 (of 10192), d = 3.38593, its = 22
i = 633 (of 10192), d = 37, its = 41
i = 634 (of 10192), d = 37, its = 37
i = 635 (of 10192), d = 37, its = 52
i = 636 (of 10192), d = 1.46182, its = 19
i = 637 (of 10192), d = 37, its = 41
i = 638 (of 10192), d = 37, its = 51
i = 639 (of 10192), d = 37, its = 55
i = 640 (of 10192), d = 37, its = 37
i = 641 (of 10192), d = 9.23302, its = 5
i = 642 (of 10192), d = 37, its = 37
i = 643 (of 10192), d = 7.77069, its = 3
i = 644 (of 10192), d = 3.19482, its = 17
i = 645 (of 10192), d = 37, its = 42
i = 646 (of 10192), d = 6.36176, its = 6
i = 647 (of 10192), d = 37, its = 53
i = 648 (of 10192), d = 37, its = 49
i = 649 (of 10192), d = 12.09, its = 6
i = 650 (of 10192), d = 37, its = 37
i = 651 (of 10192), d = 37, its = 57
i = 652 (of 10192), d = 37, its = 49
i = 653 (of 10192), d = 2.93779, its = 19
i = 654 (of 10192), d = 37, its = 56
i = 655 (of 10192), d = 37, its = 52
i = 656 (of 10192), d = 37, its = 51
i = 657 (of 10192), d = 37, its = 55
i = 658 (of 10192), d = 37, its = 40
i = 659 (of 10192), d = 37, its = 37
i = 660 (of 10192), d = 37, its = 37
i = 661 (of 10192), d = 37, its = 57
i = 662 (of 10192), d = 37, its = 52
i = 663 (of 10192), d = 37, its = 51
i = 664 (of 10192), d = 37, its = 48
i = 665 (of 10192), d = 37, its = 54
i = 666 (of 10192), d = 14.8848, its = 9
i = 667 (of 10192), d = 37, its = 39
i = 668 (of 10192), d = 37, its = 55
i = 669 (of 10192), d = 3.17881, its = 19
i = 670 (of 10192), d = 37, its = 51
i = 671 (of 10192), d = 37, its = 37
i = 672 (of 10192), d = 37, its = 52
i = 673 (of 10192), d = 37, its = 52
i = 674 (of 10192), d = 37, its = 51
i = 675 (of 10192), d = 2.31431, its = 20
i = 676 (of 10192), d = 37, its = 54
i = 677 (of 10192), d = 37, its = 53
i = 678 (of 10192), d = 37, its = 52
i = 679 (of 10192), d = 37, its = 51
i = 680 (of 10192), d = 37, its = 47
i = 681 (of 10192), d = 2.05143, its = 16
i = 682 (of 10192), d = 37, its = 50
i = 683 (of 10192), d = 2.93532, its = 17
i = 684 (of 10192), d = 5.80812, its = 6
i = 685 (of 10192), d = 37, its = 54
i = 686 (of 10192), d = 11.5598, its = 6
i = 687 (of 10192), d = 37, its = 54
i = 688 (of 10192), d = 6.51731, its = 6
i = 689 (of 10192), d = 37, its = 40
i = 690 (of 10192), d = 37, its = 55
i = 691 (of 10192), d = 37, its = 53
i = 692 (of 10192), d = 37, its = 49
i = 693 (of 10192), d = 37, its = 52
i = 694 (of 10192), d = 14.1242, its = 7
i = 695 (of 10192), d = 37, its = 37
i = 696 (of 10192), d = 2.68517, its = 20
i = 697 (of 10192), d = 37, its = 51
i = 698 (of 10192), d = 3.86232, its = 18
i = 699 (of 10192), d = 37, its = 54
i = 700 (of 10192), d = 37, its = 53
i = 701 (of 10192), d = 37, its = 54
i = 702 (of 10192), d = 10.3793, its = 5
i = 703 (of 10192), d = 4.65744, its = 21
i = 704 (of 10192), d = 37, its = 42
i = 705 (of 10192), d = 37, its = 54
i = 706 (of 10192), d = 37, its = 50
i = 707 (of 10192), d = 37, its = 37
i = 708 (of 10192), d = 11.9985, its = 6
i = 709 (of 10192), d = 37, its = 40
i = 710 (of 10192), d = 37, its = 37
i = 711 (of 10192), d = 37, its = 56
i = 712 (of 10192), d = 10.9438, its = 6
i = 713 (of 10192), d = 11.2018, its = 5
i = 714 (of 10192), d = 4.11197, its = 19
i = 715 (of 10192), d = 37, its = 37
i = 716 (of 10192), d = 4.67867, its = 19
i = 717 (of 10192), d = 3.44515, its = 22
i = 718 (of 10192), d = 37, its = 50
i = 719 (of 10192), d = 37, its = 54
i = 720 (of 10192), d = 37, its = 54
i = 721 (of 10192), d = 37, its = 41
i = 722 (of 10192), d = 37, its = 52
i = 723 (of 10192), d = 37, its = 54
i = 724 (of 10192), d = 37, its = 48
i = 725 (of 10192), d = 9.52995, its = 5
i = 726 (of 10192), d = 37, its = 53
i = 727 (of 10192), d = 37, its = 37
i = 728 (of 10192), d = 5.72396, its = 6
i = 729 (of 10192), d = 37, its = 37
i = 730 (of 10192), d = 37, its = 54
i = 731 (of 10192), d = 37, its = 50
i = 732 (of 10192), d = 37, its = 51
i = 733 (of 10192), d = 8.67601, its = 4
i = 734 (of 10192), d = 37, its = 55
i = 735 (of 10192), d = 37, its = 52
i = 736 (of 10192), d = 37, its = 37
i = 737 (of 10192), d = 7.50955, its = 4
i = 738 (of 10192), d = 37, its = 53
i = 739 (of 10192), d = 37, its = 40
i = 740 (of 10192), d = 37, its = 52
i = 741 (of 10192), d = 37, its = 49
i = 742 (of 10192), d = 7.71755, its = 3
i = 743 (of 10192), d = 8.41322, its = 4
i = 744 (of 10192), d = 37, its = 52
i = 745 (of 10192), d = 9.38555, its = 4
i = 746 (of 10192), d = 37, its = 51
i = 747 (of 10192), d = 37, its = 50
i = 748 (of 10192), d = 37, its = 39
i = 749 (of 10192), d = 37, its = 53
i = 750 (of 10192), d = 2.0672, its = 16
i = 751 (of 10192), d = 4.83632, its = 24
i = 752 (of 10192), d = 37, its = 37
i = 753 (of 10192), d = 37, its = 51
i = 754 (of 10192), d = 9.51288, its = 4
i = 755 (of 10192), d = 37, its = 53
i = 756 (of 10192), d = 37, its = 51
i = 757 (of 10192), d = 37, its = 41
i = 758 (of 10192), d = 37, its = 39
i = 759 (of 10192), d = 37, its = 53
i = 760 (of 10192), d = 37, its = 54
i = 761 (of 10192), d = 16.5926, its = 7
i = 762 (of 10192), d = 5.82326, its = 6
i = 763 (of 10192), d = 37, its = 37
i = 764 (of 10192), d = 6.09136, its = 7
i = 765 (of 10192), d = 13.7071, its = 6
i = 766 (of 10192), d = 37, its = 53
i = 767 (of 10192), d = 37, its = 53
i = 768 (of 10192), d = 37, its = 42
i = 769 (of 10192), d = 37, its = 49
i = 770 (of 10192), d = 37, its = 54
i = 771 (of 10192), d = 37, its = 53
i = 772 (of 10192), d = 37, its = 52
i = 773 (of 10192), d = 37, its = 52
i = 774 (of 10192), d = 37, its = 53
i = 775 (of 10192), d = 6.24125, its = 6
i = 776 (of 10192), d = 37, its = 57
i = 777 (of 10192), d = 37, its = 51
i = 778 (of 10192), d = 37, its = 52
i = 779 (of 10192), d = 37, its = 48
i = 780 (of 10192), d = 37, its = 51
i = 781 (of 10192), d = 7.26232, its = 4
i = 782 (of 10192), d = 37, its = 49
i = 783 (of 10192), d = 37, its = 54
i = 784 (of 10192), d = 37, its = 52
i = 785 (of 10192), d = 37, its = 53
i = 786 (of 10192), d = 2.56973, its = 21
i = 787 (of 10192), d = 7.40542, its = 4
i = 788 (of 10192), d = 4.51352, its = 24
i = 789 (of 10192), d = 37, its = 55
i = 790 (of 10192), d = 7.45404, its = 4
i = 791 (of 10192), d = 37, its = 37
i = 792 (of 10192), d = 37, its = 52
i = 793 (of 10192), d = 37, its = 54
i = 794 (of 10192), d = 37, its = 51
i = 795 (of 10192), d = 37, its = 39
i = 796 (of 10192), d = 37, its = 39
i = 797 (of 10192), d = 37, its = 51
i = 798 (of 10192), d = 37, its = 54
i = 799 (of 10192), d = 37, its = 51
i = 800 (of 10192), d = 37, its = 54
i = 801 (of 10192), d = 37, its = 54
i = 802 (of 10192), d = 37, its = 40
i = 803 (of 10192), d = 37, its = 37
i = 804 (of 10192), d = 37, its = 37
i = 805 (of 10192), d = 37, its = 53
i = 806 (of 10192), d = 37, its = 55
i = 807 (of 10192), d = 37, its = 37
i = 808 (of 10192), d = 35.2126, its = 9
i = 809 (of 10192), d = 6.36599, its = 6
i = 810 (of 10192), d = 37, its = 41
i = 811 (of 10192), d = 37, its = 49
i = 812 (of 10192), d = 37, its = 37
i = 813 (of 10192), d = 37, its = 52
i = 814 (of 10192), d = 5.4787, its = 6
i = 815 (of 10192), d = 37, its = 39
i = 816 (of 10192), d = 37, its = 56
i = 817 (of 10192), d = 37, its = 58
i = 818 (of 10192), d = 7.70352, its = 4
i = 819 (of 10192), d = 3.26758, its = 21
i = 820 (of 10192), d = 37, its = 53
i = 821 (of 10192), d = 37, its = 56
i = 822 (of 10192), d = 5.94717, its = 7
i = 823 (of 10192), d = 8.71161, its = 4
i = 824 (of 10192), d = 8.90864, its = 4
i = 825 (of 10192), d = 37, its = 51
i = 826 (of 10192), d = 37, its = 44
i = 827 (of 10192), d = 37, its = 51
i = 828 (of 10192), d = 37, its = 54
i = 829 (of 10192), d = 37, its = 42
i = 830 (of 10192), d = 37, its = 52
i = 831 (of 10192), d = 37, its = 56
i = 832 (of 10192), d = 37, its = 56
i = 833 (of 10192), d = 2.89593, its = 20
i = 834 (of 10192), d = 3.98849, its = 18
i = 835 (of 10192), d = 37, its = 39
i = 836 (of 10192), d = 6.72125, its = 6
i = 837 (of 10192), d = 37, its = 39
i = 838 (of 10192), d = 2.33903, its = 19
i = 839 (of 10192), d = 37, its = 54
i = 840 (of 10192), d = 37, its = 54
i = 841 (of 10192), d = 37, its = 54
i = 842 (of 10192), d = 37, its = 37
i = 843 (of 10192), d = 37, its = 50
i = 844 (of 10192), d = 37, its = 37
i = 845 (of 10192), d = 37, its = 53
i = 846 (of 10192), d = 37, its = 55
i = 847 (of 10192), d = 37, its = 52
i = 848 (of 10192), d = 37, its = 54
i = 849 (of 10192), d = 37, its = 37
i = 850 (of 10192), d = 10.4127, its = 5
i = 851 (of 10192), d = 3.82845, its = 19
i = 852 (of 10192), d = 37, its = 55
i = 853 (of 10192), d = 37, its = 53
i = 854 (of 10192), d = 2.70612, its = 16
i = 855 (of 10192), d = 37, its = 55
i = 856 (of 10192), d = 1.32503, its = 28
i = 857 (of 10192), d = 37, its = 40
i = 858 (of 10192), d = 37, its = 55
i = 859 (of 10192), d = 37, its = 39
i = 860 (of 10192), d = 12.9558, its = 7
i = 861 (of 10192), d = 11.8446, its = 6
i = 862 (of 10192), d = 37, its = 44
i = 863 (of 10192), d = 11.7826, its = 7
i = 864 (of 10192), d = 37, its = 50
i = 865 (of 10192), d = 37, its = 52
i = 866 (of 10192), d = 37, its = 37
i = 867 (of 10192), d = 4.67605, its = 19
i = 868 (of 10192), d = 37, its = 52
i = 869 (of 10192), d = 37, its = 55
i = 870 (of 10192), d = 37, its = 55
i = 871 (of 10192), d = 37, its = 53
i = 872 (of 10192), d = 5.31515, its = 5
i = 873 (of 10192), d = 37, its = 52
i = 874 (of 10192), d = 37, its = 55
i = 875 (of 10192), d = 4.83925, its = 8
i = 876 (of 10192), d = 5.46827, its = 7
i = 877 (of 10192), d = 13.4256, its = 6
i = 878 (of 10192), d = 7.18202, its = 4
i = 879 (of 10192), d = 4.88374, its = 6
i = 880 (of 10192), d = 37, its = 56
i = 881 (of 10192), d = 37, its = 37
i = 882 (of 10192), d = 37, its = 52
i = 883 (of 10192), d = 7.59964, its = 4
i = 884 (of 10192), d = 37, its = 48
i = 885 (of 10192), d = 37, its = 48
i = 886 (of 10192), d = 37, its = 54
i = 887 (of 10192), d = 37, its = 53
i = 888 (of 10192), d = 2.99964, its = 17
i = 889 (of 10192), d = 4.80184, its = 23
i = 890 (of 10192), d = 11.2616, its = 6
i = 891 (of 10192), d = 9.47884, its = 5
i = 892 (of 10192), d = 37, its = 51
i = 893 (of 10192), d = 37, its = 54
i = 894 (of 10192), d = 37, its = 45
i = 895 (of 10192), d = 37, its = 50
i = 896 (of 10192), d = 37, its = 37
i = 897 (of 10192), d = 4.9867, its = 18
i = 898 (of 10192), d = 37, its = 54
i = 899 (of 10192), d = 37, its = 49
i = 900 (of 10192), d = 37, its = 37
i = 901 (of 10192), d = 4.3043, its = 21
i = 902 (of 10192), d = 37, its = 49
i = 903 (of 10192), d = 37, its = 52
i = 904 (of 10192), d = 7.78051, its = 3
i = 905 (of 10192), d = 7.78986, its = 3
i = 906 (of 10192), d = 37, its = 51
i = 907 (of 10192), d = 37, its = 39
i = 908 (of 10192), d = 37, its = 55
i = 909 (of 10192), d = 18.3619, its = 7
i = 910 (of 10192), d = 37, its = 55
i = 911 (of 10192), d = 37, its = 53
i = 912 (of 10192), d = 37, its = 53
i = 913 (of 10192), d = 6.88573, its = 6
i = 914 (of 10192), d = 37, its = 50
i = 915 (of 10192), d = 37, its = 53
i = 916 (of 10192), d = 37, its = 37
i = 917 (of 10192), d = 37, its = 39
i = 918 (of 10192), d = 37, its = 53
i = 919 (of 10192), d = 37, its = 56
i = 920 (of 10192), d = 37, its = 37
i = 921 (of 10192), d = 10.3218, its = 5
i = 922 (of 10192), d = 37, its = 48
i = 923 (of 10192), d = 37, its = 37
i = 924 (of 10192), d = 37, its = 37
i = 925 (of 10192), d = 37, its = 50
i = 926 (of 10192), d = 37, its = 39
i = 927 (of 10192), d = 37, its = 38
i = 928 (of 10192), d = 3.92787, its = 19
i = 929 (of 10192), d = 37, its = 53
i = 930 (of 10192), d = 37, its = 54
i = 931 (of 10192), d = 37, its = 60
i = 932 (of 10192), d = 37, its = 50
i = 933 (of 10192), d = 5.48394, its = 7
i = 934 (of 10192), d = 37, its = 37
i = 935 (of 10192), d = 2.62897, its = 24
i = 936 (of 10192), d = 2.15185, its = 19
i = 937 (of 10192), d = 37, its = 49
i = 938 (of 10192), d = 2.89768, its = 22
i = 939 (of 10192), d = 1.65455, its = 22
i = 940 (of 10192), d = 4.1213, its = 16
i = 941 (of 10192), d = 37, its = 54
i = 942 (of 10192), d = 37, its = 55
i = 943 (of 10192), d = 2.72534, its = 17
i = 944 (of 10192), d = 37, its = 56
i = 945 (of 10192), d = 3.95723, its = 16
i = 946 (of 10192), d = 1.40567, its = 28
i = 947 (of 10192), d = 37, its = 37
i = 948 (of 10192), d = 3.01349, its = 29
i = 949 (of 10192), d = 37, its = 37
i = 950 (of 10192), d = 6.64725, its = 6
i = 951 (of 10192), d = 37, its = 47
i = 952 (of 10192), d = 37, its = 53
i = 953 (of 10192), d = 37, its = 52
i = 954 (of 10192), d = 37, its = 39
i = 955 (of 10192), d = 37, its = 50
i = 956 (of 10192), d = 37, its = 50
i = 957 (of 10192), d = 37, its = 37
i = 958 (of 10192), d = 37, its = 55
i = 959 (of 10192), d = 37, its = 44
i = 960 (of 10192), d = 3.41595, its = 18
i = 961 (of 10192), d = 37, its = 56
i = 962 (of 10192), d = 25.1227, its = 8
i = 963 (of 10192), d = 37, its = 49
i = 964 (of 10192), d = 11.0219, its = 5
i = 965 (of 10192), d = 3.24537, its = 21
i = 966 (of 10192), d = 37, its = 40
i = 967 (of 10192), d = 0.931102, its = 18
i = 968 (of 10192), d = 4.10487, its = 20
i = 969 (of 10192), d = 37, its = 57
i = 970 (of 10192), d = 37, its = 51
i = 971 (of 10192), d = 37, its = 39
i = 972 (of 10192), d = 37, its = 54
i = 973 (of 10192), d = 37, its = 48
i = 974 (of 10192), d = 37, its = 39
i = 975 (of 10192), d = 37, its = 48
i = 976 (of 10192), d = 20.6846, its = 8
i = 977 (of 10192), d = 37, its = 55
i = 978 (of 10192), d = 37, its = 37
i = 979 (of 10192), d = 6.53057, its = 6
i = 980 (of 10192), d = 37, its = 37
i = 981 (of 10192), d = 4.02536, its = 23
i = 982 (of 10192), d = 7.09382, its = 5
i = 983 (of 10192), d = 37, its = 54
i = 984 (of 10192), d = 37, its = 40
i = 985 (of 10192), d = 6.67303, its = 6
i = 986 (of 10192), d = 37, its = 57
i = 987 (of 10192), d = 37, its = 39
i = 988 (of 10192), d = 37, its = 37
i = 989 (of 10192), d = 4.15755, its = 18
i = 990 (of 10192), d = 37, its = 37
i = 991 (of 10192), d = 37, its = 51
i = 992 (of 10192), d = 8.42811, its = 4
i = 993 (of 10192), d = 37, its = 37
i = 994 (of 10192), d = 3.28427, its = 23
i = 995 (of 10192), d = 2.92874, its = 17
i = 996 (of 10192), d = 37, its = 51
i = 997 (of 10192), d = 37, its = 57
i = 998 (of 10192), d = 8.73346, its = 4
i = 999 (of 10192), d = 37, its = 37
i = 1000 (of 10192), d = 37, its = 52
i = 1001 (of 10192), d = 37, its = 51
i = 1002 (of 10192), d = 2.34673, its = 21
i = 1003 (of 10192), d = 37, its = 51
i = 1004 (of 10192), d = 5.60229, its = 6
i = 1005 (of 10192), d = 37, its = 50
i = 1006 (of 10192), d = 37, its = 51
i = 1007 (of 10192), d = 37, its = 49
i = 1008 (of 10192), d = 37, its = 37
i = 1009 (of 10192), d = 5.01656, its = 7
i = 1010 (of 10192), d = 2.843, its = 20
i = 1011 (of 10192), d = 6.58251, its = 6
i = 1012 (of 10192), d = 37, its = 53
i = 1013 (of 10192), d = 37, its = 53
i = 1014 (of 10192), d = 37, its = 54
i = 1015 (of 10192), d = 37, its = 53
i = 1016 (of 10192), d = 37, its = 37
i = 1017 (of 10192), d = 37, its = 40
i = 1018 (of 10192), d = 37, its = 55
i = 1019 (of 10192), d = 37, its = 52
i = 1020 (of 10192), d = 37, its = 51
i = 1021 (of 10192), d = 37, its = 50
i = 1022 (of 10192), d = 37, its = 52
i = 1023 (of 10192), d = 37, its = 37
i = 1024 (of 10192), d = 7.84612, its = 3
i = 1025 (of 10192), d = 37, its = 51
i = 1026 (of 10192), d = 37, its = 55
i = 1027 (of 10192), d = 2.98631, its = 15
i = 1028 (of 10192), d = 37, its = 57
i = 1029 (of 10192), d = 37, its = 51
i = 1030 (of 10192), d = 37, its = 51
i = 1031 (of 10192), d = 37, its = 49
i = 1032 (of 10192), d = 37, its = 37
i = 1033 (of 10192), d = 3.3203, its = 19
i = 1034 (of 10192), d = 37, its = 50
i = 1035 (of 10192), d = 37, its = 56
i = 1036 (of 10192), d = 37, its = 55
i = 1037 (of 10192), d = 37, its = 37
i = 1038 (of 10192), d = 37, its = 39
i = 1039 (of 10192), d = 37, its = 41
i = 1040 (of 10192), d = 37, its = 37
i = 1041 (of 10192), d = 3.12814, its = 20
i = 1042 (of 10192), d = 3.61367, its = 19
i = 1043 (of 10192), d = 37, its = 41
i = 1044 (of 10192), d = 2.80556, its = 16
i = 1045 (of 10192), d = 37, its = 37
i = 1046 (of 10192), d = 37, its = 51
i = 1047 (of 10192), d = 6.05097, its = 21
i = 1048 (of 10192), d = 37, its = 52
i = 1049 (of 10192), d = 7.52255, its = 4
i = 1050 (of 10192), d = 37, its = 52
i = 1051 (of 10192), d = 37, its = 57
i = 1052 (of 10192), d = 37, its = 53
i = 1053 (of 10192), d = 37, its = 54
i = 1054 (of 10192), d = 3.8008, its = 29
i = 1055 (of 10192), d = 12.632, its = 6
i = 1056 (of 10192), d = 4.0819, its = 17
i = 1057 (of 10192), d = 37, its = 37
i = 1058 (of 10192), d = 2.36061, its = 19
i = 1059 (of 10192), d = 37, its = 53
i = 1060 (of 10192), d = 2.33667, its = 22
i = 1061 (of 10192), d = 37, its = 52
i = 1062 (of 10192), d = 37, its = 55
i = 1063 (of 10192), d = 37, its = 49
i = 1064 (of 10192), d = 37, its = 51
i = 1065 (of 10192), d = 37, its = 54
i = 1066 (of 10192), d = 4.83632, its = 24
i = 1067 (of 10192), d = 6.77297, its = 5
i = 1068 (of 10192), d = 37, its = 51
i = 1069 (of 10192), d = 4.07846, its = 17
i = 1070 (of 10192), d = 10.9951, its = 5
i = 1071 (of 10192), d = 37, its = 39
i = 1072 (of 10192), d = 37, its = 48
i = 1073 (of 10192), d = 37, its = 52
i = 1074 (of 10192), d = 8.25534, its = 3
i = 1075 (of 10192), d = 37, its = 51
i = 1076 (of 10192), d = 37, its = 52
i = 1077 (of 10192), d = 37, its = 55
i = 1078 (of 10192), d = 23.3345, its = 8
i = 1079 (of 10192), d = 37, its = 51
i = 1080 (of 10192), d = 2.03393, its = 17
i = 1081 (of 10192), d = 37, its = 54
i = 1082 (of 10192), d = 37, its = 37
i = 1083 (of 10192), d = 37, its = 54
i = 1084 (of 10192), d = 37, its = 56
i = 1085 (of 10192), d = 17.7614, its = 7
i = 1086 (of 10192), d = 37, its = 48
i = 1087 (of 10192), d = 1.49747, its = 20
i = 1088 (of 10192), d = 3.29112, its = 16
i = 1089 (of 10192), d = 37, its = 51
i = 1090 (of 10192), d = 7.00488, its = 5
i = 1091 (of 10192), d = 37, its = 54
i = 1092 (of 10192), d = 37, its = 51
i = 1093 (of 10192), d = 37, its = 54
i = 1094 (of 10192), d = 37, its = 48
i = 1095 (of 10192), d = 37, its = 53
i = 1096 (of 10192), d = 37, its = 51
i = 1097 (of 10192), d = 37, its = 37
i = 1098 (of 10192), d = 37, its = 49
i = 1099 (of 10192), d = 2.55536, its = 18
i = 1100 (of 10192), d = 37, its = 52
i = 1101 (of 10192), d = 7.24559, its = 5
i = 1102 (of 10192), d = 5.98456, its = 6
i = 1103 (of 10192), d = 4.81898, its = 17
i = 1104 (of 10192), d = 37, its = 41
i = 1105 (of 10192), d = 37, its = 41
i = 1106 (of 10192), d = 37, its = 40
i = 1107 (of 10192), d = 12.4861, its = 6
i = 1108 (of 10192), d = 37, its = 51
i = 1109 (of 10192), d = 37, its = 49
i = 1110 (of 10192), d = 37, its = 52
i = 1111 (of 10192), d = 2.25275, its = 17
i = 1112 (of 10192), d = 37, its = 51
i = 1113 (of 10192), d = 37, its = 37
i = 1114 (of 10192), d = 37, its = 52
i = 1115 (of 10192), d = 37, its = 50
i = 1116 (of 10192), d = 37, its = 52
i = 1117 (of 10192), d = 37, its = 52
i = 1118 (of 10192), d = 10.1556, its = 5
i = 1119 (of 10192), d = 15.0887, its = 8
i = 1120 (of 10192), d = 7.98224, its = 2
i = 1121 (of 10192), d = 37, its = 37
i = 1122 (of 10192), d = 37, its = 54
i = 1123 (of 10192), d = 37, its = 41
i = 1124 (of 10192), d = 16.1852, its = 7
i = 1125 (of 10192), d = 37, its = 56
i = 1126 (of 10192), d = 37, its = 40
i = 1127 (of 10192), d = 37, its = 55
i = 1128 (of 10192), d = 37, its = 52
i = 1129 (of 10192), d = 37, its = 41
i = 1130 (of 10192), d = 3.02214, its = 16
i = 1131 (of 10192), d = 37, its = 53
i = 1132 (of 10192), d = 37, its = 52
i = 1133 (of 10192), d = 37, its = 51
i = 1134 (of 10192), d = 37, its = 53
i = 1135 (of 10192), d = 5.92102, its = 6
i = 1136 (of 10192), d = 2.7354, its = 18
i = 1137 (of 10192), d = 37, its = 52
i = 1138 (of 10192), d = 37, its = 40
i = 1139 (of 10192), d = 23.9381, its = 8
i = 1140 (of 10192), d = 14.1719, its = 7
i = 1141 (of 10192), d = 6.0014, its = 6
i = 1142 (of 10192), d = 37, its = 51
i = 1143 (of 10192), d = 37, its = 37
i = 1144 (of 10192), d = 37, its = 50
i = 1145 (of 10192), d = 37, its = 51
i = 1146 (of 10192), d = 37, its = 50
i = 1147 (of 10192), d = 37, its = 49
i = 1148 (of 10192), d = 37, its = 54
i = 1149 (of 10192), d = 37, its = 54
i = 1150 (of 10192), d = 37, its = 54
i = 1151 (of 10192), d = 37, its = 49
i = 1152 (of 10192), d = 37, its = 49
i = 1153 (of 10192), d = 10.4665, its = 5
i = 1154 (of 10192), d = 3.50119, its = 18
i = 1155 (of 10192), d = 37, its = 54
i = 1156 (of 10192), d = 18.1323, its = 7
i = 1157 (of 10192), d = 37, its = 48
i = 1158 (of 10192), d = 21.9297, its = 25
i = 1159 (of 10192), d = 37, its = 37
i = 1160 (of 10192), d = 3.55715, its = 22
i = 1161 (of 10192), d = 37, its = 37
i = 1162 (of 10192), d = 37, its = 37
i = 1163 (of 10192), d = 37, its = 52
i = 1164 (of 10192), d = 5.16612, its = 6
i = 1165 (of 10192), d = 37, its = 52
i = 1166 (of 10192), d = 7.50202, its = 4
i = 1167 (of 10192), d = 6.49525, its = 6
i = 1168 (of 10192), d = 37, its = 43
i = 1169 (of 10192), d = 37, its = 54
i = 1170 (of 10192), d = 37, its = 50
i = 1171 (of 10192), d = 6.19842, its = 6
i = 1172 (of 10192), d = 37, its = 50
i = 1173 (of 10192), d = 37, its = 37
i = 1174 (of 10192), d = 37, its = 48
i = 1175 (of 10192), d = 37, its = 37
i = 1176 (of 10192), d = 5.44626, its = 8
i = 1177 (of 10192), d = 37, its = 54
i = 1178 (of 10192), d = 37, its = 51
i = 1179 (of 10192), d = 7.81771, its = 3
i = 1180 (of 10192), d = 37, its = 40
i = 1181 (of 10192), d = 4.0819, its = 17
i = 1182 (of 10192), d = 37, its = 54
i = 1183 (of 10192), d = 3.12758, its = 20
i = 1184 (of 10192), d = 37, its = 52
i = 1185 (of 10192), d = 37, its = 54
i = 1186 (of 10192), d = 37, its = 54
i = 1187 (of 10192), d = 37, its = 53
i = 1188 (of 10192), d = 37, its = 54
i = 1189 (of 10192), d = 37, its = 53
i = 1190 (of 10192), d = 10.5454, its = 5
i = 1191 (of 10192), d = 2.78524, its = 21
i = 1192 (of 10192), d = 37, its = 47
i = 1193 (of 10192), d = 37, its = 52
i = 1194 (of 10192), d = 37, its = 54
i = 1195 (of 10192), d = 6.07671, its = 6
i = 1196 (of 10192), d = 22.1366, its = 7
i = 1197 (of 10192), d = 37, its = 52
i = 1198 (of 10192), d = 37, its = 43
i = 1199 (of 10192), d = 37, its = 52
i = 1200 (of 10192), d = 37, its = 54
i = 1201 (of 10192), d = 37, its = 50
i = 1202 (of 10192), d = 37, its = 55
i = 1203 (of 10192), d = 37, its = 37
i = 1204 (of 10192), d = 6.78139, its = 5
i = 1205 (of 10192), d = 7.26048, its = 4
i = 1206 (of 10192), d = 37, its = 41
i = 1207 (of 10192), d = 37, its = 54
i = 1208 (of 10192), d = 37, its = 50
i = 1209 (of 10192), d = 37, its = 52
i = 1210 (of 10192), d = 37, its = 40
i = 1211 (of 10192), d = 8.43962, its = 4
i = 1212 (of 10192), d = 15.9321, its = 6
i = 1213 (of 10192), d = 37, its = 41
i = 1214 (of 10192), d = 2.77982, its = 21
i = 1215 (of 10192), d = 37, its = 37
i = 1216 (of 10192), d = 37, its = 55
i = 1217 (of 10192), d = 37, its = 52
i = 1218 (of 10192), d = 37, its = 37
i = 1219 (of 10192), d = 37, its = 50
i = 1220 (of 10192), d = 2.61207, its = 18
i = 1221 (of 10192), d = 37, its = 37
i = 1222 (of 10192), d = 37, its = 37
i = 1223 (of 10192), d = 37, its = 52
i = 1224 (of 10192), d = 37, its = 56
i = 1225 (of 10192), d = 37, its = 53
i = 1226 (of 10192), d = 9.0873, its = 4
i = 1227 (of 10192), d = 5.76087, its = 8
i = 1228 (of 10192), d = 37, its = 54
i = 1229 (of 10192), d = 7.80084, its = 3
i = 1230 (of 10192), d = 37, its = 37
i = 1231 (of 10192), d = 37, its = 37
i = 1232 (of 10192), d = 3.7524, its = 19
i = 1233 (of 10192), d = 6.48841, its = 6
i = 1234 (of 10192), d = 37, its = 53
i = 1235 (of 10192), d = 3.16105, its = 19
i = 1236 (of 10192), d = 37, its = 40
i = 1237 (of 10192), d = 3.01887, its = 18
i = 1238 (of 10192), d = 37, its = 42
i = 1239 (of 10192), d = 37, its = 54
i = 1240 (of 10192), d = 8.69039, its = 4
i = 1241 (of 10192), d = 37, its = 43
i = 1242 (of 10192), d = 2.69609, its = 24
i = 1243 (of 10192), d = 8.17706, its = 3
i = 1244 (of 10192), d = 37, its = 55
i = 1245 (of 10192), d = 37, its = 54
i = 1246 (of 10192), d = 37, its = 56
i = 1247 (of 10192), d = 37, its = 52
i = 1248 (of 10192), d = 37, its = 54
i = 1249 (of 10192), d = 34.4378, its = 9
i = 1250 (of 10192), d = 37, its = 53
i = 1251 (of 10192), d = 37, its = 56
i = 1252 (of 10192), d = 6.31113, its = 6
i = 1253 (of 10192), d = 37, its = 51
i = 1254 (of 10192), d = 13.318, its = 7
i = 1255 (of 10192), d = 37, its = 39
i = 1256 (of 10192), d = 37, its = 38
i = 1257 (of 10192), d = 37, its = 38
i = 1258 (of 10192), d = 22.0948, its = 8
i = 1259 (of 10192), d = 37, its = 49
i = 1260 (of 10192), d = 2.87022, its = 19
i = 1261 (of 10192), d = 8.3298, its = 3
i = 1262 (of 10192), d = 9.9704, its = 5
i = 1263 (of 10192), d = 37, its = 54
i = 1264 (of 10192), d = 37, its = 50
i = 1265 (of 10192), d = 37, its = 38
i = 1266 (of 10192), d = 37, its = 56
i = 1267 (of 10192), d = 37, its = 39
i = 1268 (of 10192), d = 37, its = 51
i = 1269 (of 10192), d = 37, its = 53
i = 1270 (of 10192), d = 8.85298, its = 4
i = 1271 (of 10192), d = 37, its = 38
i = 1272 (of 10192), d = 37, its = 53
i = 1273 (of 10192), d = 12.456, its = 6
i = 1274 (of 10192), d = 37, its = 52
i = 1275 (of 10192), d = 37, its = 49
i = 1276 (of 10192), d = 7.5667, its = 4
i = 1277 (of 10192), d = 37, its = 37
i = 1278 (of 10192), d = 6.02747, its = 6
i = 1279 (of 10192), d = 15.9986, its = 7
i = 1280 (of 10192), d = 37, its = 52
i = 1281 (of 10192), d = 37, its = 37
i = 1282 (of 10192), d = 37, its = 55
i = 1283 (of 10192), d = 37, its = 54
i = 1284 (of 10192), d = 37, its = 50
i = 1285 (of 10192), d = 37, its = 56
i = 1286 (of 10192), d = 4.72555, its = 7
i = 1287 (of 10192), d = 37, its = 40
i = 1288 (of 10192), d = 37, its = 53
i = 1289 (of 10192), d = 37, its = 51
i = 1290 (of 10192), d = 37, its = 41
i = 1291 (of 10192), d = 37, its = 37
i = 1292 (of 10192), d = 37, its = 40
i = 1293 (of 10192), d = 37, its = 51
i = 1294 (of 10192), d = 37, its = 40
i = 1295 (of 10192), d = 37, its = 57
i = 1296 (of 10192), d = 37, its = 54
i = 1297 (of 10192), d = 37, its = 37
i = 1298 (of 10192), d = 9.14033, its = 5
i = 1299 (of 10192), d = 13.2923, its = 6
i = 1300 (of 10192), d = 37, its = 42
i = 1301 (of 10192), d = 37, its = 40
i = 1302 (of 10192), d = 37, its = 41
i = 1303 (of 10192), d = 37, its = 37
i = 1304 (of 10192), d = 37, its = 37
i = 1305 (of 10192), d = 37, its = 54
i = 1306 (of 10192), d = 37, its = 37
i = 1307 (of 10192), d = 1.56754, its = 18
i = 1308 (of 10192), d = 37, its = 51
i = 1309 (of 10192), d = 4.10487, its = 20
i = 1310 (of 10192), d = 37, its = 37
i = 1311 (of 10192), d = 7.10535, its = 5
i = 1312 (of 10192), d = 37, its = 37
i = 1313 (of 10192), d = 37, its = 39
i = 1314 (of 10192), d = 6.40931, its = 7
i = 1315 (of 10192), d = 37, its = 37
i = 1316 (of 10192), d = 37, its = 54
i = 1317 (of 10192), d = 3.40393, its = 18
i = 1318 (of 10192), d = 37, its = 58
i = 1319 (of 10192), d = 37, its = 57
i = 1320 (of 10192), d = 9.26196, its = 4
i = 1321 (of 10192), d = 1.59915, its = 21
i = 1322 (of 10192), d = 37, its = 51
i = 1323 (of 10192), d = 37, its = 55
i = 1324 (of 10192), d = 37, its = 49
i = 1325 (of 10192), d = 6.73214, its = 5
i = 1326 (of 10192), d = 2.33833, its = 22
i = 1327 (of 10192), d = 37, its = 40
i = 1328 (of 10192), d = 37, its = 37
i = 1329 (of 10192), d = 37, its = 37
i = 1330 (of 10192), d = 8.69283, its = 4
i = 1331 (of 10192), d = 37, its = 59
i = 1332 (of 10192), d = 37, its = 54
i = 1333 (of 10192), d = 8.67963, its = 4
i = 1334 (of 10192), d = 37, its = 55
i = 1335 (of 10192), d = 37, its = 57
i = 1336 (of 10192), d = 37, its = 53
i = 1337 (of 10192), d = 37, its = 56
i = 1338 (of 10192), d = 37, its = 37
i = 1339 (of 10192), d = 37, its = 53
i = 1340 (of 10192), d = 2.88022, its = 18
i = 1341 (of 10192), d = 37, its = 52
i = 1342 (of 10192), d = 37, its = 53
i = 1343 (of 10192), d = 37, its = 40
i = 1344 (of 10192), d = 13.5822, its = 6
i = 1345 (of 10192), d = 37, its = 47
i = 1346 (of 10192), d = 37, its = 39
i = 1347 (of 10192), d = 37, its = 41
i = 1348 (of 10192), d = 37, its = 54
i = 1349 (of 10192), d = 37, its = 37
i = 1350 (of 10192), d = 37, its = 53
i = 1351 (of 10192), d = 2.16762, its = 18
i = 1352 (of 10192), d = 37, its = 53
i = 1353 (of 10192), d = 37, its = 37
i = 1354 (of 10192), d = 37, its = 54
i = 1355 (of 10192), d = 37, its = 37
i = 1356 (of 10192), d = 37, its = 52
i = 1357 (of 10192), d = 37, its = 41
i = 1358 (of 10192), d = 37, its = 54
i = 1359 (of 10192), d = 3.86885, its = 22
i = 1360 (of 10192), d = 4.42192, its = 19
i = 1361 (of 10192), d = 37, its = 51
i = 1362 (of 10192), d = 37, its = 52
i = 1363 (of 10192), d = 37, its = 48
i = 1364 (of 10192), d = 37, its = 54
i = 1365 (of 10192), d = 37, its = 57
i = 1366 (of 10192), d = 37, its = 54
i = 1367 (of 10192), d = 5.17531, its = 6
i = 1368 (of 10192), d = 16.1852, its = 7
i = 1369 (of 10192), d = 37, its = 41
i = 1370 (of 10192), d = 37, its = 51
i = 1371 (of 10192), d = 37, its = 55
i = 1372 (of 10192), d = 4.50764, its = 16
i = 1373 (of 10192), d = 37, its = 51
i = 1374 (of 10192), d = 1.60507, its = 24
i = 1375 (of 10192), d = 37, its = 40
i = 1376 (of 10192), d = 10.0436, its = 5
i = 1377 (of 10192), d = 37, its = 37
i = 1378 (of 10192), d = 3.23261, its = 26
i = 1379 (of 10192), d = 37, its = 52
i = 1380 (of 10192), d = 37, its = 41
i = 1381 (of 10192), d = 37, its = 52
i = 1382 (of 10192), d = 37, its = 54
i = 1383 (of 10192), d = 2.02525, its = 20
i = 1384 (of 10192), d = 37, its = 37
i = 1385 (of 10192), d = 37, its = 43
i = 1386 (of 10192), d = 10.6099, its = 5
i = 1387 (of 10192), d = 37, its = 54
i = 1388 (of 10192), d = 37, its = 39
i = 1389 (of 10192), d = 37, its = 37
i = 1390 (of 10192), d = 23.0876, its = 8
i = 1391 (of 10192), d = 37, its = 54
i = 1392 (of 10192), d = 37, its = 41
i = 1393 (of 10192), d = 37, its = 54
i = 1394 (of 10192), d = 37, its = 57
i = 1395 (of 10192), d = 37, its = 53
i = 1396 (of 10192), d = 37, its = 39
i = 1397 (of 10192), d = 3.00157, its = 16
i = 1398 (of 10192), d = 2.46198, its = 26
i = 1399 (of 10192), d = 37, its = 37
i = 1400 (of 10192), d = 37, its = 52
i = 1401 (of 10192), d = 37, its = 54
i = 1402 (of 10192), d = 37, its = 40
i = 1403 (of 10192), d = 37, its = 50
i = 1404 (of 10192), d = 8.97843, its = 6
i = 1405 (of 10192), d = 37, its = 57
i = 1406 (of 10192), d = 37, its = 50
i = 1407 (of 10192), d = 37, its = 46
i = 1408 (of 10192), d = 37, its = 51
i = 1409 (of 10192), d = 37, its = 41
i = 1410 (of 10192), d = 37, its = 54
i = 1411 (of 10192), d = 37, its = 37
i = 1412 (of 10192), d = 37, its = 39
i = 1413 (of 10192), d = 37, its = 50
i = 1414 (of 10192), d = 15.6958, its = 6
i = 1415 (of 10192), d = 37, its = 56
i = 1416 (of 10192), d = 37, its = 51
i = 1417 (of 10192), d = 37, its = 55
i = 1418 (of 10192), d = 37, its = 37
i = 1419 (of 10192), d = 11.9793, its = 6
i = 1420 (of 10192), d = 2.26344, its = 22
i = 1421 (of 10192), d = 37, its = 50
i = 1422 (of 10192), d = 37, its = 49
i = 1423 (of 10192), d = 6.24585, its = 7
i = 1424 (of 10192), d = 2.72595, its = 20
i = 1425 (of 10192), d = 37, its = 52
i = 1426 (of 10192), d = 8.95169, its = 4
i = 1427 (of 10192), d = 37, its = 52
i = 1428 (of 10192), d = 8.73987, its = 4
i = 1429 (of 10192), d = 37, its = 52
i = 1430 (of 10192), d = 37, its = 53
i = 1431 (of 10192), d = 37, its = 51
i = 1432 (of 10192), d = 9.73871, its = 5
i = 1433 (of 10192), d = 37, its = 47
i = 1434 (of 10192), d = 8.4489, its = 4
i = 1435 (of 10192), d = 11.7645, its = 6
i = 1436 (of 10192), d = 37, its = 53
i = 1437 (of 10192), d = 37, its = 52
i = 1438 (of 10192), d = 37, its = 57
i = 1439 (of 10192), d = 8.57049, its = 4
i = 1440 (of 10192), d = 8.62332, its = 4
i = 1441 (of 10192), d = 6.7109, its = 5
i = 1442 (of 10192), d = 9.15856, its = 4
i = 1443 (of 10192), d = 37, its = 53
i = 1444 (of 10192), d = 37, its = 40
i = 1445 (of 10192), d = 37, its = 55
i = 1446 (of 10192), d = 37, its = 53
i = 1447 (of 10192), d = 6.0705, its = 6
i = 1448 (of 10192), d = 37, its = 49
i = 1449 (of 10192), d = 37, its = 55
i = 1450 (of 10192), d = 37, its = 55
i = 1451 (of 10192), d = 37, its = 37
i = 1452 (of 10192), d = 37, its = 51
i = 1453 (of 10192), d = 27.5369, its = 8
i = 1454 (of 10192), d = 37, its = 55
i = 1455 (of 10192), d = 2.0909, its = 17
i = 1456 (of 10192), d = 3.53592, its = 18
i = 1457 (of 10192), d = 37, its = 40
i = 1458 (of 10192), d = 37, its = 54
i = 1459 (of 10192), d = 7.20131, its = 7
i = 1460 (of 10192), d = 37, its = 53
i = 1461 (of 10192), d = 37, its = 52
i = 1462 (of 10192), d = 37, its = 53
i = 1463 (of 10192), d = 37, its = 56
i = 1464 (of 10192), d = 9.71979, its = 5
i = 1465 (of 10192), d = 12.6981, its = 6
i = 1466 (of 10192), d = 8.85946, its = 4
i = 1467 (of 10192), d = 37, its = 51
i = 1468 (of 10192), d = 37, its = 50
i = 1469 (of 10192), d = 37, its = 53
i = 1470 (of 10192), d = 37, its = 52
i = 1471 (of 10192), d = 37, its = 37
i = 1472 (of 10192), d = 37, its = 56
i = 1473 (of 10192), d = 37, its = 54
i = 1474 (of 10192), d = 8.67544, its = 4
i = 1475 (of 10192), d = 4.78032, its = 19
i = 1476 (of 10192), d = 37, its = 38
i = 1477 (of 10192), d = 37, its = 37
i = 1478 (of 10192), d = 37, its = 37
i = 1479 (of 10192), d = 7.84395, its = 3
i = 1480 (of 10192), d = 37, its = 40
i = 1481 (of 10192), d = 37, its = 40
i = 1482 (of 10192), d = 1.5142, its = 18
i = 1483 (of 10192), d = 37, its = 50
i = 1484 (of 10192), d = 6.06265, its = 6
i = 1485 (of 10192), d = 37, its = 53
i = 1486 (of 10192), d = 2.70723, its = 26
i = 1487 (of 10192), d = 4.16866, its = 21
i = 1488 (of 10192), d = 37, its = 52
i = 1489 (of 10192), d = 37, its = 49
i = 1490 (of 10192), d = 37, its = 50
i = 1491 (of 10192), d = 6.85471, its = 5
i = 1492 (of 10192), d = 37, its = 52
i = 1493 (of 10192), d = 3.21018, its = 26
i = 1494 (of 10192), d = 37, its = 56
i = 1495 (of 10192), d = 2.75987, its = 17
i = 1496 (of 10192), d = 37, its = 53
i = 1497 (of 10192), d = 37, its = 38
i = 1498 (of 10192), d = 37, its = 53
i = 1499 (of 10192), d = 8.56834, its = 4
i = 1500 (of 10192), d = 37, its = 37
i = 1501 (of 10192), d = 8.22851, its = 3
i = 1502 (of 10192), d = 37, its = 42
i = 1503 (of 10192), d = 8.24184, its = 3
i = 1504 (of 10192), d = 37, its = 56
i = 1505 (of 10192), d = 37, its = 37
i = 1506 (of 10192), d = 37, its = 48
i = 1507 (of 10192), d = 10.9193, its = 5
i = 1508 (of 10192), d = 37, its = 41
i = 1509 (of 10192), d = 37, its = 57
i = 1510 (of 10192), d = 37, its = 55
i = 1511 (of 10192), d = 37, its = 53
i = 1512 (of 10192), d = 37, its = 52
i = 1513 (of 10192), d = 37, its = 56
i = 1514 (of 10192), d = 37, its = 55
i = 1515 (of 10192), d = 37, its = 51
i = 1516 (of 10192), d = 37, its = 55
i = 1517 (of 10192), d = 37, its = 52
i = 1518 (of 10192), d = 37, its = 54
i = 1519 (of 10192), d = 37, its = 43
i = 1520 (of 10192), d = 37, its = 37
i = 1521 (of 10192), d = 37, its = 52
i = 1522 (of 10192), d = 37, its = 42
i = 1523 (of 10192), d = 15.4239, its = 7
i = 1524 (of 10192), d = 37, its = 52
i = 1525 (of 10192), d = 37, its = 50
i = 1526 (of 10192), d = 37, its = 52
i = 1527 (of 10192), d = 37, its = 50
i = 1528 (of 10192), d = 2.59998, its = 16
i = 1529 (of 10192), d = 5.94719, its = 6
i = 1530 (of 10192), d = 37, its = 55
i = 1531 (of 10192), d = 37, its = 39
i = 1532 (of 10192), d = 37, its = 57
i = 1533 (of 10192), d = 37, its = 57
i = 1534 (of 10192), d = 37, its = 55
i = 1535 (of 10192), d = 37, its = 48
i = 1536 (of 10192), d = 3.3607, its = 18
i = 1537 (of 10192), d = 3.24526, its = 18
i = 1538 (of 10192), d = 37, its = 37
i = 1539 (of 10192), d = 37, its = 53
i = 1540 (of 10192), d = 37, its = 49
i = 1541 (of 10192), d = 2.87885, its = 18
i = 1542 (of 10192), d = 37, its = 37
i = 1543 (of 10192), d = 37, its = 47
i = 1544 (of 10192), d = 3.79798, its = 19
i = 1545 (of 10192), d = 2.65692, its = 21
i = 1546 (of 10192), d = 37, its = 37
i = 1547 (of 10192), d = 6.84892, its = 5
i = 1548 (of 10192), d = 37, its = 49
i = 1549 (of 10192), d = 37, its = 51
i = 1550 (of 10192), d = 5.13911, its = 7
i = 1551 (of 10192), d = 37, its = 56
i = 1552 (of 10192), d = 37, its = 41
i = 1553 (of 10192), d = 37, its = 53
i = 1554 (of 10192), d = 37, its = 54
i = 1555 (of 10192), d = 10.9246, its = 7
i = 1556 (of 10192), d = 37, its = 52
i = 1557 (of 10192), d = 37, its = 54
i = 1558 (of 10192), d = 37, its = 51
i = 1559 (of 10192), d = 37, its = 56
i = 1560 (of 10192), d = 37, its = 37
i = 1561 (of 10192), d = 37, its = 54
i = 1562 (of 10192), d = 10.7775, its = 7
i = 1563 (of 10192), d = 37, its = 38
i = 1564 (of 10192), d = 8.71891, its = 5
i = 1565 (of 10192), d = 37, its = 52
i = 1566 (of 10192), d = 37, its = 39
i = 1567 (of 10192), d = 37, its = 37
i = 1568 (of 10192), d = 3.85955, its = 27
i = 1569 (of 10192), d = 37, its = 54
i = 1570 (of 10192), d = 3.27417, its = 26
i = 1571 (of 10192), d = 6.8935, its = 5
i = 1572 (of 10192), d = 2.47046, its = 17
i = 1573 (of 10192), d = 37, its = 54
i = 1574 (of 10192), d = 37, its = 40
i = 1575 (of 10192), d = 37, its = 52
i = 1576 (of 10192), d = 37, its = 37
i = 1577 (of 10192), d = 37, its = 53
i = 1578 (of 10192), d = 37, its = 54
i = 1579 (of 10192), d = 37, its = 51
i = 1580 (of 10192), d = 9.73611, its = 5
i = 1581 (of 10192), d = 5.42073, its = 6
i = 1582 (of 10192), d = 37, its = 51
i = 1583 (of 10192), d = 5.78214, its = 6
i = 1584 (of 10192), d = 4.54045, its = 19
i = 1585 (of 10192), d = 37, its = 37
i = 1586 (of 10192), d = 5.79546, its = 6
i = 1587 (of 10192), d = 37, its = 51
i = 1588 (of 10192), d = 37, its = 57
i = 1589 (of 10192), d = 37, its = 37
i = 1590 (of 10192), d = 37, its = 41
i = 1591 (of 10192), d = 37, its = 37
i = 1592 (of 10192), d = 37, its = 37
i = 1593 (of 10192), d = 2.97731, its = 18
i = 1594 (of 10192), d = 10.6222, its = 5
i = 1595 (of 10192), d = 37, its = 37
i = 1596 (of 10192), d = 5.82111, its = 9
i = 1597 (of 10192), d = 37, its = 52
i = 1598 (of 10192), d = 37, its = 52
i = 1599 (of 10192), d = 37, its = 51
i = 1600 (of 10192), d = 37, its = 51
i = 1601 (of 10192), d = 37, its = 37
i = 1602 (of 10192), d = 37, its = 54
i = 1603 (of 10192), d = 37, its = 53
i = 1604 (of 10192), d = 37, its = 48
i = 1605 (of 10192), d = 37, its = 41
i = 1606 (of 10192), d = 37, its = 37
i = 1607 (of 10192), d = 37, its = 37
i = 1608 (of 10192), d = 37, its = 42
i = 1609 (of 10192), d = 37, its = 55
i = 1610 (of 10192), d = 37, its = 54
i = 1611 (of 10192), d = 7.62818, its = 4
i = 1612 (of 10192), d = 37, its = 37
i = 1613 (of 10192), d = 6.21591, its = 7
i = 1614 (of 10192), d = 37, its = 51
i = 1615 (of 10192), d = 3.14506, its = 25
i = 1616 (of 10192), d = 37, its = 55
i = 1617 (of 10192), d = 37, its = 38
i = 1618 (of 10192), d = 37, its = 41
i = 1619 (of 10192), d = 37, its = 42
i = 1620 (of 10192), d = 37, its = 41
i = 1621 (of 10192), d = 37, its = 48
i = 1622 (of 10192), d = 37, its = 37
i = 1623 (of 10192), d = 7.23749, its = 5
i = 1624 (of 10192), d = 37, its = 53
i = 1625 (of 10192), d = 3.75491, its = 22
i = 1626 (of 10192), d = 37, its = 40
i = 1627 (of 10192), d = 37, its = 54
i = 1628 (of 10192), d = 37, its = 52
i = 1629 (of 10192), d = 37, its = 50
i = 1630 (of 10192), d = 37, its = 51
i = 1631 (of 10192), d = 3.16811, its = 18
i = 1632 (of 10192), d = 2.58423, its = 20
i = 1633 (of 10192), d = 5.11345, its = 28
i = 1634 (of 10192), d = 37, its = 51
i = 1635 (of 10192), d = 37, its = 51
i = 1636 (of 10192), d = 37, its = 51
i = 1637 (of 10192), d = 37, its = 56
i = 1638 (of 10192), d = 2.85445, its = 23
i = 1639 (of 10192), d = 7.45806, its = 4
i = 1640 (of 10192), d = 37, its = 54
i = 1641 (of 10192), d = 11.6253, its = 5
i = 1642 (of 10192), d = 37, its = 55
i = 1643 (of 10192), d = 37, its = 48
i = 1644 (of 10192), d = 37, its = 51
i = 1645 (of 10192), d = 37, its = 37
i = 1646 (of 10192), d = 37, its = 48
i = 1647 (of 10192), d = 1.94969, its = 18
i = 1648 (of 10192), d = 37, its = 53
i = 1649 (of 10192), d = 37, its = 48
i = 1650 (of 10192), d = 37, its = 39
i = 1651 (of 10192), d = 37, its = 48
i = 1652 (of 10192), d = 7.05269, its = 5
i = 1653 (of 10192), d = 37, its = 51
i = 1654 (of 10192), d = 37, its = 56
i = 1655 (of 10192), d = 37, its = 50
i = 1656 (of 10192), d = 37, its = 40
i = 1657 (of 10192), d = 37, its = 41
i = 1658 (of 10192), d = 5.88664, its = 6
i = 1659 (of 10192), d = 37, its = 37
i = 1660 (of 10192), d = 37, its = 37
i = 1661 (of 10192), d = 37, its = 43
i = 1662 (of 10192), d = 2.56326, its = 18
i = 1663 (of 10192), d = 37, its = 55
i = 1664 (of 10192), d = 37, its = 53
i = 1665 (of 10192), d = 37, its = 50
i = 1666 (of 10192), d = 4.36017, its = 19
i = 1667 (of 10192), d = 10.6733, its = 5
i = 1668 (of 10192), d = 37, its = 37
i = 1669 (of 10192), d = 37, its = 51
i = 1670 (of 10192), d = 37, its = 39
i = 1671 (of 10192), d = 37, its = 54
i = 1672 (of 10192), d = 37, its = 42
i = 1673 (of 10192), d = 8.61985, its = 4
i = 1674 (of 10192), d = 37, its = 51
i = 1675 (of 10192), d = 37, its = 56
i = 1676 (of 10192), d = 37, its = 53
i = 1677 (of 10192), d = 9.3185, its = 4
i = 1678 (of 10192), d = 37, its = 52
i = 1679 (of 10192), d = 37, its = 52
i = 1680 (of 10192), d = 9.78317, its = 5
i = 1681 (of 10192), d = 37, its = 49
i = 1682 (of 10192), d = 3.56266, its = 17
i = 1683 (of 10192), d = 37, its = 37
i = 1684 (of 10192), d = 9.76822, its = 5
i = 1685 (of 10192), d = 6.78339, its = 5
i = 1686 (of 10192), d = 37, its = 53
i = 1687 (of 10192), d = 37, its = 51
i = 1688 (of 10192), d = 2.41409, its = 19
i = 1689 (of 10192), d = 37, its = 37
i = 1690 (of 10192), d = 3.08466, its = 19
i = 1691 (of 10192), d = 37, its = 53
i = 1692 (of 10192), d = 37, its = 49
i = 1693 (of 10192), d = 37, its = 20
i = 1694 (of 10192), d = 1.68365, its = 19
i = 1695 (of 10192), d = 37, its = 56
i = 1696 (of 10192), d = 37, its = 42
i = 1697 (of 10192), d = 8.7277, its = 4
i = 1698 (of 10192), d = 37, its = 37
i = 1699 (of 10192), d = 11.3701, its = 6
i = 1700 (of 10192), d = 37, its = 39
i = 1701 (of 10192), d = 37, its = 37
i = 1702 (of 10192), d = 37, its = 54
i = 1703 (of 10192), d = 19.4825, its = 7
i = 1704 (of 10192), d = 37, its = 52
i = 1705 (of 10192), d = 6.08695, its = 7
i = 1706 (of 10192), d = 37, its = 52
i = 1707 (of 10192), d = 10.8328, its = 6
i = 1708 (of 10192), d = 37, its = 37
i = 1709 (of 10192), d = 18.0347, its = 7
i = 1710 (of 10192), d = 37, its = 37
i = 1711 (of 10192), d = 37, its = 46
i = 1712 (of 10192), d = 4.81247, its = 7
i = 1713 (of 10192), d = 37, its = 50
i = 1714 (of 10192), d = 37, its = 38
i = 1715 (of 10192), d = 37, its = 37
i = 1716 (of 10192), d = 4.40612, its = 20
i = 1717 (of 10192), d = 37, its = 37
i = 1718 (of 10192), d = 37, its = 52
i = 1719 (of 10192), d = 9.84841, its = 5
i = 1720 (of 10192), d = 9.62858, its = 5
i = 1721 (of 10192), d = 37, its = 53
i = 1722 (of 10192), d = 37, its = 54
i = 1723 (of 10192), d = 37, its = 51
i = 1724 (of 10192), d = 37, its = 56
i = 1725 (of 10192), d = 37, its = 41
i = 1726 (of 10192), d = 37, its = 51
i = 1727 (of 10192), d = 4.64283, its = 6
i = 1728 (of 10192), d = 37, its = 46
i = 1729 (of 10192), d = 37, its = 55
i = 1730 (of 10192), d = 37, its = 50
i = 1731 (of 10192), d = 37, its = 51
i = 1732 (of 10192), d = 37, its = 37
i = 1733 (of 10192), d = 37, its = 52
i = 1734 (of 10192), d = 37, its = 37
i = 1735 (of 10192), d = 37, its = 56
i = 1736 (of 10192), d = 37, its = 57
i = 1737 (of 10192), d = 2.76953, its = 17
i = 1738 (of 10192), d = 8.12751, its = 3
i = 1739 (of 10192), d = 11.666, its = 6
i = 1740 (of 10192), d = 37, its = 43
i = 1741 (of 10192), d = 37, its = 41
i = 1742 (of 10192), d = 37, its = 51
i = 1743 (of 10192), d = 4.41062, its = 17
i = 1744 (of 10192), d = 37, its = 53
i = 1745 (of 10192), d = 37, its = 53
i = 1746 (of 10192), d = 37, its = 53
i = 1747 (of 10192), d = 37, its = 53
i = 1748 (of 10192), d = 37, its = 39
i = 1749 (of 10192), d = 23.5487, its = 20
i = 1750 (of 10192), d = 37, its = 37
i = 1751 (of 10192), d = 37, its = 57
i = 1752 (of 10192), d = 37, its = 54
i = 1753 (of 10192), d = 37, its = 37
i = 1754 (of 10192), d = 2.13471, its = 17
i = 1755 (of 10192), d = 13.3059, its = 6
i = 1756 (of 10192), d = 12.1276, its = 6
i = 1757 (of 10192), d = 6.18732, its = 6
i = 1758 (of 10192), d = 7.97642, its = 2
i = 1759 (of 10192), d = 37, its = 53
i = 1760 (of 10192), d = 7.18661, its = 4
i = 1761 (of 10192), d = 37, its = 38
i = 1762 (of 10192), d = 37, its = 38
i = 1763 (of 10192), d = 37, its = 37
i = 1764 (of 10192), d = 37, its = 53
i = 1765 (of 10192), d = 37, its = 51
i = 1766 (of 10192), d = 37, its = 54
i = 1767 (of 10192), d = 9.94383, its = 5
i = 1768 (of 10192), d = 37, its = 52
i = 1769 (of 10192), d = 37, its = 56
i = 1770 (of 10192), d = 6.45097, its = 7
i = 1771 (of 10192), d = 37, its = 48
i = 1772 (of 10192), d = 37, its = 56
i = 1773 (of 10192), d = 5.26612, its = 5
i = 1774 (of 10192), d = 37, its = 44
i = 1775 (of 10192), d = 37, its = 54
i = 1776 (of 10192), d = 37, its = 37
i = 1777 (of 10192), d = 37, its = 38
i = 1778 (of 10192), d = 37, its = 53
i = 1779 (of 10192), d = 8.37442, its = 4
i = 1780 (of 10192), d = 17.2454, its = 6
i = 1781 (of 10192), d = 37, its = 55
i = 1782 (of 10192), d = 37, its = 53
i = 1783 (of 10192), d = 11.5608, its = 6
i = 1784 (of 10192), d = 37, its = 53
i = 1785 (of 10192), d = 37, its = 55
i = 1786 (of 10192), d = 37, its = 53
i = 1787 (of 10192), d = 37, its = 42
i = 1788 (of 10192), d = 37, its = 52
i = 1789 (of 10192), d = 5.11697, its = 6
i = 1790 (of 10192), d = 37, its = 54
i = 1791 (of 10192), d = 37, its = 39
i = 1792 (of 10192), d = 37, its = 54
i = 1793 (of 10192), d = 8.47299, its = 4
i = 1794 (of 10192), d = 37, its = 55
i = 1795 (of 10192), d = 7.84686, its = 3
i = 1796 (of 10192), d = 37, its = 53
i = 1797 (of 10192), d = 37, its = 50
i = 1798 (of 10192), d = 3.94634, its = 18
i = 1799 (of 10192), d = 37, its = 42
i = 1800 (of 10192), d = 10.1128, its = 5
i = 1801 (of 10192), d = 2.44749, its = 17
i = 1802 (of 10192), d = 2.81655, its = 19
i = 1803 (of 10192), d = 37, its = 50
i = 1804 (of 10192), d = 8.67601, its = 4
i = 1805 (of 10192), d = 6.08071, its = 6
i = 1806 (of 10192), d = 37, its = 57
i = 1807 (of 10192), d = 37, its = 50
i = 1808 (of 10192), d = 37, its = 38
i = 1809 (of 10192), d = 37, its = 53
i = 1810 (of 10192), d = 37, its = 55
i = 1811 (of 10192), d = 37, its = 38
i = 1812 (of 10192), d = 37, its = 53
i = 1813 (of 10192), d = 37, its = 37
i = 1814 (of 10192), d = 37, its = 53
i = 1815 (of 10192), d = 37, its = 49
i = 1816 (of 10192), d = 10.1514, its = 5
i = 1817 (of 10192), d = 37, its = 54
i = 1818 (of 10192), d = 6.74372, its = 5
i = 1819 (of 10192), d = 37, its = 49
i = 1820 (of 10192), d = 6.77941, its = 5
i = 1821 (of 10192), d = 37, its = 52
i = 1822 (of 10192), d = 37, its = 53
i = 1823 (of 10192), d = 37, its = 37
i = 1824 (of 10192), d = 3.71149, its = 20
i = 1825 (of 10192), d = 37, its = 37
i = 1826 (of 10192), d = 37, its = 51
i = 1827 (of 10192), d = 37, its = 55
i = 1828 (of 10192), d = 37, its = 41
i = 1829 (of 10192), d = 9.94403, its = 5
i = 1830 (of 10192), d = 3.72443, its = 18
i = 1831 (of 10192), d = 37, its = 51
i = 1832 (of 10192), d = 37, its = 50
i = 1833 (of 10192), d = 37, its = 54
i = 1834 (of 10192), d = 1.76911, its = 17
i = 1835 (of 10192), d = 37, its = 52
i = 1836 (of 10192), d = 37, its = 41
i = 1837 (of 10192), d = 32.6215, its = 23
i = 1838 (of 10192), d = 20.0152, its = 8
i = 1839 (of 10192), d = 37, its = 49
i = 1840 (of 10192), d = 37, its = 39
i = 1841 (of 10192), d = 37, its = 50
i = 1842 (of 10192), d = 3.06002, its = 17
i = 1843 (of 10192), d = 4.88539, its = 25
i = 1844 (of 10192), d = 37, its = 53
i = 1845 (of 10192), d = 37, its = 53
i = 1846 (of 10192), d = 37, its = 37
i = 1847 (of 10192), d = 37, its = 44
i = 1848 (of 10192), d = 37, its = 37
i = 1849 (of 10192), d = 37, its = 55
i = 1850 (of 10192), d = 37, its = 48
i = 1851 (of 10192), d = 37, its = 52
i = 1852 (of 10192), d = 37, its = 50
i = 1853 (of 10192), d = 37, its = 54
i = 1854 (of 10192), d = 4.7084, its = 6
i = 1855 (of 10192), d = 37, its = 37
i = 1856 (of 10192), d = 37, its = 48
i = 1857 (of 10192), d = 37, its = 53
i = 1858 (of 10192), d = 37, its = 40
i = 1859 (of 10192), d = 37, its = 51
i = 1860 (of 10192), d = 37, its = 43
i = 1861 (of 10192), d = 37, its = 56
i = 1862 (of 10192), d = 3.24347, its = 17
i = 1863 (of 10192), d = 37, its = 55
i = 1864 (of 10192), d = 5.53407, its = 6
i = 1865 (of 10192), d = 37, its = 49
i = 1866 (of 10192), d = 37, its = 53
i = 1867 (of 10192), d = 3.67873, its = 18
i = 1868 (of 10192), d = 37, its = 52
i = 1869 (of 10192), d = 37, its = 37
i = 1870 (of 10192), d = 2.44508, its = 20
i = 1871 (of 10192), d = 37, its = 43
i = 1872 (of 10192), d = 37, its = 37
i = 1873 (of 10192), d = 8.5557, its = 4
i = 1874 (of 10192), d = 37, its = 52
i = 1875 (of 10192), d = 2.37345, its = 27
i = 1876 (of 10192), d = 2.25523, its = 20
i = 1877 (of 10192), d = 9.04173, its = 4
i = 1878 (of 10192), d = 37, its = 37
i = 1879 (of 10192), d = 37, its = 42
i = 1880 (of 10192), d = 5.56067, its = 6
i = 1881 (of 10192), d = 37, its = 55
i = 1882 (of 10192), d = 37, its = 54
i = 1883 (of 10192), d = 37, its = 54
i = 1884 (of 10192), d = 37, its = 40
i = 1885 (of 10192), d = 37, its = 38
i = 1886 (of 10192), d = 37, its = 52
i = 1887 (of 10192), d = 37, its = 53
i = 1888 (of 10192), d = 7.40585, its = 4
i = 1889 (of 10192), d = 7.89233, its = 3
i = 1890 (of 10192), d = 4.78961, its = 7
i = 1891 (of 10192), d = 37, its = 51
i = 1892 (of 10192), d = 37, its = 50
i = 1893 (of 10192), d = 37, its = 49
i = 1894 (of 10192), d = 37, its = 47
i = 1895 (of 10192), d = 37, its = 40
i = 1896 (of 10192), d = 37, its = 47
i = 1897 (of 10192), d = 37, its = 53
i = 1898 (of 10192), d = 37, its = 37
i = 1899 (of 10192), d = 37, its = 40
i = 1900 (of 10192), d = 9.82409, its = 5
i = 1901 (of 10192), d = 37, its = 59
i = 1902 (of 10192), d = 7.99258, its = 2
i = 1903 (of 10192), d = 37, its = 52
i = 1904 (of 10192), d = 37, its = 51
i = 1905 (of 10192), d = 7.78051, its = 3
i = 1906 (of 10192), d = 5.34352, its = 7
i = 1907 (of 10192), d = 37, its = 55
i = 1908 (of 10192), d = 37, its = 51
i = 1909 (of 10192), d = 37, its = 39
i = 1910 (of 10192), d = 37, its = 49
i = 1911 (of 10192), d = 37, its = 53
i = 1912 (of 10192), d = 37, its = 55
i = 1913 (of 10192), d = 37, its = 49
i = 1914 (of 10192), d = 4.79607, its = 25
i = 1915 (of 10192), d = 37, its = 40
i = 1916 (of 10192), d = 3.41692, its = 18
i = 1917 (of 10192), d = 3.63793, its = 22
i = 1918 (of 10192), d = 37, its = 57
i = 1919 (of 10192), d = 37, its = 54
i = 1920 (of 10192), d = 37, its = 58
i = 1921 (of 10192), d = 37, its = 55
i = 1922 (of 10192), d = 37, its = 53
i = 1923 (of 10192), d = 37, its = 52
i = 1924 (of 10192), d = 37, its = 51
i = 1925 (of 10192), d = 37, its = 37
i = 1926 (of 10192), d = 37, its = 40
i = 1927 (of 10192), d = 37, its = 40
i = 1928 (of 10192), d = 37, its = 49
i = 1929 (of 10192), d = 37, its = 56
i = 1930 (of 10192), d = 37, its = 48
i = 1931 (of 10192), d = 37, its = 39
i = 1932 (of 10192), d = 37, its = 40
i = 1933 (of 10192), d = 37, its = 46
i = 1934 (of 10192), d = 4.66368, its = 20
i = 1935 (of 10192), d = 37, its = 37
i = 1936 (of 10192), d = 37, its = 51
i = 1937 (of 10192), d = 37, its = 55
i = 1938 (of 10192), d = 37, its = 52
i = 1939 (of 10192), d = 37, its = 54
i = 1940 (of 10192), d = 2.34095, its = 21
i = 1941 (of 10192), d = 37, its = 51
i = 1942 (of 10192), d = 37, its = 52
i = 1943 (of 10192), d = 37, its = 37
i = 1944 (of 10192), d = 37, its = 37
i = 1945 (of 10192), d = 3.17871, its = 17
i = 1946 (of 10192), d = 37, its = 54
i = 1947 (of 10192), d = 37, its = 41
i = 1948 (of 10192), d = 8.63316, its = 4
i = 1949 (of 10192), d = 7.38326, its = 4
i = 1950 (of 10192), d = 37, its = 54
i = 1951 (of 10192), d = 11.8544, its = 6
i = 1952 (of 10192), d = 33.2509, its = 8
i = 1953 (of 10192), d = 37, its = 54
i = 1954 (of 10192), d = 5.13331, its = 21
i = 1955 (of 10192), d = 3.45041, its = 21
i = 1956 (of 10192), d = 8.23217, its = 3
i = 1957 (of 10192), d = 37, its = 50
i = 1958 (of 10192), d = 13.1681, its = 7
i = 1959 (of 10192), d = 37, its = 49
i = 1960 (of 10192), d = 28.3312, its = 21
i = 1961 (of 10192), d = 37, its = 58
i = 1962 (of 10192), d = 37, its = 56
i = 1963 (of 10192), d = 37, its = 55
i = 1964 (of 10192), d = 37, its = 52
i = 1965 (of 10192), d = 37, its = 55
i = 1966 (of 10192), d = 2.59531, its = 20
i = 1967 (of 10192), d = 2.50393, its = 18
i = 1968 (of 10192), d = 37, its = 40
i = 1969 (of 10192), d = 37, its = 37
i = 1970 (of 10192), d = 37, its = 37
i = 1971 (of 10192), d = 37, its = 41
i = 1972 (of 10192), d = 37, its = 50
i = 1973 (of 10192), d = 7.09263, its = 5
i = 1974 (of 10192), d = 37, its = 52
i = 1975 (of 10192), d = 37, its = 37
i = 1976 (of 10192), d = 5.5638, its = 6
i = 1977 (of 10192), d = 37, its = 52
i = 1978 (of 10192), d = 37, its = 52
i = 1979 (of 10192), d = 37, its = 56
i = 1980 (of 10192), d = 37, its = 53
i = 1981 (of 10192), d = 37, its = 40
i = 1982 (of 10192), d = 37, its = 53
i = 1983 (of 10192), d = 37, its = 50
i = 1984 (of 10192), d = 37, its = 54
i = 1985 (of 10192), d = 29.4927, its = 20
i = 1986 (of 10192), d = 37, its = 51
i = 1987 (of 10192), d = 8.20293, its = 3
i = 1988 (of 10192), d = 10.5445, its = 6
i = 1989 (of 10192), d = 37, its = 39
i = 1990 (of 10192), d = 37, its = 49
i = 1991 (of 10192), d = 37, its = 51
i = 1992 (of 10192), d = 8.28229, its = 4
i = 1993 (of 10192), d = 2.68048, its = 17
i = 1994 (of 10192), d = 37, its = 57
i = 1995 (of 10192), d = 37, its = 55
i = 1996 (of 10192), d = 4.85588, its = 23
i = 1997 (of 10192), d = 2.76681, its = 22
i = 1998 (of 10192), d = 37, its = 56
i = 1999 (of 10192), d = 37, its = 37
i = 2000 (of 10192), d = 37, its = 55
i = 2001 (of 10192), d = 37, its = 51
i = 2002 (of 10192), d = 37, its = 40
i = 2003 (of 10192), d = 37, its = 41
i = 2004 (of 10192), d = 37, its = 54
i = 2005 (of 10192), d = 1.50256, its = 18
i = 2006 (of 10192), d = 6.59998, its = 7
i = 2007 (of 10192), d = 4.16556, its = 23
i = 2008 (of 10192), d = 37, its = 54
i = 2009 (of 10192), d = 7.05321, its = 5
i = 2010 (of 10192), d = 37, its = 37
i = 2011 (of 10192), d = 1.35966, its = 30
i = 2012 (of 10192), d = 4.07642, its = 18
i = 2013 (of 10192), d = 16.2471, its = 7
i = 2014 (of 10192), d = 5.01769, its = 19
i = 2015 (of 10192), d = 37, its = 50
i = 2016 (of 10192), d = 37, its = 53
i = 2017 (of 10192), d = 37, its = 48
i = 2018 (of 10192), d = 6.47178, its = 5
i = 2019 (of 10192), d = 7.65871, its = 4
i = 2020 (of 10192), d = 37, its = 41
i = 2021 (of 10192), d = 10.9524, its = 6
i = 2022 (of 10192), d = 37, its = 54
i = 2023 (of 10192), d = 37, its = 40
i = 2024 (of 10192), d = 37, its = 37
i = 2025 (of 10192), d = 37, its = 51
i = 2026 (of 10192), d = 3.02713, its = 29
i = 2027 (of 10192), d = 37, its = 50
i = 2028 (of 10192), d = 37, its = 52
i = 2029 (of 10192), d = 37, its = 51
i = 2030 (of 10192), d = 13.2617, its = 6
i = 2031 (of 10192), d = 37, its = 56
i = 2032 (of 10192), d = 10.203, its = 5
i = 2033 (of 10192), d = 37, its = 53
i = 2034 (of 10192), d = 7.34274, its = 4
i = 2035 (of 10192), d = 37, its = 37
i = 2036 (of 10192), d = 3.55429, its = 23
i = 2037 (of 10192), d = 37, its = 54
i = 2038 (of 10192), d = 10.1299, its = 5
i = 2039 (of 10192), d = 37, its = 39
i = 2040 (of 10192), d = 3.16105, its = 19
i = 2041 (of 10192), d = 37, its = 57
i = 2042 (of 10192), d = 5.19633, its = 6
i = 2043 (of 10192), d = 37, its = 56
i = 2044 (of 10192), d = 5.27852, its = 6
i = 2045 (of 10192), d = 10.3446, its = 6
i = 2046 (of 10192), d = 8.90864, its = 4
i = 2047 (of 10192), d = 1.99512, its = 17
i = 2048 (of 10192), d = 35.6483, its = 8
i = 2049 (of 10192), d = 37, its = 37
i = 2050 (of 10192), d = 12.4408, its = 6
i = 2051 (of 10192), d = 37, its = 52
i = 2052 (of 10192), d = 37, its = 54
i = 2053 (of 10192), d = 37, its = 52
i = 2054 (of 10192), d = 9.51917, its = 4
i = 2055 (of 10192), d = 37, its = 56
i = 2056 (of 10192), d = 37, its = 48
i = 2057 (of 10192), d = 37, its = 51
i = 2058 (of 10192), d = 11.5991, its = 6
i = 2059 (of 10192), d = 37, its = 52
i = 2060 (of 10192), d = 37, its = 56
i = 2061 (of 10192), d = 37, its = 56
i = 2062 (of 10192), d = 37, its = 37
i = 2063 (of 10192), d = 37, its = 37
i = 2064 (of 10192), d = 2.08559, its = 17
i = 2065 (of 10192), d = 37, its = 52
i = 2066 (of 10192), d = 37, its = 50
i = 2067 (of 10192), d = 6.90828, its = 5
i = 2068 (of 10192), d = 9.41865, its = 5
i = 2069 (of 10192), d = 37, its = 37
i = 2070 (of 10192), d = 11.0951, its = 5
i = 2071 (of 10192), d = 37, its = 54
i = 2072 (of 10192), d = 7.77647, its = 3
i = 2073 (of 10192), d = 37, its = 53
i = 2074 (of 10192), d = 37, its = 53
i = 2075 (of 10192), d = 37, its = 53
i = 2076 (of 10192), d = 37, its = 39
i = 2077 (of 10192), d = 37, its = 50
i = 2078 (of 10192), d = 27.1234, its = 9
i = 2079 (of 10192), d = 37, its = 54
i = 2080 (of 10192), d = 2.65692, its = 21
i = 2081 (of 10192), d = 8.62172, its = 4
i = 2082 (of 10192), d = 4.41988, its = 20
i = 2083 (of 10192), d = 4.74673, its = 19
i = 2084 (of 10192), d = 37, its = 53
i = 2085 (of 10192), d = 4.82703, its = 22
i = 2086 (of 10192), d = 37, its = 52
i = 2087 (of 10192), d = 2.42559, its = 21
i = 2088 (of 10192), d = 37, its = 53
i = 2089 (of 10192), d = 37, its = 55
i = 2090 (of 10192), d = 37, its = 38
i = 2091 (of 10192), d = 37, its = 57
i = 2092 (of 10192), d = 37, its = 37
i = 2093 (of 10192), d = 1.50527, its = 25
i = 2094 (of 10192), d = 37, its = 57
i = 2095 (of 10192), d = 3.86576, its = 20
i = 2096 (of 10192), d = 37, its = 41
i = 2097 (of 10192), d = 37, its = 53
i = 2098 (of 10192), d = 37, its = 54
i = 2099 (of 10192), d = 37, its = 37
i = 2100 (of 10192), d = 37, its = 58
i = 2101 (of 10192), d = 37, its = 49
i = 2102 (of 10192), d = 12.6218, its = 7
i = 2103 (of 10192), d = 37, its = 49
i = 2104 (of 10192), d = 37, its = 54
i = 2105 (of 10192), d = 12.9211, its = 6
i = 2106 (of 10192), d = 37, its = 38
i = 2107 (of 10192), d = 4.01644, its = 16
i = 2108 (of 10192), d = 3.17881, its = 19
i = 2109 (of 10192), d = 37, its = 51
i = 2110 (of 10192), d = 37, its = 51
i = 2111 (of 10192), d = 37, its = 52
i = 2112 (of 10192), d = 37, its = 40
i = 2113 (of 10192), d = 37, its = 51
i = 2114 (of 10192), d = 1.33133, its = 18
i = 2115 (of 10192), d = 2.53879, its = 16
i = 2116 (of 10192), d = 8.70997, its = 4
i = 2117 (of 10192), d = 3.70424, its = 18
i = 2118 (of 10192), d = 37, its = 37
i = 2119 (of 10192), d = 37, its = 53
i = 2120 (of 10192), d = 16.8365, its = 7
i = 2121 (of 10192), d = 37, its = 39
i = 2122 (of 10192), d = 37, its = 47
i = 2123 (of 10192), d = 37, its = 51
i = 2124 (of 10192), d = 8.79469, its = 4
i = 2125 (of 10192), d = 8.90834, its = 4
i = 2126 (of 10192), d = 4.45674, its = 23
i = 2127 (of 10192), d = 37, its = 56
i = 2128 (of 10192), d = 37, its = 52
i = 2129 (of 10192), d = 8.53111, its = 4
i = 2130 (of 10192), d = 7.10747, its = 5
i = 2131 (of 10192), d = 37, its = 55
i = 2132 (of 10192), d = 37, its = 37
i = 2133 (of 10192), d = 37, its = 37
i = 2134 (of 10192), d = 7.04317, its = 5
i = 2135 (of 10192), d = 14.0998, its = 6
i = 2136 (of 10192), d = 37, its = 50
i = 2137 (of 10192), d = 37, its = 53
i = 2138 (of 10192), d = 37, its = 54
i = 2139 (of 10192), d = 3.18584, its = 17
i = 2140 (of 10192), d = 7.75246, its = 4
i = 2141 (of 10192), d = 37, its = 39
i = 2142 (of 10192), d = 6.23329, its = 6
i = 2143 (of 10192), d = 37, its = 37
i = 2144 (of 10192), d = 7.41465, its = 4
i = 2145 (of 10192), d = 37, its = 51
i = 2146 (of 10192), d = 37, its = 37
i = 2147 (of 10192), d = 10.008, its = 5
i = 2148 (of 10192), d = 37, its = 37
i = 2149 (of 10192), d = 7.49743, its = 4
i = 2150 (of 10192), d = 37, its = 53
i = 2151 (of 10192), d = 37, its = 50
i = 2152 (of 10192), d = 27.1234, its = 9
i = 2153 (of 10192), d = 4.6434, its = 18
i = 2154 (of 10192), d = 37, its = 52
i = 2155 (of 10192), d = 3.99956, its = 18
i = 2156 (of 10192), d = 2.25833, its = 17
i = 2157 (of 10192), d = 4.81192, its = 17
i = 2158 (of 10192), d = 3.51973, its = 23
i = 2159 (of 10192), d = 9.04105, its = 4
i = 2160 (of 10192), d = 37, its = 37
i = 2161 (of 10192), d = 13.2792, its = 6
i = 2162 (of 10192), d = 37, its = 49
i = 2163 (of 10192), d = 37, its = 55
i = 2164 (of 10192), d = 3.59471, its = 20
i = 2165 (of 10192), d = 37, its = 53
i = 2166 (of 10192), d = 9.16676, its = 4
i = 2167 (of 10192), d = 2.69515, its = 18
i = 2168 (of 10192), d = 37, its = 51
i = 2169 (of 10192), d = 37, its = 37
i = 2170 (of 10192), d = 37, its = 53
i = 2171 (of 10192), d = 2.07884, its = 19
i = 2172 (of 10192), d = 4.16121, its = 17
i = 2173 (of 10192), d = 37, its = 53
i = 2174 (of 10192), d = 37, its = 38
i = 2175 (of 10192), d = 37, its = 50
i = 2176 (of 10192), d = 37, its = 37
i = 2177 (of 10192), d = 37, its = 50
i = 2178 (of 10192), d = 37, its = 53
i = 2179 (of 10192), d = 37, its = 37
i = 2180 (of 10192), d = 12.2282, its = 6
i = 2181 (of 10192), d = 37, its = 39
i = 2182 (of 10192), d = 37, its = 37
i = 2183 (of 10192), d = 37, its = 51
i = 2184 (of 10192), d = 37, its = 55
i = 2185 (of 10192), d = 37, its = 37
i = 2186 (of 10192), d = 37, its = 50
i = 2187 (of 10192), d = 37, its = 57
i = 2188 (of 10192), d = 4.3043, its = 21
i = 2189 (of 10192), d = 37, its = 54
i = 2190 (of 10192), d = 2.68339, its = 17
i = 2191 (of 10192), d = 37, its = 49
i = 2192 (of 10192), d = 37, its = 37
i = 2193 (of 10192), d = 37, its = 42
i = 2194 (of 10192), d = 37, its = 52
i = 2195 (of 10192), d = 37, its = 50
i = 2196 (of 10192), d = 15.9508, its = 7
i = 2197 (of 10192), d = 37, its = 55
i = 2198 (of 10192), d = 37, its = 43
i = 2199 (of 10192), d = 37, its = 53
i = 2200 (of 10192), d = 4.64722, its = 17
i = 2201 (of 10192), d = 18.6177, its = 7
i = 2202 (of 10192), d = 3.75802, its = 19
i = 2203 (of 10192), d = 37, its = 41
i = 2204 (of 10192), d = 6.05562, its = 6
i = 2205 (of 10192), d = 37, its = 55
i = 2206 (of 10192), d = 26.5836, its = 7
i = 2207 (of 10192), d = 2.44811, its = 17
i = 2208 (of 10192), d = 37, its = 50
i = 2209 (of 10192), d = 37, its = 55
i = 2210 (of 10192), d = 37, its = 55
i = 2211 (of 10192), d = 37, its = 49
i = 2212 (of 10192), d = 37, its = 54
i = 2213 (of 10192), d = 37, its = 50
i = 2214 (of 10192), d = 37, its = 37
i = 2215 (of 10192), d = 8.15621, its = 3
i = 2216 (of 10192), d = 37, its = 55
i = 2217 (of 10192), d = 37, its = 51
i = 2218 (of 10192), d = 37, its = 38
i = 2219 (of 10192), d = 4.40276, its = 23
i = 2220 (of 10192), d = 37, its = 53
i = 2221 (of 10192), d = 37, its = 51
i = 2222 (of 10192), d = 5.26245, its = 8
i = 2223 (of 10192), d = 8.13458, its = 3
i = 2224 (of 10192), d = 37, its = 54
i = 2225 (of 10192), d = 37, its = 41
i = 2226 (of 10192), d = 37, its = 53
i = 2227 (of 10192), d = 37, its = 41
i = 2228 (of 10192), d = 6.16565, its = 6
i = 2229 (of 10192), d = 2.44694, its = 17
i = 2230 (of 10192), d = 37, its = 51
i = 2231 (of 10192), d = 37, its = 51
i = 2232 (of 10192), d = 37, its = 48
i = 2233 (of 10192), d = 4.67501, its = 15
i = 2234 (of 10192), d = 12.1907, its = 6
i = 2235 (of 10192), d = 2.48559, its = 17
i = 2236 (of 10192), d = 37, its = 37
i = 2237 (of 10192), d = 10.8026, its = 6
i = 2238 (of 10192), d = 37, its = 53
i = 2239 (of 10192), d = 37, its = 55
i = 2240 (of 10192), d = 6.19576, its = 6
i = 2241 (of 10192), d = 2.65155, its = 27
i = 2242 (of 10192), d = 37, its = 54
i = 2243 (of 10192), d = 37, its = 53
i = 2244 (of 10192), d = 37, its = 37
i = 2245 (of 10192), d = 37, its = 52
i = 2246 (of 10192), d = 37, its = 50
i = 2247 (of 10192), d = 37, its = 40
i = 2248 (of 10192), d = 37, its = 37
i = 2249 (of 10192), d = 5.33768, its = 6
i = 2250 (of 10192), d = 37, its = 39
i = 2251 (of 10192), d = 8.23805, its = 4
i = 2252 (of 10192), d = 37, its = 51
i = 2253 (of 10192), d = 7.30832, its = 4
i = 2254 (of 10192), d = 37, its = 54
i = 2255 (of 10192), d = 37, its = 48
i = 2256 (of 10192), d = 37, its = 54
i = 2257 (of 10192), d = 37, its = 55
i = 2258 (of 10192), d = 37, its = 53
i = 2259 (of 10192), d = 37, its = 49
i = 2260 (of 10192), d = 37, its = 49
i = 2261 (of 10192), d = 37, its = 52
i = 2262 (of 10192), d = 37, its = 39
i = 2263 (of 10192), d = 9.41866, its = 5
i = 2264 (of 10192), d = 3.46396, its = 26
i = 2265 (of 10192), d = 4.40206, its = 21
i = 2266 (of 10192), d = 4.04039, its = 20
i = 2267 (of 10192), d = 37, its = 58
i = 2268 (of 10192), d = 37, its = 44
i = 2269 (of 10192), d = 14.5803, its = 6
i = 2270 (of 10192), d = 37, its = 51
i = 2271 (of 10192), d = 37, its = 52
i = 2272 (of 10192), d = 37, its = 50
i = 2273 (of 10192), d = 37, its = 37
i = 2274 (of 10192), d = 37, its = 54
i = 2275 (of 10192), d = 10.192, its = 5
i = 2276 (of 10192), d = 37, its = 49
i = 2277 (of 10192), d = 37, its = 41
i = 2278 (of 10192), d = 6.36091, its = 7
i = 2279 (of 10192), d = 37, its = 52
i = 2280 (of 10192), d = 4.38852, its = 17
i = 2281 (of 10192), d = 37, its = 57
i = 2282 (of 10192), d = 2.79435, its = 16
i = 2283 (of 10192), d = 37, its = 53
i = 2284 (of 10192), d = 37, its = 37
i = 2285 (of 10192), d = 37, its = 48
i = 2286 (of 10192), d = 37, its = 40
i = 2287 (of 10192), d = 6.99687, its = 5
i = 2288 (of 10192), d = 37, its = 50
i = 2289 (of 10192), d = 14.2388, its = 6
i = 2290 (of 10192), d = 37, its = 53
i = 2291 (of 10192), d = 37, its = 52
i = 2292 (of 10192), d = 37, its = 57
i = 2293 (of 10192), d = 37, its = 57
i = 2294 (of 10192), d = 2.88134, its = 19
i = 2295 (of 10192), d = 37, its = 54
i = 2296 (of 10192), d = 8.56154, its = 4
i = 2297 (of 10192), d = 37, its = 37
i = 2298 (of 10192), d = 37, its = 37
i = 2299 (of 10192), d = 4.60338, its = 17
i = 2300 (of 10192), d = 37, its = 40
i = 2301 (of 10192), d = 9.11465, its = 4
i = 2302 (of 10192), d = 9.5152, its = 4
i = 2303 (of 10192), d = 7.56218, its = 4
i = 2304 (of 10192), d = 15.1028, its = 7
i = 2305 (of 10192), d = 9.32765, its = 4
i = 2306 (of 10192), d = 37, its = 55
i = 2307 (of 10192), d = 5.98984, its = 6
i = 2308 (of 10192), d = 6.41254, its = 6
i = 2309 (of 10192), d = 5.80635, its = 9
i = 2310 (of 10192), d = 37, its = 49
i = 2311 (of 10192), d = 37, its = 50
i = 2312 (of 10192), d = 37, its = 37
i = 2313 (of 10192), d = 37, its = 54
i = 2314 (of 10192), d = 3.02027, its = 19
i = 2315 (of 10192), d = 2.43476, its = 18
i = 2316 (of 10192), d = 37, its = 53
i = 2317 (of 10192), d = 37, its = 49
i = 2318 (of 10192), d = 6.88599, its = 5
i = 2319 (of 10192), d = 11.0083, its = 5
i = 2320 (of 10192), d = 10.9646, its = 6
i = 2321 (of 10192), d = 14.3889, its = 7
i = 2322 (of 10192), d = 11.5332, its = 5
i = 2323 (of 10192), d = 37, its = 49
i = 2324 (of 10192), d = 15.9743, its = 7
i = 2325 (of 10192), d = 37, its = 54
i = 2326 (of 10192), d = 4.07562, its = 20
i = 2327 (of 10192), d = 6.08892, its = 6
i = 2328 (of 10192), d = 37, its = 49
i = 2329 (of 10192), d = 37, its = 51
i = 2330 (of 10192), d = 37, its = 37
i = 2331 (of 10192), d = 3.04192, its = 19
i = 2332 (of 10192), d = 37, its = 37
i = 2333 (of 10192), d = 37, its = 53
i = 2334 (of 10192), d = 12.3055, its = 7
i = 2335 (of 10192), d = 37, its = 49
i = 2336 (of 10192), d = 4.69714, its = 21
i = 2337 (of 10192), d = 1.78165, its = 17
i = 2338 (of 10192), d = 2.76444, its = 18
i = 2339 (of 10192), d = 37, its = 54
i = 2340 (of 10192), d = 37, its = 37
i = 2341 (of 10192), d = 37, its = 37
i = 2342 (of 10192), d = 12.0618, its = 6
i = 2343 (of 10192), d = 37, its = 55
i = 2344 (of 10192), d = 37, its = 49
i = 2345 (of 10192), d = 37, its = 54
i = 2346 (of 10192), d = 37, its = 41
i = 2347 (of 10192), d = 3.23823, its = 16
i = 2348 (of 10192), d = 37, its = 37
i = 2349 (of 10192), d = 2.56002, its = 17
i = 2350 (of 10192), d = 37, its = 54
i = 2351 (of 10192), d = 37, its = 54
i = 2352 (of 10192), d = 37, its = 48
i = 2353 (of 10192), d = 37, its = 51
i = 2354 (of 10192), d = 37, its = 39
i = 2355 (of 10192), d = 37, its = 37
i = 2356 (of 10192), d = 37, its = 50
i = 2357 (of 10192), d = 8.79965, its = 4
i = 2358 (of 10192), d = 37, its = 38
i = 2359 (of 10192), d = 37, its = 40
i = 2360 (of 10192), d = 37, its = 38
i = 2361 (of 10192), d = 37, its = 41
i = 2362 (of 10192), d = 37, its = 53
i = 2363 (of 10192), d = 37, its = 37
i = 2364 (of 10192), d = 37, its = 39
i = 2365 (of 10192), d = 37, its = 40
i = 2366 (of 10192), d = 37, its = 37
i = 2367 (of 10192), d = 6.30597, its = 6
i = 2368 (of 10192), d = 37, its = 53
i = 2369 (of 10192), d = 37, its = 52
i = 2370 (of 10192), d = 2.3205, its = 20
i = 2371 (of 10192), d = 37, its = 55
i = 2372 (of 10192), d = 37, its = 56
i = 2373 (of 10192), d = 4.41976, its = 19
i = 2374 (of 10192), d = 37, its = 56
i = 2375 (of 10192), d = 37, its = 54
i = 2376 (of 10192), d = 8.46662, its = 4
i = 2377 (of 10192), d = 37, its = 41
i = 2378 (of 10192), d = 37, its = 55
i = 2379 (of 10192), d = 37, its = 37
i = 2380 (of 10192), d = 37, its = 56
i = 2381 (of 10192), d = 37, its = 39
i = 2382 (of 10192), d = 37, its = 51
i = 2383 (of 10192), d = 37, its = 50
i = 2384 (of 10192), d = 10.0825, its = 6
i = 2385 (of 10192), d = 8.78992, its = 4
i = 2386 (of 10192), d = 37, its = 37
i = 2387 (of 10192), d = 37, its = 53
i = 2388 (of 10192), d = 37, its = 51
i = 2389 (of 10192), d = 37, its = 49
i = 2390 (of 10192), d = 2.77467, its = 17
i = 2391 (of 10192), d = 12.3575, its = 6
i = 2392 (of 10192), d = 37, its = 53
i = 2393 (of 10192), d = 37, its = 55
i = 2394 (of 10192), d = 37, its = 54
i = 2395 (of 10192), d = 7.36101, its = 4
i = 2396 (of 10192), d = 37, its = 41
i = 2397 (of 10192), d = 37, its = 53
i = 2398 (of 10192), d = 37, its = 42
i = 2399 (of 10192), d = 37, its = 40
i = 2400 (of 10192), d = 2.32589, its = 18
i = 2401 (of 10192), d = 37, its = 57
i = 2402 (of 10192), d = 37, its = 41
i = 2403 (of 10192), d = 37, its = 53
i = 2404 (of 10192), d = 37, its = 54
i = 2405 (of 10192), d = 37, its = 37
i = 2406 (of 10192), d = 9.42231, its = 5
i = 2407 (of 10192), d = 5.88898, its = 6
i = 2408 (of 10192), d = 7.66753, its = 4
i = 2409 (of 10192), d = 37, its = 56
i = 2410 (of 10192), d = 7.22929, its = 4
i = 2411 (of 10192), d = 37, its = 52
i = 2412 (of 10192), d = 37, its = 53
i = 2413 (of 10192), d = 37, its = 45
i = 2414 (of 10192), d = 37, its = 51
i = 2415 (of 10192), d = 1.10344, its = 21
i = 2416 (of 10192), d = 37, its = 55
i = 2417 (of 10192), d = 37, its = 56
i = 2418 (of 10192), d = 37, its = 39
i = 2419 (of 10192), d = 37, its = 54
i = 2420 (of 10192), d = 37, its = 37
i = 2421 (of 10192), d = 37, its = 50
i = 2422 (of 10192), d = 8.16866, its = 3
i = 2423 (of 10192), d = 37, its = 37
i = 2424 (of 10192), d = 2.7417, its = 18
i = 2425 (of 10192), d = 37, its = 54
i = 2426 (of 10192), d = 37, its = 56
i = 2427 (of 10192), d = 5.97834, its = 6
i = 2428 (of 10192), d = 37, its = 37
i = 2429 (of 10192), d = 37, its = 55
i = 2430 (of 10192), d = 9.51917, its = 4
i = 2431 (of 10192), d = 37, its = 53
i = 2432 (of 10192), d = 37, its = 55
i = 2433 (of 10192), d = 37, its = 53
i = 2434 (of 10192), d = 37, its = 53
i = 2435 (of 10192), d = 37, its = 40
i = 2436 (of 10192), d = 7.94093, its = 3
i = 2437 (of 10192), d = 13.0998, its = 7
i = 2438 (of 10192), d = 37, its = 48
i = 2439 (of 10192), d = 37, its = 54
i = 2440 (of 10192), d = 37, its = 51
i = 2441 (of 10192), d = 37, its = 54
i = 2442 (of 10192), d = 2.54456, its = 24
i = 2443 (of 10192), d = 37, its = 51
i = 2444 (of 10192), d = 37, its = 47
i = 2445 (of 10192), d = 37, its = 52
i = 2446 (of 10192), d = 37, its = 55
i = 2447 (of 10192), d = 37, its = 56
i = 2448 (of 10192), d = 2.85809, its = 18
i = 2449 (of 10192), d = 37, its = 42
i = 2450 (of 10192), d = 37, its = 48
i = 2451 (of 10192), d = 37, its = 53
i = 2452 (of 10192), d = 1.47671, its = 17
i = 2453 (of 10192), d = 7.63972, its = 4
i = 2454 (of 10192), d = 37, its = 53
i = 2455 (of 10192), d = 1.98074, its = 19
i = 2456 (of 10192), d = 13.5277, its = 6
i = 2457 (of 10192), d = 37, its = 37
i = 2458 (of 10192), d = 37, its = 51
i = 2459 (of 10192), d = 2.82371, its = 20
i = 2460 (of 10192), d = 37, its = 51
i = 2461 (of 10192), d = 3.63643, its = 16
i = 2462 (of 10192), d = 37, its = 37
i = 2463 (of 10192), d = 37, its = 37
i = 2464 (of 10192), d = 32.7132, its = 7
i = 2465 (of 10192), d = 11.5819, its = 6
i = 2466 (of 10192), d = 37, its = 40
i = 2467 (of 10192), d = 37, its = 51
i = 2468 (of 10192), d = 37, its = 48
i = 2469 (of 10192), d = 37, its = 54
i = 2470 (of 10192), d = 37, its = 52
i = 2471 (of 10192), d = 37, its = 53
i = 2472 (of 10192), d = 37, its = 53
i = 2473 (of 10192), d = 3.25426, its = 19
i = 2474 (of 10192), d = 37, its = 49
i = 2475 (of 10192), d = 37, its = 42
i = 2476 (of 10192), d = 23.9616, its = 8
i = 2477 (of 10192), d = 1.4239, its = 28
i = 2478 (of 10192), d = 37, its = 51
i = 2479 (of 10192), d = 16.4642, its = 7
i = 2480 (of 10192), d = 37, its = 37
i = 2481 (of 10192), d = 37, its = 49
i = 2482 (of 10192), d = 7.10535, its = 5
i = 2483 (of 10192), d = 6.44383, its = 6
i = 2484 (of 10192), d = 37, its = 53
i = 2485 (of 10192), d = 5.56575, its = 6
i = 2486 (of 10192), d = 9.53116, its = 5
i = 2487 (of 10192), d = 37, its = 53
i = 2488 (of 10192), d = 37, its = 54
i = 2489 (of 10192), d = 37, its = 41
i = 2490 (of 10192), d = 37, its = 42
i = 2491 (of 10192), d = 37, its = 54
i = 2492 (of 10192), d = 37, its = 51
i = 2493 (of 10192), d = 37, its = 37
i = 2494 (of 10192), d = 37, its = 37
i = 2495 (of 10192), d = 37, its = 43
i = 2496 (of 10192), d = 37, its = 57
i = 2497 (of 10192), d = 37, its = 37
i = 2498 (of 10192), d = 7.47922, its = 4
i = 2499 (of 10192), d = 37, its = 48
i = 2500 (of 10192), d = 4.60934, its = 15
i = 2501 (of 10192), d = 9.52983, its = 5
i = 2502 (of 10192), d = 37, its = 52
i = 2503 (of 10192), d = 37, its = 39
i = 2504 (of 10192), d = 37, its = 37
i = 2505 (of 10192), d = 7.07921, its = 5
i = 2506 (of 10192), d = 37, its = 50
i = 2507 (of 10192), d = 3.7524, its = 19
i = 2508 (of 10192), d = 37, its = 54
i = 2509 (of 10192), d = 35.8939, its = 10
i = 2510 (of 10192), d = 37, its = 52
i = 2511 (of 10192), d = 37, its = 48
i = 2512 (of 10192), d = 37, its = 56
i = 2513 (of 10192), d = 10.1926, its = 5
i = 2514 (of 10192), d = 18.0187, its = 7
i = 2515 (of 10192), d = 11.5805, its = 5
i = 2516 (of 10192), d = 37, its = 55
i = 2517 (of 10192), d = 37, its = 55
i = 2518 (of 10192), d = 37, its = 37
i = 2519 (of 10192), d = 37, its = 53
i = 2520 (of 10192), d = 10.6393, its = 6
i = 2521 (of 10192), d = 37, its = 37
i = 2522 (of 10192), d = 37, its = 40
i = 2523 (of 10192), d = 37, its = 51
i = 2524 (of 10192), d = 37, its = 55
i = 2525 (of 10192), d = 6.75, its = 6
i = 2526 (of 10192), d = 37, its = 48
i = 2527 (of 10192), d = 37, its = 55
i = 2528 (of 10192), d = 7.39526, its = 4
i = 2529 (of 10192), d = 9.79972, its = 5
i = 2530 (of 10192), d = 37, its = 58
i = 2531 (of 10192), d = 37, its = 40
i = 2532 (of 10192), d = 37, its = 58
i = 2533 (of 10192), d = 37, its = 55
i = 2534 (of 10192), d = 3.36108, its = 18
i = 2535 (of 10192), d = 37, its = 47
i = 2536 (of 10192), d = 37, its = 51
i = 2537 (of 10192), d = 37, its = 56
i = 2538 (of 10192), d = 16.3316, its = 7
i = 2539 (of 10192), d = 37, its = 42
i = 2540 (of 10192), d = 2.99481, its = 15
i = 2541 (of 10192), d = 37, its = 50
i = 2542 (of 10192), d = 37, its = 55
i = 2543 (of 10192), d = 8.07803, its = 3
i = 2544 (of 10192), d = 37, its = 54
i = 2545 (of 10192), d = 37, its = 53
i = 2546 (of 10192), d = 37, its = 37
i = 2547 (of 10192), d = 37, its = 56
i = 2548 (of 10192), d = 7.25373, its = 4
i = 2549 (of 10192), d = 37, its = 41
i = 2550 (of 10192), d = 37, its = 53
i = 2551 (of 10192), d = 37, its = 52
i = 2552 (of 10192), d = 6.62186, its = 6
i = 2553 (of 10192), d = 13.103, its = 6
i = 2554 (of 10192), d = 37, its = 41
i = 2555 (of 10192), d = 37, its = 37
i = 2556 (of 10192), d = 8.6827, its = 4
i = 2557 (of 10192), d = 37, its = 52
i = 2558 (of 10192), d = 2.65992, its = 19
i = 2559 (of 10192), d = 37, its = 54
i = 2560 (of 10192), d = 37, its = 50
i = 2561 (of 10192), d = 37, its = 57
i = 2562 (of 10192), d = 2.92249, its = 18
i = 2563 (of 10192), d = 37, its = 45
i = 2564 (of 10192), d = 2.34494, its = 18
i = 2565 (of 10192), d = 37, its = 44
i = 2566 (of 10192), d = 37, its = 40
i = 2567 (of 10192), d = 37, its = 37
i = 2568 (of 10192), d = 37, its = 53
i = 2569 (of 10192), d = 37, its = 47
i = 2570 (of 10192), d = 5.61891, its = 6
i = 2571 (of 10192), d = 37, its = 53
i = 2572 (of 10192), d = 37, its = 54
i = 2573 (of 10192), d = 37, its = 55
i = 2574 (of 10192), d = 37, its = 55
i = 2575 (of 10192), d = 37, its = 53
i = 2576 (of 10192), d = 37, its = 49
i = 2577 (of 10192), d = 37, its = 51
i = 2578 (of 10192), d = 37, its = 55
i = 2579 (of 10192), d = 37, its = 55
i = 2580 (of 10192), d = 37, its = 40
i = 2581 (of 10192), d = 37, its = 37
i = 2582 (of 10192), d = 37, its = 52
i = 2583 (of 10192), d = 37, its = 37
i = 2584 (of 10192), d = 37, its = 48
i = 2585 (of 10192), d = 6.08058, its = 7
i = 2586 (of 10192), d = 37, its = 51
i = 2587 (of 10192), d = 37, its = 49
i = 2588 (of 10192), d = 37, its = 49
i = 2589 (of 10192), d = 37, its = 37
i = 2590 (of 10192), d = 37, its = 52
i = 2591 (of 10192), d = 37, its = 53
i = 2592 (of 10192), d = 37, its = 37
i = 2593 (of 10192), d = 37, its = 37
i = 2594 (of 10192), d = 37, its = 37
i = 2595 (of 10192), d = 37, its = 37
i = 2596 (of 10192), d = 7.38281, its = 6
i = 2597 (of 10192), d = 17.6608, its = 8
i = 2598 (of 10192), d = 37, its = 37
i = 2599 (of 10192), d = 25.8414, its = 8
i = 2600 (of 10192), d = 2.36113, its = 19
i = 2601 (of 10192), d = 37, its = 53
i = 2602 (of 10192), d = 37, its = 56
i = 2603 (of 10192), d = 37, its = 42
i = 2604 (of 10192), d = 37, its = 55
i = 2605 (of 10192), d = 37, its = 55
i = 2606 (of 10192), d = 8.62392, its = 4
i = 2607 (of 10192), d = 37, its = 51
i = 2608 (of 10192), d = 7.70852, its = 4
i = 2609 (of 10192), d = 4.69511, its = 24
i = 2610 (of 10192), d = 37, its = 52
i = 2611 (of 10192), d = 4.29836, its = 21
i = 2612 (of 10192), d = 37, its = 49
i = 2613 (of 10192), d = 37, its = 40
i = 2614 (of 10192), d = 37, its = 37
i = 2615 (of 10192), d = 37, its = 56
i = 2616 (of 10192), d = 3.66121, its = 21
i = 2617 (of 10192), d = 13.9589, its = 7
i = 2618 (of 10192), d = 37, its = 52
i = 2619 (of 10192), d = 37, its = 42
i = 2620 (of 10192), d = 37, its = 52
i = 2621 (of 10192), d = 3.30256, its = 19
i = 2622 (of 10192), d = 2.72129, its = 27
i = 2623 (of 10192), d = 37, its = 51
i = 2624 (of 10192), d = 37, its = 54
i = 2625 (of 10192), d = 37, its = 50
i = 2626 (of 10192), d = 37, its = 53
i = 2627 (of 10192), d = 6.61637, its = 5
i = 2628 (of 10192), d = 37, its = 54
i = 2629 (of 10192), d = 37, its = 53
i = 2630 (of 10192), d = 37, its = 53
i = 2631 (of 10192), d = 37, its = 47
i = 2632 (of 10192), d = 4.02653, its = 18
i = 2633 (of 10192), d = 5.14162, its = 6
i = 2634 (of 10192), d = 32.3847, its = 10
i = 2635 (of 10192), d = 37, its = 51
i = 2636 (of 10192), d = 4.89683, its = 6
i = 2637 (of 10192), d = 37, its = 37
i = 2638 (of 10192), d = 4.91335, its = 22
i = 2639 (of 10192), d = 37, its = 53
i = 2640 (of 10192), d = 37, its = 40
i = 2641 (of 10192), d = 3.4257, its = 18
i = 2642 (of 10192), d = 7.07514, its = 4
i = 2643 (of 10192), d = 37, its = 54
i = 2644 (of 10192), d = 7.51859, its = 4
i = 2645 (of 10192), d = 11.81, its = 5
i = 2646 (of 10192), d = 2.58601, its = 19
i = 2647 (of 10192), d = 37, its = 55
i = 2648 (of 10192), d = 2.20099, its = 18
i = 2649 (of 10192), d = 37, its = 57
i = 2650 (of 10192), d = 37, its = 47
i = 2651 (of 10192), d = 37, its = 51
i = 2652 (of 10192), d = 37, its = 37
i = 2653 (of 10192), d = 37, its = 42
i = 2654 (of 10192), d = 3.02389, its = 18
i = 2655 (of 10192), d = 11.2517, its = 6
i = 2656 (of 10192), d = 37, its = 53
i = 2657 (of 10192), d = 8.81232, its = 4
i = 2658 (of 10192), d = 10.6349, its = 5
i = 2659 (of 10192), d = 37, its = 55
i = 2660 (of 10192), d = 37, its = 50
i = 2661 (of 10192), d = 10.7407, its = 5
i = 2662 (of 10192), d = 10.4127, its = 5
i = 2663 (of 10192), d = 37, its = 52
i = 2664 (of 10192), d = 4.80694, its = 6
i = 2665 (of 10192), d = 37, its = 53
i = 2666 (of 10192), d = 37, its = 37
i = 2667 (of 10192), d = 37, its = 51
i = 2668 (of 10192), d = 37, its = 37
i = 2669 (of 10192), d = 37, its = 50
i = 2670 (of 10192), d = 37, its = 54
i = 2671 (of 10192), d = 8.58897, its = 4
i = 2672 (of 10192), d = 2.87397, its = 18
i = 2673 (of 10192), d = 37, its = 56
i = 2674 (of 10192), d = 37, its = 37
i = 2675 (of 10192), d = 37, its = 52
i = 2676 (of 10192), d = 37, its = 53
i = 2677 (of 10192), d = 5.69775, its = 6
i = 2678 (of 10192), d = 37, its = 48
i = 2679 (of 10192), d = 37, its = 53
i = 2680 (of 10192), d = 13.3559, its = 6
i = 2681 (of 10192), d = 37, its = 54
i = 2682 (of 10192), d = 10.3548, its = 5
i = 2683 (of 10192), d = 2.84955, its = 22
i = 2684 (of 10192), d = 37, its = 52
i = 2685 (of 10192), d = 37, its = 54
i = 2686 (of 10192), d = 5.82298, its = 21
i = 2687 (of 10192), d = 37, its = 50
i = 2688 (of 10192), d = 10.1608, its = 5
i = 2689 (of 10192), d = 37, its = 53
i = 2690 (of 10192), d = 37, its = 37
i = 2691 (of 10192), d = 37, its = 54
i = 2692 (of 10192), d = 23.1199, its = 9
i = 2693 (of 10192), d = 4.39377, its = 23
i = 2694 (of 10192), d = 37, its = 37
i = 2695 (of 10192), d = 37, its = 49
i = 2696 (of 10192), d = 3.27711, its = 16
i = 2697 (of 10192), d = 37, its = 51
i = 2698 (of 10192), d = 37, its = 37
i = 2699 (of 10192), d = 6.31766, its = 7
i = 2700 (of 10192), d = 37, its = 56
i = 2701 (of 10192), d = 3.16945, its = 24
i = 2702 (of 10192), d = 37, its = 52
i = 2703 (of 10192), d = 37, its = 50
i = 2704 (of 10192), d = 2.98368, its = 16
i = 2705 (of 10192), d = 37, its = 54
i = 2706 (of 10192), d = 37, its = 52
i = 2707 (of 10192), d = 37, its = 53
i = 2708 (of 10192), d = 37, its = 57
i = 2709 (of 10192), d = 11.4919, its = 5
i = 2710 (of 10192), d = 37, its = 38
i = 2711 (of 10192), d = 37, its = 48
i = 2712 (of 10192), d = 2.00289, its = 17
i = 2713 (of 10192), d = 37, its = 37
i = 2714 (of 10192), d = 37, its = 52
i = 2715 (of 10192), d = 5.19065, its = 5
i = 2716 (of 10192), d = 37, its = 50
i = 2717 (of 10192), d = 37, its = 55
i = 2718 (of 10192), d = 6.87453, its = 5
i = 2719 (of 10192), d = 37, its = 45
i = 2720 (of 10192), d = 7.57966, its = 4
i = 2721 (of 10192), d = 37, its = 52
i = 2722 (of 10192), d = 12.3061, its = 6
i = 2723 (of 10192), d = 10.0556, its = 6
i = 2724 (of 10192), d = 37, its = 48
i = 2725 (of 10192), d = 11.3532, its = 6
i = 2726 (of 10192), d = 37, its = 53
i = 2727 (of 10192), d = 37, its = 52
i = 2728 (of 10192), d = 7.98222, its = 2
i = 2729 (of 10192), d = 37, its = 37
i = 2730 (of 10192), d = 37, its = 52
i = 2731 (of 10192), d = 37, its = 55
i = 2732 (of 10192), d = 6.251, its = 6
i = 2733 (of 10192), d = 37, its = 37
i = 2734 (of 10192), d = 37, its = 40
i = 2735 (of 10192), d = 4.38538, its = 23
i = 2736 (of 10192), d = 5.51272, its = 7
i = 2737 (of 10192), d = 37, its = 40
i = 2738 (of 10192), d = 37, its = 53
i = 2739 (of 10192), d = 9.30662, its = 5
i = 2740 (of 10192), d = 37, its = 41
i = 2741 (of 10192), d = 37, its = 37
i = 2742 (of 10192), d = 37, its = 41
i = 2743 (of 10192), d = 37, its = 51
i = 2744 (of 10192), d = 2.12999, its = 21
i = 2745 (of 10192), d = 11.5441, its = 6
i = 2746 (of 10192), d = 37, its = 37
i = 2747 (of 10192), d = 37, its = 54
i = 2748 (of 10192), d = 12.8661, its = 6
i = 2749 (of 10192), d = 37, its = 37
i = 2750 (of 10192), d = 2.30581, its = 18
i = 2751 (of 10192), d = 4.85199, its = 19
i = 2752 (of 10192), d = 37, its = 49
i = 2753 (of 10192), d = 37, its = 48
i = 2754 (of 10192), d = 37, its = 53
i = 2755 (of 10192), d = 37, its = 54
i = 2756 (of 10192), d = 12.2584, its = 6
i = 2757 (of 10192), d = 37, its = 42
i = 2758 (of 10192), d = 5.73898, its = 7
i = 2759 (of 10192), d = 6.41734, its = 6
i = 2760 (of 10192), d = 37, its = 41
i = 2761 (of 10192), d = 1.46939, its = 24
i = 2762 (of 10192), d = 0.952605, its = 20
i = 2763 (of 10192), d = 37, its = 37
i = 2764 (of 10192), d = 37, its = 37
i = 2765 (of 10192), d = 37, its = 54
i = 2766 (of 10192), d = 37, its = 57
i = 2767 (of 10192), d = 37, its = 57
i = 2768 (of 10192), d = 37, its = 49
i = 2769 (of 10192), d = 12.0745, its = 5
i = 2770 (of 10192), d = 37, its = 53
i = 2771 (of 10192), d = 37, its = 55
i = 2772 (of 10192), d = 37, its = 37
i = 2773 (of 10192), d = 9.29323, its = 4
i = 2774 (of 10192), d = 2.83932, its = 19
i = 2775 (of 10192), d = 8.3259, its = 4
i = 2776 (of 10192), d = 37, its = 40
i = 2777 (of 10192), d = 37, its = 53
i = 2778 (of 10192), d = 37, its = 54
i = 2779 (of 10192), d = 3.04214, its = 21
i = 2780 (of 10192), d = 7.25987, its = 5
i = 2781 (of 10192), d = 9.9667, its = 6
i = 2782 (of 10192), d = 37, its = 54
i = 2783 (of 10192), d = 7.81771, its = 3
i = 2784 (of 10192), d = 37, its = 37
i = 2785 (of 10192), d = 37, its = 53
i = 2786 (of 10192), d = 37, its = 37
i = 2787 (of 10192), d = 37, its = 41
i = 2788 (of 10192), d = 5.15918, its = 6
i = 2789 (of 10192), d = 3.20073, its = 24
i = 2790 (of 10192), d = 2.89762, its = 22
i = 2791 (of 10192), d = 3.22503, its = 17
i = 2792 (of 10192), d = 37, its = 58
i = 2793 (of 10192), d = 1.91138, its = 19
i = 2794 (of 10192), d = 37, its = 52
i = 2795 (of 10192), d = 2.48644, its = 20
i = 2796 (of 10192), d = 37, its = 51
i = 2797 (of 10192), d = 37, its = 54
i = 2798 (of 10192), d = 3.0904, its = 21
i = 2799 (of 10192), d = 37, its = 50
i = 2800 (of 10192), d = 37, its = 51
i = 2801 (of 10192), d = 37, its = 53
i = 2802 (of 10192), d = 37, its = 41
i = 2803 (of 10192), d = 37, its = 37
i = 2804 (of 10192), d = 37, its = 47
i = 2805 (of 10192), d = 37, its = 39
i = 2806 (of 10192), d = 37, its = 58
i = 2807 (of 10192), d = 37, its = 41
i = 2808 (of 10192), d = 37, its = 37
i = 2809 (of 10192), d = 5.32417, its = 5
i = 2810 (of 10192), d = 37, its = 51
i = 2811 (of 10192), d = 14.1281, its = 6
i = 2812 (of 10192), d = 4.01792, its = 17
i = 2813 (of 10192), d = 37, its = 37
i = 2814 (of 10192), d = 37, its = 56
i = 2815 (of 10192), d = 37, its = 53
i = 2816 (of 10192), d = 37, its = 55
i = 2817 (of 10192), d = 37, its = 54
i = 2818 (of 10192), d = 37, its = 37
i = 2819 (of 10192), d = 37, its = 53
i = 2820 (of 10192), d = 37, its = 40
i = 2821 (of 10192), d = 1.84645, its = 23
i = 2822 (of 10192), d = 37, its = 54
i = 2823 (of 10192), d = 37, its = 56
i = 2824 (of 10192), d = 37, its = 44
i = 2825 (of 10192), d = 37, its = 39
i = 2826 (of 10192), d = 37, its = 56
i = 2827 (of 10192), d = 37, its = 37
i = 2828 (of 10192), d = 37, its = 52
i = 2829 (of 10192), d = 37, its = 59
i = 2830 (of 10192), d = 14.1718, its = 6
i = 2831 (of 10192), d = 37, its = 37
i = 2832 (of 10192), d = 37, its = 52
i = 2833 (of 10192), d = 2.23871, its = 18
i = 2834 (of 10192), d = 37, its = 50
i = 2835 (of 10192), d = 37, its = 56
i = 2836 (of 10192), d = 13.2408, its = 6
i = 2837 (of 10192), d = 36.9528, its = 9
i = 2838 (of 10192), d = 37, its = 57
i = 2839 (of 10192), d = 37, its = 48
i = 2840 (of 10192), d = 9.14455, its = 4
i = 2841 (of 10192), d = 9.35922, its = 4
i = 2842 (of 10192), d = 3.21671, its = 25
i = 2843 (of 10192), d = 37, its = 37
i = 2844 (of 10192), d = 37, its = 55
i = 2845 (of 10192), d = 15.7985, its = 8
i = 2846 (of 10192), d = 37, its = 52
i = 2847 (of 10192), d = 6.50016, its = 6
i = 2848 (of 10192), d = 37, its = 50
i = 2849 (of 10192), d = 37, its = 39
i = 2850 (of 10192), d = 37, its = 55
i = 2851 (of 10192), d = 37, its = 51
i = 2852 (of 10192), d = 37, its = 37
i = 2853 (of 10192), d = 37, its = 53
i = 2854 (of 10192), d = 6.02569, its = 6
i = 2855 (of 10192), d = 37, its = 48
i = 2856 (of 10192), d = 37, its = 52
i = 2857 (of 10192), d = 37, its = 37
i = 2858 (of 10192), d = 2.1749, its = 18
i = 2859 (of 10192), d = 37, its = 40
i = 2860 (of 10192), d = 6.94867, its = 7
i = 2861 (of 10192), d = 37, its = 40
i = 2862 (of 10192), d = 37, its = 41
i = 2863 (of 10192), d = 37, its = 58
i = 2864 (of 10192), d = 37, its = 50
i = 2865 (of 10192), d = 37, its = 37
i = 2866 (of 10192), d = 37, its = 37
i = 2867 (of 10192), d = 37, its = 37
i = 2868 (of 10192), d = 37, its = 40
i = 2869 (of 10192), d = 37, its = 52
i = 2870 (of 10192), d = 37, its = 54
i = 2871 (of 10192), d = 2.37798, its = 20
i = 2872 (of 10192), d = 37, its = 51
i = 2873 (of 10192), d = 7.4182, its = 4
i = 2874 (of 10192), d = 37, its = 41
i = 2875 (of 10192), d = 37, its = 55
i = 2876 (of 10192), d = 37, its = 56
i = 2877 (of 10192), d = 37, its = 37
i = 2878 (of 10192), d = 1.8126, its = 17
i = 2879 (of 10192), d = 13.55, its = 6
i = 2880 (of 10192), d = 4.82077, its = 16
i = 2881 (of 10192), d = 37, its = 54
i = 2882 (of 10192), d = 18.6289, its = 7
i = 2883 (of 10192), d = 6.4724, its = 6
i = 2884 (of 10192), d = 37, its = 53
i = 2885 (of 10192), d = 6.66354, its = 6
i = 2886 (of 10192), d = 37, its = 52
i = 2887 (of 10192), d = 37, its = 49
i = 2888 (of 10192), d = 2.46928, its = 19
i = 2889 (of 10192), d = 9.00565, its = 4
i = 2890 (of 10192), d = 37, its = 37
i = 2891 (of 10192), d = 13.4052, its = 6
i = 2892 (of 10192), d = 37, its = 53
i = 2893 (of 10192), d = 37, its = 42
i = 2894 (of 10192), d = 3.03014, its = 17
i = 2895 (of 10192), d = 37, its = 52
i = 2896 (of 10192), d = 37, its = 40
i = 2897 (of 10192), d = 37, its = 55
i = 2898 (of 10192), d = 37, its = 54
i = 2899 (of 10192), d = 37, its = 56
i = 2900 (of 10192), d = 3.32177, its = 18
i = 2901 (of 10192), d = 2.93324, its = 19
i = 2902 (of 10192), d = 37, its = 55
i = 2903 (of 10192), d = 37, its = 40
i = 2904 (of 10192), d = 28.3894, its = 7
i = 2905 (of 10192), d = 13.1493, its = 6
i = 2906 (of 10192), d = 5.63019, its = 6
i = 2907 (of 10192), d = 2.01223, its = 19
i = 2908 (of 10192), d = 12.2398, its = 6
i = 2909 (of 10192), d = 37, its = 39
i = 2910 (of 10192), d = 6.07285, its = 6
i = 2911 (of 10192), d = 37, its = 51
i = 2912 (of 10192), d = 37, its = 54
i = 2913 (of 10192), d = 37, its = 52
i = 2914 (of 10192), d = 37, its = 37
i = 2915 (of 10192), d = 8.99377, its = 5
i = 2916 (of 10192), d = 3.47765, its = 26
i = 2917 (of 10192), d = 37, its = 50
i = 2918 (of 10192), d = 37, its = 55
i = 2919 (of 10192), d = 37, its = 54
i = 2920 (of 10192), d = 37, its = 41
i = 2921 (of 10192), d = 7.31581, its = 4
i = 2922 (of 10192), d = 2.9795, its = 17
i = 2923 (of 10192), d = 37, its = 53
i = 2924 (of 10192), d = 37, its = 58
i = 2925 (of 10192), d = 37, its = 55
i = 2926 (of 10192), d = 37, its = 52
i = 2927 (of 10192), d = 37, its = 38
i = 2928 (of 10192), d = 37, its = 37
i = 2929 (of 10192), d = 37, its = 49
i = 2930 (of 10192), d = 37, its = 52
i = 2931 (of 10192), d = 37, its = 56
i = 2932 (of 10192), d = 37, its = 58
i = 2933 (of 10192), d = 2.38679, its = 18
i = 2934 (of 10192), d = 37, its = 54
i = 2935 (of 10192), d = 13.3821, its = 7
i = 2936 (of 10192), d = 37, its = 52
i = 2937 (of 10192), d = 37, its = 49
i = 2938 (of 10192), d = 9.47884, its = 5
i = 2939 (of 10192), d = 2.91167, its = 19
i = 2940 (of 10192), d = 37, its = 40
i = 2941 (of 10192), d = 37, its = 55
i = 2942 (of 10192), d = 37, its = 43
i = 2943 (of 10192), d = 37, its = 54
i = 2944 (of 10192), d = 37, its = 49
i = 2945 (of 10192), d = 37, its = 37
i = 2946 (of 10192), d = 5.96192, its = 6
i = 2947 (of 10192), d = 37, its = 53
i = 2948 (of 10192), d = 37, its = 50
i = 2949 (of 10192), d = 37, its = 50
i = 2950 (of 10192), d = 10.881, its = 7
i = 2951 (of 10192), d = 2.12189, its = 19
i = 2952 (of 10192), d = 37, its = 50
i = 2953 (of 10192), d = 7.49607, its = 4
i = 2954 (of 10192), d = 4.09615, its = 20
i = 2955 (of 10192), d = 37, its = 40
i = 2956 (of 10192), d = 37, its = 38
i = 2957 (of 10192), d = 3.99646, its = 18
i = 2958 (of 10192), d = 7.46509, its = 4
i = 2959 (of 10192), d = 1.70174, its = 18
i = 2960 (of 10192), d = 37, its = 37
i = 2961 (of 10192), d = 37, its = 40
i = 2962 (of 10192), d = 8.10251, its = 3
i = 2963 (of 10192), d = 37, its = 39
i = 2964 (of 10192), d = 37, its = 50
i = 2965 (of 10192), d = 1.22832, its = 19
i = 2966 (of 10192), d = 37, its = 50
i = 2967 (of 10192), d = 37, its = 51
i = 2968 (of 10192), d = 37, its = 55
i = 2969 (of 10192), d = 37, its = 37
i = 2970 (of 10192), d = 7.40021, its = 4
i = 2971 (of 10192), d = 37, its = 40
i = 2972 (of 10192), d = 37, its = 39
i = 2973 (of 10192), d = 7.4431, its = 4
i = 2974 (of 10192), d = 2.85445, its = 23
i = 2975 (of 10192), d = 37, its = 37
i = 2976 (of 10192), d = 6.5148, its = 5
i = 2977 (of 10192), d = 37, its = 53
i = 2978 (of 10192), d = 37, its = 54
i = 2979 (of 10192), d = 37, its = 51
i = 2980 (of 10192), d = 10.5205, its = 5
i = 2981 (of 10192), d = 37, its = 53
i = 2982 (of 10192), d = 37, its = 51
i = 2983 (of 10192), d = 3.80676, its = 17
i = 2984 (of 10192), d = 3.99582, its = 17
i = 2985 (of 10192), d = 37, its = 55
i = 2986 (of 10192), d = 37, its = 55
i = 2987 (of 10192), d = 2.95831, its = 16
i = 2988 (of 10192), d = 37, its = 40
i = 2989 (of 10192), d = 9.60639, its = 5
i = 2990 (of 10192), d = 37, its = 52
i = 2991 (of 10192), d = 7.2319, its = 5
i = 2992 (of 10192), d = 37, its = 53
i = 2993 (of 10192), d = 37, its = 52
i = 2994 (of 10192), d = 37, its = 56
i = 2995 (of 10192), d = 37, its = 50
i = 2996 (of 10192), d = 1.7456, its = 19
i = 2997 (of 10192), d = 37, its = 52
i = 2998 (of 10192), d = 22.3315, its = 8
i = 2999 (of 10192), d = 37, its = 37
i = 3000 (of 10192), d = 37, its = 37
i = 3001 (of 10192), d = 37, its = 53
i = 3002 (of 10192), d = 2.58902, its = 26
i = 3003 (of 10192), d = 1.7832, its = 17
i = 3004 (of 10192), d = 37, its = 53
i = 3005 (of 10192), d = 37, its = 50
i = 3006 (of 10192), d = 37, its = 37
i = 3007 (of 10192), d = 37, its = 37
i = 3008 (of 10192), d = 7.07614, its = 5
i = 3009 (of 10192), d = 37, its = 55
i = 3010 (of 10192), d = 12.9839, its = 6
i = 3011 (of 10192), d = 3.50745, its = 23
i = 3012 (of 10192), d = 37, its = 40
i = 3013 (of 10192), d = 37, its = 54
i = 3014 (of 10192), d = 37, its = 50
i = 3015 (of 10192), d = 37, its = 37
i = 3016 (of 10192), d = 37, its = 41
i = 3017 (of 10192), d = 37, its = 43
i = 3018 (of 10192), d = 37, its = 50
i = 3019 (of 10192), d = 37, its = 55
i = 3020 (of 10192), d = 2.6371, its = 18
i = 3021 (of 10192), d = 37, its = 55
i = 3022 (of 10192), d = 37, its = 55
i = 3023 (of 10192), d = 37, its = 52
i = 3024 (of 10192), d = 37, its = 56
i = 3025 (of 10192), d = 37, its = 52
i = 3026 (of 10192), d = 37, its = 49
i = 3027 (of 10192), d = 37, its = 48
i = 3028 (of 10192), d = 37, its = 56
i = 3029 (of 10192), d = 4.27252, its = 19
i = 3030 (of 10192), d = 37, its = 57
i = 3031 (of 10192), d = 37, its = 52
i = 3032 (of 10192), d = 37, its = 40
i = 3033 (of 10192), d = 37, its = 53
i = 3034 (of 10192), d = 37, its = 52
i = 3035 (of 10192), d = 37, its = 39
i = 3036 (of 10192), d = 37, its = 52
i = 3037 (of 10192), d = 37, its = 43
i = 3038 (of 10192), d = 7.26775, its = 5
i = 3039 (of 10192), d = 37, its = 53
i = 3040 (of 10192), d = 8.84111, its = 4
i = 3041 (of 10192), d = 37, its = 50
i = 3042 (of 10192), d = 37, its = 39
i = 3043 (of 10192), d = 37, its = 39
i = 3044 (of 10192), d = 2.843, its = 20
i = 3045 (of 10192), d = 37, its = 52
i = 3046 (of 10192), d = 7.19481, its = 4
i = 3047 (of 10192), d = 3.41692, its = 18
i = 3048 (of 10192), d = 2.14634, its = 18
i = 3049 (of 10192), d = 37, its = 52
i = 3050 (of 10192), d = 37, its = 53
i = 3051 (of 10192), d = 6.78249, its = 5
i = 3052 (of 10192), d = 37, its = 54
i = 3053 (of 10192), d = 37, its = 37
i = 3054 (of 10192), d = 2.26564, its = 19
i = 3055 (of 10192), d = 37, its = 51
i = 3056 (of 10192), d = 7.29365, its = 4
i = 3057 (of 10192), d = 37, its = 52
i = 3058 (of 10192), d = 37, its = 55
i = 3059 (of 10192), d = 37, its = 37
i = 3060 (of 10192), d = 6.36829, its = 6
i = 3061 (of 10192), d = 6.67991, its = 5
i = 3062 (of 10192), d = 37, its = 44
i = 3063 (of 10192), d = 37, its = 49
i = 3064 (of 10192), d = 37, its = 41
i = 3065 (of 10192), d = 37, its = 57
i = 3066 (of 10192), d = 37, its = 57
i = 3067 (of 10192), d = 12.7963, its = 6
i = 3068 (of 10192), d = 37, its = 39
i = 3069 (of 10192), d = 37, its = 43
i = 3070 (of 10192), d = 7.18661, its = 4
i = 3071 (of 10192), d = 37, its = 51
i = 3072 (of 10192), d = 14.1465, its = 7
i = 3073 (of 10192), d = 37, its = 37
i = 3074 (of 10192), d = 2.79882, its = 19
i = 3075 (of 10192), d = 5.47386, its = 21
i = 3076 (of 10192), d = 8.23785, its = 4
i = 3077 (of 10192), d = 37, its = 38
i = 3078 (of 10192), d = 37, its = 52
i = 3079 (of 10192), d = 37, its = 48
i = 3080 (of 10192), d = 1.98074, its = 19
i = 3081 (of 10192), d = 1.81727, its = 17
i = 3082 (of 10192), d = 37, its = 49
i = 3083 (of 10192), d = 37, its = 51
i = 3084 (of 10192), d = 2.95238, its = 20
i = 3085 (of 10192), d = 37, its = 50
i = 3086 (of 10192), d = 37, its = 49
i = 3087 (of 10192), d = 4.46865, its = 20
i = 3088 (of 10192), d = 37, its = 52
i = 3089 (of 10192), d = 37, its = 37
i = 3090 (of 10192), d = 6.21422, its = 6
i = 3091 (of 10192), d = 37, its = 56
i = 3092 (of 10192), d = 37, its = 56
i = 3093 (of 10192), d = 3.86832, its = 17
i = 3094 (of 10192), d = 37, its = 40
i = 3095 (of 10192), d = 37, its = 55
i = 3096 (of 10192), d = 20.9437, its = 8
i = 3097 (of 10192), d = 12.4119, its = 7
i = 3098 (of 10192), d = 37, its = 53
i = 3099 (of 10192), d = 17.2277, its = 7
i = 3100 (of 10192), d = 1.38636, its = 17
i = 3101 (of 10192), d = 37, its = 39
i = 3102 (of 10192), d = 9.87287, its = 5
i = 3103 (of 10192), d = 37, its = 55
i = 3104 (of 10192), d = 37, its = 56
i = 3105 (of 10192), d = 4.8362, its = 6
i = 3106 (of 10192), d = 37, its = 43
i = 3107 (of 10192), d = 10.7407, its = 5
i = 3108 (of 10192), d = 37, its = 53
i = 3109 (of 10192), d = 37, its = 50
i = 3110 (of 10192), d = 37, its = 51
i = 3111 (of 10192), d = 37, its = 52
i = 3112 (of 10192), d = 37, its = 55
i = 3113 (of 10192), d = 37, its = 50
i = 3114 (of 10192), d = 2.69885, its = 18
i = 3115 (of 10192), d = 37, its = 37
i = 3116 (of 10192), d = 37, its = 54
i = 3117 (of 10192), d = 5.60565, its = 6
i = 3118 (of 10192), d = 10.3891, its = 5
i = 3119 (of 10192), d = 8.63339, its = 4
i = 3120 (of 10192), d = 3.67746, its = 22
i = 3121 (of 10192), d = 37, its = 49
i = 3122 (of 10192), d = 37, its = 37
i = 3123 (of 10192), d = 1.54626, its = 18
i = 3124 (of 10192), d = 37, its = 53
i = 3125 (of 10192), d = 17.6879, its = 7
i = 3126 (of 10192), d = 6.82031, its = 5
i = 3127 (of 10192), d = 37, its = 53
i = 3128 (of 10192), d = 37, its = 54
i = 3129 (of 10192), d = 37, its = 54
i = 3130 (of 10192), d = 37, its = 55
i = 3131 (of 10192), d = 37, its = 54
i = 3132 (of 10192), d = 10.2742, its = 5
i = 3133 (of 10192), d = 37, its = 54
i = 3134 (of 10192), d = 3.12234, its = 21
i = 3135 (of 10192), d = 37, its = 51
i = 3136 (of 10192), d = 37, its = 37
i = 3137 (of 10192), d = 1.26812, its = 22
i = 3138 (of 10192), d = 2.86827, its = 21
i = 3139 (of 10192), d = 4.35384, its = 18
i = 3140 (of 10192), d = 3.36656, its = 19
i = 3141 (of 10192), d = 37, its = 39
i = 3142 (of 10192), d = 37, its = 52
i = 3143 (of 10192), d = 12.7613, its = 6
i = 3144 (of 10192), d = 37, its = 52
i = 3145 (of 10192), d = 2.27428, its = 22
i = 3146 (of 10192), d = 0.71195, its = 28
i = 3147 (of 10192), d = 37, its = 53
i = 3148 (of 10192), d = 37, its = 55
i = 3149 (of 10192), d = 37, its = 53
i = 3150 (of 10192), d = 5.00965, its = 6
i = 3151 (of 10192), d = 33.6839, its = 9
i = 3152 (of 10192), d = 37, its = 53
i = 3153 (of 10192), d = 2.68217, its = 22
i = 3154 (of 10192), d = 37, its = 37
i = 3155 (of 10192), d = 6.43967, its = 6
i = 3156 (of 10192), d = 37, its = 52
i = 3157 (of 10192), d = 1.19187, its = 18
i = 3158 (of 10192), d = 37, its = 37
i = 3159 (of 10192), d = 37, its = 37
i = 3160 (of 10192), d = 8.9341, its = 4
i = 3161 (of 10192), d = 37, its = 39
i = 3162 (of 10192), d = 3.02214, its = 16
i = 3163 (of 10192), d = 37, its = 55
i = 3164 (of 10192), d = 37, its = 59
i = 3165 (of 10192), d = 4.1588, its = 30
i = 3166 (of 10192), d = 8.84804, its = 4
i = 3167 (of 10192), d = 5.68278, its = 8
i = 3168 (of 10192), d = 37, its = 47
i = 3169 (of 10192), d = 37, its = 37
i = 3170 (of 10192), d = 4.99111, its = 21
i = 3171 (of 10192), d = 37, its = 37
i = 3172 (of 10192), d = 6.97768, its = 5
i = 3173 (of 10192), d = 37, its = 47
i = 3174 (of 10192), d = 37, its = 41
i = 3175 (of 10192), d = 37, its = 52
i = 3176 (of 10192), d = 37, its = 49
i = 3177 (of 10192), d = 37, its = 58
i = 3178 (of 10192), d = 37, its = 37
i = 3179 (of 10192), d = 37, its = 49
i = 3180 (of 10192), d = 37, its = 51
i = 3181 (of 10192), d = 37, its = 54
i = 3182 (of 10192), d = 37, its = 49
i = 3183 (of 10192), d = 3.76996, its = 19
i = 3184 (of 10192), d = 37, its = 47
i = 3185 (of 10192), d = 5.7026, its = 6
i = 3186 (of 10192), d = 37, its = 48
i = 3187 (of 10192), d = 37, its = 53
i = 3188 (of 10192), d = 9.54111, its = 5
i = 3189 (of 10192), d = 37, its = 55
i = 3190 (of 10192), d = 37, its = 58
i = 3191 (of 10192), d = 5.6244, its = 7
i = 3192 (of 10192), d = 37, its = 52
i = 3193 (of 10192), d = 37, its = 41
i = 3194 (of 10192), d = 37, its = 53
i = 3195 (of 10192), d = 37, its = 54
i = 3196 (of 10192), d = 37, its = 37
i = 3197 (of 10192), d = 12.1169, its = 5
i = 3198 (of 10192), d = 37, its = 37
i = 3199 (of 10192), d = 8.23389, its = 3
i = 3200 (of 10192), d = 37, its = 53
i = 3201 (of 10192), d = 3.73575, its = 17
i = 3202 (of 10192), d = 37, its = 53
i = 3203 (of 10192), d = 37, its = 37
i = 3204 (of 10192), d = 37, its = 51
i = 3205 (of 10192), d = 37, its = 55
i = 3206 (of 10192), d = 4.03786, its = 24
i = 3207 (of 10192), d = 9.98385, its = 5
i = 3208 (of 10192), d = 7.01173, its = 5
i = 3209 (of 10192), d = 5.79727, its = 6
i = 3210 (of 10192), d = 37, its = 49
i = 3211 (of 10192), d = 5.96374, its = 6
i = 3212 (of 10192), d = 37, its = 37
i = 3213 (of 10192), d = 37, its = 39
i = 3214 (of 10192), d = 37, its = 40
i = 3215 (of 10192), d = 37, its = 49
i = 3216 (of 10192), d = 6.76846, its = 5
i = 3217 (of 10192), d = 37, its = 37
i = 3218 (of 10192), d = 37, its = 54
i = 3219 (of 10192), d = 37, its = 39
i = 3220 (of 10192), d = 3.35014, its = 18
i = 3221 (of 10192), d = 37, its = 40
i = 3222 (of 10192), d = 37, its = 54
i = 3223 (of 10192), d = 37, its = 53
i = 3224 (of 10192), d = 8.68799, its = 4
i = 3225 (of 10192), d = 9.04623, its = 4
i = 3226 (of 10192), d = 37, its = 48
i = 3227 (of 10192), d = 37, its = 50
i = 3228 (of 10192), d = 37, its = 57
i = 3229 (of 10192), d = 9.96435, its = 5
i = 3230 (of 10192), d = 6.95155, its = 5
i = 3231 (of 10192), d = 37, its = 39
i = 3232 (of 10192), d = 7.29201, its = 4
i = 3233 (of 10192), d = 37, its = 54
i = 3234 (of 10192), d = 37, its = 54
i = 3235 (of 10192), d = 37, its = 37
i = 3236 (of 10192), d = 8.45109, its = 4
i = 3237 (of 10192), d = 37, its = 40
i = 3238 (of 10192), d = 37, its = 55
i = 3239 (of 10192), d = 37, its = 48
i = 3240 (of 10192), d = 8.87065, its = 4
i = 3241 (of 10192), d = 37, its = 50
i = 3242 (of 10192), d = 2.53068, its = 23
i = 3243 (of 10192), d = 37, its = 55
i = 3244 (of 10192), d = 37, its = 53
i = 3245 (of 10192), d = 2.63132, its = 23
i = 3246 (of 10192), d = 7.72507, its = 4
i = 3247 (of 10192), d = 37, its = 55
i = 3248 (of 10192), d = 37, its = 49
i = 3249 (of 10192), d = 37, its = 56
i = 3250 (of 10192), d = 37, its = 48
i = 3251 (of 10192), d = 2.49581, its = 21
i = 3252 (of 10192), d = 3.44313, its = 22
i = 3253 (of 10192), d = 24.1793, its = 8
i = 3254 (of 10192), d = 37, its = 59
i = 3255 (of 10192), d = 37, its = 53
i = 3256 (of 10192), d = 6.35421, its = 6
i = 3257 (of 10192), d = 37, its = 52
i = 3258 (of 10192), d = 2.72354, its = 17
i = 3259 (of 10192), d = 37, its = 48
i = 3260 (of 10192), d = 37, its = 51
i = 3261 (of 10192), d = 13.7481, its = 7
i = 3262 (of 10192), d = 37, its = 37
i = 3263 (of 10192), d = 37, its = 50
i = 3264 (of 10192), d = 37, its = 56
i = 3265 (of 10192), d = 37, its = 49
i = 3266 (of 10192), d = 6.58739, its = 6
i = 3267 (of 10192), d = 37, its = 48
i = 3268 (of 10192), d = 5.14995, its = 6
i = 3269 (of 10192), d = 3.54122, its = 26
i = 3270 (of 10192), d = 37, its = 54
i = 3271 (of 10192), d = 37, its = 37
i = 3272 (of 10192), d = 10.2204, its = 5
i = 3273 (of 10192), d = 37, its = 41
i = 3274 (of 10192), d = 9.80222, its = 5
i = 3275 (of 10192), d = 37, its = 52
i = 3276 (of 10192), d = 37, its = 52
i = 3277 (of 10192), d = 2.35452, its = 24
i = 3278 (of 10192), d = 37, its = 51
i = 3279 (of 10192), d = 37, its = 54
i = 3280 (of 10192), d = 37, its = 48
i = 3281 (of 10192), d = 7.9446, its = 3
i = 3282 (of 10192), d = 37, its = 55
i = 3283 (of 10192), d = 37, its = 56
i = 3284 (of 10192), d = 3.81191, its = 21
i = 3285 (of 10192), d = 11.5598, its = 6
i = 3286 (of 10192), d = 37, its = 54
i = 3287 (of 10192), d = 3.39864, its = 27
i = 3288 (of 10192), d = 37, its = 48
i = 3289 (of 10192), d = 37, its = 37
i = 3290 (of 10192), d = 3.94284, its = 19
i = 3291 (of 10192), d = 14.125, its = 6
i = 3292 (of 10192), d = 37, its = 37
i = 3293 (of 10192), d = 37, its = 49
i = 3294 (of 10192), d = 37, its = 55
i = 3295 (of 10192), d = 37, its = 48
i = 3296 (of 10192), d = 37, its = 42
i = 3297 (of 10192), d = 37, its = 51
i = 3298 (of 10192), d = 11.1539, its = 7
i = 3299 (of 10192), d = 37, its = 54
i = 3300 (of 10192), d = 8.47299, its = 4
i = 3301 (of 10192), d = 37, its = 53
i = 3302 (of 10192), d = 37, its = 54
i = 3303 (of 10192), d = 37, its = 55
i = 3304 (of 10192), d = 37, its = 41
i = 3305 (of 10192), d = 1.58632, its = 18
i = 3306 (of 10192), d = 37, its = 42
i = 3307 (of 10192), d = 37, its = 52
i = 3308 (of 10192), d = 13.8975, its = 7
i = 3309 (of 10192), d = 37, its = 54
i = 3310 (of 10192), d = 3.18108, its = 17
i = 3311 (of 10192), d = 37, its = 50
i = 3312 (of 10192), d = 37, its = 40
i = 3313 (of 10192), d = 4.56393, its = 7
i = 3314 (of 10192), d = 16.7731, its = 7
i = 3315 (of 10192), d = 37, its = 41
i = 3316 (of 10192), d = 37, its = 37
i = 3317 (of 10192), d = 7.14633, its = 5
i = 3318 (of 10192), d = 37, its = 37
i = 3319 (of 10192), d = 37, its = 58
i = 3320 (of 10192), d = 6.93389, its = 5
i = 3321 (of 10192), d = 37, its = 40
i = 3322 (of 10192), d = 37, its = 55
i = 3323 (of 10192), d = 6.9663, its = 5
i = 3324 (of 10192), d = 37, its = 51
i = 3325 (of 10192), d = 37, its = 54
i = 3326 (of 10192), d = 10.4427, its = 5
i = 3327 (of 10192), d = 11.9985, its = 6
i = 3328 (of 10192), d = 37, its = 51
i = 3329 (of 10192), d = 37, its = 52
i = 3330 (of 10192), d = 37, its = 52
i = 3331 (of 10192), d = 4.97013, its = 21
i = 3332 (of 10192), d = 37, its = 56
i = 3333 (of 10192), d = 2.16444, its = 17
i = 3334 (of 10192), d = 37, its = 51
i = 3335 (of 10192), d = 37, its = 56
i = 3336 (of 10192), d = 37, its = 52
i = 3337 (of 10192), d = 37, its = 52
i = 3338 (of 10192), d = 37, its = 50
i = 3339 (of 10192), d = 11.7448, its = 5
i = 3340 (of 10192), d = 3.50187, its = 18
i = 3341 (of 10192), d = 10.1926, its = 5
i = 3342 (of 10192), d = 37, its = 55
i = 3343 (of 10192), d = 3.63447, its = 19
i = 3344 (of 10192), d = 2.57642, its = 18
i = 3345 (of 10192), d = 37, its = 43
i = 3346 (of 10192), d = 7.26775, its = 5
i = 3347 (of 10192), d = 37, its = 37
i = 3348 (of 10192), d = 37, its = 37
i = 3349 (of 10192), d = 37, its = 37
i = 3350 (of 10192), d = 37, its = 55
i = 3351 (of 10192), d = 37, its = 52
i = 3352 (of 10192), d = 37, its = 51
i = 3353 (of 10192), d = 37, its = 37
i = 3354 (of 10192), d = 37, its = 49
i = 3355 (of 10192), d = 3.22989, its = 17
i = 3356 (of 10192), d = 37, its = 54
i = 3357 (of 10192), d = 9.41973, its = 5
i = 3358 (of 10192), d = 37, its = 54
i = 3359 (of 10192), d = 37, its = 37
i = 3360 (of 10192), d = 1.83229, its = 22
i = 3361 (of 10192), d = 37, its = 37
i = 3362 (of 10192), d = 9.67157, its = 5
i = 3363 (of 10192), d = 37, its = 54
i = 3364 (of 10192), d = 37, its = 55
i = 3365 (of 10192), d = 37, its = 37
i = 3366 (of 10192), d = 37, its = 52
i = 3367 (of 10192), d = 7.51442, its = 4
i = 3368 (of 10192), d = 3.82845, its = 19
i = 3369 (of 10192), d = 37, its = 37
i = 3370 (of 10192), d = 37, its = 53
i = 3371 (of 10192), d = 37, its = 37
i = 3372 (of 10192), d = 37, its = 38
i = 3373 (of 10192), d = 12.8069, its = 6
i = 3374 (of 10192), d = 37, its = 37
i = 3375 (of 10192), d = 37, its = 37
i = 3376 (of 10192), d = 37, its = 38
i = 3377 (of 10192), d = 37, its = 37
i = 3378 (of 10192), d = 14.3309, its = 7
i = 3379 (of 10192), d = 37, its = 50
i = 3380 (of 10192), d = 37, its = 40
i = 3381 (of 10192), d = 37, its = 51
i = 3382 (of 10192), d = 8.90864, its = 4
i = 3383 (of 10192), d = 37, its = 55
i = 3384 (of 10192), d = 4.57239, its = 20
i = 3385 (of 10192), d = 37, its = 50
i = 3386 (of 10192), d = 37, its = 56
i = 3387 (of 10192), d = 8.11, its = 3
i = 3388 (of 10192), d = 37, its = 54
i = 3389 (of 10192), d = 37, its = 54
i = 3390 (of 10192), d = 37, its = 53
i = 3391 (of 10192), d = 37, its = 39
i = 3392 (of 10192), d = 2.70467, its = 17
i = 3393 (of 10192), d = 23.3582, its = 8
i = 3394 (of 10192), d = 37, its = 50
i = 3395 (of 10192), d = 37, its = 39
i = 3396 (of 10192), d = 37, its = 55
i = 3397 (of 10192), d = 37, its = 50
i = 3398 (of 10192), d = 37, its = 40
i = 3399 (of 10192), d = 37, its = 53
i = 3400 (of 10192), d = 6.19852, its = 7
i = 3401 (of 10192), d = 37, its = 55
i = 3402 (of 10192), d = 37, its = 56
i = 3403 (of 10192), d = 37, its = 40
i = 3404 (of 10192), d = 37, its = 49
i = 3405 (of 10192), d = 2.60172, its = 19
i = 3406 (of 10192), d = 37, its = 56
i = 3407 (of 10192), d = 37, its = 37
i = 3408 (of 10192), d = 5.60565, its = 6
i = 3409 (of 10192), d = 37, its = 40
i = 3410 (of 10192), d = 37, its = 44
i = 3411 (of 10192), d = 3.68188, its = 21
i = 3412 (of 10192), d = 14.6542, its = 6
i = 3413 (of 10192), d = 10.0983, its = 5
i = 3414 (of 10192), d = 37, its = 49
i = 3415 (of 10192), d = 37, its = 40
i = 3416 (of 10192), d = 37, its = 48
i = 3417 (of 10192), d = 4.93475, its = 6
i = 3418 (of 10192), d = 37, its = 49
i = 3419 (of 10192), d = 37, its = 52
i = 3420 (of 10192), d = 2.69309, its = 17
i = 3421 (of 10192), d = 6.8918, its = 5
i = 3422 (of 10192), d = 1.26549, its = 18
i = 3423 (of 10192), d = 37, its = 54
i = 3424 (of 10192), d = 8.44178, its = 4
i = 3425 (of 10192), d = 37, its = 52
i = 3426 (of 10192), d = 37, its = 55
i = 3427 (of 10192), d = 37, its = 37
i = 3428 (of 10192), d = 3.17654, its = 19
i = 3429 (of 10192), d = 7.00625, its = 5
i = 3430 (of 10192), d = 37, its = 38
i = 3431 (of 10192), d = 4.94542, its = 7
i = 3432 (of 10192), d = 37, its = 48
i = 3433 (of 10192), d = 37, its = 39
i = 3434 (of 10192), d = 1.47118, its = 17
i = 3435 (of 10192), d = 37, its = 37
i = 3436 (of 10192), d = 37, its = 40
i = 3437 (of 10192), d = 2.07062, its = 17
i = 3438 (of 10192), d = 37, its = 37
i = 3439 (of 10192), d = 37, its = 56
i = 3440 (of 10192), d = 37, its = 54
i = 3441 (of 10192), d = 37, its = 51
i = 3442 (of 10192), d = 12.0471, its = 5
i = 3443 (of 10192), d = 37, its = 54
i = 3444 (of 10192), d = 37, its = 58
i = 3445 (of 10192), d = 5.98579, its = 7
i = 3446 (of 10192), d = 3.38431, its = 19
i = 3447 (of 10192), d = 37, its = 52
i = 3448 (of 10192), d = 7.17033, its = 6
i = 3449 (of 10192), d = 20.449, its = 8
i = 3450 (of 10192), d = 37, its = 55
i = 3451 (of 10192), d = 7.82919, its = 3
i = 3452 (of 10192), d = 37, its = 37
i = 3453 (of 10192), d = 37, its = 51
i = 3454 (of 10192), d = 11.5608, its = 6
i = 3455 (of 10192), d = 37, its = 51
i = 3456 (of 10192), d = 37, its = 37
i = 3457 (of 10192), d = 8.78224, its = 4
i = 3458 (of 10192), d = 37, its = 51
i = 3459 (of 10192), d = 2.70467, its = 17
i = 3460 (of 10192), d = 37, its = 37
i = 3461 (of 10192), d = 37, its = 59
i = 3462 (of 10192), d = 5.95346, its = 6
i = 3463 (of 10192), d = 37, its = 58
i = 3464 (of 10192), d = 37, its = 39
i = 3465 (of 10192), d = 9.95414, its = 5
i = 3466 (of 10192), d = 3.35764, its = 18
i = 3467 (of 10192), d = 7.84462, its = 3
i = 3468 (of 10192), d = 37, its = 56
i = 3469 (of 10192), d = 37, its = 53
i = 3470 (of 10192), d = 37, its = 37
i = 3471 (of 10192), d = 37, its = 56
i = 3472 (of 10192), d = 37, its = 48
i = 3473 (of 10192), d = 2.37334, its = 19
i = 3474 (of 10192), d = 11.9891, its = 6
i = 3475 (of 10192), d = 7.70423, its = 4
i = 3476 (of 10192), d = 7.35039, its = 4
i = 3477 (of 10192), d = 4.9897, its = 7
i = 3478 (of 10192), d = 9.27856, its = 5
i = 3479 (of 10192), d = 37, its = 37
i = 3480 (of 10192), d = 37, its = 55
i = 3481 (of 10192), d = 37, its = 54
i = 3482 (of 10192), d = 6.82848, its = 5
i = 3483 (of 10192), d = 4.09615, its = 20
i = 3484 (of 10192), d = 37, its = 55
i = 3485 (of 10192), d = 37, its = 49
i = 3486 (of 10192), d = 37, its = 52
i = 3487 (of 10192), d = 37, its = 54
i = 3488 (of 10192), d = 37, its = 42
i = 3489 (of 10192), d = 37, its = 52
i = 3490 (of 10192), d = 37, its = 51
i = 3491 (of 10192), d = 9.67655, its = 5
i = 3492 (of 10192), d = 37, its = 54
i = 3493 (of 10192), d = 37, its = 49
i = 3494 (of 10192), d = 37, its = 49
i = 3495 (of 10192), d = 37, its = 51
i = 3496 (of 10192), d = 37, its = 55
i = 3497 (of 10192), d = 37, its = 55
i = 3498 (of 10192), d = 37, its = 37
i = 3499 (of 10192), d = 37, its = 37
i = 3500 (of 10192), d = 37, its = 53
i = 3501 (of 10192), d = 37, its = 53
i = 3502 (of 10192), d = 11.9627, its = 6
i = 3503 (of 10192), d = 37, its = 52
i = 3504 (of 10192), d = 37, its = 54
i = 3505 (of 10192), d = 37, its = 54
i = 3506 (of 10192), d = 37, its = 54
i = 3507 (of 10192), d = 9.80222, its = 5
i = 3508 (of 10192), d = 37, its = 51
i = 3509 (of 10192), d = 5.52558, its = 6
i = 3510 (of 10192), d = 37, its = 49
i = 3511 (of 10192), d = 37, its = 49
i = 3512 (of 10192), d = 6.29313, its = 7
i = 3513 (of 10192), d = 2.82666, its = 19
i = 3514 (of 10192), d = 37, its = 51
i = 3515 (of 10192), d = 37, its = 53
i = 3516 (of 10192), d = 13.6845, its = 6
i = 3517 (of 10192), d = 2.97604, its = 26
i = 3518 (of 10192), d = 37, its = 50
i = 3519 (of 10192), d = 2.17338, its = 21
i = 3520 (of 10192), d = 37, its = 51
i = 3521 (of 10192), d = 37, its = 49
i = 3522 (of 10192), d = 11.0362, its = 6
i = 3523 (of 10192), d = 2.86089, its = 19
i = 3524 (of 10192), d = 2.71904, its = 17
i = 3525 (of 10192), d = 37, its = 49
i = 3526 (of 10192), d = 1.02199, its = 19
i = 3527 (of 10192), d = 2.98532, its = 19
i = 3528 (of 10192), d = 37, its = 40
i = 3529 (of 10192), d = 37, its = 54
i = 3530 (of 10192), d = 37, its = 50
i = 3531 (of 10192), d = 37, its = 49
i = 3532 (of 10192), d = 37, its = 40
i = 3533 (of 10192), d = 22.0948, its = 8
i = 3534 (of 10192), d = 37, its = 40
i = 3535 (of 10192), d = 9.19318, its = 4
i = 3536 (of 10192), d = 37, its = 54
i = 3537 (of 10192), d = 37, its = 52
i = 3538 (of 10192), d = 37, its = 39
i = 3539 (of 10192), d = 37, its = 52
i = 3540 (of 10192), d = 37, its = 55
i = 3541 (of 10192), d = 7.63972, its = 4
i = 3542 (of 10192), d = 37, its = 53
i = 3543 (of 10192), d = 37, its = 51
i = 3544 (of 10192), d = 10.0683, its = 5
i = 3545 (of 10192), d = 15.7064, its = 7
i = 3546 (of 10192), d = 37, its = 48
i = 3547 (of 10192), d = 10.4423, its = 5
i = 3548 (of 10192), d = 37, its = 54
i = 3549 (of 10192), d = 37, its = 49
i = 3550 (of 10192), d = 37, its = 37
i = 3551 (of 10192), d = 37, its = 55
i = 3552 (of 10192), d = 5.42377, its = 19
i = 3553 (of 10192), d = 37, its = 49
i = 3554 (of 10192), d = 37, its = 57
i = 3555 (of 10192), d = 6.33538, its = 6
i = 3556 (of 10192), d = 37, its = 38
i = 3557 (of 10192), d = 37, its = 53
i = 3558 (of 10192), d = 37, its = 55
i = 3559 (of 10192), d = 37, its = 53
i = 3560 (of 10192), d = 37, its = 38
i = 3561 (of 10192), d = 37, its = 51
i = 3562 (of 10192), d = 37, its = 53
i = 3563 (of 10192), d = 1.9074, its = 17
i = 3564 (of 10192), d = 37, its = 41
i = 3565 (of 10192), d = 37, its = 53
i = 3566 (of 10192), d = 37, its = 37
i = 3567 (of 10192), d = 37, its = 37
i = 3568 (of 10192), d = 37, its = 47
i = 3569 (of 10192), d = 37, its = 49
i = 3570 (of 10192), d = 37, its = 37
i = 3571 (of 10192), d = 37, its = 56
i = 3572 (of 10192), d = 3.51177, its = 25
i = 3573 (of 10192), d = 37, its = 53
i = 3574 (of 10192), d = 37, its = 52
i = 3575 (of 10192), d = 37, its = 50
i = 3576 (of 10192), d = 37, its = 53
i = 3577 (of 10192), d = 37, its = 37
i = 3578 (of 10192), d = 3.44349, its = 20
i = 3579 (of 10192), d = 37, its = 51
i = 3580 (of 10192), d = 37, its = 54
i = 3581 (of 10192), d = 2.4637, its = 20
i = 3582 (of 10192), d = 8.34872, its = 3
i = 3583 (of 10192), d = 37, its = 56
i = 3584 (of 10192), d = 6.52134, its = 6
i = 3585 (of 10192), d = 37, its = 48
i = 3586 (of 10192), d = 7.77852, its = 3
i = 3587 (of 10192), d = 6.33093, its = 6
i = 3588 (of 10192), d = 14.7317, its = 7
i = 3589 (of 10192), d = 37, its = 50
i = 3590 (of 10192), d = 37, its = 50
i = 3591 (of 10192), d = 2.8432, its = 20
i = 3592 (of 10192), d = 17.7782, its = 7
i = 3593 (of 10192), d = 6.5809, its = 5
i = 3594 (of 10192), d = 37, its = 57
i = 3595 (of 10192), d = 37, its = 53
i = 3596 (of 10192), d = 37, its = 52
i = 3597 (of 10192), d = 37, its = 42
i = 3598 (of 10192), d = 1.99512, its = 17
i = 3599 (of 10192), d = 37, its = 58
i = 3600 (of 10192), d = 37, its = 56
i = 3601 (of 10192), d = 3.62707, its = 17
i = 3602 (of 10192), d = 37, its = 52
i = 3603 (of 10192), d = 37, its = 49
i = 3604 (of 10192), d = 37, its = 54
i = 3605 (of 10192), d = 37, its = 54
i = 3606 (of 10192), d = 37, its = 37
i = 3607 (of 10192), d = 37, its = 54
i = 3608 (of 10192), d = 3.96596, its = 17
i = 3609 (of 10192), d = 15.6886, its = 7
i = 3610 (of 10192), d = 37, its = 55
i = 3611 (of 10192), d = 37, its = 55
i = 3612 (of 10192), d = 37, its = 59
i = 3613 (of 10192), d = 37, its = 57
i = 3614 (of 10192), d = 37, its = 52
i = 3615 (of 10192), d = 2.79983, its = 17
i = 3616 (of 10192), d = 37, its = 50
i = 3617 (of 10192), d = 37, its = 40
i = 3618 (of 10192), d = 37, its = 51
i = 3619 (of 10192), d = 8.10529, its = 3
i = 3620 (of 10192), d = 1.26017, its = 19
i = 3621 (of 10192), d = 11.2517, its = 6
i = 3622 (of 10192), d = 37, its = 51
i = 3623 (of 10192), d = 17.5473, its = 7
i = 3624 (of 10192), d = 7.94692, its = 3
i = 3625 (of 10192), d = 8.86761, its = 4
i = 3626 (of 10192), d = 37, its = 53
i = 3627 (of 10192), d = 12.6619, its = 7
i = 3628 (of 10192), d = 37, its = 58
i = 3629 (of 10192), d = 37, its = 54
i = 3630 (of 10192), d = 9.6689, its = 4
i = 3631 (of 10192), d = 37, its = 37
i = 3632 (of 10192), d = 37, its = 51
i = 3633 (of 10192), d = 7.94391, its = 3
i = 3634 (of 10192), d = 37, its = 52
i = 3635 (of 10192), d = 37, its = 37
i = 3636 (of 10192), d = 37, its = 51
i = 3637 (of 10192), d = 37, its = 55
i = 3638 (of 10192), d = 37, its = 37
i = 3639 (of 10192), d = 37, its = 54
i = 3640 (of 10192), d = 6.04989, its = 6
i = 3641 (of 10192), d = 37, its = 43
i = 3642 (of 10192), d = 37, its = 51
i = 3643 (of 10192), d = 11.5755, its = 5
i = 3644 (of 10192), d = 13.5948, its = 6
i = 3645 (of 10192), d = 4.73898, its = 18
i = 3646 (of 10192), d = 2.29523, its = 17
i = 3647 (of 10192), d = 37, its = 51
i = 3648 (of 10192), d = 37, its = 47
i = 3649 (of 10192), d = 1.83229, its = 22
i = 3650 (of 10192), d = 37, its = 53
i = 3651 (of 10192), d = 6.95564, its = 5
i = 3652 (of 10192), d = 37, its = 49
i = 3653 (of 10192), d = 3.62574, its = 19
i = 3654 (of 10192), d = 37, its = 50
i = 3655 (of 10192), d = 37, its = 52
i = 3656 (of 10192), d = 37, its = 51
i = 3657 (of 10192), d = 3.14253, its = 17
i = 3658 (of 10192), d = 37, its = 37
i = 3659 (of 10192), d = 7.5843, its = 4
i = 3660 (of 10192), d = 2.07597, its = 16
i = 3661 (of 10192), d = 37, its = 53
i = 3662 (of 10192), d = 37, its = 41
i = 3663 (of 10192), d = 2.05278, its = 17
i = 3664 (of 10192), d = 37, its = 37
i = 3665 (of 10192), d = 37, its = 52
i = 3666 (of 10192), d = 2.4637, its = 20
i = 3667 (of 10192), d = 37, its = 57
i = 3668 (of 10192), d = 5.58337, its = 6
i = 3669 (of 10192), d = 37, its = 53
i = 3670 (of 10192), d = 37, its = 55
i = 3671 (of 10192), d = 13.3807, its = 7
i = 3672 (of 10192), d = 37, its = 51
i = 3673 (of 10192), d = 37, its = 53
i = 3674 (of 10192), d = 37, its = 47
i = 3675 (of 10192), d = 37, its = 51
i = 3676 (of 10192), d = 37, its = 52
i = 3677 (of 10192), d = 37, its = 53
i = 3678 (of 10192), d = 37, its = 55
i = 3679 (of 10192), d = 37, its = 52
i = 3680 (of 10192), d = 37, its = 51
i = 3681 (of 10192), d = 1.65445, its = 20
i = 3682 (of 10192), d = 37, its = 54
i = 3683 (of 10192), d = 6.70696, its = 7
i = 3684 (of 10192), d = 37, its = 41
i = 3685 (of 10192), d = 37, its = 37
i = 3686 (of 10192), d = 37, its = 54
i = 3687 (of 10192), d = 16.6908, its = 7
i = 3688 (of 10192), d = 3.37082, its = 20
i = 3689 (of 10192), d = 37, its = 52
i = 3690 (of 10192), d = 37, its = 43
i = 3691 (of 10192), d = 37, its = 54
i = 3692 (of 10192), d = 37, its = 50
i = 3693 (of 10192), d = 3.97053, its = 19
i = 3694 (of 10192), d = 37, its = 53
i = 3695 (of 10192), d = 37, its = 51
i = 3696 (of 10192), d = 8.57451, its = 4
i = 3697 (of 10192), d = 37, its = 50
i = 3698 (of 10192), d = 2.80698, its = 17
i = 3699 (of 10192), d = 37, its = 55
i = 3700 (of 10192), d = 5.32826, its = 6
i = 3701 (of 10192), d = 37, its = 52
i = 3702 (of 10192), d = 36.2378, its = 8
i = 3703 (of 10192), d = 37, its = 52
i = 3704 (of 10192), d = 5.33443, its = 6
i = 3705 (of 10192), d = 12.9155, its = 6
i = 3706 (of 10192), d = 37, its = 37
i = 3707 (of 10192), d = 3.36131, its = 21
i = 3708 (of 10192), d = 37, its = 41
i = 3709 (of 10192), d = 3.22503, its = 17
i = 3710 (of 10192), d = 2.13235, its = 22
i = 3711 (of 10192), d = 10.0001, its = 5
i = 3712 (of 10192), d = 3.70862, its = 18
i = 3713 (of 10192), d = 37, its = 49
i = 3714 (of 10192), d = 37, its = 54
i = 3715 (of 10192), d = 9.8938, its = 5
i = 3716 (of 10192), d = 37, its = 40
i = 3717 (of 10192), d = 37, its = 50
i = 3718 (of 10192), d = 2.38679, its = 18
i = 3719 (of 10192), d = 9.26846, its = 5
i = 3720 (of 10192), d = 3.32679, its = 18
i = 3721 (of 10192), d = 9.63055, its = 5
i = 3722 (of 10192), d = 37, its = 57
i = 3723 (of 10192), d = 37, its = 49
i = 3724 (of 10192), d = 37, its = 54
i = 3725 (of 10192), d = 7.2701, its = 5
i = 3726 (of 10192), d = 2.5417, its = 18
i = 3727 (of 10192), d = 37, its = 50
i = 3728 (of 10192), d = 16.7863, its = 7
i = 3729 (of 10192), d = 13.6748, its = 6
i = 3730 (of 10192), d = 37, its = 56
i = 3731 (of 10192), d = 7.76119, its = 4
i = 3732 (of 10192), d = 37, its = 59
i = 3733 (of 10192), d = 37, its = 52
i = 3734 (of 10192), d = 2.09936, its = 22
i = 3735 (of 10192), d = 37, its = 37
i = 3736 (of 10192), d = 37, its = 37
i = 3737 (of 10192), d = 37, its = 50
i = 3738 (of 10192), d = 37, its = 52
i = 3739 (of 10192), d = 37, its = 50
i = 3740 (of 10192), d = 10.2637, its = 5
i = 3741 (of 10192), d = 37, its = 50
i = 3742 (of 10192), d = 3.44591, its = 19
i = 3743 (of 10192), d = 12.4119, its = 7
i = 3744 (of 10192), d = 37, its = 50
i = 3745 (of 10192), d = 37, its = 56
i = 3746 (of 10192), d = 37, its = 40
i = 3747 (of 10192), d = 4.22265, its = 16
i = 3748 (of 10192), d = 3.82787, its = 16
i = 3749 (of 10192), d = 37, its = 51
i = 3750 (of 10192), d = 37, its = 37
i = 3751 (of 10192), d = 37, its = 52
i = 3752 (of 10192), d = 8.79419, its = 4
i = 3753 (of 10192), d = 37, its = 55
i = 3754 (of 10192), d = 9.23302, its = 5
i = 3755 (of 10192), d = 12.5887, its = 6
i = 3756 (of 10192), d = 12.4839, its = 6
i = 3757 (of 10192), d = 7.15433, its = 5
i = 3758 (of 10192), d = 4.50437, its = 18
i = 3759 (of 10192), d = 37, its = 37
i = 3760 (of 10192), d = 37, its = 37
i = 3761 (of 10192), d = 2.09936, its = 22
i = 3762 (of 10192), d = 37, its = 53
i = 3763 (of 10192), d = 37, its = 51
i = 3764 (of 10192), d = 37, its = 55
i = 3765 (of 10192), d = 37, its = 37
i = 3766 (of 10192), d = 37, its = 37
i = 3767 (of 10192), d = 30.751, its = 18
i = 3768 (of 10192), d = 37, its = 53
i = 3769 (of 10192), d = 37, its = 41
i = 3770 (of 10192), d = 37, its = 48
i = 3771 (of 10192), d = 37, its = 53
i = 3772 (of 10192), d = 37, its = 53
i = 3773 (of 10192), d = 37, its = 44
i = 3774 (of 10192), d = 37, its = 51
i = 3775 (of 10192), d = 37, its = 57
i = 3776 (of 10192), d = 37, its = 40
i = 3777 (of 10192), d = 4.45656, its = 19
i = 3778 (of 10192), d = 5.7214, its = 6
i = 3779 (of 10192), d = 3.11417, its = 19
i = 3780 (of 10192), d = 37, its = 37
i = 3781 (of 10192), d = 37, its = 54
i = 3782 (of 10192), d = 6.09105, its = 6
i = 3783 (of 10192), d = 37, its = 37
i = 3784 (of 10192), d = 7.68347, its = 3
i = 3785 (of 10192), d = 14.3005, its = 6
i = 3786 (of 10192), d = 2.15822, its = 19
i = 3787 (of 10192), d = 37, its = 56
i = 3788 (of 10192), d = 37, its = 41
i = 3789 (of 10192), d = 37, its = 43
i = 3790 (of 10192), d = 37, its = 37
i = 3791 (of 10192), d = 7.45404, its = 4
i = 3792 (of 10192), d = 36.4524, its = 32
i = 3793 (of 10192), d = 5.84245, its = 7
i = 3794 (of 10192), d = 37, its = 52
i = 3795 (of 10192), d = 3.7942, its = 18
i = 3796 (of 10192), d = 6.98198, its = 5
i = 3797 (of 10192), d = 37, its = 50
i = 3798 (of 10192), d = 37, its = 40
i = 3799 (of 10192), d = 37, its = 50
i = 3800 (of 10192), d = 37, its = 37
i = 3801 (of 10192), d = 9.36282, its = 4
i = 3802 (of 10192), d = 2.3205, its = 20
i = 3803 (of 10192), d = 9.6933, its = 5
i = 3804 (of 10192), d = 37, its = 55
i = 3805 (of 10192), d = 2.37347, its = 18
i = 3806 (of 10192), d = 37, its = 51
i = 3807 (of 10192), d = 37, its = 50
i = 3808 (of 10192), d = 8.78847, its = 4
i = 3809 (of 10192), d = 1.26549, its = 18
i = 3810 (of 10192), d = 37, its = 41
i = 3811 (of 10192), d = 37, its = 47
i = 3812 (of 10192), d = 37, its = 53
i = 3813 (of 10192), d = 2.14199, its = 17
i = 3814 (of 10192), d = 37, its = 54
i = 3815 (of 10192), d = 37, its = 41
i = 3816 (of 10192), d = 37, its = 37
i = 3817 (of 10192), d = 37, its = 50
i = 3818 (of 10192), d = 37, its = 52
i = 3819 (of 10192), d = 37, its = 55
i = 3820 (of 10192), d = 37, its = 37
i = 3821 (of 10192), d = 37, its = 51
i = 3822 (of 10192), d = 37, its = 49
i = 3823 (of 10192), d = 37, its = 54
i = 3824 (of 10192), d = 37, its = 51
i = 3825 (of 10192), d = 37, its = 52
i = 3826 (of 10192), d = 37, its = 51
i = 3827 (of 10192), d = 37, its = 37
i = 3828 (of 10192), d = 3.63087, its = 18
i = 3829 (of 10192), d = 2.3345, its = 18
i = 3830 (of 10192), d = 22.2955, its = 8
i = 3831 (of 10192), d = 37, its = 51
i = 3832 (of 10192), d = 37, its = 54
i = 3833 (of 10192), d = 37, its = 49
i = 3834 (of 10192), d = 37, its = 50
i = 3835 (of 10192), d = 37, its = 49
i = 3836 (of 10192), d = 37, its = 53
i = 3837 (of 10192), d = 37, its = 37
i = 3838 (of 10192), d = 37, its = 37
i = 3839 (of 10192), d = 37, its = 50
i = 3840 (of 10192), d = 1.78465, its = 18
i = 3841 (of 10192), d = 37, its = 40
i = 3842 (of 10192), d = 37, its = 52
i = 3843 (of 10192), d = 7.58608, its = 4
i = 3844 (of 10192), d = 37, its = 48
i = 3845 (of 10192), d = 37, its = 53
i = 3846 (of 10192), d = 37, its = 50
i = 3847 (of 10192), d = 37, its = 49
i = 3848 (of 10192), d = 3.99128, its = 16
i = 3849 (of 10192), d = 37, its = 53
i = 3850 (of 10192), d = 12.4804, its = 6
i = 3851 (of 10192), d = 37, its = 53
i = 3852 (of 10192), d = 7.69897, its = 3
i = 3853 (of 10192), d = 37, its = 37
i = 3854 (of 10192), d = 37, its = 37
i = 3855 (of 10192), d = 5.29583, its = 6
i = 3856 (of 10192), d = 1.10881, its = 18
i = 3857 (of 10192), d = 37, its = 37
i = 3858 (of 10192), d = 37, its = 53
i = 3859 (of 10192), d = 7.59663, its = 4
i = 3860 (of 10192), d = 37, its = 54
i = 3861 (of 10192), d = 6.86879, its = 5
i = 3862 (of 10192), d = 37, its = 51
i = 3863 (of 10192), d = 37, its = 50
i = 3864 (of 10192), d = 4.99818, its = 7
i = 3865 (of 10192), d = 37, its = 53
i = 3866 (of 10192), d = 8.24184, its = 3
i = 3867 (of 10192), d = 37, its = 52
i = 3868 (of 10192), d = 37, its = 45
i = 3869 (of 10192), d = 37, its = 53
i = 3870 (of 10192), d = 9.62856, its = 5
i = 3871 (of 10192), d = 37, its = 52
i = 3872 (of 10192), d = 37, its = 52
i = 3873 (of 10192), d = 37, its = 54
i = 3874 (of 10192), d = 37, its = 50
i = 3875 (of 10192), d = 37, its = 55
i = 3876 (of 10192), d = 10.7775, its = 7
i = 3877 (of 10192), d = 37, its = 37
i = 3878 (of 10192), d = 37, its = 40
i = 3879 (of 10192), d = 37, its = 52
i = 3880 (of 10192), d = 37, its = 55
i = 3881 (of 10192), d = 5.96853, its = 6
i = 3882 (of 10192), d = 37, its = 56
i = 3883 (of 10192), d = 37, its = 53
i = 3884 (of 10192), d = 37, its = 37
i = 3885 (of 10192), d = 4.67126, its = 16
i = 3886 (of 10192), d = 37, its = 38
i = 3887 (of 10192), d = 37, its = 37
i = 3888 (of 10192), d = 37, its = 40
i = 3889 (of 10192), d = 37, its = 53
i = 3890 (of 10192), d = 37, its = 41
i = 3891 (of 10192), d = 37, its = 51
i = 3892 (of 10192), d = 37, its = 57
i = 3893 (of 10192), d = 37, its = 37
i = 3894 (of 10192), d = 37, its = 52
i = 3895 (of 10192), d = 6.90513, its = 5
i = 3896 (of 10192), d = 8.46203, its = 4
i = 3897 (of 10192), d = 8.66479, its = 4
i = 3898 (of 10192), d = 9.67101, its = 5
i = 3899 (of 10192), d = 37, its = 51
i = 3900 (of 10192), d = 2.43603, its = 18
i = 3901 (of 10192), d = 5.29081, its = 6
i = 3902 (of 10192), d = 37, its = 49
i = 3903 (of 10192), d = 6.59588, its = 5
i = 3904 (of 10192), d = 37, its = 54
i = 3905 (of 10192), d = 21.9908, its = 7
i = 3906 (of 10192), d = 37, its = 40
i = 3907 (of 10192), d = 8.3737, its = 4
i = 3908 (of 10192), d = 37, its = 56
i = 3909 (of 10192), d = 3.50745, its = 23
i = 3910 (of 10192), d = 37, its = 52
i = 3911 (of 10192), d = 4.85426, its = 19
i = 3912 (of 10192), d = 37, its = 39
i = 3913 (of 10192), d = 37, its = 52
i = 3914 (of 10192), d = 37, its = 52
i = 3915 (of 10192), d = 4.47709, its = 17
i = 3916 (of 10192), d = 4.01889, its = 20
i = 3917 (of 10192), d = 1.80051, its = 25
i = 3918 (of 10192), d = 37, its = 51
i = 3919 (of 10192), d = 7.51692, its = 4
i = 3920 (of 10192), d = 37, its = 37
i = 3921 (of 10192), d = 37, its = 51
i = 3922 (of 10192), d = 4.12785, its = 17
i = 3923 (of 10192), d = 4.15042, its = 21
i = 3924 (of 10192), d = 4.73705, its = 20
i = 3925 (of 10192), d = 37, its = 37
i = 3926 (of 10192), d = 2.85541, its = 21
i = 3927 (of 10192), d = 37, its = 53
i = 3928 (of 10192), d = 37, its = 41
i = 3929 (of 10192), d = 37, its = 37
i = 3930 (of 10192), d = 8.1191, its = 3
i = 3931 (of 10192), d = 3.75786, its = 17
i = 3932 (of 10192), d = 3.55939, its = 17
i = 3933 (of 10192), d = 37, its = 52
i = 3934 (of 10192), d = 37, its = 49
i = 3935 (of 10192), d = 37, its = 52
i = 3936 (of 10192), d = 37, its = 39
i = 3937 (of 10192), d = 37, its = 54
i = 3938 (of 10192), d = 37, its = 54
i = 3939 (of 10192), d = 37, its = 53
i = 3940 (of 10192), d = 37, its = 53
i = 3941 (of 10192), d = 37, its = 37
i = 3942 (of 10192), d = 7.10177, its = 4
i = 3943 (of 10192), d = 3.11417, its = 19
i = 3944 (of 10192), d = 37, its = 55
i = 3945 (of 10192), d = 37, its = 37
i = 3946 (of 10192), d = 37, its = 52
i = 3947 (of 10192), d = 37, its = 57
i = 3948 (of 10192), d = 4.32167, its = 22
i = 3949 (of 10192), d = 37, its = 41
i = 3950 (of 10192), d = 27.3491, its = 10
i = 3951 (of 10192), d = 2.47753, its = 19
i = 3952 (of 10192), d = 37, its = 53
i = 3953 (of 10192), d = 37, its = 39
i = 3954 (of 10192), d = 37, its = 51
i = 3955 (of 10192), d = 37, its = 40
i = 3956 (of 10192), d = 37, its = 43
i = 3957 (of 10192), d = 3.3154, its = 20
i = 3958 (of 10192), d = 6.323, its = 6
i = 3959 (of 10192), d = 37, its = 50
i = 3960 (of 10192), d = 37, its = 37
i = 3961 (of 10192), d = 19.8764, its = 7
i = 3962 (of 10192), d = 37, its = 53
i = 3963 (of 10192), d = 37, its = 49
i = 3964 (of 10192), d = 17.0563, its = 7
i = 3965 (of 10192), d = 37, its = 49
i = 3966 (of 10192), d = 37, its = 37
i = 3967 (of 10192), d = 10.2218, its = 5
i = 3968 (of 10192), d = 7.81771, its = 3
i = 3969 (of 10192), d = 5.56213, its = 6
i = 3970 (of 10192), d = 37, its = 52
i = 3971 (of 10192), d = 37, its = 51
i = 3972 (of 10192), d = 37, its = 53
i = 3973 (of 10192), d = 14.9911, its = 7
i = 3974 (of 10192), d = 37, its = 49
i = 3975 (of 10192), d = 37, its = 54
i = 3976 (of 10192), d = 3.45268, its = 18
i = 3977 (of 10192), d = 37, its = 51
i = 3978 (of 10192), d = 7.04041, its = 5
i = 3979 (of 10192), d = 37, its = 49
i = 3980 (of 10192), d = 37, its = 54
i = 3981 (of 10192), d = 37, its = 52
i = 3982 (of 10192), d = 37, its = 50
i = 3983 (of 10192), d = 37, its = 37
i = 3984 (of 10192), d = 37, its = 37
i = 3985 (of 10192), d = 37, its = 52
i = 3986 (of 10192), d = 5.45946, its = 7
i = 3987 (of 10192), d = 37, its = 52
i = 3988 (of 10192), d = 37, its = 37
i = 3989 (of 10192), d = 37, its = 51
i = 3990 (of 10192), d = 37, its = 52
i = 3991 (of 10192), d = 37, its = 37
i = 3992 (of 10192), d = 3.3858, its = 17
i = 3993 (of 10192), d = 37, its = 53
i = 3994 (of 10192), d = 37, its = 58
i = 3995 (of 10192), d = 3.79677, its = 16
i = 3996 (of 10192), d = 37, its = 52
i = 3997 (of 10192), d = 37, its = 56
i = 3998 (of 10192), d = 37, its = 54
i = 3999 (of 10192), d = 37, its = 40
i = 4000 (of 10192), d = 37, its = 40
i = 4001 (of 10192), d = 37, its = 56
i = 4002 (of 10192), d = 37, its = 49
i = 4003 (of 10192), d = 4.51664, its = 18
i = 4004 (of 10192), d = 2.3205, its = 20
i = 4005 (of 10192), d = 37, its = 41
i = 4006 (of 10192), d = 37, its = 51
i = 4007 (of 10192), d = 11.7826, its = 7
i = 4008 (of 10192), d = 3.75194, its = 24
i = 4009 (of 10192), d = 37, its = 41
i = 4010 (of 10192), d = 37, its = 51
i = 4011 (of 10192), d = 37, its = 37
i = 4012 (of 10192), d = 37, its = 52
i = 4013 (of 10192), d = 37, its = 52
i = 4014 (of 10192), d = 4.69151, its = 22
i = 4015 (of 10192), d = 37, its = 53
i = 4016 (of 10192), d = 7.93906, its = 3
i = 4017 (of 10192), d = 37, its = 37
i = 4018 (of 10192), d = 3.21735, its = 20
i = 4019 (of 10192), d = 37, its = 41
i = 4020 (of 10192), d = 37, its = 54
i = 4021 (of 10192), d = 37, its = 50
i = 4022 (of 10192), d = 37, its = 54
i = 4023 (of 10192), d = 13.1648, its = 6
i = 4024 (of 10192), d = 7.67184, its = 4
i = 4025 (of 10192), d = 37, its = 40
i = 4026 (of 10192), d = 37, its = 54
i = 4027 (of 10192), d = 3.33084, its = 20
i = 4028 (of 10192), d = 37, its = 54
i = 4029 (of 10192), d = 4.22825, its = 18
i = 4030 (of 10192), d = 37, its = 51
i = 4031 (of 10192), d = 37, its = 51
i = 4032 (of 10192), d = 37, its = 52
i = 4033 (of 10192), d = 37, its = 50
i = 4034 (of 10192), d = 4.62931, its = 23
i = 4035 (of 10192), d = 37, its = 50
i = 4036 (of 10192), d = 6.58421, its = 5
i = 4037 (of 10192), d = 37, its = 56
i = 4038 (of 10192), d = 37, its = 51
i = 4039 (of 10192), d = 3.9837, its = 19
i = 4040 (of 10192), d = 14.1086, its = 6
i = 4041 (of 10192), d = 1.3205, its = 17
i = 4042 (of 10192), d = 1.76334, its = 17
i = 4043 (of 10192), d = 37, its = 56
i = 4044 (of 10192), d = 37, its = 52
i = 4045 (of 10192), d = 37, its = 54
i = 4046 (of 10192), d = 3.27432, its = 20
i = 4047 (of 10192), d = 37, its = 37
i = 4048 (of 10192), d = 10.0465, its = 5
i = 4049 (of 10192), d = 37, its = 49
i = 4050 (of 10192), d = 37, its = 37
i = 4051 (of 10192), d = 37, its = 39
i = 4052 (of 10192), d = 22.188, its = 8
i = 4053 (of 10192), d = 37, its = 53
i = 4054 (of 10192), d = 8.18456, its = 3
i = 4055 (of 10192), d = 16.7922, its = 7
i = 4056 (of 10192), d = 37, its = 57
i = 4057 (of 10192), d = 37, its = 39
i = 4058 (of 10192), d = 37, its = 51
i = 4059 (of 10192), d = 37, its = 37
i = 4060 (of 10192), d = 17.8004, its = 7
i = 4061 (of 10192), d = 7.10278, its = 4
i = 4062 (of 10192), d = 37, its = 52
i = 4063 (of 10192), d = 37, its = 54
i = 4064 (of 10192), d = 37, its = 54
i = 4065 (of 10192), d = 37, its = 55
i = 4066 (of 10192), d = 37, its = 52
i = 4067 (of 10192), d = 37, its = 39
i = 4068 (of 10192), d = 9.03014, its = 4
i = 4069 (of 10192), d = 3.29004, its = 19
i = 4070 (of 10192), d = 37, its = 40
i = 4071 (of 10192), d = 5.99888, its = 6
i = 4072 (of 10192), d = 11.2487, its = 7
i = 4073 (of 10192), d = 37, its = 53
i = 4074 (of 10192), d = 18.3619, its = 7
i = 4075 (of 10192), d = 9.18989, its = 4
i = 4076 (of 10192), d = 2.99711, its = 16
i = 4077 (of 10192), d = 37, its = 54
i = 4078 (of 10192), d = 37, its = 37
i = 4079 (of 10192), d = 37, its = 52
i = 4080 (of 10192), d = 37, its = 37
i = 4081 (of 10192), d = 37, its = 50
i = 4082 (of 10192), d = 37, its = 51
i = 4083 (of 10192), d = 37, its = 52
i = 4084 (of 10192), d = 37, its = 53
i = 4085 (of 10192), d = 9.98774, its = 5
i = 4086 (of 10192), d = 37, its = 39
i = 4087 (of 10192), d = 37, its = 53
i = 4088 (of 10192), d = 37, its = 49
i = 4089 (of 10192), d = 37, its = 56
i = 4090 (of 10192), d = 37, its = 53
i = 4091 (of 10192), d = 37, its = 52
i = 4092 (of 10192), d = 1.90555, its = 22
i = 4093 (of 10192), d = 37, its = 50
i = 4094 (of 10192), d = 37, its = 50
i = 4095 (of 10192), d = 37, its = 37
i = 4096 (of 10192), d = 37, its = 57
i = 4097 (of 10192), d = 2.8754, its = 23
i = 4098 (of 10192), d = 37, its = 52
i = 4099 (of 10192), d = 37, its = 53
i = 4100 (of 10192), d = 37, its = 53
i = 4101 (of 10192), d = 12.6218, its = 7
i = 4102 (of 10192), d = 37, its = 37
i = 4103 (of 10192), d = 7.38892, its = 4
i = 4104 (of 10192), d = 37, its = 54
i = 4105 (of 10192), d = 37, its = 52
i = 4106 (of 10192), d = 37, its = 37
i = 4107 (of 10192), d = 37, its = 53
i = 4108 (of 10192), d = 6.36202, its = 6
i = 4109 (of 10192), d = 37, its = 52
i = 4110 (of 10192), d = 37, its = 50
i = 4111 (of 10192), d = 6.84892, its = 5
i = 4112 (of 10192), d = 37, its = 37
i = 4113 (of 10192), d = 37, its = 40
i = 4114 (of 10192), d = 37, its = 52
i = 4115 (of 10192), d = 37, its = 50
i = 4116 (of 10192), d = 8.82295, its = 4
i = 4117 (of 10192), d = 37, its = 55
i = 4118 (of 10192), d = 8.96648, its = 4
i = 4119 (of 10192), d = 3.14935, its = 17
i = 4120 (of 10192), d = 4.87889, its = 18
i = 4121 (of 10192), d = 12.8375, its = 6
i = 4122 (of 10192), d = 37, its = 53
i = 4123 (of 10192), d = 37, its = 37
i = 4124 (of 10192), d = 13.3821, its = 7
i = 4125 (of 10192), d = 37, its = 37
i = 4126 (of 10192), d = 6.16007, its = 8
i = 4127 (of 10192), d = 37, its = 53
i = 4128 (of 10192), d = 37, its = 43
i = 4129 (of 10192), d = 3.85806, its = 17
i = 4130 (of 10192), d = 3.50228, its = 21
i = 4131 (of 10192), d = 3.13707, its = 20
i = 4132 (of 10192), d = 37, its = 51
i = 4133 (of 10192), d = 37, its = 55
i = 4134 (of 10192), d = 37, its = 50
i = 4135 (of 10192), d = 37, its = 51
i = 4136 (of 10192), d = 37, its = 40
i = 4137 (of 10192), d = 37, its = 54
i = 4138 (of 10192), d = 10.3457, its = 5
i = 4139 (of 10192), d = 6.92478, its = 7
i = 4140 (of 10192), d = 37, its = 48
i = 4141 (of 10192), d = 2.60358, its = 17
i = 4142 (of 10192), d = 5.9118, its = 6
i = 4143 (of 10192), d = 37, its = 53
i = 4144 (of 10192), d = 9.88891, its = 5
i = 4145 (of 10192), d = 21.9788, its = 10
i = 4146 (of 10192), d = 37, its = 56
i = 4147 (of 10192), d = 37, its = 42
i = 4148 (of 10192), d = 2.25537, its = 18
i = 4149 (of 10192), d = 8.31871, its = 3
i = 4150 (of 10192), d = 37, its = 37
i = 4151 (of 10192), d = 37, its = 52
i = 4152 (of 10192), d = 12.2662, its = 5
i = 4153 (of 10192), d = 37, its = 55
i = 4154 (of 10192), d = 37, its = 56
i = 4155 (of 10192), d = 7.35701, its = 4
i = 4156 (of 10192), d = 37, its = 56
i = 4157 (of 10192), d = 37, its = 37
i = 4158 (of 10192), d = 37, its = 37
i = 4159 (of 10192), d = 37, its = 51
i = 4160 (of 10192), d = 37, its = 57
i = 4161 (of 10192), d = 37, its = 53
i = 4162 (of 10192), d = 37, its = 52
i = 4163 (of 10192), d = 37, its = 53
i = 4164 (of 10192), d = 37, its = 37
i = 4165 (of 10192), d = 37, its = 41
i = 4166 (of 10192), d = 37, its = 45
i = 4167 (of 10192), d = 7.49508, its = 4
i = 4168 (of 10192), d = 37, its = 53
i = 4169 (of 10192), d = 37, its = 37
i = 4170 (of 10192), d = 3.78558, its = 21
i = 4171 (of 10192), d = 37, its = 53
i = 4172 (of 10192), d = 37, its = 42
i = 4173 (of 10192), d = 1.76907, its = 17
i = 4174 (of 10192), d = 37, its = 52
i = 4175 (of 10192), d = 11.5441, its = 6
i = 4176 (of 10192), d = 37, its = 55
i = 4177 (of 10192), d = 37, its = 56
i = 4178 (of 10192), d = 37, its = 54
i = 4179 (of 10192), d = 37, its = 56
i = 4180 (of 10192), d = 2.02659, its = 17
i = 4181 (of 10192), d = 37, its = 39
i = 4182 (of 10192), d = 37, its = 56
i = 4183 (of 10192), d = 37, its = 54
i = 4184 (of 10192), d = 37, its = 50
i = 4185 (of 10192), d = 37, its = 51
i = 4186 (of 10192), d = 8.84363, its = 4
i = 4187 (of 10192), d = 37, its = 53
i = 4188 (of 10192), d = 37, its = 52
i = 4189 (of 10192), d = 37, its = 52
i = 4190 (of 10192), d = 8.01522, its = 2
i = 4191 (of 10192), d = 11.7605, its = 5
i = 4192 (of 10192), d = 10.4423, its = 5
i = 4193 (of 10192), d = 37, its = 37
i = 4194 (of 10192), d = 11.019, its = 5
i = 4195 (of 10192), d = 4.11236, its = 20
i = 4196 (of 10192), d = 6.318, its = 6
i = 4197 (of 10192), d = 9.30137, its = 4
i = 4198 (of 10192), d = 9.28384, its = 4
i = 4199 (of 10192), d = 37, its = 38
i = 4200 (of 10192), d = 11.1459, its = 6
i = 4201 (of 10192), d = 2.4088, its = 21
i = 4202 (of 10192), d = 2.37973, its = 18
i = 4203 (of 10192), d = 9.15795, its = 4
i = 4204 (of 10192), d = 37, its = 49
i = 4205 (of 10192), d = 37, its = 55
i = 4206 (of 10192), d = 37, its = 39
i = 4207 (of 10192), d = 37, its = 51
i = 4208 (of 10192), d = 37, its = 50
i = 4209 (of 10192), d = 9.13297, its = 5
i = 4210 (of 10192), d = 7.32156, its = 4
i = 4211 (of 10192), d = 6.30697, its = 6
i = 4212 (of 10192), d = 8.84804, its = 4
i = 4213 (of 10192), d = 37, its = 40
i = 4214 (of 10192), d = 3.97748, its = 18
i = 4215 (of 10192), d = 37, its = 54
i = 4216 (of 10192), d = 37, its = 40
i = 4217 (of 10192), d = 37, its = 52
i = 4218 (of 10192), d = 4.13752, its = 20
i = 4219 (of 10192), d = 6.29577, its = 6
i = 4220 (of 10192), d = 4.36296, its = 22
i = 4221 (of 10192), d = 28.9308, its = 9
i = 4222 (of 10192), d = 37, its = 54
i = 4223 (of 10192), d = 1.81613, its = 17
i = 4224 (of 10192), d = 37, its = 57
i = 4225 (of 10192), d = 37, its = 55
i = 4226 (of 10192), d = 2.13526, its = 17
i = 4227 (of 10192), d = 37, its = 55
i = 4228 (of 10192), d = 37, its = 56
i = 4229 (of 10192), d = 37, its = 37
i = 4230 (of 10192), d = 2.4176, its = 18
i = 4231 (of 10192), d = 37, its = 56
i = 4232 (of 10192), d = 2.71168, its = 19
i = 4233 (of 10192), d = 37, its = 55
i = 4234 (of 10192), d = 37, its = 41
i = 4235 (of 10192), d = 37, its = 51
i = 4236 (of 10192), d = 37, its = 40
i = 4237 (of 10192), d = 5.94111, its = 7
i = 4238 (of 10192), d = 37, its = 50
i = 4239 (of 10192), d = 37, its = 48
i = 4240 (of 10192), d = 1.19174, its = 19
i = 4241 (of 10192), d = 37, its = 55
i = 4242 (of 10192), d = 2.62163, its = 17
i = 4243 (of 10192), d = 37, its = 49
i = 4244 (of 10192), d = 3.86232, its = 18
i = 4245 (of 10192), d = 6.63931, its = 5
i = 4246 (of 10192), d = 37, its = 51
i = 4247 (of 10192), d = 37, its = 54
i = 4248 (of 10192), d = 37, its = 49
i = 4249 (of 10192), d = 37, its = 51
i = 4250 (of 10192), d = 37, its = 50
i = 4251 (of 10192), d = 6.27927, its = 6
i = 4252 (of 10192), d = 37, its = 50
i = 4253 (of 10192), d = 37, its = 37
i = 4254 (of 10192), d = 37, its = 55
i = 4255 (of 10192), d = 3.74655, its = 18
i = 4256 (of 10192), d = 37, its = 51
i = 4257 (of 10192), d = 37, its = 37
i = 4258 (of 10192), d = 37, its = 53
i = 4259 (of 10192), d = 37, its = 53
i = 4260 (of 10192), d = 12.3223, its = 7
i = 4261 (of 10192), d = 37, its = 55
i = 4262 (of 10192), d = 12.3918, its = 6
i = 4263 (of 10192), d = 37, its = 57
i = 4264 (of 10192), d = 37, its = 51
i = 4265 (of 10192), d = 37, its = 43
i = 4266 (of 10192), d = 6.1805, its = 6
i = 4267 (of 10192), d = 37, its = 51
i = 4268 (of 10192), d = 2.78763, its = 20
i = 4269 (of 10192), d = 37, its = 39
i = 4270 (of 10192), d = 4.42514, its = 21
i = 4271 (of 10192), d = 37, its = 37
i = 4272 (of 10192), d = 37, its = 55
i = 4273 (of 10192), d = 37, its = 57
i = 4274 (of 10192), d = 37, its = 51
i = 4275 (of 10192), d = 37, its = 53
i = 4276 (of 10192), d = 37, its = 54
i = 4277 (of 10192), d = 37, its = 55
i = 4278 (of 10192), d = 37, its = 53
i = 4279 (of 10192), d = 1.92166, its = 20
i = 4280 (of 10192), d = 12.9211, its = 6
i = 4281 (of 10192), d = 37, its = 51
i = 4282 (of 10192), d = 37, its = 53
i = 4283 (of 10192), d = 17.7821, its = 7
i = 4284 (of 10192), d = 37, its = 52
i = 4285 (of 10192), d = 37, its = 50
i = 4286 (of 10192), d = 9.25619, its = 5
i = 4287 (of 10192), d = 2.82766, its = 20
i = 4288 (of 10192), d = 37, its = 44
i = 4289 (of 10192), d = 2.6919, its = 27
i = 4290 (of 10192), d = 37, its = 50
i = 4291 (of 10192), d = 37, its = 40
i = 4292 (of 10192), d = 37, its = 49
i = 4293 (of 10192), d = 8.8172, its = 4
i = 4294 (of 10192), d = 3.85512, its = 17
i = 4295 (of 10192), d = 37, its = 56
i = 4296 (of 10192), d = 37, its = 38
i = 4297 (of 10192), d = 4.55488, its = 17
i = 4298 (of 10192), d = 37, its = 48
i = 4299 (of 10192), d = 11.4747, its = 5
i = 4300 (of 10192), d = 37, its = 37
i = 4301 (of 10192), d = 37, its = 54
i = 4302 (of 10192), d = 37, its = 56
i = 4303 (of 10192), d = 37, its = 53
i = 4304 (of 10192), d = 37, its = 51
i = 4305 (of 10192), d = 37, its = 50
i = 4306 (of 10192), d = 1.53191, its = 19
i = 4307 (of 10192), d = 37, its = 38
i = 4308 (of 10192), d = 37, its = 56
i = 4309 (of 10192), d = 37, its = 58
i = 4310 (of 10192), d = 37, its = 52
i = 4311 (of 10192), d = 9.30902, its = 5
i = 4312 (of 10192), d = 9.96532, its = 5
i = 4313 (of 10192), d = 9.56014, its = 5
i = 4314 (of 10192), d = 37, its = 53
i = 4315 (of 10192), d = 37, its = 56
i = 4316 (of 10192), d = 37, its = 37
i = 4317 (of 10192), d = 37, its = 50
i = 4318 (of 10192), d = 7.48746, its = 4
i = 4319 (of 10192), d = 37, its = 56
i = 4320 (of 10192), d = 37, its = 37
i = 4321 (of 10192), d = 3.83618, its = 19
i = 4322 (of 10192), d = 37, its = 37
i = 4323 (of 10192), d = 37, its = 40
i = 4324 (of 10192), d = 37, its = 54
i = 4325 (of 10192), d = 4.33565, its = 23
i = 4326 (of 10192), d = 37, its = 55
i = 4327 (of 10192), d = 37, its = 48
i = 4328 (of 10192), d = 37, its = 49
i = 4329 (of 10192), d = 37, its = 52
i = 4330 (of 10192), d = 37, its = 55
i = 4331 (of 10192), d = 5.33443, its = 6
i = 4332 (of 10192), d = 37, its = 53
i = 4333 (of 10192), d = 5.88113, its = 6
i = 4334 (of 10192), d = 4.47365, its = 18
i = 4335 (of 10192), d = 37, its = 56
i = 4336 (of 10192), d = 9.9408, its = 5
i = 4337 (of 10192), d = 37, its = 52
i = 4338 (of 10192), d = 9.92721, its = 5
i = 4339 (of 10192), d = 37, its = 55
i = 4340 (of 10192), d = 6.42308, its = 6
i = 4341 (of 10192), d = 37, its = 37
i = 4342 (of 10192), d = 37, its = 37
i = 4343 (of 10192), d = 37, its = 51
i = 4344 (of 10192), d = 37, its = 53
i = 4345 (of 10192), d = 37, its = 55
i = 4346 (of 10192), d = 37, its = 55
i = 4347 (of 10192), d = 37, its = 38
i = 4348 (of 10192), d = 4.06053, its = 21
i = 4349 (of 10192), d = 37, its = 50
i = 4350 (of 10192), d = 37, its = 53
i = 4351 (of 10192), d = 9.36309, its = 4
i = 4352 (of 10192), d = 1.86066, its = 17
i = 4353 (of 10192), d = 37, its = 51
i = 4354 (of 10192), d = 3.3844, its = 19
i = 4355 (of 10192), d = 37, its = 40
i = 4356 (of 10192), d = 37, its = 40
i = 4357 (of 10192), d = 37, its = 54
i = 4358 (of 10192), d = 37, its = 37
i = 4359 (of 10192), d = 37, its = 55
i = 4360 (of 10192), d = 37, its = 51
i = 4361 (of 10192), d = 37, its = 51
i = 4362 (of 10192), d = 37, its = 51
i = 4363 (of 10192), d = 37, its = 53
i = 4364 (of 10192), d = 37, its = 48
i = 4365 (of 10192), d = 37, its = 52
i = 4366 (of 10192), d = 37, its = 56
i = 4367 (of 10192), d = 37, its = 55
i = 4368 (of 10192), d = 5.11697, its = 6
i = 4369 (of 10192), d = 37, its = 57
i = 4370 (of 10192), d = 37, its = 53
i = 4371 (of 10192), d = 37, its = 55
i = 4372 (of 10192), d = 37, its = 53
i = 4373 (of 10192), d = 5.86289, its = 6
i = 4374 (of 10192), d = 37, its = 50
i = 4375 (of 10192), d = 37, its = 52
i = 4376 (of 10192), d = 37, its = 37
i = 4377 (of 10192), d = 37, its = 52
i = 4378 (of 10192), d = 4.30402, its = 19
i = 4379 (of 10192), d = 37, its = 54
i = 4380 (of 10192), d = 37, its = 37
i = 4381 (of 10192), d = 37, its = 56
i = 4382 (of 10192), d = 3.21411, its = 17
i = 4383 (of 10192), d = 37, its = 52
i = 4384 (of 10192), d = 37, its = 53
i = 4385 (of 10192), d = 5.33583, its = 6
i = 4386 (of 10192), d = 34.1263, its = 30
i = 4387 (of 10192), d = 5.51271, its = 7
i = 4388 (of 10192), d = 37, its = 39
i = 4389 (of 10192), d = 2.22597, its = 19
i = 4390 (of 10192), d = 37, its = 53
i = 4391 (of 10192), d = 37, its = 37
i = 4392 (of 10192), d = 37, its = 51
i = 4393 (of 10192), d = 37, its = 41
i = 4394 (of 10192), d = 37, its = 39
i = 4395 (of 10192), d = 3.06654, its = 20
i = 4396 (of 10192), d = 37, its = 53
i = 4397 (of 10192), d = 37, its = 53
i = 4398 (of 10192), d = 37, its = 42
i = 4399 (of 10192), d = 37, its = 55
i = 4400 (of 10192), d = 8.4528, its = 4
i = 4401 (of 10192), d = 37, its = 52
i = 4402 (of 10192), d = 37, its = 53
i = 4403 (of 10192), d = 37, its = 51
i = 4404 (of 10192), d = 37, its = 40
i = 4405 (of 10192), d = 4.35469, its = 19
i = 4406 (of 10192), d = 37, its = 51
i = 4407 (of 10192), d = 37, its = 37
i = 4408 (of 10192), d = 4.52788, its = 15
i = 4409 (of 10192), d = 37, its = 57
i = 4410 (of 10192), d = 37, its = 37
i = 4411 (of 10192), d = 37, its = 53
i = 4412 (of 10192), d = 37, its = 52
i = 4413 (of 10192), d = 37, its = 56
i = 4414 (of 10192), d = 37, its = 54
i = 4415 (of 10192), d = 37, its = 54
i = 4416 (of 10192), d = 37, its = 53
i = 4417 (of 10192), d = 37, its = 39
i = 4418 (of 10192), d = 3.81243, its = 27
i = 4419 (of 10192), d = 37, its = 40
i = 4420 (of 10192), d = 37, its = 51
i = 4421 (of 10192), d = 3.5822, its = 21
i = 4422 (of 10192), d = 6.49258, its = 5
i = 4423 (of 10192), d = 37, its = 55
i = 4424 (of 10192), d = 4.75989, its = 19
i = 4425 (of 10192), d = 2.51188, its = 24
i = 4426 (of 10192), d = 37, its = 51
i = 4427 (of 10192), d = 37, its = 41
i = 4428 (of 10192), d = 37, its = 37
i = 4429 (of 10192), d = 13.5475, its = 6
i = 4430 (of 10192), d = 6.47178, its = 5
i = 4431 (of 10192), d = 37, its = 37
i = 4432 (of 10192), d = 37, its = 54
i = 4433 (of 10192), d = 10.2141, its = 5
i = 4434 (of 10192), d = 37, its = 54
i = 4435 (of 10192), d = 1.51194, its = 23
i = 4436 (of 10192), d = 8.15953, its = 3
i = 4437 (of 10192), d = 37, its = 53
i = 4438 (of 10192), d = 5.93713, its = 6
i = 4439 (of 10192), d = 9.38792, its = 5
i = 4440 (of 10192), d = 16.5476, its = 6
i = 4441 (of 10192), d = 37, its = 52
i = 4442 (of 10192), d = 1.70495, its = 18
i = 4443 (of 10192), d = 37, its = 58
i = 4444 (of 10192), d = 37, its = 52
i = 4445 (of 10192), d = 37, its = 56
i = 4446 (of 10192), d = 9.09404, its = 5
i = 4447 (of 10192), d = 37, its = 37
i = 4448 (of 10192), d = 37, its = 53
i = 4449 (of 10192), d = 7.98338, its = 3
i = 4450 (of 10192), d = 37, its = 54
i = 4451 (of 10192), d = 12.9155, its = 6
i = 4452 (of 10192), d = 3.34561, its = 18
i = 4453 (of 10192), d = 4.20566, its = 21
i = 4454 (of 10192), d = 37, its = 55
i = 4455 (of 10192), d = 37, its = 37
i = 4456 (of 10192), d = 37, its = 54
i = 4457 (of 10192), d = 8.96794, its = 4
i = 4458 (of 10192), d = 2.72354, its = 17
i = 4459 (of 10192), d = 11.8673, its = 5
i = 4460 (of 10192), d = 37, its = 53
i = 4461 (of 10192), d = 37, its = 52
i = 4462 (of 10192), d = 37, its = 51
i = 4463 (of 10192), d = 37, its = 51
i = 4464 (of 10192), d = 37, its = 51
i = 4465 (of 10192), d = 37, its = 52
i = 4466 (of 10192), d = 37, its = 42
i = 4467 (of 10192), d = 37, its = 56
i = 4468 (of 10192), d = 37, its = 56
i = 4469 (of 10192), d = 3.20229, its = 16
i = 4470 (of 10192), d = 3.04105, its = 15
i = 4471 (of 10192), d = 3.24146, its = 26
i = 4472 (of 10192), d = 37, its = 52
i = 4473 (of 10192), d = 37, its = 54
i = 4474 (of 10192), d = 37, its = 41
i = 4475 (of 10192), d = 37, its = 39
i = 4476 (of 10192), d = 37, its = 37
i = 4477 (of 10192), d = 37, its = 38
i = 4478 (of 10192), d = 22.7145, its = 8
i = 4479 (of 10192), d = 37, its = 37
i = 4480 (of 10192), d = 28.4883, its = 7
i = 4481 (of 10192), d = 37, its = 55
i = 4482 (of 10192), d = 37, its = 55
i = 4483 (of 10192), d = 37, its = 40
i = 4484 (of 10192), d = 37, its = 43
i = 4485 (of 10192), d = 37, its = 54
i = 4486 (of 10192), d = 37, its = 53
i = 4487 (of 10192), d = 37, its = 50
i = 4488 (of 10192), d = 37, its = 55
i = 4489 (of 10192), d = 37, its = 53
i = 4490 (of 10192), d = 36.2242, its = 8
i = 4491 (of 10192), d = 6.20065, its = 6
i = 4492 (of 10192), d = 7.88275, its = 3
i = 4493 (of 10192), d = 37, its = 52
i = 4494 (of 10192), d = 37, its = 53
i = 4495 (of 10192), d = 37, its = 41
i = 4496 (of 10192), d = 6.91222, its = 5
i = 4497 (of 10192), d = 7.11678, its = 5
i = 4498 (of 10192), d = 37, its = 49
i = 4499 (of 10192), d = 37, its = 54
i = 4500 (of 10192), d = 37, its = 42
i = 4501 (of 10192), d = 2.9795, its = 17
i = 4502 (of 10192), d = 37, its = 54
i = 4503 (of 10192), d = 37, its = 51
i = 4504 (of 10192), d = 37, its = 51
i = 4505 (of 10192), d = 37, its = 57
i = 4506 (of 10192), d = 37, its = 37
i = 4507 (of 10192), d = 37, its = 42
i = 4508 (of 10192), d = 2.53973, its = 16
i = 4509 (of 10192), d = 37, its = 37
i = 4510 (of 10192), d = 37, its = 40
i = 4511 (of 10192), d = 37, its = 48
i = 4512 (of 10192), d = 37, its = 55
i = 4513 (of 10192), d = 37, its = 55
i = 4514 (of 10192), d = 37, its = 58
i = 4515 (of 10192), d = 37, its = 55
i = 4516 (of 10192), d = 37, its = 48
i = 4517 (of 10192), d = 26.6212, its = 8
i = 4518 (of 10192), d = 37, its = 38
i = 4519 (of 10192), d = 37, its = 53
i = 4520 (of 10192), d = 37, its = 54
i = 4521 (of 10192), d = 37, its = 42
i = 4522 (of 10192), d = 6.58503, its = 5
i = 4523 (of 10192), d = 6.31336, its = 6
i = 4524 (of 10192), d = 37, its = 52
i = 4525 (of 10192), d = 37, its = 48
i = 4526 (of 10192), d = 37, its = 53
i = 4527 (of 10192), d = 37, its = 39
i = 4528 (of 10192), d = 5.32417, its = 5
i = 4529 (of 10192), d = 37, its = 37
i = 4530 (of 10192), d = 5.41105, its = 6
i = 4531 (of 10192), d = 37, its = 37
i = 4532 (of 10192), d = 11.8185, its = 6
i = 4533 (of 10192), d = 37, its = 57
i = 4534 (of 10192), d = 37, its = 37
i = 4535 (of 10192), d = 37, its = 41
i = 4536 (of 10192), d = 2.21109, its = 18
i = 4537 (of 10192), d = 10.9192, its = 5
i = 4538 (of 10192), d = 37, its = 54
i = 4539 (of 10192), d = 11.7181, its = 6
i = 4540 (of 10192), d = 8.66479, its = 4
i = 4541 (of 10192), d = 37, its = 40
i = 4542 (of 10192), d = 10.9352, its = 6
i = 4543 (of 10192), d = 37, its = 37
i = 4544 (of 10192), d = 37, its = 47
i = 4545 (of 10192), d = 4.53772, its = 20
i = 4546 (of 10192), d = 9.16259, its = 4
i = 4547 (of 10192), d = 37, its = 51
i = 4548 (of 10192), d = 37, its = 55
i = 4549 (of 10192), d = 7.47812, its = 4
i = 4550 (of 10192), d = 3.61997, its = 20
i = 4551 (of 10192), d = 37, its = 50
i = 4552 (of 10192), d = 37, its = 54
i = 4553 (of 10192), d = 37, its = 52
i = 4554 (of 10192), d = 10.455, its = 5
i = 4555 (of 10192), d = 37, its = 54
i = 4556 (of 10192), d = 37, its = 49
i = 4557 (of 10192), d = 37, its = 41
i = 4558 (of 10192), d = 8.73987, its = 4
i = 4559 (of 10192), d = 3.22738, its = 24
i = 4560 (of 10192), d = 37, its = 41
i = 4561 (of 10192), d = 19.4825, its = 7
i = 4562 (of 10192), d = 3.82787, its = 16
i = 4563 (of 10192), d = 37, its = 50
i = 4564 (of 10192), d = 37, its = 40
i = 4565 (of 10192), d = 14.3423, its = 6
i = 4566 (of 10192), d = 6.56557, its = 6
i = 4567 (of 10192), d = 4.3043, its = 21
i = 4568 (of 10192), d = 37, its = 56
i = 4569 (of 10192), d = 37, its = 37
i = 4570 (of 10192), d = 37, its = 55
i = 4571 (of 10192), d = 9.30146, its = 5
i = 4572 (of 10192), d = 37, its = 55
i = 4573 (of 10192), d = 37, its = 34
i = 4574 (of 10192), d = 37, its = 37
i = 4575 (of 10192), d = 37, its = 55
i = 4576 (of 10192), d = 4.55858, its = 19
i = 4577 (of 10192), d = 37, its = 54
i = 4578 (of 10192), d = 37, its = 37
i = 4579 (of 10192), d = 37, its = 47
i = 4580 (of 10192), d = 37, its = 40
i = 4581 (of 10192), d = 37, its = 51
i = 4582 (of 10192), d = 37, its = 37
i = 4583 (of 10192), d = 37, its = 43
i = 4584 (of 10192), d = 7.86447, its = 3
i = 4585 (of 10192), d = 9.06717, its = 5
i = 4586 (of 10192), d = 37, its = 47
i = 4587 (of 10192), d = 12.8459, its = 6
i = 4588 (of 10192), d = 37, its = 53
i = 4589 (of 10192), d = 3.67484, its = 20
i = 4590 (of 10192), d = 9.03703, its = 4
i = 4591 (of 10192), d = 5.15243, its = 19
i = 4592 (of 10192), d = 37, its = 40
i = 4593 (of 10192), d = 13.679, its = 6
i = 4594 (of 10192), d = 8.73987, its = 4
i = 4595 (of 10192), d = 37, its = 37
i = 4596 (of 10192), d = 7.48746, its = 4
i = 4597 (of 10192), d = 37, its = 50
i = 4598 (of 10192), d = 7.98419, its = 2
i = 4599 (of 10192), d = 2.69282, its = 17
i = 4600 (of 10192), d = 37, its = 39
i = 4601 (of 10192), d = 37, its = 50
i = 4602 (of 10192), d = 2.32836, its = 20
i = 4603 (of 10192), d = 8.0097, its = 2
i = 4604 (of 10192), d = 37, its = 40
i = 4605 (of 10192), d = 37, its = 52
i = 4606 (of 10192), d = 2.96988, its = 25
i = 4607 (of 10192), d = 5.32432, its = 21
i = 4608 (of 10192), d = 37, its = 50
i = 4609 (of 10192), d = 37, its = 54
i = 4610 (of 10192), d = 37, its = 41
i = 4611 (of 10192), d = 37, its = 55
i = 4612 (of 10192), d = 37, its = 50
i = 4613 (of 10192), d = 37, its = 56
i = 4614 (of 10192), d = 8.51251, its = 4
i = 4615 (of 10192), d = 37, its = 51
i = 4616 (of 10192), d = 37, its = 49
i = 4617 (of 10192), d = 2.8027, its = 17
i = 4618 (of 10192), d = 37, its = 50
i = 4619 (of 10192), d = 36.9794, its = 8
i = 4620 (of 10192), d = 37, its = 50
i = 4621 (of 10192), d = 37, its = 53
i = 4622 (of 10192), d = 37, its = 53
i = 4623 (of 10192), d = 8.44466, its = 4
i = 4624 (of 10192), d = 37, its = 43
i = 4625 (of 10192), d = 6.82896, its = 5
i = 4626 (of 10192), d = 7.84096, its = 3
i = 4627 (of 10192), d = 3.08055, its = 18
i = 4628 (of 10192), d = 5.37114, its = 6
i = 4629 (of 10192), d = 17.8449, its = 7
i = 4630 (of 10192), d = 1.66958, its = 19
i = 4631 (of 10192), d = 37, its = 53
i = 4632 (of 10192), d = 37, its = 37
i = 4633 (of 10192), d = 37, its = 47
i = 4634 (of 10192), d = 37, its = 53
i = 4635 (of 10192), d = 37, its = 54
i = 4636 (of 10192), d = 37, its = 39
i = 4637 (of 10192), d = 37, its = 55
i = 4638 (of 10192), d = 10.1226, its = 5
i = 4639 (of 10192), d = 4.85683, its = 7
i = 4640 (of 10192), d = 37, its = 56
i = 4641 (of 10192), d = 7.6759, its = 4
i = 4642 (of 10192), d = 37, its = 37
i = 4643 (of 10192), d = 37, its = 54
i = 4644 (of 10192), d = 37, its = 37
i = 4645 (of 10192), d = 37, its = 55
i = 4646 (of 10192), d = 7.61305, its = 4
i = 4647 (of 10192), d = 4.04039, its = 20
i = 4648 (of 10192), d = 37, its = 37
i = 4649 (of 10192), d = 37, its = 40
i = 4650 (of 10192), d = 37, its = 37
i = 4651 (of 10192), d = 18.37, its = 7
i = 4652 (of 10192), d = 37, its = 41
i = 4653 (of 10192), d = 18.2648, its = 9
i = 4654 (of 10192), d = 4.15654, its = 16
i = 4655 (of 10192), d = 37, its = 39
i = 4656 (of 10192), d = 37, its = 53
i = 4657 (of 10192), d = 37, its = 53
i = 4658 (of 10192), d = 37, its = 54
i = 4659 (of 10192), d = 37, its = 51
i = 4660 (of 10192), d = 37, its = 37
i = 4661 (of 10192), d = 37, its = 37
i = 4662 (of 10192), d = 37, its = 37
i = 4663 (of 10192), d = 37, its = 52
i = 4664 (of 10192), d = 37, its = 50
i = 4665 (of 10192), d = 13.7629, its = 7
i = 4666 (of 10192), d = 3.97546, its = 23
i = 4667 (of 10192), d = 2.84955, its = 22
i = 4668 (of 10192), d = 37, its = 54
i = 4669 (of 10192), d = 0.694657, its = 22
i = 4670 (of 10192), d = 37, its = 37
i = 4671 (of 10192), d = 37, its = 55
i = 4672 (of 10192), d = 37, its = 56
i = 4673 (of 10192), d = 37, its = 40
i = 4674 (of 10192), d = 37, its = 37
i = 4675 (of 10192), d = 4.40464, its = 21
i = 4676 (of 10192), d = 37, its = 49
i = 4677 (of 10192), d = 10.7375, its = 6
i = 4678 (of 10192), d = 37, its = 55
i = 4679 (of 10192), d = 6.02747, its = 6
i = 4680 (of 10192), d = 37, its = 52
i = 4681 (of 10192), d = 37, its = 37
i = 4682 (of 10192), d = 37, its = 55
i = 4683 (of 10192), d = 37, its = 50
i = 4684 (of 10192), d = 5.19633, its = 6
i = 4685 (of 10192), d = 7.75368, its = 3
i = 4686 (of 10192), d = 37, its = 55
i = 4687 (of 10192), d = 37, its = 37
i = 4688 (of 10192), d = 37, its = 56
i = 4689 (of 10192), d = 37, its = 50
i = 4690 (of 10192), d = 37, its = 53
i = 4691 (of 10192), d = 37, its = 52
i = 4692 (of 10192), d = 37, its = 54
i = 4693 (of 10192), d = 37, its = 58
i = 4694 (of 10192), d = 7.66315, its = 4
i = 4695 (of 10192), d = 37, its = 41
i = 4696 (of 10192), d = 37, its = 54
i = 4697 (of 10192), d = 37, its = 52
i = 4698 (of 10192), d = 37, its = 37
i = 4699 (of 10192), d = 25.9966, its = 8
i = 4700 (of 10192), d = 37, its = 54
i = 4701 (of 10192), d = 37, its = 54
i = 4702 (of 10192), d = 37, its = 50
i = 4703 (of 10192), d = 5.53567, its = 6
i = 4704 (of 10192), d = 37, its = 42
i = 4705 (of 10192), d = 37, its = 49
i = 4706 (of 10192), d = 7.16355, its = 4
i = 4707 (of 10192), d = 37, its = 53
i = 4708 (of 10192), d = 37, its = 37
i = 4709 (of 10192), d = 37, its = 54
i = 4710 (of 10192), d = 4.08394, its = 19
i = 4711 (of 10192), d = 37, its = 51
i = 4712 (of 10192), d = 37, its = 51
i = 4713 (of 10192), d = 9.50042, its = 4
i = 4714 (of 10192), d = 7.78335, its = 3
i = 4715 (of 10192), d = 37, its = 52
i = 4716 (of 10192), d = 37, its = 55
i = 4717 (of 10192), d = 3.57269, its = 21
i = 4718 (of 10192), d = 37, its = 52
i = 4719 (of 10192), d = 37, its = 37
i = 4720 (of 10192), d = 37, its = 37
i = 4721 (of 10192), d = 37, its = 53
i = 4722 (of 10192), d = 37, its = 40
i = 4723 (of 10192), d = 37, its = 38
i = 4724 (of 10192), d = 8.15176, its = 3
i = 4725 (of 10192), d = 37, its = 52
i = 4726 (of 10192), d = 37, its = 40
i = 4727 (of 10192), d = 37, its = 40
i = 4728 (of 10192), d = 37, its = 51
i = 4729 (of 10192), d = 37, its = 54
i = 4730 (of 10192), d = 2.81054, its = 21
i = 4731 (of 10192), d = 3.35891, its = 18
i = 4732 (of 10192), d = 37, its = 51
i = 4733 (of 10192), d = 2.25825, its = 18
i = 4734 (of 10192), d = 6.0918, its = 6
i = 4735 (of 10192), d = 37, its = 50
i = 4736 (of 10192), d = 37, its = 50
i = 4737 (of 10192), d = 37, its = 37
i = 4738 (of 10192), d = 37, its = 37
i = 4739 (of 10192), d = 37, its = 53
i = 4740 (of 10192), d = 7.2375, its = 6
i = 4741 (of 10192), d = 37, its = 55
i = 4742 (of 10192), d = 17.7623, its = 7
i = 4743 (of 10192), d = 3.57133, its = 20
i = 4744 (of 10192), d = 37, its = 50
i = 4745 (of 10192), d = 35.5974, its = 22
i = 4746 (of 10192), d = 37, its = 54
i = 4747 (of 10192), d = 7.40889, its = 4
i = 4748 (of 10192), d = 37, its = 50
i = 4749 (of 10192), d = 37, its = 53
i = 4750 (of 10192), d = 37, its = 49
i = 4751 (of 10192), d = 8.53869, its = 4
i = 4752 (of 10192), d = 37, its = 54
i = 4753 (of 10192), d = 4.3487, its = 21
i = 4754 (of 10192), d = 37, its = 37
i = 4755 (of 10192), d = 37, its = 37
i = 4756 (of 10192), d = 5.87253, its = 6
i = 4757 (of 10192), d = 10.8771, its = 6
i = 4758 (of 10192), d = 11.7549, its = 5
i = 4759 (of 10192), d = 5.20568, its = 6
i = 4760 (of 10192), d = 8.46603, its = 4
i = 4761 (of 10192), d = 37, its = 52
i = 4762 (of 10192), d = 37, its = 39
i = 4763 (of 10192), d = 37, its = 46
i = 4764 (of 10192), d = 5.51413, its = 6
i = 4765 (of 10192), d = 37, its = 52
i = 4766 (of 10192), d = 1.33133, its = 18
i = 4767 (of 10192), d = 37, its = 50
i = 4768 (of 10192), d = 2.54753, its = 17
i = 4769 (of 10192), d = 37, its = 56
i = 4770 (of 10192), d = 37, its = 55
i = 4771 (of 10192), d = 37, its = 55
i = 4772 (of 10192), d = 6.64287, its = 6
i = 4773 (of 10192), d = 7.78212, its = 5
i = 4774 (of 10192), d = 5.14211, its = 6
i = 4775 (of 10192), d = 37, its = 54
i = 4776 (of 10192), d = 3.7942, its = 18
i = 4777 (of 10192), d = 37, its = 39
i = 4778 (of 10192), d = 37, its = 50
i = 4779 (of 10192), d = 37, its = 40
i = 4780 (of 10192), d = 37, its = 58
i = 4781 (of 10192), d = 37, its = 37
i = 4782 (of 10192), d = 37, its = 51
i = 4783 (of 10192), d = 11.7437, its = 6
i = 4784 (of 10192), d = 37, its = 53
i = 4785 (of 10192), d = 35.7294, its = 35
i = 4786 (of 10192), d = 37, its = 37
i = 4787 (of 10192), d = 37, its = 37
i = 4788 (of 10192), d = 37, its = 45
i = 4789 (of 10192), d = 6.73564, its = 5
i = 4790 (of 10192), d = 37, its = 57
i = 4791 (of 10192), d = 3.20697, its = 16
i = 4792 (of 10192), d = 37, its = 55
i = 4793 (of 10192), d = 37, its = 54
i = 4794 (of 10192), d = 37, its = 37
i = 4795 (of 10192), d = 3.71149, its = 20
i = 4796 (of 10192), d = 3.30151, its = 24
i = 4797 (of 10192), d = 37, its = 37
i = 4798 (of 10192), d = 3.91003, its = 21
i = 4799 (of 10192), d = 5.36554, its = 8
i = 4800 (of 10192), d = 37, its = 56
i = 4801 (of 10192), d = 37, its = 58
i = 4802 (of 10192), d = 37, its = 54
i = 4803 (of 10192), d = 37, its = 50
i = 4804 (of 10192), d = 37, its = 53
i = 4805 (of 10192), d = 37, its = 37
i = 4806 (of 10192), d = 37, its = 47
i = 4807 (of 10192), d = 10.8386, its = 5
i = 4808 (of 10192), d = 37, its = 54
i = 4809 (of 10192), d = 4.04315, its = 18
i = 4810 (of 10192), d = 37, its = 54
i = 4811 (of 10192), d = 15.1924, its = 7
i = 4812 (of 10192), d = 37, its = 58
i = 4813 (of 10192), d = 37, its = 53
i = 4814 (of 10192), d = 37, its = 51
i = 4815 (of 10192), d = 37, its = 41
i = 4816 (of 10192), d = 11.6472, its = 6
i = 4817 (of 10192), d = 37, its = 53
i = 4818 (of 10192), d = 37, its = 54
i = 4819 (of 10192), d = 5.13611, its = 6
i = 4820 (of 10192), d = 8.93651, its = 4
i = 4821 (of 10192), d = 37, its = 52
i = 4822 (of 10192), d = 37, its = 55
i = 4823 (of 10192), d = 1.46269, its = 19
i = 4824 (of 10192), d = 9.90201, its = 5
i = 4825 (of 10192), d = 3.72222, its = 19
i = 4826 (of 10192), d = 37, its = 37
i = 4827 (of 10192), d = 37, its = 50
i = 4828 (of 10192), d = 37, its = 37
i = 4829 (of 10192), d = 2.77147, its = 18
i = 4830 (of 10192), d = 4.02048, its = 16
i = 4831 (of 10192), d = 0.931102, its = 18
i = 4832 (of 10192), d = 37, its = 38
i = 4833 (of 10192), d = 8.28603, its = 3
i = 4834 (of 10192), d = 37, its = 51
i = 4835 (of 10192), d = 37, its = 53
i = 4836 (of 10192), d = 5.56291, its = 6
i = 4837 (of 10192), d = 37, its = 37
i = 4838 (of 10192), d = 7.76119, its = 4
i = 4839 (of 10192), d = 2.91818, its = 18
i = 4840 (of 10192), d = 37, its = 56
i = 4841 (of 10192), d = 10.9145, its = 6
i = 4842 (of 10192), d = 2.17777, its = 20
i = 4843 (of 10192), d = 6.89137, its = 7
i = 4844 (of 10192), d = 12.6226, its = 6
i = 4845 (of 10192), d = 3.22401, its = 16
i = 4846 (of 10192), d = 37, its = 51
i = 4847 (of 10192), d = 37, its = 37
i = 4848 (of 10192), d = 37, its = 55
i = 4849 (of 10192), d = 3.21021, its = 17
i = 4850 (of 10192), d = 9.33497, its = 5
i = 4851 (of 10192), d = 37, its = 54
i = 4852 (of 10192), d = 37, its = 48
i = 4853 (of 10192), d = 6.17038, its = 6
i = 4854 (of 10192), d = 37, its = 39
i = 4855 (of 10192), d = 37, its = 38
i = 4856 (of 10192), d = 37, its = 37
i = 4857 (of 10192), d = 37, its = 37
i = 4858 (of 10192), d = 3.97089, its = 19
i = 4859 (of 10192), d = 8.52976, its = 4
i = 4860 (of 10192), d = 37, its = 55
i = 4861 (of 10192), d = 37, its = 54
i = 4862 (of 10192), d = 37, its = 37
i = 4863 (of 10192), d = 37, its = 41
i = 4864 (of 10192), d = 4.08223, its = 18
i = 4865 (of 10192), d = 37, its = 53
i = 4866 (of 10192), d = 37, its = 37
i = 4867 (of 10192), d = 11.5904, its = 5
i = 4868 (of 10192), d = 37, its = 48
i = 4869 (of 10192), d = 10.4192, its = 5
i = 4870 (of 10192), d = 37, its = 54
i = 4871 (of 10192), d = 7.74907, its = 4
i = 4872 (of 10192), d = 37, its = 50
i = 4873 (of 10192), d = 7.59663, its = 4
i = 4874 (of 10192), d = 37, its = 39
i = 4875 (of 10192), d = 37, its = 37
i = 4876 (of 10192), d = 37, its = 52
i = 4877 (of 10192), d = 37, its = 50
i = 4878 (of 10192), d = 37, its = 55
i = 4879 (of 10192), d = 37, its = 37
i = 4880 (of 10192), d = 5.69611, its = 6
i = 4881 (of 10192), d = 37, its = 37
i = 4882 (of 10192), d = 37, its = 53
i = 4883 (of 10192), d = 11.0696, its = 5
i = 4884 (of 10192), d = 3.93709, its = 19
i = 4885 (of 10192), d = 37, its = 56
i = 4886 (of 10192), d = 37, its = 53
i = 4887 (of 10192), d = 37, its = 52
i = 4888 (of 10192), d = 37, its = 54
i = 4889 (of 10192), d = 37, its = 50
i = 4890 (of 10192), d = 37, its = 55
i = 4891 (of 10192), d = 37, its = 41
i = 4892 (of 10192), d = 37, its = 37
i = 4893 (of 10192), d = 37, its = 57
i = 4894 (of 10192), d = 37, its = 56
i = 4895 (of 10192), d = 37, its = 53
i = 4896 (of 10192), d = 37, its = 55
i = 4897 (of 10192), d = 37, its = 50
i = 4898 (of 10192), d = 37, its = 52
i = 4899 (of 10192), d = 37, its = 51
i = 4900 (of 10192), d = 37, its = 53
i = 4901 (of 10192), d = 2.8027, its = 17
i = 4902 (of 10192), d = 37, its = 53
i = 4903 (of 10192), d = 3.16105, its = 19
i = 4904 (of 10192), d = 37, its = 50
i = 4905 (of 10192), d = 8.64185, its = 4
i = 4906 (of 10192), d = 7.08042, its = 5
i = 4907 (of 10192), d = 37, its = 55
i = 4908 (of 10192), d = 37, its = 56
i = 4909 (of 10192), d = 5.50292, its = 6
i = 4910 (of 10192), d = 37, its = 50
i = 4911 (of 10192), d = 37, its = 52
i = 4912 (of 10192), d = 37, its = 53
i = 4913 (of 10192), d = 37, its = 54
i = 4914 (of 10192), d = 37, its = 40
i = 4915 (of 10192), d = 3.70466, its = 23
i = 4916 (of 10192), d = 23.7528, its = 7
i = 4917 (of 10192), d = 37, its = 53
i = 4918 (of 10192), d = 7.87842, its = 3
i = 4919 (of 10192), d = 12.4105, its = 6
i = 4920 (of 10192), d = 10.4576, its = 5
i = 4921 (of 10192), d = 37, its = 50
i = 4922 (of 10192), d = 1.32046, its = 28
i = 4923 (of 10192), d = 37, its = 51
i = 4924 (of 10192), d = 6.97567, its = 5
i = 4925 (of 10192), d = 2.11628, its = 18
i = 4926 (of 10192), d = 37, its = 50
i = 4927 (of 10192), d = 12.0941, its = 7
i = 4928 (of 10192), d = 37, its = 52
i = 4929 (of 10192), d = 37, its = 55
i = 4930 (of 10192), d = 37, its = 41
i = 4931 (of 10192), d = 37, its = 39
i = 4932 (of 10192), d = 37, its = 40
i = 4933 (of 10192), d = 37, its = 50
i = 4934 (of 10192), d = 37, its = 37
i = 4935 (of 10192), d = 37, its = 47
i = 4936 (of 10192), d = 37, its = 50
i = 4937 (of 10192), d = 37, its = 37
i = 4938 (of 10192), d = 37, its = 37
i = 4939 (of 10192), d = 5.38254, its = 20
i = 4940 (of 10192), d = 37, its = 48
i = 4941 (of 10192), d = 37, its = 52
i = 4942 (of 10192), d = 37, its = 47
i = 4943 (of 10192), d = 37, its = 53
i = 4944 (of 10192), d = 37, its = 54
i = 4945 (of 10192), d = 37, its = 52
i = 4946 (of 10192), d = 37, its = 37
i = 4947 (of 10192), d = 37, its = 38
i = 4948 (of 10192), d = 37, its = 55
i = 4949 (of 10192), d = 2.89481, its = 20
i = 4950 (of 10192), d = 37, its = 55
i = 4951 (of 10192), d = 3.12997, its = 19
i = 4952 (of 10192), d = 37, its = 37
i = 4953 (of 10192), d = 10.4501, its = 5
i = 4954 (of 10192), d = 37, its = 49
i = 4955 (of 10192), d = 8.51698, its = 4
i = 4956 (of 10192), d = 5.99025, its = 7
i = 4957 (of 10192), d = 37, its = 49
i = 4958 (of 10192), d = 37, its = 40
i = 4959 (of 10192), d = 37, its = 37
i = 4960 (of 10192), d = 9.41542, its = 5
i = 4961 (of 10192), d = 5.2707, its = 7
i = 4962 (of 10192), d = 37, its = 37
i = 4963 (of 10192), d = 37, its = 37
i = 4964 (of 10192), d = 37, its = 41
i = 4965 (of 10192), d = 37, its = 37
i = 4966 (of 10192), d = 37, its = 40
i = 4967 (of 10192), d = 7.22404, its = 4
i = 4968 (of 10192), d = 2.26797, its = 27
i = 4969 (of 10192), d = 11.6039, its = 6
i = 4970 (of 10192), d = 6.88323, its = 5
i = 4971 (of 10192), d = 7.78096, its = 3
i = 4972 (of 10192), d = 37, its = 52
i = 4973 (of 10192), d = 37, its = 39
i = 4974 (of 10192), d = 37, its = 37
i = 4975 (of 10192), d = 37, its = 52
i = 4976 (of 10192), d = 37, its = 37
i = 4977 (of 10192), d = 37, its = 40
i = 4978 (of 10192), d = 10.269, its = 6
i = 4979 (of 10192), d = 37, its = 56
i = 4980 (of 10192), d = 37, its = 55
i = 4981 (of 10192), d = 7.38261, its = 4
i = 4982 (of 10192), d = 37, its = 50
i = 4983 (of 10192), d = 37, its = 53
i = 4984 (of 10192), d = 20.4795, its = 7
i = 4985 (of 10192), d = 37, its = 37
i = 4986 (of 10192), d = 37, its = 37
i = 4987 (of 10192), d = 37, its = 52
i = 4988 (of 10192), d = 12.339, its = 6
i = 4989 (of 10192), d = 37, its = 47
i = 4990 (of 10192), d = 37, its = 55
i = 4991 (of 10192), d = 37, its = 40
i = 4992 (of 10192), d = 37, its = 51
i = 4993 (of 10192), d = 1.6048, its = 20
i = 4994 (of 10192), d = 37, its = 40
i = 4995 (of 10192), d = 37, its = 56
i = 4996 (of 10192), d = 9.57431, its = 5
i = 4997 (of 10192), d = 8.38185, its = 5
i = 4998 (of 10192), d = 37, its = 56
i = 4999 (of 10192), d = 37, its = 49
i = 5000 (of 10192), d = 37, its = 49
i = 5001 (of 10192), d = 37, its = 52
i = 5002 (of 10192), d = 37, its = 51
i = 5003 (of 10192), d = 7.48267, its = 4
i = 5004 (of 10192), d = 37, its = 51
i = 5005 (of 10192), d = 37, its = 51
i = 5006 (of 10192), d = 5.78138, its = 7
i = 5007 (of 10192), d = 37, its = 55
i = 5008 (of 10192), d = 37, its = 39
i = 5009 (of 10192), d = 6.31086, its = 6
i = 5010 (of 10192), d = 5.28294, its = 7
i = 5011 (of 10192), d = 37, its = 41
i = 5012 (of 10192), d = 37, its = 50
i = 5013 (of 10192), d = 37, its = 53
i = 5014 (of 10192), d = 37, its = 55
i = 5015 (of 10192), d = 37, its = 55
i = 5016 (of 10192), d = 37, its = 37
i = 5017 (of 10192), d = 8.57967, its = 4
i = 5018 (of 10192), d = 37, its = 51
i = 5019 (of 10192), d = 37, its = 43
i = 5020 (of 10192), d = 3.54861, its = 17
i = 5021 (of 10192), d = 37, its = 37
i = 5022 (of 10192), d = 9.9704, its = 5
i = 5023 (of 10192), d = 37, its = 37
i = 5024 (of 10192), d = 37, its = 53
i = 5025 (of 10192), d = 37, its = 53
i = 5026 (of 10192), d = 37, its = 48
i = 5027 (of 10192), d = 37, its = 53
i = 5028 (of 10192), d = 2.40372, its = 27
i = 5029 (of 10192), d = 37, its = 51
i = 5030 (of 10192), d = 1.27603, its = 18
i = 5031 (of 10192), d = 2.36061, its = 19
i = 5032 (of 10192), d = 7.41192, its = 4
i = 5033 (of 10192), d = 37, its = 55
i = 5034 (of 10192), d = 37, its = 51
i = 5035 (of 10192), d = 37, its = 53
i = 5036 (of 10192), d = 10.6036, its = 5
i = 5037 (of 10192), d = 37, its = 40
i = 5038 (of 10192), d = 37, its = 51
i = 5039 (of 10192), d = 37, its = 37
i = 5040 (of 10192), d = 6.86712, its = 5
i = 5041 (of 10192), d = 37, its = 51
i = 5042 (of 10192), d = 37, its = 55
i = 5043 (of 10192), d = 37, its = 57
i = 5044 (of 10192), d = 37, its = 53
i = 5045 (of 10192), d = 37, its = 51
i = 5046 (of 10192), d = 37, its = 55
i = 5047 (of 10192), d = 2.07901, its = 17
i = 5048 (of 10192), d = 2.24221, its = 19
i = 5049 (of 10192), d = 3.93267, its = 20
i = 5050 (of 10192), d = 37, its = 38
i = 5051 (of 10192), d = 4.68139, its = 18
i = 5052 (of 10192), d = 2.42776, its = 17
i = 5053 (of 10192), d = 37, its = 53
i = 5054 (of 10192), d = 37, its = 50
i = 5055 (of 10192), d = 37, its = 39
i = 5056 (of 10192), d = 7.82923, its = 3
i = 5057 (of 10192), d = 37, its = 52
i = 5058 (of 10192), d = 37, its = 41
i = 5059 (of 10192), d = 7.97466, its = 2
i = 5060 (of 10192), d = 37, its = 52
i = 5061 (of 10192), d = 37, its = 37
i = 5062 (of 10192), d = 37, its = 48
i = 5063 (of 10192), d = 37, its = 54
i = 5064 (of 10192), d = 37, its = 37
i = 5065 (of 10192), d = 37, its = 37
i = 5066 (of 10192), d = 9.18517, its = 4
i = 5067 (of 10192), d = 37, its = 49
i = 5068 (of 10192), d = 37, its = 51
i = 5069 (of 10192), d = 37, its = 52
i = 5070 (of 10192), d = 37, its = 53
i = 5071 (of 10192), d = 37, its = 47
i = 5072 (of 10192), d = 37, its = 55
i = 5073 (of 10192), d = 37, its = 53
i = 5074 (of 10192), d = 4.02169, its = 19
i = 5075 (of 10192), d = 37, its = 37
i = 5076 (of 10192), d = 2.59345, its = 23
i = 5077 (of 10192), d = 9.03702, its = 5
i = 5078 (of 10192), d = 7.61381, its = 4
i = 5079 (of 10192), d = 37, its = 54
i = 5080 (of 10192), d = 6.61738, its = 5
i = 5081 (of 10192), d = 37, its = 53
i = 5082 (of 10192), d = 37, its = 51
i = 5083 (of 10192), d = 37, its = 49
i = 5084 (of 10192), d = 20.8893, its = 7
i = 5085 (of 10192), d = 7.8855, its = 3
i = 5086 (of 10192), d = 37, its = 58
i = 5087 (of 10192), d = 37, its = 54
i = 5088 (of 10192), d = 37, its = 55
i = 5089 (of 10192), d = 37, its = 53
i = 5090 (of 10192), d = 37, its = 56
i = 5091 (of 10192), d = 37, its = 49
i = 5092 (of 10192), d = 37, its = 55
i = 5093 (of 10192), d = 4.18121, its = 23
i = 5094 (of 10192), d = 8.81914, its = 4
i = 5095 (of 10192), d = 6.96828, its = 5
i = 5096 (of 10192), d = 37, its = 52
i = 5097 (of 10192), d = 37, its = 56
i = 5098 (of 10192), d = 37, its = 45
i = 5099 (of 10192), d = 37, its = 49
i = 5100 (of 10192), d = 37, its = 52
i = 5101 (of 10192), d = 37, its = 53
i = 5102 (of 10192), d = 37, its = 56
i = 5103 (of 10192), d = 37, its = 54
i = 5104 (of 10192), d = 37, its = 37
i = 5105 (of 10192), d = 37, its = 40
i = 5106 (of 10192), d = 9.09792, its = 5
i = 5107 (of 10192), d = 37, its = 53
i = 5108 (of 10192), d = 37, its = 58
i = 5109 (of 10192), d = 8.61329, its = 4
i = 5110 (of 10192), d = 2.09577, its = 17
i = 5111 (of 10192), d = 37, its = 50
i = 5112 (of 10192), d = 6.99763, its = 6
i = 5113 (of 10192), d = 37, its = 37
i = 5114 (of 10192), d = 37, its = 37
i = 5115 (of 10192), d = 3.34182, its = 19
i = 5116 (of 10192), d = 6.16027, its = 6
i = 5117 (of 10192), d = 37, its = 51
i = 5118 (of 10192), d = 6.96964, its = 5
i = 5119 (of 10192), d = 37, its = 51
i = 5120 (of 10192), d = 37, its = 37
i = 5121 (of 10192), d = 37, its = 50
i = 5122 (of 10192), d = 8.1333, its = 3
i = 5123 (of 10192), d = 37, its = 37
i = 5124 (of 10192), d = 37, its = 50
i = 5125 (of 10192), d = 2.13752, its = 19
i = 5126 (of 10192), d = 3.66372, its = 18
i = 5127 (of 10192), d = 37, its = 37
i = 5128 (of 10192), d = 37, its = 54
i = 5129 (of 10192), d = 5.17821, its = 7
i = 5130 (of 10192), d = 37, its = 54
i = 5131 (of 10192), d = 2.82666, its = 19
i = 5132 (of 10192), d = 37, its = 51
i = 5133 (of 10192), d = 37, its = 42
i = 5134 (of 10192), d = 37, its = 52
i = 5135 (of 10192), d = 37, its = 55
i = 5136 (of 10192), d = 37, its = 49
i = 5137 (of 10192), d = 37, its = 53
i = 5138 (of 10192), d = 37, its = 45
i = 5139 (of 10192), d = 37, its = 54
i = 5140 (of 10192), d = 37, its = 53
i = 5141 (of 10192), d = 37, its = 54
i = 5142 (of 10192), d = 6.27116, its = 6
i = 5143 (of 10192), d = 37, its = 50
i = 5144 (of 10192), d = 4.1213, its = 16
i = 5145 (of 10192), d = 4.21157, its = 22
i = 5146 (of 10192), d = 10.6934, its = 5
i = 5147 (of 10192), d = 37, its = 37
i = 5148 (of 10192), d = 3.63489, its = 21
i = 5149 (of 10192), d = 3.45519, its = 19
i = 5150 (of 10192), d = 37, its = 38
i = 5151 (of 10192), d = 1.8848, its = 19
i = 5152 (of 10192), d = 37, its = 53
i = 5153 (of 10192), d = 9.81176, its = 5
i = 5154 (of 10192), d = 37, its = 38
i = 5155 (of 10192), d = 37, its = 40
i = 5156 (of 10192), d = 37, its = 48
i = 5157 (of 10192), d = 37, its = 54
i = 5158 (of 10192), d = 37, its = 54
i = 5159 (of 10192), d = 37, its = 53
i = 5160 (of 10192), d = 5.4228, its = 7
i = 5161 (of 10192), d = 37, its = 41
i = 5162 (of 10192), d = 37, its = 37
i = 5163 (of 10192), d = 37, its = 57
i = 5164 (of 10192), d = 3.78156, its = 18
i = 5165 (of 10192), d = 37, its = 55
i = 5166 (of 10192), d = 2.65155, its = 27
i = 5167 (of 10192), d = 8.03196, its = 3
i = 5168 (of 10192), d = 2.85435, its = 23
i = 5169 (of 10192), d = 2.51247, its = 23
i = 5170 (of 10192), d = 37, its = 49
i = 5171 (of 10192), d = 6.03976, its = 6
i = 5172 (of 10192), d = 37, its = 50
i = 5173 (of 10192), d = 37, its = 53
i = 5174 (of 10192), d = 37, its = 49
i = 5175 (of 10192), d = 37, its = 56
i = 5176 (of 10192), d = 37, its = 53
i = 5177 (of 10192), d = 15.0947, its = 7
i = 5178 (of 10192), d = 37, its = 53
i = 5179 (of 10192), d = 37, its = 40
i = 5180 (of 10192), d = 37, its = 48
i = 5181 (of 10192), d = 2.84088, its = 19
i = 5182 (of 10192), d = 37, its = 52
i = 5183 (of 10192), d = 8.07344, its = 3
i = 5184 (of 10192), d = 37, its = 37
i = 5185 (of 10192), d = 37, its = 39
i = 5186 (of 10192), d = 16.2034, its = 7
i = 5187 (of 10192), d = 37, its = 53
i = 5188 (of 10192), d = 37, its = 50
i = 5189 (of 10192), d = 37, its = 56
i = 5190 (of 10192), d = 9.65958, its = 6
i = 5191 (of 10192), d = 37, its = 37
i = 5192 (of 10192), d = 37, its = 37
i = 5193 (of 10192), d = 5.5168, its = 7
i = 5194 (of 10192), d = 37, its = 54
i = 5195 (of 10192), d = 37, its = 57
i = 5196 (of 10192), d = 37, its = 49
i = 5197 (of 10192), d = 37, its = 53
i = 5198 (of 10192), d = 16.9689, its = 7
i = 5199 (of 10192), d = 15.3647, its = 9
i = 5200 (of 10192), d = 37, its = 53
i = 5201 (of 10192), d = 4.71043, its = 19
i = 5202 (of 10192), d = 37, its = 56
i = 5203 (of 10192), d = 3.28319, its = 18
i = 5204 (of 10192), d = 37, its = 56
i = 5205 (of 10192), d = 37, its = 53
i = 5206 (of 10192), d = 37, its = 57
i = 5207 (of 10192), d = 37, its = 50
i = 5208 (of 10192), d = 37, its = 40
i = 5209 (of 10192), d = 7.98901, its = 2
i = 5210 (of 10192), d = 37, its = 51
i = 5211 (of 10192), d = 2.24169, its = 19
i = 5212 (of 10192), d = 37, its = 49
i = 5213 (of 10192), d = 37, its = 53
i = 5214 (of 10192), d = 37, its = 54
i = 5215 (of 10192), d = 37, its = 37
i = 5216 (of 10192), d = 11.6142, its = 5
i = 5217 (of 10192), d = 37, its = 37
i = 5218 (of 10192), d = 37, its = 53
i = 5219 (of 10192), d = 37, its = 60
i = 5220 (of 10192), d = 4.57741, its = 15
i = 5221 (of 10192), d = 37, its = 52
i = 5222 (of 10192), d = 37, its = 52
i = 5223 (of 10192), d = 37, its = 53
i = 5224 (of 10192), d = 9.04522, its = 5
i = 5225 (of 10192), d = 37, its = 53
i = 5226 (of 10192), d = 37, its = 54
i = 5227 (of 10192), d = 10.2926, its = 5
i = 5228 (of 10192), d = 37, its = 37
i = 5229 (of 10192), d = 37, its = 37
i = 5230 (of 10192), d = 37, its = 54
i = 5231 (of 10192), d = 37, its = 54
i = 5232 (of 10192), d = 8.313, its = 3
i = 5233 (of 10192), d = 18.7732, its = 9
i = 5234 (of 10192), d = 37, its = 49
i = 5235 (of 10192), d = 37, its = 51
i = 5236 (of 10192), d = 37, its = 54
i = 5237 (of 10192), d = 4.01559, its = 19
i = 5238 (of 10192), d = 37, its = 19
i = 5239 (of 10192), d = 37, its = 55
i = 5240 (of 10192), d = 6.98094, its = 5
i = 5241 (of 10192), d = 6.95341, its = 5
i = 5242 (of 10192), d = 37, its = 37
i = 5243 (of 10192), d = 9.9667, its = 6
i = 5244 (of 10192), d = 37, its = 53
i = 5245 (of 10192), d = 37, its = 39
i = 5246 (of 10192), d = 37, its = 51
i = 5247 (of 10192), d = 4.59683, its = 18
i = 5248 (of 10192), d = 37, its = 56
i = 5249 (of 10192), d = 3.54291, its = 21
i = 5250 (of 10192), d = 37, its = 52
i = 5251 (of 10192), d = 37, its = 58
i = 5252 (of 10192), d = 2.38521, its = 18
i = 5253 (of 10192), d = 37, its = 37
i = 5254 (of 10192), d = 37, its = 58
i = 5255 (of 10192), d = 17.7614, its = 7
i = 5256 (of 10192), d = 8.04484, its = 3
i = 5257 (of 10192), d = 37, its = 50
i = 5258 (of 10192), d = 37, its = 42
i = 5259 (of 10192), d = 5.28656, its = 6
i = 5260 (of 10192), d = 37, its = 56
i = 5261 (of 10192), d = 37, its = 53
i = 5262 (of 10192), d = 6.86712, its = 5
i = 5263 (of 10192), d = 37, its = 37
i = 5264 (of 10192), d = 37, its = 51
i = 5265 (of 10192), d = 37, its = 52
i = 5266 (of 10192), d = 37, its = 50
i = 5267 (of 10192), d = 2.10128, its = 17
i = 5268 (of 10192), d = 37, its = 48
i = 5269 (of 10192), d = 11.7355, its = 5
i = 5270 (of 10192), d = 4.93943, its = 21
i = 5271 (of 10192), d = 3.58821, its = 19
i = 5272 (of 10192), d = 4.87804, its = 6
i = 5273 (of 10192), d = 1.85705, its = 17
i = 5274 (of 10192), d = 37, its = 49
i = 5275 (of 10192), d = 37, its = 50
i = 5276 (of 10192), d = 37, its = 37
i = 5277 (of 10192), d = 37, its = 37
i = 5278 (of 10192), d = 37, its = 52
i = 5279 (of 10192), d = 37, its = 52
i = 5280 (of 10192), d = 37, its = 52
i = 5281 (of 10192), d = 37, its = 52
i = 5282 (of 10192), d = 37, its = 37
i = 5283 (of 10192), d = 16.7951, its = 7
i = 5284 (of 10192), d = 8.52215, its = 4
i = 5285 (of 10192), d = 37, its = 37
i = 5286 (of 10192), d = 37, its = 52
i = 5287 (of 10192), d = 37, its = 56
i = 5288 (of 10192), d = 37, its = 50
i = 5289 (of 10192), d = 10.3783, its = 5
i = 5290 (of 10192), d = 30.2748, its = 19
i = 5291 (of 10192), d = 37, its = 51
i = 5292 (of 10192), d = 18.0473, its = 8
i = 5293 (of 10192), d = 37, its = 56
i = 5294 (of 10192), d = 4.46865, its = 20
i = 5295 (of 10192), d = 37, its = 42
i = 5296 (of 10192), d = 16.6908, its = 7
i = 5297 (of 10192), d = 37, its = 52
i = 5298 (of 10192), d = 37, its = 55
i = 5299 (of 10192), d = 37, its = 41
i = 5300 (of 10192), d = 37, its = 41
i = 5301 (of 10192), d = 15.8389, its = 8
i = 5302 (of 10192), d = 37, its = 54
i = 5303 (of 10192), d = 37, its = 48
i = 5304 (of 10192), d = 6.55706, its = 5
i = 5305 (of 10192), d = 37, its = 50
i = 5306 (of 10192), d = 4.15042, its = 21
i = 5307 (of 10192), d = 37, its = 37
i = 5308 (of 10192), d = 7.71651, its = 3
i = 5309 (of 10192), d = 16.9278, its = 7
i = 5310 (of 10192), d = 37, its = 52
i = 5311 (of 10192), d = 7.09814, its = 5
i = 5312 (of 10192), d = 37, its = 53
i = 5313 (of 10192), d = 37, its = 49
i = 5314 (of 10192), d = 37, its = 54
i = 5315 (of 10192), d = 37, its = 52
i = 5316 (of 10192), d = 37, its = 52
i = 5317 (of 10192), d = 7.35762, its = 5
i = 5318 (of 10192), d = 37, its = 52
i = 5319 (of 10192), d = 37, its = 50
i = 5320 (of 10192), d = 10.1012, its = 6
i = 5321 (of 10192), d = 37, its = 52
i = 5322 (of 10192), d = 37, its = 53
i = 5323 (of 10192), d = 37, its = 48
i = 5324 (of 10192), d = 7.49918, its = 4
i = 5325 (of 10192), d = 37, its = 51
i = 5326 (of 10192), d = 37, its = 37
i = 5327 (of 10192), d = 1.54492, its = 18
i = 5328 (of 10192), d = 37, its = 53
i = 5329 (of 10192), d = 37, its = 55
i = 5330 (of 10192), d = 1.68462, its = 20
i = 5331 (of 10192), d = 37, its = 41
i = 5332 (of 10192), d = 2.84362, its = 25
i = 5333 (of 10192), d = 37, its = 55
i = 5334 (of 10192), d = 37, its = 54
i = 5335 (of 10192), d = 37, its = 48
i = 5336 (of 10192), d = 2.65403, its = 21
i = 5337 (of 10192), d = 5.69611, its = 6
i = 5338 (of 10192), d = 37, its = 51
i = 5339 (of 10192), d = 37, its = 37
i = 5340 (of 10192), d = 37, its = 49
i = 5341 (of 10192), d = 4.07846, its = 17
i = 5342 (of 10192), d = 37, its = 57
i = 5343 (of 10192), d = 8.99742, its = 4
i = 5344 (of 10192), d = 22.4633, its = 7
i = 5345 (of 10192), d = 37, its = 52
i = 5346 (of 10192), d = 37, its = 40
i = 5347 (of 10192), d = 37, its = 37
i = 5348 (of 10192), d = 37, its = 37
i = 5349 (of 10192), d = 37, its = 53
i = 5350 (of 10192), d = 37, its = 54
i = 5351 (of 10192), d = 2.53303, its = 19
i = 5352 (of 10192), d = 37, its = 52
i = 5353 (of 10192), d = 3.04064, its = 18
i = 5354 (of 10192), d = 7.58036, its = 4
i = 5355 (of 10192), d = 37, its = 51
i = 5356 (of 10192), d = 37, its = 50
i = 5357 (of 10192), d = 27.5369, its = 8
i = 5358 (of 10192), d = 37, its = 49
i = 5359 (of 10192), d = 2.80128, its = 19
i = 5360 (of 10192), d = 26.5006, its = 10
i = 5361 (of 10192), d = 6.8011, its = 5
i = 5362 (of 10192), d = 37, its = 53
i = 5363 (of 10192), d = 37, its = 42
i = 5364 (of 10192), d = 37, its = 50
i = 5365 (of 10192), d = 37, its = 53
i = 5366 (of 10192), d = 3.09638, its = 17
i = 5367 (of 10192), d = 37, its = 39
i = 5368 (of 10192), d = 37, its = 37
i = 5369 (of 10192), d = 37, its = 54
i = 5370 (of 10192), d = 37, its = 53
i = 5371 (of 10192), d = 37, its = 52
i = 5372 (of 10192), d = 37, its = 51
i = 5373 (of 10192), d = 37, its = 37
i = 5374 (of 10192), d = 37, its = 51
i = 5375 (of 10192), d = 37, its = 53
i = 5376 (of 10192), d = 37, its = 52
i = 5377 (of 10192), d = 37, its = 53
i = 5378 (of 10192), d = 37, its = 53
i = 5379 (of 10192), d = 37, its = 37
i = 5380 (of 10192), d = 3.83107, its = 19
i = 5381 (of 10192), d = 37, its = 52
i = 5382 (of 10192), d = 37, its = 58
i = 5383 (of 10192), d = 3.67406, its = 19
i = 5384 (of 10192), d = 37, its = 50
i = 5385 (of 10192), d = 37, its = 49
i = 5386 (of 10192), d = 37, its = 55
i = 5387 (of 10192), d = 3.11236, its = 17
i = 5388 (of 10192), d = 37, its = 52
i = 5389 (of 10192), d = 37, its = 53
i = 5390 (of 10192), d = 3.52131, its = 27
i = 5391 (of 10192), d = 8.8697, its = 4
i = 5392 (of 10192), d = 3.75523, its = 16
i = 5393 (of 10192), d = 5.85549, its = 22
i = 5394 (of 10192), d = 37, its = 51
i = 5395 (of 10192), d = 37, its = 52
i = 5396 (of 10192), d = 37, its = 51
i = 5397 (of 10192), d = 37, its = 53
i = 5398 (of 10192), d = 37, its = 56
i = 5399 (of 10192), d = 37, its = 40
i = 5400 (of 10192), d = 37, its = 51
i = 5401 (of 10192), d = 37, its = 57
i = 5402 (of 10192), d = 37, its = 49
i = 5403 (of 10192), d = 37, its = 37
i = 5404 (of 10192), d = 37, its = 37
i = 5405 (of 10192), d = 37, its = 37
i = 5406 (of 10192), d = 3.94969, its = 21
i = 5407 (of 10192), d = 8.25534, its = 3
i = 5408 (of 10192), d = 4.5862, its = 19
i = 5409 (of 10192), d = 37, its = 37
i = 5410 (of 10192), d = 37, its = 37
i = 5411 (of 10192), d = 37, its = 37
i = 5412 (of 10192), d = 37, its = 37
i = 5413 (of 10192), d = 37, its = 57
i = 5414 (of 10192), d = 10.2293, its = 5
i = 5415 (of 10192), d = 6.17078, its = 6
i = 5416 (of 10192), d = 37, its = 54
i = 5417 (of 10192), d = 37, its = 49
i = 5418 (of 10192), d = 37, its = 37
i = 5419 (of 10192), d = 37, its = 43
i = 5420 (of 10192), d = 37, its = 59
i = 5421 (of 10192), d = 2.19423, its = 20
i = 5422 (of 10192), d = 37, its = 48
i = 5423 (of 10192), d = 37, its = 42
i = 5424 (of 10192), d = 3.19596, its = 17
i = 5425 (of 10192), d = 10.3395, its = 5
i = 5426 (of 10192), d = 6.19706, its = 6
i = 5427 (of 10192), d = 11.664, its = 5
i = 5428 (of 10192), d = 3.99984, its = 21
i = 5429 (of 10192), d = 37, its = 54
i = 5430 (of 10192), d = 13.5231, its = 6
i = 5431 (of 10192), d = 37, its = 52
i = 5432 (of 10192), d = 2.297, its = 18
i = 5433 (of 10192), d = 37, its = 37
i = 5434 (of 10192), d = 37, its = 53
i = 5435 (of 10192), d = 2.22047, its = 17
i = 5436 (of 10192), d = 37, its = 37
i = 5437 (of 10192), d = 37, its = 52
i = 5438 (of 10192), d = 1.91438, its = 21
i = 5439 (of 10192), d = 7.50123, its = 4
i = 5440 (of 10192), d = 1.76334, its = 17
i = 5441 (of 10192), d = 3.43162, its = 19
i = 5442 (of 10192), d = 37, its = 54
i = 5443 (of 10192), d = 13.0343, its = 8
i = 5444 (of 10192), d = 37, its = 39
i = 5445 (of 10192), d = 6.86416, its = 5
i = 5446 (of 10192), d = 5.54299, its = 6
i = 5447 (of 10192), d = 10.1327, its = 5
i = 5448 (of 10192), d = 37, its = 52
i = 5449 (of 10192), d = 12.5449, its = 6
i = 5450 (of 10192), d = 37, its = 54
i = 5451 (of 10192), d = 37, its = 57
i = 5452 (of 10192), d = 37, its = 49
i = 5453 (of 10192), d = 2.25743, its = 17
i = 5454 (of 10192), d = 9.30242, its = 4
i = 5455 (of 10192), d = 37, its = 37
i = 5456 (of 10192), d = 3.34885, its = 21
i = 5457 (of 10192), d = 37, its = 37
i = 5458 (of 10192), d = 3.11612, its = 19
i = 5459 (of 10192), d = 37, its = 55
i = 5460 (of 10192), d = 37, its = 40
i = 5461 (of 10192), d = 37, its = 50
i = 5462 (of 10192), d = 2.83629, its = 19
i = 5463 (of 10192), d = 37, its = 51
i = 5464 (of 10192), d = 9.49586, its = 5
i = 5465 (of 10192), d = 37, its = 51
i = 5466 (of 10192), d = 9.90692, its = 5
i = 5467 (of 10192), d = 3.36886, its = 18
i = 5468 (of 10192), d = 37, its = 37
i = 5469 (of 10192), d = 8.73987, its = 4
i = 5470 (of 10192), d = 37, its = 50
i = 5471 (of 10192), d = 37, its = 50
i = 5472 (of 10192), d = 37, its = 37
i = 5473 (of 10192), d = 2.50917, its = 22
i = 5474 (of 10192), d = 8.30706, its = 4
i = 5475 (of 10192), d = 37, its = 51
i = 5476 (of 10192), d = 3.24765, its = 21
i = 5477 (of 10192), d = 37, its = 39
i = 5478 (of 10192), d = 4.9766, its = 6
i = 5479 (of 10192), d = 5.88272, its = 7
i = 5480 (of 10192), d = 37, its = 57
i = 5481 (of 10192), d = 4.73489, its = 7
i = 5482 (of 10192), d = 6.74152, its = 7
i = 5483 (of 10192), d = 8.65287, its = 4
i = 5484 (of 10192), d = 37, its = 51
i = 5485 (of 10192), d = 37, its = 50
i = 5486 (of 10192), d = 37, its = 51
i = 5487 (of 10192), d = 5.49295, its = 21
i = 5488 (of 10192), d = 37, its = 43
i = 5489 (of 10192), d = 10.5827, its = 5
i = 5490 (of 10192), d = 6.39643, its = 6
i = 5491 (of 10192), d = 37, its = 43
i = 5492 (of 10192), d = 37, its = 52
i = 5493 (of 10192), d = 37, its = 52
i = 5494 (of 10192), d = 37, its = 37
i = 5495 (of 10192), d = 37, its = 40
i = 5496 (of 10192), d = 37, its = 54
i = 5497 (of 10192), d = 37, its = 53
i = 5498 (of 10192), d = 37, its = 58
i = 5499 (of 10192), d = 37, its = 37
i = 5500 (of 10192), d = 2.35987, its = 19
i = 5501 (of 10192), d = 37, its = 52
i = 5502 (of 10192), d = 37, its = 37
i = 5503 (of 10192), d = 37, its = 37
i = 5504 (of 10192), d = 37, its = 37
i = 5505 (of 10192), d = 37, its = 54
i = 5506 (of 10192), d = 37, its = 50
i = 5507 (of 10192), d = 37, its = 48
i = 5508 (of 10192), d = 37, its = 52
i = 5509 (of 10192), d = 37, its = 43
i = 5510 (of 10192), d = 37, its = 54
i = 5511 (of 10192), d = 37, its = 47
i = 5512 (of 10192), d = 37, its = 51
i = 5513 (of 10192), d = 37, its = 37
i = 5514 (of 10192), d = 37, its = 52
i = 5515 (of 10192), d = 37, its = 53
i = 5516 (of 10192), d = 37, its = 47
i = 5517 (of 10192), d = 11.4144, its = 6
i = 5518 (of 10192), d = 37, its = 43
i = 5519 (of 10192), d = 37, its = 48
i = 5520 (of 10192), d = 11.317, its = 6
i = 5521 (of 10192), d = 37, its = 39
i = 5522 (of 10192), d = 37, its = 37
i = 5523 (of 10192), d = 17.3943, its = 7
i = 5524 (of 10192), d = 6.70815, its = 5
i = 5525 (of 10192), d = 6.6278, its = 7
i = 5526 (of 10192), d = 37, its = 53
i = 5527 (of 10192), d = 8.12692, its = 3
i = 5528 (of 10192), d = 37, its = 50
i = 5529 (of 10192), d = 37, its = 55
i = 5530 (of 10192), d = 37, its = 57
i = 5531 (of 10192), d = 37, its = 37
i = 5532 (of 10192), d = 37, its = 53
i = 5533 (of 10192), d = 2.05495, its = 17
i = 5534 (of 10192), d = 37, its = 53
i = 5535 (of 10192), d = 37, its = 54
i = 5536 (of 10192), d = 37, its = 37
i = 5537 (of 10192), d = 37, its = 58
i = 5538 (of 10192), d = 37, its = 53
i = 5539 (of 10192), d = 37, its = 39
i = 5540 (of 10192), d = 37, its = 49
i = 5541 (of 10192), d = 2.64695, its = 19
i = 5542 (of 10192), d = 37, its = 48
i = 5543 (of 10192), d = 37, its = 58
i = 5544 (of 10192), d = 37, its = 37
i = 5545 (of 10192), d = 37, its = 41
i = 5546 (of 10192), d = 37, its = 52
i = 5547 (of 10192), d = 2.60138, its = 17
i = 5548 (of 10192), d = 37, its = 55
i = 5549 (of 10192), d = 37, its = 40
i = 5550 (of 10192), d = 37, its = 48
i = 5551 (of 10192), d = 37, its = 55
i = 5552 (of 10192), d = 37, its = 41
i = 5553 (of 10192), d = 7.36336, its = 7
i = 5554 (of 10192), d = 5.79864, its = 6
i = 5555 (of 10192), d = 3.27054, its = 19
i = 5556 (of 10192), d = 37, its = 51
i = 5557 (of 10192), d = 7.52431, its = 4
i = 5558 (of 10192), d = 37, its = 53
i = 5559 (of 10192), d = 37, its = 54
i = 5560 (of 10192), d = 37, its = 57
i = 5561 (of 10192), d = 37, its = 54
i = 5562 (of 10192), d = 9.47884, its = 5
i = 5563 (of 10192), d = 37, its = 37
i = 5564 (of 10192), d = 37, its = 55
i = 5565 (of 10192), d = 37, its = 54
i = 5566 (of 10192), d = 37, its = 55
i = 5567 (of 10192), d = 37, its = 52
i = 5568 (of 10192), d = 37, its = 51
i = 5569 (of 10192), d = 37, its = 40
i = 5570 (of 10192), d = 37, its = 37
i = 5571 (of 10192), d = 37, its = 54
i = 5572 (of 10192), d = 3.50959, its = 18
i = 5573 (of 10192), d = 37, its = 50
i = 5574 (of 10192), d = 2.88007, its = 18
i = 5575 (of 10192), d = 37, its = 56
i = 5576 (of 10192), d = 10.1041, its = 5
i = 5577 (of 10192), d = 9.05121, its = 5
i = 5578 (of 10192), d = 1.44714, its = 17
i = 5579 (of 10192), d = 37, its = 41
i = 5580 (of 10192), d = 37, its = 51
i = 5581 (of 10192), d = 37, its = 51
i = 5582 (of 10192), d = 4.1422, its = 18
i = 5583 (of 10192), d = 37, its = 52
i = 5584 (of 10192), d = 3.04367, its = 19
i = 5585 (of 10192), d = 10.113, its = 5
i = 5586 (of 10192), d = 37, its = 37
i = 5587 (of 10192), d = 37, its = 53
i = 5588 (of 10192), d = 37, its = 37
i = 5589 (of 10192), d = 37, its = 55
i = 5590 (of 10192), d = 37, its = 55
i = 5591 (of 10192), d = 37, its = 55
i = 5592 (of 10192), d = 37, its = 43
i = 5593 (of 10192), d = 7.25987, its = 5
i = 5594 (of 10192), d = 37, its = 37
i = 5595 (of 10192), d = 37, its = 50
i = 5596 (of 10192), d = 37, its = 37
i = 5597 (of 10192), d = 37, its = 37
i = 5598 (of 10192), d = 2.8074, its = 17
i = 5599 (of 10192), d = 37, its = 37
i = 5600 (of 10192), d = 37, its = 51
i = 5601 (of 10192), d = 37, its = 49
i = 5602 (of 10192), d = 12.5973, its = 7
i = 5603 (of 10192), d = 3.12355, its = 18
i = 5604 (of 10192), d = 37, its = 56
i = 5605 (of 10192), d = 37, its = 51
i = 5606 (of 10192), d = 37, its = 37
i = 5607 (of 10192), d = 5.34764, its = 5
i = 5608 (of 10192), d = 6.90193, its = 5
i = 5609 (of 10192), d = 37, its = 50
i = 5610 (of 10192), d = 10.8787, its = 5
i = 5611 (of 10192), d = 37, its = 47
i = 5612 (of 10192), d = 32.3847, its = 10
i = 5613 (of 10192), d = 37, its = 50
i = 5614 (of 10192), d = 37, its = 53
i = 5615 (of 10192), d = 37, its = 55
i = 5616 (of 10192), d = 1.76907, its = 17
i = 5617 (of 10192), d = 3.14316, its = 25
i = 5618 (of 10192), d = 37, its = 39
i = 5619 (of 10192), d = 37, its = 57
i = 5620 (of 10192), d = 37, its = 56
i = 5621 (of 10192), d = 37, its = 40
i = 5622 (of 10192), d = 37, its = 37
i = 5623 (of 10192), d = 37, its = 50
i = 5624 (of 10192), d = 37, its = 53
i = 5625 (of 10192), d = 37, its = 50
i = 5626 (of 10192), d = 12.3223, its = 7
i = 5627 (of 10192), d = 9.98858, its = 6
i = 5628 (of 10192), d = 6.11931, its = 6
i = 5629 (of 10192), d = 37, its = 54
i = 5630 (of 10192), d = 37, its = 55
i = 5631 (of 10192), d = 4.2464, its = 19
i = 5632 (of 10192), d = 37, its = 54
i = 5633 (of 10192), d = 37, its = 54
i = 5634 (of 10192), d = 37, its = 53
i = 5635 (of 10192), d = 37, its = 58
i = 5636 (of 10192), d = 8.3456, its = 3
i = 5637 (of 10192), d = 8.73879, its = 4
i = 5638 (of 10192), d = 37, its = 54
i = 5639 (of 10192), d = 7.31102, its = 4
i = 5640 (of 10192), d = 37, its = 42
i = 5641 (of 10192), d = 37, its = 37
i = 5642 (of 10192), d = 37, its = 51
i = 5643 (of 10192), d = 37, its = 51
i = 5644 (of 10192), d = 37, its = 55
i = 5645 (of 10192), d = 4.59731, its = 19
i = 5646 (of 10192), d = 37, its = 37
i = 5647 (of 10192), d = 2.20585, its = 17
i = 5648 (of 10192), d = 2.08571, its = 16
i = 5649 (of 10192), d = 37, its = 53
i = 5650 (of 10192), d = 37, its = 37
i = 5651 (of 10192), d = 2.89721, its = 18
i = 5652 (of 10192), d = 37, its = 37
i = 5653 (of 10192), d = 37, its = 53
i = 5654 (of 10192), d = 37, its = 52
i = 5655 (of 10192), d = 37, its = 51
i = 5656 (of 10192), d = 37, its = 55
i = 5657 (of 10192), d = 9.64179, its = 4
i = 5658 (of 10192), d = 4.71391, its = 18
i = 5659 (of 10192), d = 37, its = 54
i = 5660 (of 10192), d = 3.09416, its = 17
i = 5661 (of 10192), d = 37, its = 52
i = 5662 (of 10192), d = 37, its = 51
i = 5663 (of 10192), d = 37, its = 52
i = 5664 (of 10192), d = 7.89094, its = 3
i = 5665 (of 10192), d = 37, its = 37
i = 5666 (of 10192), d = 6.63227, its = 6
i = 5667 (of 10192), d = 37, its = 54
i = 5668 (of 10192), d = 37, its = 41
i = 5669 (of 10192), d = 13.3547, its = 6
i = 5670 (of 10192), d = 37, its = 54
i = 5671 (of 10192), d = 37, its = 39
i = 5672 (of 10192), d = 37, its = 53
i = 5673 (of 10192), d = 37, its = 40
i = 5674 (of 10192), d = 37, its = 54
i = 5675 (of 10192), d = 37, its = 37
i = 5676 (of 10192), d = 37, its = 52
i = 5677 (of 10192), d = 6.21417, its = 6
i = 5678 (of 10192), d = 37, its = 51
i = 5679 (of 10192), d = 10.1208, its = 6
i = 5680 (of 10192), d = 37, its = 50
i = 5681 (of 10192), d = 37, its = 55
i = 5682 (of 10192), d = 37, its = 39
i = 5683 (of 10192), d = 12.9619, its = 6
i = 5684 (of 10192), d = 7.57182, its = 4
i = 5685 (of 10192), d = 37, its = 55
i = 5686 (of 10192), d = 11.7428, its = 5
i = 5687 (of 10192), d = 8.34959, its = 4
i = 5688 (of 10192), d = 37, its = 53
i = 5689 (of 10192), d = 37, its = 55
i = 5690 (of 10192), d = 3.2819, its = 18
i = 5691 (of 10192), d = 3.75077, its = 17
i = 5692 (of 10192), d = 37, its = 56
i = 5693 (of 10192), d = 37, its = 50
i = 5694 (of 10192), d = 37, its = 54
i = 5695 (of 10192), d = 37, its = 54
i = 5696 (of 10192), d = 37, its = 54
i = 5697 (of 10192), d = 37, its = 51
i = 5698 (of 10192), d = 37, its = 37
i = 5699 (of 10192), d = 2.77654, its = 16
i = 5700 (of 10192), d = 37, its = 51
i = 5701 (of 10192), d = 37, its = 53
i = 5702 (of 10192), d = 10.465, its = 5
i = 5703 (of 10192), d = 7.65863, its = 4
i = 5704 (of 10192), d = 2.84638, its = 22
i = 5705 (of 10192), d = 37, its = 54
i = 5706 (of 10192), d = 37, its = 40
i = 5707 (of 10192), d = 37, its = 54
i = 5708 (of 10192), d = 37, its = 54
i = 5709 (of 10192), d = 2.66863, its = 19
i = 5710 (of 10192), d = 13.9934, its = 6
i = 5711 (of 10192), d = 37, its = 54
i = 5712 (of 10192), d = 37, its = 53
i = 5713 (of 10192), d = 37, its = 62
i = 5714 (of 10192), d = 11.4948, its = 6
i = 5715 (of 10192), d = 11.9729, its = 7
i = 5716 (of 10192), d = 37, its = 49
i = 5717 (of 10192), d = 9.18586, its = 4
i = 5718 (of 10192), d = 37, its = 53
i = 5719 (of 10192), d = 37, its = 51
i = 5720 (of 10192), d = 11.2636, its = 7
i = 5721 (of 10192), d = 37, its = 41
i = 5722 (of 10192), d = 37, its = 53
i = 5723 (of 10192), d = 2.40115, its = 19
i = 5724 (of 10192), d = 8.96565, its = 4
i = 5725 (of 10192), d = 2.82488, its = 19
i = 5726 (of 10192), d = 37, its = 37
i = 5727 (of 10192), d = 37, its = 51
i = 5728 (of 10192), d = 37, its = 53
i = 5729 (of 10192), d = 1.9607, its = 18
i = 5730 (of 10192), d = 5.35624, its = 6
i = 5731 (of 10192), d = 35.2411, its = 9
i = 5732 (of 10192), d = 37, its = 47
i = 5733 (of 10192), d = 37, its = 52
i = 5734 (of 10192), d = 3.44644, its = 19
i = 5735 (of 10192), d = 37, its = 39
i = 5736 (of 10192), d = 37, its = 49
i = 5737 (of 10192), d = 37, its = 37
i = 5738 (of 10192), d = 37, its = 51
i = 5739 (of 10192), d = 14.0009, its = 6
i = 5740 (of 10192), d = 37, its = 49
i = 5741 (of 10192), d = 37, its = 48
i = 5742 (of 10192), d = 20.1329, its = 8
i = 5743 (of 10192), d = 37, its = 37
i = 5744 (of 10192), d = 37, its = 52
i = 5745 (of 10192), d = 3.66257, its = 18
i = 5746 (of 10192), d = 37, its = 53
i = 5747 (of 10192), d = 37, its = 58
i = 5748 (of 10192), d = 37, its = 53
i = 5749 (of 10192), d = 37, its = 53
i = 5750 (of 10192), d = 9.53116, its = 5
i = 5751 (of 10192), d = 25.0939, its = 8
i = 5752 (of 10192), d = 4.97826, its = 25
i = 5753 (of 10192), d = 9.39597, its = 5
i = 5754 (of 10192), d = 37, its = 51
i = 5755 (of 10192), d = 37, its = 50
i = 5756 (of 10192), d = 37, its = 55
i = 5757 (of 10192), d = 3.5242, its = 19
i = 5758 (of 10192), d = 37, its = 54
i = 5759 (of 10192), d = 37, its = 50
i = 5760 (of 10192), d = 37, its = 52
i = 5761 (of 10192), d = 37, its = 53
i = 5762 (of 10192), d = 37, its = 37
i = 5763 (of 10192), d = 37, its = 52
i = 5764 (of 10192), d = 1.8239, its = 17
i = 5765 (of 10192), d = 37, its = 55
i = 5766 (of 10192), d = 37, its = 41
i = 5767 (of 10192), d = 10.1926, its = 5
i = 5768 (of 10192), d = 37, its = 54
i = 5769 (of 10192), d = 15.007, its = 6
i = 5770 (of 10192), d = 37, its = 53
i = 5771 (of 10192), d = 6.21422, its = 6
i = 5772 (of 10192), d = 37, its = 37
i = 5773 (of 10192), d = 37, its = 55
i = 5774 (of 10192), d = 37, its = 37
i = 5775 (of 10192), d = 37, its = 50
i = 5776 (of 10192), d = 10.0515, its = 5
i = 5777 (of 10192), d = 37, its = 37
i = 5778 (of 10192), d = 37, its = 34
i = 5779 (of 10192), d = 6.30566, its = 7
i = 5780 (of 10192), d = 37, its = 51
i = 5781 (of 10192), d = 4.97826, its = 25
i = 5782 (of 10192), d = 19.1215, its = 9
i = 5783 (of 10192), d = 37, its = 52
i = 5784 (of 10192), d = 37, its = 53
i = 5785 (of 10192), d = 37, its = 54
i = 5786 (of 10192), d = 37, its = 46
i = 5787 (of 10192), d = 3.16958, its = 20
i = 5788 (of 10192), d = 37, its = 49
i = 5789 (of 10192), d = 37, its = 50
i = 5790 (of 10192), d = 37, its = 52
i = 5791 (of 10192), d = 1.34761, its = 18
i = 5792 (of 10192), d = 1.25534, its = 18
i = 5793 (of 10192), d = 37, its = 52
i = 5794 (of 10192), d = 7.77, its = 3
i = 5795 (of 10192), d = 10.8176, its = 5
i = 5796 (of 10192), d = 17.8986, its = 7
i = 5797 (of 10192), d = 7.28629, its = 4
i = 5798 (of 10192), d = 14.0203, its = 6
i = 5799 (of 10192), d = 37, its = 60
i = 5800 (of 10192), d = 37, its = 56
i = 5801 (of 10192), d = 37, its = 54
i = 5802 (of 10192), d = 37, its = 37
i = 5803 (of 10192), d = 1.38589, its = 19
i = 5804 (of 10192), d = 37, its = 52
i = 5805 (of 10192), d = 37, its = 37
i = 5806 (of 10192), d = 3.68357, its = 18
i = 5807 (of 10192), d = 3.26438, its = 18
i = 5808 (of 10192), d = 6.58503, its = 5
i = 5809 (of 10192), d = 2.99169, its = 16
i = 5810 (of 10192), d = 37, its = 55
i = 5811 (of 10192), d = 7.08898, its = 5
i = 5812 (of 10192), d = 37, its = 52
i = 5813 (of 10192), d = 37, its = 46
i = 5814 (of 10192), d = 37, its = 54
i = 5815 (of 10192), d = 37, its = 51
i = 5816 (of 10192), d = 37, its = 45
i = 5817 (of 10192), d = 37, its = 37
i = 5818 (of 10192), d = 37, its = 54
i = 5819 (of 10192), d = 37, its = 52
i = 5820 (of 10192), d = 6.16388, its = 6
i = 5821 (of 10192), d = 7.87437, its = 3
i = 5822 (of 10192), d = 37, its = 41
i = 5823 (of 10192), d = 22.2096, its = 8
i = 5824 (of 10192), d = 4.58623, its = 24
i = 5825 (of 10192), d = 37, its = 51
i = 5826 (of 10192), d = 37, its = 37
i = 5827 (of 10192), d = 8.07033, its = 3
i = 5828 (of 10192), d = 37, its = 54
i = 5829 (of 10192), d = 37, its = 37
i = 5830 (of 10192), d = 11.3637, its = 7
i = 5831 (of 10192), d = 37, its = 54
i = 5832 (of 10192), d = 37, its = 58
i = 5833 (of 10192), d = 3.85326, its = 18
i = 5834 (of 10192), d = 2.54252, its = 16
i = 5835 (of 10192), d = 37, its = 42
i = 5836 (of 10192), d = 37, its = 37
i = 5837 (of 10192), d = 37, its = 55
i = 5838 (of 10192), d = 37, its = 37
i = 5839 (of 10192), d = 37, its = 49
i = 5840 (of 10192), d = 6.89472, its = 5
i = 5841 (of 10192), d = 7.84819, its = 3
i = 5842 (of 10192), d = 37, its = 52
i = 5843 (of 10192), d = 37, its = 50
i = 5844 (of 10192), d = 37, its = 37
i = 5845 (of 10192), d = 37, its = 48
i = 5846 (of 10192), d = 37, its = 54
i = 5847 (of 10192), d = 37, its = 56
i = 5848 (of 10192), d = 37, its = 47
i = 5849 (of 10192), d = 37, its = 37
i = 5850 (of 10192), d = 3.98709, its = 17
i = 5851 (of 10192), d = 7.89426, its = 3
i = 5852 (of 10192), d = 37, its = 41
i = 5853 (of 10192), d = 37, its = 46
i = 5854 (of 10192), d = 6.479, its = 6
i = 5855 (of 10192), d = 37, its = 54
i = 5856 (of 10192), d = 37, its = 51
i = 5857 (of 10192), d = 37, its = 39
i = 5858 (of 10192), d = 37, its = 53
i = 5859 (of 10192), d = 5.88512, its = 7
i = 5860 (of 10192), d = 11.2605, its = 6
i = 5861 (of 10192), d = 8.96648, its = 4
i = 5862 (of 10192), d = 37, its = 51
i = 5863 (of 10192), d = 37, its = 54
i = 5864 (of 10192), d = 37, its = 50
i = 5865 (of 10192), d = 37, its = 51
i = 5866 (of 10192), d = 37, its = 37
i = 5867 (of 10192), d = 2.78947, its = 20
i = 5868 (of 10192), d = 16.6889, its = 8
i = 5869 (of 10192), d = 37, its = 50
i = 5870 (of 10192), d = 1.94619, its = 21
i = 5871 (of 10192), d = 37, its = 53
i = 5872 (of 10192), d = 37, its = 40
i = 5873 (of 10192), d = 37, its = 60
i = 5874 (of 10192), d = 4.15887, its = 17
i = 5875 (of 10192), d = 3.02098, its = 16
i = 5876 (of 10192), d = 2.65837, its = 20
i = 5877 (of 10192), d = 37, its = 53
i = 5878 (of 10192), d = 37, its = 37
i = 5879 (of 10192), d = 37, its = 52
i = 5880 (of 10192), d = 37, its = 55
i = 5881 (of 10192), d = 37, its = 37
i = 5882 (of 10192), d = 3.19482, its = 17
i = 5883 (of 10192), d = 9.54151, its = 4
i = 5884 (of 10192), d = 37, its = 51
i = 5885 (of 10192), d = 7.61842, its = 4
i = 5886 (of 10192), d = 37, its = 51
i = 5887 (of 10192), d = 37, its = 52
i = 5888 (of 10192), d = 37, its = 52
i = 5889 (of 10192), d = 7.74717, its = 4
i = 5890 (of 10192), d = 37, its = 51
i = 5891 (of 10192), d = 37, its = 51
i = 5892 (of 10192), d = 5.27632, its = 6
i = 5893 (of 10192), d = 11.363, its = 6
i = 5894 (of 10192), d = 37, its = 40
i = 5895 (of 10192), d = 14.2687, its = 6
i = 5896 (of 10192), d = 37, its = 39
i = 5897 (of 10192), d = 37, its = 50
i = 5898 (of 10192), d = 37, its = 51
i = 5899 (of 10192), d = 8.71629, its = 4
i = 5900 (of 10192), d = 37, its = 38
i = 5901 (of 10192), d = 37, its = 52
i = 5902 (of 10192), d = 1.06466, its = 28
i = 5903 (of 10192), d = 37, its = 57
i = 5904 (of 10192), d = 37, its = 37
i = 5905 (of 10192), d = 37, its = 50
i = 5906 (of 10192), d = 7.47126, its = 4
i = 5907 (of 10192), d = 37, its = 51
i = 5908 (of 10192), d = 37, its = 39
i = 5909 (of 10192), d = 2.43984, its = 18
i = 5910 (of 10192), d = 37, its = 51
i = 5911 (of 10192), d = 37, its = 40
i = 5912 (of 10192), d = 37, its = 54
i = 5913 (of 10192), d = 37, its = 53
i = 5914 (of 10192), d = 37, its = 52
i = 5915 (of 10192), d = 37, its = 37
i = 5916 (of 10192), d = 37, its = 53
i = 5917 (of 10192), d = 37, its = 52
i = 5918 (of 10192), d = 37, its = 39
i = 5919 (of 10192), d = 6.36898, its = 7
i = 5920 (of 10192), d = 37, its = 53
i = 5921 (of 10192), d = 13.006, its = 6
i = 5922 (of 10192), d = 9.11396, its = 4
i = 5923 (of 10192), d = 37, its = 37
i = 5924 (of 10192), d = 37, its = 50
i = 5925 (of 10192), d = 9.51873, its = 5
i = 5926 (of 10192), d = 37, its = 53
i = 5927 (of 10192), d = 5.97151, its = 7
i = 5928 (of 10192), d = 6.03632, its = 6
i = 5929 (of 10192), d = 4.82712, its = 19
i = 5930 (of 10192), d = 37, its = 37
i = 5931 (of 10192), d = 7.10478, its = 4
i = 5932 (of 10192), d = 37, its = 39
i = 5933 (of 10192), d = 7.4182, its = 4
i = 5934 (of 10192), d = 37, its = 52
i = 5935 (of 10192), d = 37, its = 55
i = 5936 (of 10192), d = 4.2736, its = 17
i = 5937 (of 10192), d = 37, its = 39
i = 5938 (of 10192), d = 37, its = 53
i = 5939 (of 10192), d = 3.70684, its = 20
i = 5940 (of 10192), d = 6.95612, its = 4
i = 5941 (of 10192), d = 37, its = 52
i = 5942 (of 10192), d = 12.5187, its = 6
i = 5943 (of 10192), d = 14.0998, its = 6
i = 5944 (of 10192), d = 37, its = 38
i = 5945 (of 10192), d = 37, its = 48
i = 5946 (of 10192), d = 37, its = 51
i = 5947 (of 10192), d = 37, its = 52
i = 5948 (of 10192), d = 2.35452, its = 24
i = 5949 (of 10192), d = 37, its = 54
i = 5950 (of 10192), d = 37, its = 41
i = 5951 (of 10192), d = 37, its = 37
i = 5952 (of 10192), d = 37, its = 52
i = 5953 (of 10192), d = 37, its = 52
i = 5954 (of 10192), d = 37, its = 43
i = 5955 (of 10192), d = 37, its = 37
i = 5956 (of 10192), d = 37, its = 37
i = 5957 (of 10192), d = 37, its = 48
i = 5958 (of 10192), d = 37, its = 54
i = 5959 (of 10192), d = 37, its = 53
i = 5960 (of 10192), d = 37, its = 37
i = 5961 (of 10192), d = 37, its = 37
i = 5962 (of 10192), d = 37, its = 56
i = 5963 (of 10192), d = 37, its = 37
i = 5964 (of 10192), d = 37, its = 53
i = 5965 (of 10192), d = 6.69951, its = 5
i = 5966 (of 10192), d = 37, its = 52
i = 5967 (of 10192), d = 37, its = 52
i = 5968 (of 10192), d = 8.01025, its = 2
i = 5969 (of 10192), d = 1.7065, its = 22
i = 5970 (of 10192), d = 37, its = 38
i = 5971 (of 10192), d = 3.63302, its = 17
i = 5972 (of 10192), d = 13.3051, its = 6
i = 5973 (of 10192), d = 2.24894, its = 19
i = 5974 (of 10192), d = 37, its = 37
i = 5975 (of 10192), d = 37, its = 37
i = 5976 (of 10192), d = 37, its = 52
i = 5977 (of 10192), d = 3.86037, its = 19
i = 5978 (of 10192), d = 37, its = 50
i = 5979 (of 10192), d = 37, its = 50
i = 5980 (of 10192), d = 37, its = 52
i = 5981 (of 10192), d = 37, its = 54
i = 5982 (of 10192), d = 37, its = 52
i = 5983 (of 10192), d = 37, its = 53
i = 5984 (of 10192), d = 37, its = 51
i = 5985 (of 10192), d = 13.2038, its = 6
i = 5986 (of 10192), d = 4.80708, its = 6
i = 5987 (of 10192), d = 4.41536, its = 21
i = 5988 (of 10192), d = 37, its = 50
i = 5989 (of 10192), d = 2.46937, its = 19
i = 5990 (of 10192), d = 37, its = 54
i = 5991 (of 10192), d = 10.7574, its = 5
i = 5992 (of 10192), d = 37, its = 47
i = 5993 (of 10192), d = 5.46884, its = 7
i = 5994 (of 10192), d = 37, its = 37
i = 5995 (of 10192), d = 37, its = 51
i = 5996 (of 10192), d = 11.7494, its = 6
i = 5997 (of 10192), d = 3.81175, its = 17
i = 5998 (of 10192), d = 2.21029, its = 18
i = 5999 (of 10192), d = 37, its = 37
i = 6000 (of 10192), d = 10.1257, its = 5
i = 6001 (of 10192), d = 37, its = 55
i = 6002 (of 10192), d = 8.51628, its = 4
i = 6003 (of 10192), d = 37, its = 51
i = 6004 (of 10192), d = 37, its = 40
i = 6005 (of 10192), d = 37, its = 56
i = 6006 (of 10192), d = 3.2819, its = 18
i = 6007 (of 10192), d = 37, its = 52
i = 6008 (of 10192), d = 3.13121, its = 18
i = 6009 (of 10192), d = 37, its = 55
i = 6010 (of 10192), d = 37, its = 56
i = 6011 (of 10192), d = 37, its = 53
i = 6012 (of 10192), d = 3.73581, its = 18
i = 6013 (of 10192), d = 37, its = 52
i = 6014 (of 10192), d = 37, its = 40
i = 6015 (of 10192), d = 8.73987, its = 4
i = 6016 (of 10192), d = 37, its = 51
i = 6017 (of 10192), d = 37, its = 40
i = 6018 (of 10192), d = 13.0978, its = 8
i = 6019 (of 10192), d = 37, its = 55
i = 6020 (of 10192), d = 37, its = 54
i = 6021 (of 10192), d = 8.68594, its = 4
i = 6022 (of 10192), d = 37, its = 49
i = 6023 (of 10192), d = 18.896, its = 8
i = 6024 (of 10192), d = 8.23567, its = 3
i = 6025 (of 10192), d = 37, its = 50
i = 6026 (of 10192), d = 2.85383, its = 21
i = 6027 (of 10192), d = 2.25963, its = 18
i = 6028 (of 10192), d = 37, its = 54
i = 6029 (of 10192), d = 37, its = 41
i = 6030 (of 10192), d = 37, its = 53
i = 6031 (of 10192), d = 37, its = 56
i = 6032 (of 10192), d = 37, its = 51
i = 6033 (of 10192), d = 37, its = 37
i = 6034 (of 10192), d = 9.60182, its = 5
i = 6035 (of 10192), d = 6.50383, its = 5
i = 6036 (of 10192), d = 37, its = 41
i = 6037 (of 10192), d = 37, its = 57
i = 6038 (of 10192), d = 1.77366, its = 17
i = 6039 (of 10192), d = 37, its = 39
i = 6040 (of 10192), d = 37, its = 56
i = 6041 (of 10192), d = 6.50012, its = 6
i = 6042 (of 10192), d = 21.2955, its = 7
i = 6043 (of 10192), d = 37, its = 37
i = 6044 (of 10192), d = 37, its = 49
i = 6045 (of 10192), d = 37, its = 52
i = 6046 (of 10192), d = 37, its = 40
i = 6047 (of 10192), d = 37, its = 53
i = 6048 (of 10192), d = 37, its = 51
i = 6049 (of 10192), d = 37, its = 37
i = 6050 (of 10192), d = 37, its = 51
i = 6051 (of 10192), d = 10.2961, its = 5
i = 6052 (of 10192), d = 23.5928, its = 7
i = 6053 (of 10192), d = 14.0728, its = 8
i = 6054 (of 10192), d = 6.10793, its = 6
i = 6055 (of 10192), d = 37, its = 53
i = 6056 (of 10192), d = 37, its = 50
i = 6057 (of 10192), d = 10.5674, its = 5
i = 6058 (of 10192), d = 37, its = 54
i = 6059 (of 10192), d = 6.49426, its = 6
i = 6060 (of 10192), d = 37, its = 37
i = 6061 (of 10192), d = 4.21157, its = 22
i = 6062 (of 10192), d = 37, its = 37
i = 6063 (of 10192), d = 3.02094, its = 17
i = 6064 (of 10192), d = 10.6594, its = 6
i = 6065 (of 10192), d = 37, its = 50
i = 6066 (of 10192), d = 37, its = 52
i = 6067 (of 10192), d = 37, its = 53
i = 6068 (of 10192), d = 37, its = 55
i = 6069 (of 10192), d = 31.3976, its = 8
i = 6070 (of 10192), d = 7.03152, its = 4
i = 6071 (of 10192), d = 37, its = 54
i = 6072 (of 10192), d = 37, its = 52
i = 6073 (of 10192), d = 4.0542, its = 26
i = 6074 (of 10192), d = 37, its = 39
i = 6075 (of 10192), d = 37, its = 49
i = 6076 (of 10192), d = 37, its = 54
i = 6077 (of 10192), d = 37, its = 53
i = 6078 (of 10192), d = 37, its = 55
i = 6079 (of 10192), d = 37, its = 53
i = 6080 (of 10192), d = 4.40278, its = 29
i = 6081 (of 10192), d = 37, its = 38
i = 6082 (of 10192), d = 37, its = 41
i = 6083 (of 10192), d = 37, its = 57
i = 6084 (of 10192), d = 37, its = 52
i = 6085 (of 10192), d = 37, its = 48
i = 6086 (of 10192), d = 37, its = 51
i = 6087 (of 10192), d = 37, its = 50
i = 6088 (of 10192), d = 37, its = 49
i = 6089 (of 10192), d = 37, its = 52
i = 6090 (of 10192), d = 10.7574, its = 5
i = 6091 (of 10192), d = 13.8259, its = 6
i = 6092 (of 10192), d = 37, its = 37
i = 6093 (of 10192), d = 6.61338, its = 6
i = 6094 (of 10192), d = 37, its = 37
i = 6095 (of 10192), d = 37, its = 53
i = 6096 (of 10192), d = 37, its = 54
i = 6097 (of 10192), d = 37, its = 56
i = 6098 (of 10192), d = 37, its = 42
i = 6099 (of 10192), d = 37, its = 54
i = 6100 (of 10192), d = 37, its = 52
i = 6101 (of 10192), d = 4.18081, its = 20
i = 6102 (of 10192), d = 37, its = 51
i = 6103 (of 10192), d = 7.8789, its = 3
i = 6104 (of 10192), d = 37, its = 51
i = 6105 (of 10192), d = 9.93603, its = 5
i = 6106 (of 10192), d = 37, its = 53
i = 6107 (of 10192), d = 37, its = 50
i = 6108 (of 10192), d = 37, its = 55
i = 6109 (of 10192), d = 15.4758, its = 7
i = 6110 (of 10192), d = 3.81884, its = 22
i = 6111 (of 10192), d = 37, its = 53
i = 6112 (of 10192), d = 4.82703, its = 22
i = 6113 (of 10192), d = 37, its = 56
i = 6114 (of 10192), d = 37, its = 56
i = 6115 (of 10192), d = 37, its = 53
i = 6116 (of 10192), d = 37, its = 50
i = 6117 (of 10192), d = 37, its = 53
i = 6118 (of 10192), d = 37, its = 50
i = 6119 (of 10192), d = 15.367, its = 7
i = 6120 (of 10192), d = 37, its = 49
i = 6121 (of 10192), d = 15.9537, its = 8
i = 6122 (of 10192), d = 37, its = 37
i = 6123 (of 10192), d = 37, its = 37
i = 6124 (of 10192), d = 3.98459, its = 22
i = 6125 (of 10192), d = 37, its = 51
i = 6126 (of 10192), d = 37, its = 57
i = 6127 (of 10192), d = 37, its = 37
i = 6128 (of 10192), d = 37, its = 51
i = 6129 (of 10192), d = 10.0673, its = 5
i = 6130 (of 10192), d = 37, its = 53
i = 6131 (of 10192), d = 37, its = 40
i = 6132 (of 10192), d = 8.25965, its = 3
i = 6133 (of 10192), d = 7.53204, its = 4
i = 6134 (of 10192), d = 12.0878, its = 6
i = 6135 (of 10192), d = 2.64768, its = 17
i = 6136 (of 10192), d = 37, its = 40
i = 6137 (of 10192), d = 37, its = 37
i = 6138 (of 10192), d = 37, its = 55
i = 6139 (of 10192), d = 3.30052, its = 22
i = 6140 (of 10192), d = 2.3149, its = 18
i = 6141 (of 10192), d = 37, its = 56
i = 6142 (of 10192), d = 37, its = 56
i = 6143 (of 10192), d = 3.3144, its = 19
i = 6144 (of 10192), d = 37, its = 37
i = 6145 (of 10192), d = 37, its = 37
i = 6146 (of 10192), d = 2.39835, its = 18
i = 6147 (of 10192), d = 37, its = 37
i = 6148 (of 10192), d = 1.53073, its = 18
i = 6149 (of 10192), d = 4.20578, its = 16
i = 6150 (of 10192), d = 18.896, its = 8
i = 6151 (of 10192), d = 37, its = 55
i = 6152 (of 10192), d = 4.08263, its = 16
i = 6153 (of 10192), d = 37, its = 59
i = 6154 (of 10192), d = 10.0736, its = 5
i = 6155 (of 10192), d = 7.12069, its = 4
i = 6156 (of 10192), d = 37, its = 47
i = 6157 (of 10192), d = 37, its = 51
i = 6158 (of 10192), d = 37, its = 58
i = 6159 (of 10192), d = 5.94719, its = 6
i = 6160 (of 10192), d = 37, its = 37
i = 6161 (of 10192), d = 37, its = 51
i = 6162 (of 10192), d = 37, its = 37
i = 6163 (of 10192), d = 15.8084, its = 8
i = 6164 (of 10192), d = 37, its = 37
i = 6165 (of 10192), d = 37, its = 55
i = 6166 (of 10192), d = 37, its = 43
i = 6167 (of 10192), d = 37, its = 37
i = 6168 (of 10192), d = 37, its = 40
i = 6169 (of 10192), d = 9.05854, its = 4
i = 6170 (of 10192), d = 37, its = 42
i = 6171 (of 10192), d = 37, its = 37
i = 6172 (of 10192), d = 1.26338, its = 18
i = 6173 (of 10192), d = 9.28093, its = 5
i = 6174 (of 10192), d = 37, its = 57
i = 6175 (of 10192), d = 3.35896, its = 18
i = 6176 (of 10192), d = 37, its = 51
i = 6177 (of 10192), d = 37, its = 57
i = 6178 (of 10192), d = 6.0267, its = 6
i = 6179 (of 10192), d = 37, its = 50
i = 6180 (of 10192), d = 33.5309, its = 9
i = 6181 (of 10192), d = 37, its = 49
i = 6182 (of 10192), d = 37, its = 50
i = 6183 (of 10192), d = 7.98769, its = 2
i = 6184 (of 10192), d = 3.9837, its = 19
i = 6185 (of 10192), d = 37, its = 52
i = 6186 (of 10192), d = 37, its = 52
i = 6187 (of 10192), d = 37, its = 42
i = 6188 (of 10192), d = 37, its = 54
i = 6189 (of 10192), d = 37, its = 57
i = 6190 (of 10192), d = 37, its = 53
i = 6191 (of 10192), d = 3.40985, its = 18
i = 6192 (of 10192), d = 5.88239, its = 6
i = 6193 (of 10192), d = 37, its = 57
i = 6194 (of 10192), d = 37, its = 56
i = 6195 (of 10192), d = 37, its = 54
i = 6196 (of 10192), d = 14.4185, its = 7
i = 6197 (of 10192), d = 6.82848, its = 5
i = 6198 (of 10192), d = 37, its = 37
i = 6199 (of 10192), d = 17.616, its = 6
i = 6200 (of 10192), d = 9.71317, its = 5
i = 6201 (of 10192), d = 3.76505, its = 28
i = 6202 (of 10192), d = 37, its = 52
i = 6203 (of 10192), d = 37, its = 51
i = 6204 (of 10192), d = 37, its = 54
i = 6205 (of 10192), d = 37, its = 41
i = 6206 (of 10192), d = 37, its = 46
i = 6207 (of 10192), d = 7.30135, its = 4
i = 6208 (of 10192), d = 9.57487, its = 5
i = 6209 (of 10192), d = 5.45946, its = 7
i = 6210 (of 10192), d = 37, its = 51
i = 6211 (of 10192), d = 1.35427, its = 20
i = 6212 (of 10192), d = 10.6466, its = 5
i = 6213 (of 10192), d = 15.2401, its = 6
i = 6214 (of 10192), d = 37, its = 37
i = 6215 (of 10192), d = 2.26826, its = 19
i = 6216 (of 10192), d = 37, its = 41
i = 6217 (of 10192), d = 3.71387, its = 18
i = 6218 (of 10192), d = 13.7047, its = 6
i = 6219 (of 10192), d = 4.39782, its = 20
i = 6220 (of 10192), d = 37, its = 52
i = 6221 (of 10192), d = 3.03902, its = 17
i = 6222 (of 10192), d = 37, its = 37
i = 6223 (of 10192), d = 37, its = 51
i = 6224 (of 10192), d = 2.54976, its = 24
i = 6225 (of 10192), d = 10.9512, its = 5
i = 6226 (of 10192), d = 12.9518, its = 6
i = 6227 (of 10192), d = 37, its = 53
i = 6228 (of 10192), d = 7.00259, its = 7
i = 6229 (of 10192), d = 5.50391, its = 6
i = 6230 (of 10192), d = 37, its = 54
i = 6231 (of 10192), d = 37, its = 39
i = 6232 (of 10192), d = 3.67473, its = 17
i = 6233 (of 10192), d = 5.69866, its = 6
i = 6234 (of 10192), d = 37, its = 49
i = 6235 (of 10192), d = 2.89942, its = 20
i = 6236 (of 10192), d = 13.3051, its = 6
i = 6237 (of 10192), d = 6.80296, its = 6
i = 6238 (of 10192), d = 3.41063, its = 20
i = 6239 (of 10192), d = 37, its = 44
i = 6240 (of 10192), d = 37, its = 53
i = 6241 (of 10192), d = 37, its = 43
i = 6242 (of 10192), d = 37, its = 45
i = 6243 (of 10192), d = 37, its = 52
i = 6244 (of 10192), d = 37, its = 53
i = 6245 (of 10192), d = 3.33084, its = 20
i = 6246 (of 10192), d = 37, its = 53
i = 6247 (of 10192), d = 1.6575, its = 19
i = 6248 (of 10192), d = 37, its = 50
i = 6249 (of 10192), d = 37, its = 55
i = 6250 (of 10192), d = 10.8198, its = 5
i = 6251 (of 10192), d = 35.3787, its = 8
i = 6252 (of 10192), d = 37, its = 48
i = 6253 (of 10192), d = 13.4052, its = 6
i = 6254 (of 10192), d = 10.619, its = 6
i = 6255 (of 10192), d = 5.22135, its = 6
i = 6256 (of 10192), d = 3.51032, its = 21
i = 6257 (of 10192), d = 6.67885, its = 5
i = 6258 (of 10192), d = 37, its = 37
i = 6259 (of 10192), d = 16.7922, its = 7
i = 6260 (of 10192), d = 1.27603, its = 18
i = 6261 (of 10192), d = 5.48301, its = 6
i = 6262 (of 10192), d = 8.29663, its = 3
i = 6263 (of 10192), d = 37, its = 52
i = 6264 (of 10192), d = 5.87485, its = 6
i = 6265 (of 10192), d = 37, its = 52
i = 6266 (of 10192), d = 2.30885, its = 19
i = 6267 (of 10192), d = 37, its = 41
i = 6268 (of 10192), d = 37, its = 37
i = 6269 (of 10192), d = 37, its = 42
i = 6270 (of 10192), d = 8.70997, its = 4
i = 6271 (of 10192), d = 37, its = 56
i = 6272 (of 10192), d = 3.26302, its = 18
i = 6273 (of 10192), d = 37, its = 53
i = 6274 (of 10192), d = 4.33892, its = 20
i = 6275 (of 10192), d = 37, its = 50
i = 6276 (of 10192), d = 37, its = 37
i = 6277 (of 10192), d = 37, its = 40
i = 6278 (of 10192), d = 37, its = 52
i = 6279 (of 10192), d = 37, its = 53
i = 6280 (of 10192), d = 37, its = 49
i = 6281 (of 10192), d = 37, its = 54
i = 6282 (of 10192), d = 37, its = 37
i = 6283 (of 10192), d = 37, its = 55
i = 6284 (of 10192), d = 32.8859, its = 7
i = 6285 (of 10192), d = 37, its = 52
i = 6286 (of 10192), d = 37, its = 53
i = 6287 (of 10192), d = 5.87653, its = 7
i = 6288 (of 10192), d = 37, its = 56
i = 6289 (of 10192), d = 3.2643, its = 29
i = 6290 (of 10192), d = 37, its = 37
i = 6291 (of 10192), d = 3.73581, its = 18
i = 6292 (of 10192), d = 37, its = 39
i = 6293 (of 10192), d = 10.7077, its = 6
i = 6294 (of 10192), d = 37, its = 53
i = 6295 (of 10192), d = 37, its = 53
i = 6296 (of 10192), d = 7.88539, its = 3
i = 6297 (of 10192), d = 37, its = 37
i = 6298 (of 10192), d = 10.3898, its = 5
i = 6299 (of 10192), d = 37, its = 49
i = 6300 (of 10192), d = 11.6636, its = 6
i = 6301 (of 10192), d = 37, its = 42
i = 6302 (of 10192), d = 37, its = 57
i = 6303 (of 10192), d = 15.8231, its = 7
i = 6304 (of 10192), d = 37, its = 53
i = 6305 (of 10192), d = 12.1927, its = 6
i = 6306 (of 10192), d = 37, its = 51
i = 6307 (of 10192), d = 37, its = 37
i = 6308 (of 10192), d = 7.77336, its = 3
i = 6309 (of 10192), d = 37, its = 51
i = 6310 (of 10192), d = 7.62401, its = 4
i = 6311 (of 10192), d = 4.52834, its = 24
i = 6312 (of 10192), d = 37, its = 55
i = 6313 (of 10192), d = 37, its = 41
i = 6314 (of 10192), d = 8.08749, its = 3
i = 6315 (of 10192), d = 37, its = 39
i = 6316 (of 10192), d = 37, its = 37
i = 6317 (of 10192), d = 37, its = 52
i = 6318 (of 10192), d = 37, its = 37
i = 6319 (of 10192), d = 37, its = 52
i = 6320 (of 10192), d = 37, its = 53
i = 6321 (of 10192), d = 2.16324, its = 18
i = 6322 (of 10192), d = 9.02882, its = 4
i = 6323 (of 10192), d = 37, its = 50
i = 6324 (of 10192), d = 11.7605, its = 5
i = 6325 (of 10192), d = 8.36661, its = 4
i = 6326 (of 10192), d = 37, its = 56
i = 6327 (of 10192), d = 37, its = 40
i = 6328 (of 10192), d = 37, its = 40
i = 6329 (of 10192), d = 37, its = 37
i = 6330 (of 10192), d = 37, its = 37
i = 6331 (of 10192), d = 37, its = 55
i = 6332 (of 10192), d = 2.9415, its = 20
i = 6333 (of 10192), d = 37, its = 57
i = 6334 (of 10192), d = 37, its = 37
i = 6335 (of 10192), d = 10.0948, its = 6
i = 6336 (of 10192), d = 37, its = 41
i = 6337 (of 10192), d = 37, its = 40
i = 6338 (of 10192), d = 9.83185, its = 5
i = 6339 (of 10192), d = 7.46756, its = 4
i = 6340 (of 10192), d = 37, its = 55
i = 6341 (of 10192), d = 37, its = 41
i = 6342 (of 10192), d = 37, its = 52
i = 6343 (of 10192), d = 37, its = 52
i = 6344 (of 10192), d = 37, its = 48
i = 6345 (of 10192), d = 37, its = 52
i = 6346 (of 10192), d = 37, its = 53
i = 6347 (of 10192), d = 37, its = 54
i = 6348 (of 10192), d = 37, its = 37
i = 6349 (of 10192), d = 9.57431, its = 5
i = 6350 (of 10192), d = 37, its = 37
i = 6351 (of 10192), d = 37, its = 37
i = 6352 (of 10192), d = 9.29638, its = 5
i = 6353 (of 10192), d = 9.00165, its = 4
i = 6354 (of 10192), d = 37, its = 51
i = 6355 (of 10192), d = 9.70436, its = 5
i = 6356 (of 10192), d = 14.8848, its = 9
i = 6357 (of 10192), d = 14.6412, its = 6
i = 6358 (of 10192), d = 37, its = 40
i = 6359 (of 10192), d = 37, its = 42
i = 6360 (of 10192), d = 37, its = 37
i = 6361 (of 10192), d = 37, its = 53
i = 6362 (of 10192), d = 37, its = 38
i = 6363 (of 10192), d = 37, its = 47
i = 6364 (of 10192), d = 9.82665, its = 5
i = 6365 (of 10192), d = 7.38107, its = 4
i = 6366 (of 10192), d = 15.2313, its = 7
i = 6367 (of 10192), d = 5.47522, its = 6
i = 6368 (of 10192), d = 37, its = 40
i = 6369 (of 10192), d = 37, its = 37
i = 6370 (of 10192), d = 32.4622, its = 8
i = 6371 (of 10192), d = 37, its = 53
i = 6372 (of 10192), d = 37, its = 37
i = 6373 (of 10192), d = 11.2636, its = 7
i = 6374 (of 10192), d = 37, its = 41
i = 6375 (of 10192), d = 3.14645, its = 17
i = 6376 (of 10192), d = 37, its = 54
i = 6377 (of 10192), d = 3.91529, its = 20
i = 6378 (of 10192), d = 37, its = 55
i = 6379 (of 10192), d = 37, its = 37
i = 6380 (of 10192), d = 37, its = 51
i = 6381 (of 10192), d = 37, its = 51
i = 6382 (of 10192), d = 37, its = 39
i = 6383 (of 10192), d = 37, its = 53
i = 6384 (of 10192), d = 1.93985, its = 19
i = 6385 (of 10192), d = 37, its = 37
i = 6386 (of 10192), d = 37, its = 40
i = 6387 (of 10192), d = 37, its = 57
i = 6388 (of 10192), d = 37, its = 38
i = 6389 (of 10192), d = 8.61644, its = 4
i = 6390 (of 10192), d = 37, its = 54
i = 6391 (of 10192), d = 37, its = 53
i = 6392 (of 10192), d = 37, its = 51
i = 6393 (of 10192), d = 37, its = 37
i = 6394 (of 10192), d = 29.1062, its = 8
i = 6395 (of 10192), d = 37, its = 53
i = 6396 (of 10192), d = 37, its = 51
i = 6397 (of 10192), d = 37, its = 37
i = 6398 (of 10192), d = 37, its = 50
i = 6399 (of 10192), d = 37, its = 54
i = 6400 (of 10192), d = 3.70862, its = 18
i = 6401 (of 10192), d = 2.34502, its = 24
i = 6402 (of 10192), d = 4.55578, its = 25
i = 6403 (of 10192), d = 37, its = 51
i = 6404 (of 10192), d = 7.0246, its = 5
i = 6405 (of 10192), d = 37, its = 37
i = 6406 (of 10192), d = 10.3691, its = 5
i = 6407 (of 10192), d = 37, its = 46
i = 6408 (of 10192), d = 5.32379, its = 8
i = 6409 (of 10192), d = 8.97843, its = 6
i = 6410 (of 10192), d = 3.20143, its = 16
i = 6411 (of 10192), d = 37, its = 52
i = 6412 (of 10192), d = 2.36604, its = 20
i = 6413 (of 10192), d = 1.80051, its = 25
i = 6414 (of 10192), d = 37, its = 37
i = 6415 (of 10192), d = 37, its = 54
i = 6416 (of 10192), d = 1.35104, its = 18
i = 6417 (of 10192), d = 3.40867, its = 19
i = 6418 (of 10192), d = 6.77124, its = 6
i = 6419 (of 10192), d = 13.3821, its = 7
i = 6420 (of 10192), d = 7.17617, its = 5
i = 6421 (of 10192), d = 37, its = 39
i = 6422 (of 10192), d = 37, its = 53
i = 6423 (of 10192), d = 37, its = 54
i = 6424 (of 10192), d = 37, its = 37
i = 6425 (of 10192), d = 37, its = 51
i = 6426 (of 10192), d = 37, its = 43
i = 6427 (of 10192), d = 2.25642, its = 20
i = 6428 (of 10192), d = 2.14391, its = 17
i = 6429 (of 10192), d = 15.015, its = 7
i = 6430 (of 10192), d = 37, its = 38
i = 6431 (of 10192), d = 37, its = 52
i = 6432 (of 10192), d = 3.49447, its = 19
i = 6433 (of 10192), d = 2.77515, its = 17
i = 6434 (of 10192), d = 4.04039, its = 20
i = 6435 (of 10192), d = 1.5741, its = 18
i = 6436 (of 10192), d = 37, its = 55
i = 6437 (of 10192), d = 37, its = 37
i = 6438 (of 10192), d = 37, its = 48
i = 6439 (of 10192), d = 37, its = 50
i = 6440 (of 10192), d = 37, its = 51
i = 6441 (of 10192), d = 6.41457, its = 6
i = 6442 (of 10192), d = 37, its = 37
i = 6443 (of 10192), d = 37, its = 37
i = 6444 (of 10192), d = 37, its = 55
i = 6445 (of 10192), d = 5.72592, its = 21
i = 6446 (of 10192), d = 37, its = 48
i = 6447 (of 10192), d = 37, its = 55
i = 6448 (of 10192), d = 17.2415, its = 7
i = 6449 (of 10192), d = 37, its = 52
i = 6450 (of 10192), d = 37, its = 38
i = 6451 (of 10192), d = 10.1688, its = 5
i = 6452 (of 10192), d = 37, its = 53
i = 6453 (of 10192), d = 37, its = 47
i = 6454 (of 10192), d = 37, its = 37
i = 6455 (of 10192), d = 37, its = 52
i = 6456 (of 10192), d = 37, its = 53
i = 6457 (of 10192), d = 37, its = 51
i = 6458 (of 10192), d = 1.30338, its = 18
i = 6459 (of 10192), d = 37, its = 52
i = 6460 (of 10192), d = 37, its = 53
i = 6461 (of 10192), d = 37, its = 53
i = 6462 (of 10192), d = 3.06061, its = 16
i = 6463 (of 10192), d = 37, its = 60
i = 6464 (of 10192), d = 7.71885, its = 3
i = 6465 (of 10192), d = 37, its = 37
i = 6466 (of 10192), d = 37, its = 53
i = 6467 (of 10192), d = 37, its = 51
i = 6468 (of 10192), d = 6.08219, its = 6
i = 6469 (of 10192), d = 37, its = 37
i = 6470 (of 10192), d = 37, its = 54
i = 6471 (of 10192), d = 37, its = 54
i = 6472 (of 10192), d = 37, its = 51
i = 6473 (of 10192), d = 37, its = 51
i = 6474 (of 10192), d = 9.06037, its = 4
i = 6475 (of 10192), d = 6.68093, its = 6
i = 6476 (of 10192), d = 14.1595, its = 6
i = 6477 (of 10192), d = 37, its = 37
i = 6478 (of 10192), d = 37, its = 39
i = 6479 (of 10192), d = 37, its = 37
i = 6480 (of 10192), d = 37, its = 53
i = 6481 (of 10192), d = 3.14189, its = 20
i = 6482 (of 10192), d = 37, its = 51
i = 6483 (of 10192), d = 10.687, its = 5
i = 6484 (of 10192), d = 5.10621, its = 6
i = 6485 (of 10192), d = 37, its = 56
i = 6486 (of 10192), d = 37, its = 50
i = 6487 (of 10192), d = 4.11236, its = 20
i = 6488 (of 10192), d = 8.485, its = 4
i = 6489 (of 10192), d = 2.17283, its = 20
i = 6490 (of 10192), d = 37, its = 57
i = 6491 (of 10192), d = 37, its = 41
i = 6492 (of 10192), d = 37, its = 37
i = 6493 (of 10192), d = 37, its = 52
i = 6494 (of 10192), d = 37, its = 37
i = 6495 (of 10192), d = 37, its = 48
i = 6496 (of 10192), d = 4.22475, its = 25
i = 6497 (of 10192), d = 37, its = 60
i = 6498 (of 10192), d = 37, its = 51
i = 6499 (of 10192), d = 6.41254, its = 6
i = 6500 (of 10192), d = 1.71925, its = 18
i = 6501 (of 10192), d = 37, its = 50
i = 6502 (of 10192), d = 37, its = 37
i = 6503 (of 10192), d = 2.66493, its = 21
i = 6504 (of 10192), d = 3.9701, its = 18
i = 6505 (of 10192), d = 37, its = 37
i = 6506 (of 10192), d = 7.26808, its = 4
i = 6507 (of 10192), d = 3.01947, its = 22
i = 6508 (of 10192), d = 37, its = 52
i = 6509 (of 10192), d = 37, its = 40
i = 6510 (of 10192), d = 37, its = 54
i = 6511 (of 10192), d = 3.3043, its = 16
i = 6512 (of 10192), d = 37, its = 52
i = 6513 (of 10192), d = 37, its = 37
i = 6514 (of 10192), d = 37, its = 43
i = 6515 (of 10192), d = 37, its = 53
i = 6516 (of 10192), d = 10.2829, its = 5
i = 6517 (of 10192), d = 9.35304, its = 5
i = 6518 (of 10192), d = 37, its = 38
i = 6519 (of 10192), d = 5.66546, its = 6
i = 6520 (of 10192), d = 37, its = 51
i = 6521 (of 10192), d = 37, its = 52
i = 6522 (of 10192), d = 13.7413, its = 6
i = 6523 (of 10192), d = 37, its = 54
i = 6524 (of 10192), d = 37, its = 53
i = 6525 (of 10192), d = 37, its = 52
i = 6526 (of 10192), d = 10.6007, its = 5
i = 6527 (of 10192), d = 37, its = 53
i = 6528 (of 10192), d = 37, its = 51
i = 6529 (of 10192), d = 37, its = 54
i = 6530 (of 10192), d = 37, its = 51
i = 6531 (of 10192), d = 37, its = 50
i = 6532 (of 10192), d = 4.62465, its = 18
i = 6533 (of 10192), d = 2.11054, its = 19
i = 6534 (of 10192), d = 37, its = 37
i = 6535 (of 10192), d = 37, its = 55
i = 6536 (of 10192), d = 1.62278, its = 25
i = 6537 (of 10192), d = 37, its = 51
i = 6538 (of 10192), d = 10.1012, its = 6
i = 6539 (of 10192), d = 11.7421, its = 6
i = 6540 (of 10192), d = 37, its = 39
i = 6541 (of 10192), d = 37, its = 54
i = 6542 (of 10192), d = 37, its = 55
i = 6543 (of 10192), d = 37, its = 37
i = 6544 (of 10192), d = 37, its = 57
i = 6545 (of 10192), d = 37, its = 57
i = 6546 (of 10192), d = 37, its = 54
i = 6547 (of 10192), d = 25.2955, its = 8
i = 6548 (of 10192), d = 37, its = 53
i = 6549 (of 10192), d = 37, its = 53
i = 6550 (of 10192), d = 37, its = 38
i = 6551 (of 10192), d = 37, its = 56
i = 6552 (of 10192), d = 37, its = 37
i = 6553 (of 10192), d = 11.9348, its = 7
i = 6554 (of 10192), d = 37, its = 52
i = 6555 (of 10192), d = 37, its = 56
i = 6556 (of 10192), d = 37, its = 41
i = 6557 (of 10192), d = 0.9319, its = 24
i = 6558 (of 10192), d = 37, its = 38
i = 6559 (of 10192), d = 4.59314, its = 18
i = 6560 (of 10192), d = 37, its = 37
i = 6561 (of 10192), d = 10.7593, its = 5
i = 6562 (of 10192), d = 1.94339, its = 19
i = 6563 (of 10192), d = 2.87703, its = 19
i = 6564 (of 10192), d = 37, its = 56
i = 6565 (of 10192), d = 37, its = 42
i = 6566 (of 10192), d = 37, its = 37
i = 6567 (of 10192), d = 37, its = 55
i = 6568 (of 10192), d = 37, its = 54
i = 6569 (of 10192), d = 2.81342, its = 19
i = 6570 (of 10192), d = 37, its = 37
i = 6571 (of 10192), d = 37, its = 51
i = 6572 (of 10192), d = 37, its = 41
i = 6573 (of 10192), d = 37, its = 58
i = 6574 (of 10192), d = 37, its = 50
i = 6575 (of 10192), d = 37, its = 52
i = 6576 (of 10192), d = 37, its = 37
i = 6577 (of 10192), d = 7.58938, its = 4
i = 6578 (of 10192), d = 6.41254, its = 6
i = 6579 (of 10192), d = 37, its = 57
i = 6580 (of 10192), d = 37, its = 56
i = 6581 (of 10192), d = 37, its = 43
i = 6582 (of 10192), d = 37, its = 52
i = 6583 (of 10192), d = 7.47583, its = 4
i = 6584 (of 10192), d = 37, its = 48
i = 6585 (of 10192), d = 37, its = 50
i = 6586 (of 10192), d = 3.1475, its = 19
i = 6587 (of 10192), d = 18.0187, its = 7
i = 6588 (of 10192), d = 3.91774, its = 17
i = 6589 (of 10192), d = 3.6601, its = 19
i = 6590 (of 10192), d = 37, its = 53
i = 6591 (of 10192), d = 37, its = 56
i = 6592 (of 10192), d = 37, its = 48
i = 6593 (of 10192), d = 37, its = 54
i = 6594 (of 10192), d = 37, its = 55
i = 6595 (of 10192), d = 37, its = 52
i = 6596 (of 10192), d = 37, its = 50
i = 6597 (of 10192), d = 37, its = 58
i = 6598 (of 10192), d = 37, its = 37
i = 6599 (of 10192), d = 5.74009, its = 6
i = 6600 (of 10192), d = 6.92932, its = 5
i = 6601 (of 10192), d = 37, its = 54
i = 6602 (of 10192), d = 2.41762, its = 22
i = 6603 (of 10192), d = 9.82665, its = 5
i = 6604 (of 10192), d = 8.22886, its = 3
i = 6605 (of 10192), d = 17.0504, its = 7
i = 6606 (of 10192), d = 4.089, its = 25
i = 6607 (of 10192), d = 2.75106, its = 17
i = 6608 (of 10192), d = 37, its = 51
i = 6609 (of 10192), d = 6.77986, its = 6
i = 6610 (of 10192), d = 6.74483, its = 5
i = 6611 (of 10192), d = 37, its = 54
i = 6612 (of 10192), d = 37, its = 40
i = 6613 (of 10192), d = 3.7891, its = 19
i = 6614 (of 10192), d = 4.2843, its = 21
i = 6615 (of 10192), d = 37, its = 55
i = 6616 (of 10192), d = 37, its = 55
i = 6617 (of 10192), d = 3.63447, its = 19
i = 6618 (of 10192), d = 37, its = 49
i = 6619 (of 10192), d = 37, its = 55
i = 6620 (of 10192), d = 37, its = 51
i = 6621 (of 10192), d = 37, its = 53
i = 6622 (of 10192), d = 6.47581, its = 5
i = 6623 (of 10192), d = 37, its = 54
i = 6624 (of 10192), d = 5.84464, its = 8
i = 6625 (of 10192), d = 14.9622, its = 7
i = 6626 (of 10192), d = 11.8169, its = 6
i = 6627 (of 10192), d = 1.37392, its = 20
i = 6628 (of 10192), d = 37, its = 54
i = 6629 (of 10192), d = 37, its = 53
i = 6630 (of 10192), d = 37, its = 43
i = 6631 (of 10192), d = 37, its = 50
i = 6632 (of 10192), d = 8.25943, its = 3
i = 6633 (of 10192), d = 37, its = 53
i = 6634 (of 10192), d = 37, its = 53
i = 6635 (of 10192), d = 37, its = 52
i = 6636 (of 10192), d = 37, its = 37
i = 6637 (of 10192), d = 37, its = 54
i = 6638 (of 10192), d = 37, its = 51
i = 6639 (of 10192), d = 37, its = 50
i = 6640 (of 10192), d = 7.23781, its = 4
i = 6641 (of 10192), d = 37, its = 54
i = 6642 (of 10192), d = 37, its = 41
i = 6643 (of 10192), d = 37, its = 57
i = 6644 (of 10192), d = 37, its = 55
i = 6645 (of 10192), d = 37, its = 51
i = 6646 (of 10192), d = 1.60321, its = 22
i = 6647 (of 10192), d = 37, its = 51
i = 6648 (of 10192), d = 1.81829, its = 17
i = 6649 (of 10192), d = 37, its = 52
i = 6650 (of 10192), d = 37, its = 50
i = 6651 (of 10192), d = 6.43967, its = 6
i = 6652 (of 10192), d = 37, its = 41
i = 6653 (of 10192), d = 37, its = 54
i = 6654 (of 10192), d = 37, its = 37
i = 6655 (of 10192), d = 37, its = 49
i = 6656 (of 10192), d = 37, its = 49
i = 6657 (of 10192), d = 16.1853, its = 7
i = 6658 (of 10192), d = 37, its = 51
i = 6659 (of 10192), d = 9.40262, its = 4
i = 6660 (of 10192), d = 37, its = 50
i = 6661 (of 10192), d = 37, its = 37
i = 6662 (of 10192), d = 37, its = 53
i = 6663 (of 10192), d = 9.07172, its = 4
i = 6664 (of 10192), d = 4.58623, its = 24
i = 6665 (of 10192), d = 37, its = 37
i = 6666 (of 10192), d = 37, its = 49
i = 6667 (of 10192), d = 37, its = 48
i = 6668 (of 10192), d = 37, its = 37
i = 6669 (of 10192), d = 37, its = 54
i = 6670 (of 10192), d = 37, its = 40
i = 6671 (of 10192), d = 1.76785, its = 17
i = 6672 (of 10192), d = 37, its = 57
i = 6673 (of 10192), d = 12.9228, its = 7
i = 6674 (of 10192), d = 37, its = 55
i = 6675 (of 10192), d = 37, its = 37
i = 6676 (of 10192), d = 37, its = 52
i = 6677 (of 10192), d = 37, its = 40
i = 6678 (of 10192), d = 8.17051, its = 3
i = 6679 (of 10192), d = 37, its = 53
i = 6680 (of 10192), d = 1.51194, its = 23
i = 6681 (of 10192), d = 21.943, its = 8
i = 6682 (of 10192), d = 37, its = 52
i = 6683 (of 10192), d = 37, its = 52
i = 6684 (of 10192), d = 37, its = 49
i = 6685 (of 10192), d = 2.0926, its = 21
i = 6686 (of 10192), d = 37, its = 41
i = 6687 (of 10192), d = 37, its = 54
i = 6688 (of 10192), d = 37, its = 53
i = 6689 (of 10192), d = 10.8361, its = 5
i = 6690 (of 10192), d = 37, its = 56
i = 6691 (of 10192), d = 37, its = 38
i = 6692 (of 10192), d = 37, its = 49
i = 6693 (of 10192), d = 37, its = 51
i = 6694 (of 10192), d = 37, its = 52
i = 6695 (of 10192), d = 37, its = 40
i = 6696 (of 10192), d = 14.2435, its = 6
i = 6697 (of 10192), d = 37, its = 54
i = 6698 (of 10192), d = 37, its = 52
i = 6699 (of 10192), d = 37, its = 40
i = 6700 (of 10192), d = 37, its = 53
i = 6701 (of 10192), d = 37, its = 56
i = 6702 (of 10192), d = 37, its = 38
i = 6703 (of 10192), d = 2.8041, its = 21
i = 6704 (of 10192), d = 37, its = 49
i = 6705 (of 10192), d = 37, its = 55
i = 6706 (of 10192), d = 37, its = 50
i = 6707 (of 10192), d = 37, its = 56
i = 6708 (of 10192), d = 37, its = 37
i = 6709 (of 10192), d = 10.8161, its = 6
i = 6710 (of 10192), d = 37, its = 55
i = 6711 (of 10192), d = 37, its = 54
i = 6712 (of 10192), d = 37, its = 49
i = 6713 (of 10192), d = 2.74412, its = 20
i = 6714 (of 10192), d = 6.54968, its = 6
i = 6715 (of 10192), d = 37, its = 52
i = 6716 (of 10192), d = 37, its = 37
i = 6717 (of 10192), d = 2.07427, its = 17
i = 6718 (of 10192), d = 37, its = 52
i = 6719 (of 10192), d = 37, its = 51
i = 6720 (of 10192), d = 37, its = 50
i = 6721 (of 10192), d = 37, its = 54
i = 6722 (of 10192), d = 37, its = 51
i = 6723 (of 10192), d = 7.16355, its = 4
i = 6724 (of 10192), d = 6.14877, its = 6
i = 6725 (of 10192), d = 37, its = 37
i = 6726 (of 10192), d = 5.59973, its = 6
i = 6727 (of 10192), d = 8.66233, its = 4
i = 6728 (of 10192), d = 2.15989, its = 17
i = 6729 (of 10192), d = 5.85187, its = 6
i = 6730 (of 10192), d = 37, its = 54
i = 6731 (of 10192), d = 1.22083, its = 20
i = 6732 (of 10192), d = 9.16033, its = 4
i = 6733 (of 10192), d = 37, its = 49
i = 6734 (of 10192), d = 37, its = 52
i = 6735 (of 10192), d = 37, its = 54
i = 6736 (of 10192), d = 2.98673, its = 17
i = 6737 (of 10192), d = 7.05264, its = 5
i = 6738 (of 10192), d = 3.10134, its = 22
i = 6739 (of 10192), d = 2.12089, its = 19
i = 6740 (of 10192), d = 37, its = 47
i = 6741 (of 10192), d = 3.0676, its = 25
i = 6742 (of 10192), d = 37, its = 37
i = 6743 (of 10192), d = 2.48123, its = 17
i = 6744 (of 10192), d = 37, its = 37
i = 6745 (of 10192), d = 2.83361, its = 18
i = 6746 (of 10192), d = 37, its = 37
i = 6747 (of 10192), d = 37, its = 39
i = 6748 (of 10192), d = 37, its = 55
i = 6749 (of 10192), d = 37, its = 54
i = 6750 (of 10192), d = 11.8598, its = 6
i = 6751 (of 10192), d = 4.52882, its = 27
i = 6752 (of 10192), d = 37, its = 55
i = 6753 (of 10192), d = 7.06708, its = 5
i = 6754 (of 10192), d = 10.2571, its = 5
i = 6755 (of 10192), d = 37, its = 52
i = 6756 (of 10192), d = 13.5104, its = 6
i = 6757 (of 10192), d = 37, its = 39
i = 6758 (of 10192), d = 37, its = 50
i = 6759 (of 10192), d = 37, its = 37
i = 6760 (of 10192), d = 37, its = 56
i = 6761 (of 10192), d = 6.5998, its = 7
i = 6762 (of 10192), d = 37, its = 55
i = 6763 (of 10192), d = 5.30263, its = 25
i = 6764 (of 10192), d = 37, its = 54
i = 6765 (of 10192), d = 37, its = 49
i = 6766 (of 10192), d = 37, its = 37
i = 6767 (of 10192), d = 37, its = 37
i = 6768 (of 10192), d = 37, its = 47
i = 6769 (of 10192), d = 37, its = 53
i = 6770 (of 10192), d = 37, its = 54
i = 6771 (of 10192), d = 37, its = 54
i = 6772 (of 10192), d = 8.17244, its = 3
i = 6773 (of 10192), d = 37, its = 61
i = 6774 (of 10192), d = 5.73843, its = 6
i = 6775 (of 10192), d = 37, its = 52
i = 6776 (of 10192), d = 11.2323, its = 5
i = 6777 (of 10192), d = 3.60145, its = 21
i = 6778 (of 10192), d = 37, its = 39
i = 6779 (of 10192), d = 37, its = 55
i = 6780 (of 10192), d = 37, its = 51
i = 6781 (of 10192), d = 9.03719, its = 5
i = 6782 (of 10192), d = 9.06037, its = 4
i = 6783 (of 10192), d = 37, its = 54
i = 6784 (of 10192), d = 37, its = 45
i = 6785 (of 10192), d = 37, its = 51
i = 6786 (of 10192), d = 37, its = 54
i = 6787 (of 10192), d = 0.811525, its = 26
i = 6788 (of 10192), d = 37, its = 41
i = 6789 (of 10192), d = 37, its = 54
i = 6790 (of 10192), d = 37, its = 37
i = 6791 (of 10192), d = 3.48124, its = 23
i = 6792 (of 10192), d = 37, its = 57
i = 6793 (of 10192), d = 7.05269, its = 5
i = 6794 (of 10192), d = 8.62441, its = 4
i = 6795 (of 10192), d = 37, its = 53
i = 6796 (of 10192), d = 8.75179, its = 4
i = 6797 (of 10192), d = 37, its = 37
i = 6798 (of 10192), d = 8.32868, its = 3
i = 6799 (of 10192), d = 37, its = 37
i = 6800 (of 10192), d = 10.78, its = 8
i = 6801 (of 10192), d = 37, its = 41
i = 6802 (of 10192), d = 16.6501, its = 8
i = 6803 (of 10192), d = 37, its = 39
i = 6804 (of 10192), d = 37, its = 52
i = 6805 (of 10192), d = 37, its = 51
i = 6806 (of 10192), d = 10.1926, its = 5
i = 6807 (of 10192), d = 4.69376, its = 21
i = 6808 (of 10192), d = 7.99609, its = 2
i = 6809 (of 10192), d = 37, its = 47
i = 6810 (of 10192), d = 37, its = 54
i = 6811 (of 10192), d = 37, its = 56
i = 6812 (of 10192), d = 37, its = 43
i = 6813 (of 10192), d = 37, its = 37
i = 6814 (of 10192), d = 8.51201, its = 4
i = 6815 (of 10192), d = 7.97595, its = 2
i = 6816 (of 10192), d = 37, its = 41
i = 6817 (of 10192), d = 11.3637, its = 7
i = 6818 (of 10192), d = 37, its = 38
i = 6819 (of 10192), d = 12.1907, its = 6
i = 6820 (of 10192), d = 37, its = 51
i = 6821 (of 10192), d = 37, its = 41
i = 6822 (of 10192), d = 6.93237, its = 5
i = 6823 (of 10192), d = 37, its = 37
i = 6824 (of 10192), d = 37, its = 50
i = 6825 (of 10192), d = 37, its = 40
i = 6826 (of 10192), d = 37, its = 52
i = 6827 (of 10192), d = 37, its = 50
i = 6828 (of 10192), d = 37, its = 37
i = 6829 (of 10192), d = 37, its = 51
i = 6830 (of 10192), d = 37, its = 51
i = 6831 (of 10192), d = 37, its = 40
i = 6832 (of 10192), d = 37, its = 40
i = 6833 (of 10192), d = 37, its = 39
i = 6834 (of 10192), d = 37, its = 37
i = 6835 (of 10192), d = 10.1641, its = 6
i = 6836 (of 10192), d = 9.02055, its = 4
i = 6837 (of 10192), d = 37, its = 37
i = 6838 (of 10192), d = 37, its = 37
i = 6839 (of 10192), d = 4.80851, its = 7
i = 6840 (of 10192), d = 37, its = 53
i = 6841 (of 10192), d = 37, its = 55
i = 6842 (of 10192), d = 37, its = 53
i = 6843 (of 10192), d = 11.1212, its = 5
i = 6844 (of 10192), d = 6.76634, its = 5
i = 6845 (of 10192), d = 37, its = 56
i = 6846 (of 10192), d = 37, its = 37
i = 6847 (of 10192), d = 37, its = 52
i = 6848 (of 10192), d = 8.60041, its = 4
i = 6849 (of 10192), d = 3.83618, its = 19
i = 6850 (of 10192), d = 4.09701, its = 22
i = 6851 (of 10192), d = 37, its = 51
i = 6852 (of 10192), d = 5.31691, its = 6
i = 6853 (of 10192), d = 9.54151, its = 4
i = 6854 (of 10192), d = 37, its = 51
i = 6855 (of 10192), d = 37, its = 57
i = 6856 (of 10192), d = 10.2957, its = 6
i = 6857 (of 10192), d = 12.057, its = 6
i = 6858 (of 10192), d = 37, its = 41
i = 6859 (of 10192), d = 5.13564, its = 6
i = 6860 (of 10192), d = 37, its = 54
i = 6861 (of 10192), d = 37, its = 37
i = 6862 (of 10192), d = 37, its = 51
i = 6863 (of 10192), d = 2.77467, its = 17
i = 6864 (of 10192), d = 37, its = 53
i = 6865 (of 10192), d = 37, its = 38
i = 6866 (of 10192), d = 37, its = 51
i = 6867 (of 10192), d = 37, its = 42
i = 6868 (of 10192), d = 37, its = 54
i = 6869 (of 10192), d = 7.25427, its = 5
i = 6870 (of 10192), d = 37, its = 37
i = 6871 (of 10192), d = 10.6299, its = 5
i = 6872 (of 10192), d = 37, its = 40
i = 6873 (of 10192), d = 37, its = 39
i = 6874 (of 10192), d = 37, its = 50
i = 6875 (of 10192), d = 37, its = 54
i = 6876 (of 10192), d = 9.79444, its = 5
i = 6877 (of 10192), d = 37, its = 41
i = 6878 (of 10192), d = 37, its = 53
i = 6879 (of 10192), d = 37, its = 51
i = 6880 (of 10192), d = 37, its = 53
i = 6881 (of 10192), d = 7.22703, its = 4
i = 6882 (of 10192), d = 37, its = 37
i = 6883 (of 10192), d = 8.09311, its = 3
i = 6884 (of 10192), d = 4.13752, its = 20
i = 6885 (of 10192), d = 5.81213, its = 8
i = 6886 (of 10192), d = 4.06839, its = 17
i = 6887 (of 10192), d = 37, its = 54
i = 6888 (of 10192), d = 0.926551, its = 22
i = 6889 (of 10192), d = 37, its = 55
i = 6890 (of 10192), d = 5.70793, its = 6
i = 6891 (of 10192), d = 8.06679, its = 3
i = 6892 (of 10192), d = 37, its = 53
i = 6893 (of 10192), d = 37, its = 52
i = 6894 (of 10192), d = 37, its = 52
i = 6895 (of 10192), d = 37, its = 51
i = 6896 (of 10192), d = 37, its = 57
i = 6897 (of 10192), d = 37, its = 48
i = 6898 (of 10192), d = 37, its = 39
i = 6899 (of 10192), d = 37, its = 40
i = 6900 (of 10192), d = 37, its = 39
i = 6901 (of 10192), d = 13.103, its = 6
i = 6902 (of 10192), d = 37, its = 38
i = 6903 (of 10192), d = 37, its = 51
i = 6904 (of 10192), d = 37, its = 37
i = 6905 (of 10192), d = 1.91299, its = 18
i = 6906 (of 10192), d = 6.22066, its = 6
i = 6907 (of 10192), d = 11.7836, its = 6
i = 6908 (of 10192), d = 37, its = 58
i = 6909 (of 10192), d = 37, its = 37
i = 6910 (of 10192), d = 37, its = 50
i = 6911 (of 10192), d = 37, its = 56
i = 6912 (of 10192), d = 7.88171, its = 3
i = 6913 (of 10192), d = 37, its = 37
i = 6914 (of 10192), d = 37, its = 49
i = 6915 (of 10192), d = 37, its = 55
i = 6916 (of 10192), d = 3.14955, its = 17
i = 6917 (of 10192), d = 3.861, its = 19
i = 6918 (of 10192), d = 37, its = 54
i = 6919 (of 10192), d = 37, its = 52
i = 6920 (of 10192), d = 7.61305, its = 4
i = 6921 (of 10192), d = 37, its = 37
i = 6922 (of 10192), d = 37, its = 41
i = 6923 (of 10192), d = 3.29546, its = 21
i = 6924 (of 10192), d = 0.931102, its = 18
i = 6925 (of 10192), d = 37, its = 37
i = 6926 (of 10192), d = 2.09073, its = 27
i = 6927 (of 10192), d = 3.30632, its = 22
i = 6928 (of 10192), d = 37, its = 41
i = 6929 (of 10192), d = 37, its = 56
i = 6930 (of 10192), d = 37, its = 52
i = 6931 (of 10192), d = 37, its = 37
i = 6932 (of 10192), d = 37, its = 37
i = 6933 (of 10192), d = 37, its = 55
i = 6934 (of 10192), d = 37, its = 40
i = 6935 (of 10192), d = 37, its = 52
i = 6936 (of 10192), d = 37, its = 50
i = 6937 (of 10192), d = 9.01498, its = 4
i = 6938 (of 10192), d = 37, its = 51
i = 6939 (of 10192), d = 37, its = 54
i = 6940 (of 10192), d = 37, its = 55
i = 6941 (of 10192), d = 37, its = 54
i = 6942 (of 10192), d = 37, its = 53
i = 6943 (of 10192), d = 37, its = 48
i = 6944 (of 10192), d = 37, its = 50
i = 6945 (of 10192), d = 3.27711, its = 16
i = 6946 (of 10192), d = 37, its = 51
i = 6947 (of 10192), d = 12.5449, its = 6
i = 6948 (of 10192), d = 5.68983, its = 7
i = 6949 (of 10192), d = 37, its = 53
i = 6950 (of 10192), d = 37, its = 51
i = 6951 (of 10192), d = 9.59835, its = 5
i = 6952 (of 10192), d = 12.3074, its = 6
i = 6953 (of 10192), d = 37, its = 44
i = 6954 (of 10192), d = 37, its = 50
i = 6955 (of 10192), d = 37, its = 37
i = 6956 (of 10192), d = 37, its = 37
i = 6957 (of 10192), d = 37, its = 41
i = 6958 (of 10192), d = 37, its = 40
i = 6959 (of 10192), d = 37, its = 55
i = 6960 (of 10192), d = 37, its = 40
i = 6961 (of 10192), d = 37, its = 55
i = 6962 (of 10192), d = 8.23389, its = 3
i = 6963 (of 10192), d = 37, its = 54
i = 6964 (of 10192), d = 23.9187, its = 7
i = 6965 (of 10192), d = 37, its = 50
i = 6966 (of 10192), d = 37, its = 54
i = 6967 (of 10192), d = 3.26504, its = 17
i = 6968 (of 10192), d = 4.96552, its = 29
i = 6969 (of 10192), d = 37, its = 37
i = 6970 (of 10192), d = 37, its = 53
i = 6971 (of 10192), d = 37, its = 40
i = 6972 (of 10192), d = 4.20073, its = 21
i = 6973 (of 10192), d = 37, its = 49
i = 6974 (of 10192), d = 37, its = 54
i = 6975 (of 10192), d = 37, its = 39
i = 6976 (of 10192), d = 37, its = 51
i = 6977 (of 10192), d = 37, its = 55
i = 6978 (of 10192), d = 37, its = 37
i = 6979 (of 10192), d = 4.02003, its = 18
i = 6980 (of 10192), d = 37, its = 39
i = 6981 (of 10192), d = 1.97758, its = 23
i = 6982 (of 10192), d = 37, its = 39
i = 6983 (of 10192), d = 37, its = 37
i = 6984 (of 10192), d = 2.3804, its = 18
i = 6985 (of 10192), d = 8.56677, its = 4
i = 6986 (of 10192), d = 37, its = 50
i = 6987 (of 10192), d = 37, its = 37
i = 6988 (of 10192), d = 37, its = 44
i = 6989 (of 10192), d = 9.16259, its = 4
i = 6990 (of 10192), d = 37, its = 37
i = 6991 (of 10192), d = 37, its = 37
i = 6992 (of 10192), d = 37, its = 55
i = 6993 (of 10192), d = 37, its = 54
i = 6994 (of 10192), d = 33.9461, its = 9
i = 6995 (of 10192), d = 37, its = 52
i = 6996 (of 10192), d = 37, its = 49
i = 6997 (of 10192), d = 5.96761, its = 6
i = 6998 (of 10192), d = 7.10997, its = 5
i = 6999 (of 10192), d = 37, its = 53
i = 7000 (of 10192), d = 37, its = 40
i = 7001 (of 10192), d = 37, its = 52
i = 7002 (of 10192), d = 37, its = 52
i = 7003 (of 10192), d = 37, its = 53
i = 7004 (of 10192), d = 37, its = 39
i = 7005 (of 10192), d = 37, its = 50
i = 7006 (of 10192), d = 37, its = 40
i = 7007 (of 10192), d = 37, its = 56
i = 7008 (of 10192), d = 37, its = 51
i = 7009 (of 10192), d = 37, its = 39
i = 7010 (of 10192), d = 4.75422, its = 7
i = 7011 (of 10192), d = 37, its = 53
i = 7012 (of 10192), d = 37, its = 55
i = 7013 (of 10192), d = 2.74062, its = 17
i = 7014 (of 10192), d = 11.5697, its = 5
i = 7015 (of 10192), d = 37, its = 37
i = 7016 (of 10192), d = 3.54017, its = 19
i = 7017 (of 10192), d = 37, its = 39
i = 7018 (of 10192), d = 5.27265, its = 6
i = 7019 (of 10192), d = 37, its = 37
i = 7020 (of 10192), d = 37, its = 37
i = 7021 (of 10192), d = 37, its = 53
i = 7022 (of 10192), d = 6.74152, its = 7
i = 7023 (of 10192), d = 37, its = 40
i = 7024 (of 10192), d = 37, its = 44
i = 7025 (of 10192), d = 37, its = 57
i = 7026 (of 10192), d = 37, its = 37
i = 7027 (of 10192), d = 6.0087, its = 6
i = 7028 (of 10192), d = 37, its = 41
i = 7029 (of 10192), d = 37, its = 51
i = 7030 (of 10192), d = 37, its = 37
i = 7031 (of 10192), d = 9.56491, its = 5
i = 7032 (of 10192), d = 37, its = 40
i = 7033 (of 10192), d = 1.59741, its = 18
i = 7034 (of 10192), d = 37, its = 55
i = 7035 (of 10192), d = 1.93183, its = 20
i = 7036 (of 10192), d = 2.72534, its = 17
i = 7037 (of 10192), d = 37, its = 52
i = 7038 (of 10192), d = 37, its = 55
i = 7039 (of 10192), d = 6.66836, its = 5
i = 7040 (of 10192), d = 37, its = 37
i = 7041 (of 10192), d = 2.94809, its = 18
i = 7042 (of 10192), d = 37, its = 47
i = 7043 (of 10192), d = 37, its = 42
i = 7044 (of 10192), d = 37, its = 50
i = 7045 (of 10192), d = 37, its = 52
i = 7046 (of 10192), d = 4.17546, its = 19
i = 7047 (of 10192), d = 37, its = 50
i = 7048 (of 10192), d = 37, its = 54
i = 7049 (of 10192), d = 37, its = 51
i = 7050 (of 10192), d = 37, its = 39
i = 7051 (of 10192), d = 37, its = 40
i = 7052 (of 10192), d = 37, its = 55
i = 7053 (of 10192), d = 7.4282, its = 4
i = 7054 (of 10192), d = 37, its = 39
i = 7055 (of 10192), d = 37, its = 55
i = 7056 (of 10192), d = 37, its = 52
i = 7057 (of 10192), d = 7.55289, its = 4
i = 7058 (of 10192), d = 37, its = 55
i = 7059 (of 10192), d = 37, its = 53
i = 7060 (of 10192), d = 37, its = 53
i = 7061 (of 10192), d = 37, its = 53
i = 7062 (of 10192), d = 37, its = 53
i = 7063 (of 10192), d = 37, its = 50
i = 7064 (of 10192), d = 37, its = 53
i = 7065 (of 10192), d = 37, its = 40
i = 7066 (of 10192), d = 37, its = 53
i = 7067 (of 10192), d = 2.56994, its = 17
i = 7068 (of 10192), d = 13.1288, its = 6
i = 7069 (of 10192), d = 37, its = 40
i = 7070 (of 10192), d = 26.5172, its = 8
i = 7071 (of 10192), d = 4.15756, its = 25
i = 7072 (of 10192), d = 7.67162, its = 4
i = 7073 (of 10192), d = 14.7566, its = 7
i = 7074 (of 10192), d = 9.20272, its = 4
i = 7075 (of 10192), d = 7.91364, its = 3
i = 7076 (of 10192), d = 37, its = 37
i = 7077 (of 10192), d = 7.78212, its = 5
i = 7078 (of 10192), d = 37, its = 53
i = 7079 (of 10192), d = 37, its = 50
i = 7080 (of 10192), d = 16.1435, its = 7
i = 7081 (of 10192), d = 37, its = 40
i = 7082 (of 10192), d = 37, its = 52
i = 7083 (of 10192), d = 4.78607, its = 19
i = 7084 (of 10192), d = 37, its = 51
i = 7085 (of 10192), d = 37, its = 52
i = 7086 (of 10192), d = 8.06249, its = 3
i = 7087 (of 10192), d = 37, its = 48
i = 7088 (of 10192), d = 37, its = 52
i = 7089 (of 10192), d = 37, its = 52
i = 7090 (of 10192), d = 37, its = 55
i = 7091 (of 10192), d = 37, its = 57
i = 7092 (of 10192), d = 37, its = 40
i = 7093 (of 10192), d = 37, its = 37
i = 7094 (of 10192), d = 7.55747, its = 4
i = 7095 (of 10192), d = 11.0367, its = 6
i = 7096 (of 10192), d = 37, its = 52
i = 7097 (of 10192), d = 37, its = 52
i = 7098 (of 10192), d = 37, its = 55
i = 7099 (of 10192), d = 37, its = 54
i = 7100 (of 10192), d = 37, its = 37
i = 7101 (of 10192), d = 37, its = 54
i = 7102 (of 10192), d = 4.7084, its = 6
i = 7103 (of 10192), d = 37, its = 51
i = 7104 (of 10192), d = 37, its = 52
i = 7105 (of 10192), d = 4.52882, its = 27
i = 7106 (of 10192), d = 37, its = 57
i = 7107 (of 10192), d = 5.2332, its = 6
i = 7108 (of 10192), d = 37, its = 50
i = 7109 (of 10192), d = 12.4801, its = 6
i = 7110 (of 10192), d = 2.95978, its = 18
i = 7111 (of 10192), d = 37, its = 54
i = 7112 (of 10192), d = 37, its = 49
i = 7113 (of 10192), d = 37, its = 49
i = 7114 (of 10192), d = 37, its = 49
i = 7115 (of 10192), d = 1.37536, its = 20
i = 7116 (of 10192), d = 6.82462, its = 6
i = 7117 (of 10192), d = 37, its = 37
i = 7118 (of 10192), d = 37, its = 55
i = 7119 (of 10192), d = 37, its = 50
i = 7120 (of 10192), d = 3.59335, its = 17
i = 7121 (of 10192), d = 37, its = 39
i = 7122 (of 10192), d = 37, its = 40
i = 7123 (of 10192), d = 37, its = 49
i = 7124 (of 10192), d = 37, its = 37
i = 7125 (of 10192), d = 9.11938, its = 4
i = 7126 (of 10192), d = 37, its = 53
i = 7127 (of 10192), d = 37, its = 53
i = 7128 (of 10192), d = 13.0978, its = 8
i = 7129 (of 10192), d = 2.84351, its = 20
i = 7130 (of 10192), d = 37, its = 51
i = 7131 (of 10192), d = 3.91506, its = 19
i = 7132 (of 10192), d = 37, its = 53
i = 7133 (of 10192), d = 37, its = 53
i = 7134 (of 10192), d = 6.70414, its = 6
i = 7135 (of 10192), d = 37, its = 52
i = 7136 (of 10192), d = 9.20129, its = 5
i = 7137 (of 10192), d = 37, its = 56
i = 7138 (of 10192), d = 37, its = 40
i = 7139 (of 10192), d = 37, its = 57
i = 7140 (of 10192), d = 3.82086, its = 17
i = 7141 (of 10192), d = 37, its = 41
i = 7142 (of 10192), d = 5.70096, its = 6
i = 7143 (of 10192), d = 37, its = 54
i = 7144 (of 10192), d = 37, its = 52
i = 7145 (of 10192), d = 37, its = 56
i = 7146 (of 10192), d = 37, its = 56
i = 7147 (of 10192), d = 37, its = 53
i = 7148 (of 10192), d = 37, its = 53
i = 7149 (of 10192), d = 8.18664, its = 3
i = 7150 (of 10192), d = 37, its = 52
i = 7151 (of 10192), d = 37, its = 55
i = 7152 (of 10192), d = 37, its = 40
i = 7153 (of 10192), d = 37, its = 58
i = 7154 (of 10192), d = 11.1819, its = 6
i = 7155 (of 10192), d = 37, its = 37
i = 7156 (of 10192), d = 37, its = 51
i = 7157 (of 10192), d = 37, its = 41
i = 7158 (of 10192), d = 29.2779, its = 8
i = 7159 (of 10192), d = 37, its = 52
i = 7160 (of 10192), d = 3.31085, its = 19
i = 7161 (of 10192), d = 37, its = 48
i = 7162 (of 10192), d = 37, its = 55
i = 7163 (of 10192), d = 15.2401, its = 6
i = 7164 (of 10192), d = 37, its = 50
i = 7165 (of 10192), d = 37, its = 49
i = 7166 (of 10192), d = 37, its = 40
i = 7167 (of 10192), d = 1.6122, its = 20
i = 7168 (of 10192), d = 37, its = 55
i = 7169 (of 10192), d = 1.59741, its = 18
i = 7170 (of 10192), d = 37, its = 54
i = 7171 (of 10192), d = 37, its = 39
i = 7172 (of 10192), d = 37, its = 48
i = 7173 (of 10192), d = 37, its = 53
i = 7174 (of 10192), d = 4.1422, its = 18
i = 7175 (of 10192), d = 37, its = 51
i = 7176 (of 10192), d = 37, its = 56
i = 7177 (of 10192), d = 3.45151, its = 22
i = 7178 (of 10192), d = 19.9109, its = 20
i = 7179 (of 10192), d = 37, its = 50
i = 7180 (of 10192), d = 8.74226, its = 5
i = 7181 (of 10192), d = 37, its = 54
i = 7182 (of 10192), d = 37, its = 37
i = 7183 (of 10192), d = 2.82925, its = 21
i = 7184 (of 10192), d = 12.4703, its = 7
i = 7185 (of 10192), d = 37, its = 37
i = 7186 (of 10192), d = 1.24144, its = 18
i = 7187 (of 10192), d = 37, its = 37
i = 7188 (of 10192), d = 37, its = 51
i = 7189 (of 10192), d = 37, its = 50
i = 7190 (of 10192), d = 37, its = 38
i = 7191 (of 10192), d = 5.22612, its = 6
i = 7192 (of 10192), d = 37, its = 44
i = 7193 (of 10192), d = 37, its = 56
i = 7194 (of 10192), d = 37, its = 50
i = 7195 (of 10192), d = 9.16258, its = 4
i = 7196 (of 10192), d = 13.7132, its = 7
i = 7197 (of 10192), d = 37, its = 52
i = 7198 (of 10192), d = 37, its = 52
i = 7199 (of 10192), d = 37, its = 53
i = 7200 (of 10192), d = 37, its = 52
i = 7201 (of 10192), d = 37, its = 53
i = 7202 (of 10192), d = 37, its = 55
i = 7203 (of 10192), d = 37, its = 56
i = 7204 (of 10192), d = 3.57024, its = 26
i = 7205 (of 10192), d = 37, its = 53
i = 7206 (of 10192), d = 5.82611, its = 6
i = 7207 (of 10192), d = 37, its = 37
i = 7208 (of 10192), d = 37, its = 61
i = 7209 (of 10192), d = 37, its = 52
i = 7210 (of 10192), d = 11.9351, its = 6
i = 7211 (of 10192), d = 11.8179, its = 6
i = 7212 (of 10192), d = 37, its = 54
i = 7213 (of 10192), d = 37, its = 49
i = 7214 (of 10192), d = 37, its = 37
i = 7215 (of 10192), d = 37, its = 41
i = 7216 (of 10192), d = 37, its = 37
i = 7217 (of 10192), d = 37, its = 54
i = 7218 (of 10192), d = 1.98831, its = 19
i = 7219 (of 10192), d = 37, its = 37
i = 7220 (of 10192), d = 37, its = 37
i = 7221 (of 10192), d = 37, its = 37
i = 7222 (of 10192), d = 37, its = 41
i = 7223 (of 10192), d = 37, its = 37
i = 7224 (of 10192), d = 37, its = 52
i = 7225 (of 10192), d = 37, its = 52
i = 7226 (of 10192), d = 37, its = 52
i = 7227 (of 10192), d = 37, its = 53
i = 7228 (of 10192), d = 37, its = 52
i = 7229 (of 10192), d = 37, its = 37
i = 7230 (of 10192), d = 16.5518, its = 7
i = 7231 (of 10192), d = 37, its = 55
i = 7232 (of 10192), d = 37, its = 54
i = 7233 (of 10192), d = 6.52028, its = 5
i = 7234 (of 10192), d = 37, its = 54
i = 7235 (of 10192), d = 37, its = 55
i = 7236 (of 10192), d = 16.3037, its = 6
i = 7237 (of 10192), d = 37, its = 40
i = 7238 (of 10192), d = 8.12533, its = 3
i = 7239 (of 10192), d = 3.73106, its = 17
i = 7240 (of 10192), d = 32.943, its = 9
i = 7241 (of 10192), d = 37, its = 50
i = 7242 (of 10192), d = 37, its = 39
i = 7243 (of 10192), d = 37, its = 56
i = 7244 (of 10192), d = 37, its = 37
i = 7245 (of 10192), d = 3.22554, its = 17
i = 7246 (of 10192), d = 37, its = 53
i = 7247 (of 10192), d = 37, its = 37
i = 7248 (of 10192), d = 2.65271, its = 16
i = 7249 (of 10192), d = 37, its = 50
i = 7250 (of 10192), d = 37, its = 54
i = 7251 (of 10192), d = 37, its = 55
i = 7252 (of 10192), d = 2.49581, its = 21
i = 7253 (of 10192), d = 4.61481, its = 19
i = 7254 (of 10192), d = 1.34761, its = 18
i = 7255 (of 10192), d = 37, its = 37
i = 7256 (of 10192), d = 37, its = 40
i = 7257 (of 10192), d = 37, its = 56
i = 7258 (of 10192), d = 5.96818, its = 7
i = 7259 (of 10192), d = 37, its = 49
i = 7260 (of 10192), d = 1.97064, its = 22
i = 7261 (of 10192), d = 12.7684, its = 7
i = 7262 (of 10192), d = 10.9912, its = 6
i = 7263 (of 10192), d = 21.9297, its = 25
i = 7264 (of 10192), d = 37, its = 49
i = 7265 (of 10192), d = 37, its = 37
i = 7266 (of 10192), d = 37, its = 51
i = 7267 (of 10192), d = 9.90159, its = 5
i = 7268 (of 10192), d = 37, its = 54
i = 7269 (of 10192), d = 37, its = 37
i = 7270 (of 10192), d = 37, its = 53
i = 7271 (of 10192), d = 37, its = 56
i = 7272 (of 10192), d = 7.43206, its = 4
i = 7273 (of 10192), d = 37, its = 50
i = 7274 (of 10192), d = 37, its = 54
i = 7275 (of 10192), d = 37, its = 37
i = 7276 (of 10192), d = 37, its = 42
i = 7277 (of 10192), d = 37, its = 52
i = 7278 (of 10192), d = 37, its = 37
i = 7279 (of 10192), d = 37, its = 37
i = 7280 (of 10192), d = 37, its = 51
i = 7281 (of 10192), d = 37, its = 53
i = 7282 (of 10192), d = 7.04317, its = 5
i = 7283 (of 10192), d = 37, its = 56
i = 7284 (of 10192), d = 37, its = 55
i = 7285 (of 10192), d = 3.93216, its = 18
i = 7286 (of 10192), d = 37, its = 51
i = 7287 (of 10192), d = 37, its = 39
i = 7288 (of 10192), d = 37, its = 55
i = 7289 (of 10192), d = 37, its = 42
i = 7290 (of 10192), d = 37, its = 53
i = 7291 (of 10192), d = 37, its = 52
i = 7292 (of 10192), d = 6.04557, its = 6
i = 7293 (of 10192), d = 3.76505, its = 28
i = 7294 (of 10192), d = 2.04952, its = 16
i = 7295 (of 10192), d = 37, its = 39
i = 7296 (of 10192), d = 9.32921, its = 4
i = 7297 (of 10192), d = 10.1104, its = 5
i = 7298 (of 10192), d = 21.0859, its = 7
i = 7299 (of 10192), d = 37, its = 51
i = 7300 (of 10192), d = 11.7605, its = 5
i = 7301 (of 10192), d = 37, its = 53
i = 7302 (of 10192), d = 37, its = 53
i = 7303 (of 10192), d = 6.72125, its = 6
i = 7304 (of 10192), d = 37, its = 37
i = 7305 (of 10192), d = 1.86206, its = 17
i = 7306 (of 10192), d = 37, its = 52
i = 7307 (of 10192), d = 9.65946, its = 5
i = 7308 (of 10192), d = 3.2182, its = 19
i = 7309 (of 10192), d = 37, its = 53
i = 7310 (of 10192), d = 37, its = 51
i = 7311 (of 10192), d = 7.2901, its = 4
i = 7312 (of 10192), d = 37, its = 37
i = 7313 (of 10192), d = 10.9964, its = 5
i = 7314 (of 10192), d = 37, its = 37
i = 7315 (of 10192), d = 37, its = 57
i = 7316 (of 10192), d = 37, its = 54
i = 7317 (of 10192), d = 37, its = 55
i = 7318 (of 10192), d = 37, its = 56
i = 7319 (of 10192), d = 37, its = 37
i = 7320 (of 10192), d = 4.15064, its = 19
i = 7321 (of 10192), d = 3.58172, its = 20
i = 7322 (of 10192), d = 2.34095, its = 21
i = 7323 (of 10192), d = 37, its = 52
i = 7324 (of 10192), d = 6.18539, its = 6
i = 7325 (of 10192), d = 8.91667, its = 5
i = 7326 (of 10192), d = 37, its = 37
i = 7327 (of 10192), d = 37, its = 56
i = 7328 (of 10192), d = 37, its = 37
i = 7329 (of 10192), d = 37, its = 54
i = 7330 (of 10192), d = 37, its = 50
i = 7331 (of 10192), d = 37, its = 37
i = 7332 (of 10192), d = 37, its = 54
i = 7333 (of 10192), d = 37, its = 52
i = 7334 (of 10192), d = 37, its = 40
i = 7335 (of 10192), d = 37, its = 37
i = 7336 (of 10192), d = 37, its = 53
i = 7337 (of 10192), d = 37, its = 54
i = 7338 (of 10192), d = 19.1214, its = 7
i = 7339 (of 10192), d = 10.9951, its = 5
i = 7340 (of 10192), d = 37, its = 51
i = 7341 (of 10192), d = 37, its = 53
i = 7342 (of 10192), d = 37, its = 51
i = 7343 (of 10192), d = 4.9766, its = 6
i = 7344 (of 10192), d = 37, its = 53
i = 7345 (of 10192), d = 21.7315, its = 7
i = 7346 (of 10192), d = 37, its = 41
i = 7347 (of 10192), d = 37, its = 48
i = 7348 (of 10192), d = 37, its = 43
i = 7349 (of 10192), d = 37, its = 52
i = 7350 (of 10192), d = 37, its = 53
i = 7351 (of 10192), d = 37, its = 55
i = 7352 (of 10192), d = 9.71295, its = 5
i = 7353 (of 10192), d = 37, its = 51
i = 7354 (of 10192), d = 37, its = 37
i = 7355 (of 10192), d = 1.19859, its = 22
i = 7356 (of 10192), d = 37, its = 52
i = 7357 (of 10192), d = 37, its = 37
i = 7358 (of 10192), d = 37, its = 50
i = 7359 (of 10192), d = 37, its = 39
i = 7360 (of 10192), d = 37, its = 51
i = 7361 (of 10192), d = 11.1703, its = 6
i = 7362 (of 10192), d = 37, its = 37
i = 7363 (of 10192), d = 2.83389, its = 20
i = 7364 (of 10192), d = 37, its = 55
i = 7365 (of 10192), d = 37, its = 51
i = 7366 (of 10192), d = 3.77683, its = 17
i = 7367 (of 10192), d = 37, its = 40
i = 7368 (of 10192), d = 6.70815, its = 5
i = 7369 (of 10192), d = 37, its = 42
i = 7370 (of 10192), d = 37, its = 48
i = 7371 (of 10192), d = 2.89593, its = 20
i = 7372 (of 10192), d = 3.60216, its = 18
i = 7373 (of 10192), d = 37, its = 52
i = 7374 (of 10192), d = 9.41973, its = 5
i = 7375 (of 10192), d = 3.36131, its = 21
i = 7376 (of 10192), d = 5.92224, its = 7
i = 7377 (of 10192), d = 37, its = 52
i = 7378 (of 10192), d = 37, its = 53
i = 7379 (of 10192), d = 37, its = 51
i = 7380 (of 10192), d = 37, its = 52
i = 7381 (of 10192), d = 37, its = 50
i = 7382 (of 10192), d = 3.53815, its = 18
i = 7383 (of 10192), d = 37, its = 50
i = 7384 (of 10192), d = 4.6936, its = 23
i = 7385 (of 10192), d = 37, its = 52
i = 7386 (of 10192), d = 37, its = 54
i = 7387 (of 10192), d = 37, its = 59
i = 7388 (of 10192), d = 37, its = 37
i = 7389 (of 10192), d = 37, its = 48
i = 7390 (of 10192), d = 37, its = 40
i = 7391 (of 10192), d = 4.7877, its = 19
i = 7392 (of 10192), d = 37, its = 53
i = 7393 (of 10192), d = 37, its = 58
i = 7394 (of 10192), d = 3.1763, its = 17
i = 7395 (of 10192), d = 8.54508, its = 4
i = 7396 (of 10192), d = 37, its = 48
i = 7397 (of 10192), d = 3.16916, its = 24
i = 7398 (of 10192), d = 2.58089, its = 17
i = 7399 (of 10192), d = 37, its = 44
i = 7400 (of 10192), d = 2.73377, its = 16
i = 7401 (of 10192), d = 37, its = 37
i = 7402 (of 10192), d = 4.25409, its = 21
i = 7403 (of 10192), d = 37, its = 53
i = 7404 (of 10192), d = 37, its = 37
i = 7405 (of 10192), d = 37, its = 53
i = 7406 (of 10192), d = 37, its = 50
i = 7407 (of 10192), d = 37, its = 53
i = 7408 (of 10192), d = 37, its = 53
i = 7409 (of 10192), d = 2.76681, its = 22
i = 7410 (of 10192), d = 37, its = 37
i = 7411 (of 10192), d = 9.58516, its = 5
i = 7412 (of 10192), d = 10.2993, its = 5
i = 7413 (of 10192), d = 7.4826, its = 4
i = 7414 (of 10192), d = 37, its = 55
i = 7415 (of 10192), d = 37, its = 52
i = 7416 (of 10192), d = 9.05671, its = 4
i = 7417 (of 10192), d = 3.99062, its = 19
i = 7418 (of 10192), d = 37, its = 49
i = 7419 (of 10192), d = 37, its = 51
i = 7420 (of 10192), d = 37, its = 57
i = 7421 (of 10192), d = 37, its = 54
i = 7422 (of 10192), d = 37, its = 37
i = 7423 (of 10192), d = 37, its = 51
i = 7424 (of 10192), d = 37, its = 52
i = 7425 (of 10192), d = 10.3075, its = 6
i = 7426 (of 10192), d = 37, its = 53
i = 7427 (of 10192), d = 37, its = 50
i = 7428 (of 10192), d = 37, its = 55
i = 7429 (of 10192), d = 37, its = 52
i = 7430 (of 10192), d = 37, its = 39
i = 7431 (of 10192), d = 7.90168, its = 3
i = 7432 (of 10192), d = 9.20806, its = 4
i = 7433 (of 10192), d = 37, its = 53
i = 7434 (of 10192), d = 37, its = 40
i = 7435 (of 10192), d = 37, its = 54
i = 7436 (of 10192), d = 37, its = 58
i = 7437 (of 10192), d = 37, its = 55
i = 7438 (of 10192), d = 37, its = 56
i = 7439 (of 10192), d = 37, its = 55
i = 7440 (of 10192), d = 37, its = 54
i = 7441 (of 10192), d = 6.32442, its = 7
i = 7442 (of 10192), d = 37, its = 54
i = 7443 (of 10192), d = 37, its = 51
i = 7444 (of 10192), d = 37, its = 51
i = 7445 (of 10192), d = 6.64196, its = 5
i = 7446 (of 10192), d = 11.3826, its = 7
i = 7447 (of 10192), d = 37, its = 51
i = 7448 (of 10192), d = 7.87806, its = 3
i = 7449 (of 10192), d = 37, its = 41
i = 7450 (of 10192), d = 9.09834, its = 4
i = 7451 (of 10192), d = 37, its = 49
i = 7452 (of 10192), d = 37, its = 53
i = 7453 (of 10192), d = 20.8864, its = 9
i = 7454 (of 10192), d = 5.80081, its = 6
i = 7455 (of 10192), d = 37, its = 43
i = 7456 (of 10192), d = 37, its = 37
i = 7457 (of 10192), d = 12.6307, its = 6
i = 7458 (of 10192), d = 37, its = 52
i = 7459 (of 10192), d = 37, its = 43
i = 7460 (of 10192), d = 37, its = 49
i = 7461 (of 10192), d = 37, its = 51
i = 7462 (of 10192), d = 37, its = 54
i = 7463 (of 10192), d = 7.12931, its = 5
i = 7464 (of 10192), d = 2.1922, its = 17
i = 7465 (of 10192), d = 7.65457, its = 4
i = 7466 (of 10192), d = 25.676, its = 8
i = 7467 (of 10192), d = 37, its = 53
i = 7468 (of 10192), d = 37, its = 37
i = 7469 (of 10192), d = 37, its = 49
i = 7470 (of 10192), d = 37, its = 51
i = 7471 (of 10192), d = 37, its = 54
i = 7472 (of 10192), d = 37, its = 37
i = 7473 (of 10192), d = 37, its = 37
i = 7474 (of 10192), d = 10.6099, its = 5
i = 7475 (of 10192), d = 1.90205, its = 19
i = 7476 (of 10192), d = 37, its = 50
i = 7477 (of 10192), d = 37, its = 56
i = 7478 (of 10192), d = 37, its = 57
i = 7479 (of 10192), d = 37, its = 52
i = 7480 (of 10192), d = 37, its = 38
i = 7481 (of 10192), d = 37, its = 50
i = 7482 (of 10192), d = 3.03657, its = 16
i = 7483 (of 10192), d = 37, its = 53
i = 7484 (of 10192), d = 37, its = 52
i = 7485 (of 10192), d = 37, its = 55
i = 7486 (of 10192), d = 37, its = 53
i = 7487 (of 10192), d = 14.3191, its = 7
i = 7488 (of 10192), d = 37, its = 40
i = 7489 (of 10192), d = 8.5676, its = 4
i = 7490 (of 10192), d = 37, its = 55
i = 7491 (of 10192), d = 3.18666, its = 17
i = 7492 (of 10192), d = 37, its = 49
i = 7493 (of 10192), d = 37, its = 53
i = 7494 (of 10192), d = 2.32674, its = 19
i = 7495 (of 10192), d = 3.5687, its = 22
i = 7496 (of 10192), d = 37, its = 52
i = 7497 (of 10192), d = 37, its = 54
i = 7498 (of 10192), d = 37, its = 37
i = 7499 (of 10192), d = 37, its = 52
i = 7500 (of 10192), d = 37, its = 39
i = 7501 (of 10192), d = 37, its = 53
i = 7502 (of 10192), d = 3.12807, its = 18
i = 7503 (of 10192), d = 37, its = 40
i = 7504 (of 10192), d = 10.3119, its = 5
i = 7505 (of 10192), d = 37, its = 50
i = 7506 (of 10192), d = 37, its = 50
i = 7507 (of 10192), d = 37, its = 55
i = 7508 (of 10192), d = 1.25997, its = 18
i = 7509 (of 10192), d = 37, its = 37
i = 7510 (of 10192), d = 37, its = 41
i = 7511 (of 10192), d = 37, its = 37
i = 7512 (of 10192), d = 28.6609, its = 8
i = 7513 (of 10192), d = 37, its = 50
i = 7514 (of 10192), d = 37, its = 56
i = 7515 (of 10192), d = 1.82293, its = 17
i = 7516 (of 10192), d = 37, its = 37
i = 7517 (of 10192), d = 37, its = 40
i = 7518 (of 10192), d = 37, its = 52
i = 7519 (of 10192), d = 6.27617, its = 6
i = 7520 (of 10192), d = 37, its = 37
i = 7521 (of 10192), d = 2.5135, its = 22
i = 7522 (of 10192), d = 15.2313, its = 7
i = 7523 (of 10192), d = 37, its = 54
i = 7524 (of 10192), d = 37, its = 37
i = 7525 (of 10192), d = 37, its = 55
i = 7526 (of 10192), d = 37, its = 50
i = 7527 (of 10192), d = 37, its = 53
i = 7528 (of 10192), d = 6.80188, its = 5
i = 7529 (of 10192), d = 37, its = 42
i = 7530 (of 10192), d = 37, its = 51
i = 7531 (of 10192), d = 37, its = 42
i = 7532 (of 10192), d = 37, its = 52
i = 7533 (of 10192), d = 37, its = 51
i = 7534 (of 10192), d = 37, its = 37
i = 7535 (of 10192), d = 37, its = 50
i = 7536 (of 10192), d = 37, its = 51
i = 7537 (of 10192), d = 37, its = 55
i = 7538 (of 10192), d = 37, its = 51
i = 7539 (of 10192), d = 3.13635, its = 18
i = 7540 (of 10192), d = 37, its = 37
i = 7541 (of 10192), d = 6.93237, its = 5
i = 7542 (of 10192), d = 37, its = 49
i = 7543 (of 10192), d = 8.22268, its = 3
i = 7544 (of 10192), d = 37, its = 50
i = 7545 (of 10192), d = 8.52215, its = 4
i = 7546 (of 10192), d = 3.3149, its = 20
i = 7547 (of 10192), d = 37, its = 49
i = 7548 (of 10192), d = 37, its = 49
i = 7549 (of 10192), d = 9.08053, its = 4
i = 7550 (of 10192), d = 37, its = 56
i = 7551 (of 10192), d = 37, its = 40
i = 7552 (of 10192), d = 37, its = 48
i = 7553 (of 10192), d = 37, its = 52
i = 7554 (of 10192), d = 37, its = 37
i = 7555 (of 10192), d = 37, its = 52
i = 7556 (of 10192), d = 37, its = 54
i = 7557 (of 10192), d = 37, its = 51
i = 7558 (of 10192), d = 37, its = 55
i = 7559 (of 10192), d = 37, its = 37
i = 7560 (of 10192), d = 37, its = 50
i = 7561 (of 10192), d = 7.39149, its = 4
i = 7562 (of 10192), d = 37, its = 52
i = 7563 (of 10192), d = 37, its = 47
i = 7564 (of 10192), d = 37, its = 52
i = 7565 (of 10192), d = 6.13459, its = 6
i = 7566 (of 10192), d = 8.98972, its = 4
i = 7567 (of 10192), d = 37, its = 54
i = 7568 (of 10192), d = 37, its = 51
i = 7569 (of 10192), d = 37, its = 37
i = 7570 (of 10192), d = 37, its = 57
i = 7571 (of 10192), d = 37, its = 52
i = 7572 (of 10192), d = 2.40919, its = 20
i = 7573 (of 10192), d = 37, its = 50
i = 7574 (of 10192), d = 37, its = 55
i = 7575 (of 10192), d = 37, its = 52
i = 7576 (of 10192), d = 6.18072, its = 6
i = 7577 (of 10192), d = 4.2464, its = 19
i = 7578 (of 10192), d = 37, its = 37
i = 7579 (of 10192), d = 37, its = 51
i = 7580 (of 10192), d = 37, its = 52
i = 7581 (of 10192), d = 37, its = 37
i = 7582 (of 10192), d = 37, its = 55
i = 7583 (of 10192), d = 37, its = 57
i = 7584 (of 10192), d = 37, its = 51
i = 7585 (of 10192), d = 37, its = 51
i = 7586 (of 10192), d = 37, its = 37
i = 7587 (of 10192), d = 4.2688, its = 22
i = 7588 (of 10192), d = 37, its = 48
i = 7589 (of 10192), d = 37, its = 39
i = 7590 (of 10192), d = 37, its = 37
i = 7591 (of 10192), d = 3.58228, its = 22
i = 7592 (of 10192), d = 1.21413, its = 19
i = 7593 (of 10192), d = 37, its = 37
i = 7594 (of 10192), d = 37, its = 39
i = 7595 (of 10192), d = 37, its = 50
i = 7596 (of 10192), d = 37, its = 53
i = 7597 (of 10192), d = 3.90696, its = 19
i = 7598 (of 10192), d = 37, its = 43
i = 7599 (of 10192), d = 37, its = 52
i = 7600 (of 10192), d = 8.23805, its = 4
i = 7601 (of 10192), d = 37, its = 55
i = 7602 (of 10192), d = 37, its = 54
i = 7603 (of 10192), d = 37, its = 55
i = 7604 (of 10192), d = 37, its = 51
i = 7605 (of 10192), d = 9.31764, its = 4
i = 7606 (of 10192), d = 6.18732, its = 6
i = 7607 (of 10192), d = 6.0174, its = 6
i = 7608 (of 10192), d = 37, its = 42
i = 7609 (of 10192), d = 9.28853, its = 5
i = 7610 (of 10192), d = 20.4546, its = 7
i = 7611 (of 10192), d = 2.82766, its = 20
i = 7612 (of 10192), d = 37, its = 50
i = 7613 (of 10192), d = 37, its = 53
i = 7614 (of 10192), d = 37, its = 52
i = 7615 (of 10192), d = 37, its = 53
i = 7616 (of 10192), d = 37, its = 37
i = 7617 (of 10192), d = 3.55601, its = 18
i = 7618 (of 10192), d = 37, its = 54
i = 7619 (of 10192), d = 37, its = 37
i = 7620 (of 10192), d = 37, its = 41
i = 7621 (of 10192), d = 37, its = 37
i = 7622 (of 10192), d = 37, its = 56
i = 7623 (of 10192), d = 37, its = 37
i = 7624 (of 10192), d = 37, its = 54
i = 7625 (of 10192), d = 9.15795, its = 4
i = 7626 (of 10192), d = 37, its = 54
i = 7627 (of 10192), d = 37, its = 52
i = 7628 (of 10192), d = 6.13459, its = 6
i = 7629 (of 10192), d = 37, its = 54
i = 7630 (of 10192), d = 37, its = 37
i = 7631 (of 10192), d = 37, its = 37
i = 7632 (of 10192), d = 1.92166, its = 20
i = 7633 (of 10192), d = 37, its = 50
i = 7634 (of 10192), d = 37, its = 37
i = 7635 (of 10192), d = 37, its = 41
i = 7636 (of 10192), d = 3.53295, its = 24
i = 7637 (of 10192), d = 37, its = 52
i = 7638 (of 10192), d = 2.59685, its = 18
i = 7639 (of 10192), d = 37, its = 55
i = 7640 (of 10192), d = 37, its = 37
i = 7641 (of 10192), d = 37, its = 57
i = 7642 (of 10192), d = 37, its = 55
i = 7643 (of 10192), d = 7.70852, its = 4
i = 7644 (of 10192), d = 4.40206, its = 21
i = 7645 (of 10192), d = 37, its = 52
i = 7646 (of 10192), d = 37, its = 37
i = 7647 (of 10192), d = 37, its = 52
i = 7648 (of 10192), d = 37, its = 51
i = 7649 (of 10192), d = 14.7895, its = 7
i = 7650 (of 10192), d = 2.03803, its = 26
i = 7651 (of 10192), d = 37, its = 50
i = 7652 (of 10192), d = 37, its = 53
i = 7653 (of 10192), d = 37, its = 55
i = 7654 (of 10192), d = 11.1427, its = 6
i = 7655 (of 10192), d = 37, its = 39
i = 7656 (of 10192), d = 14.8188, its = 7
i = 7657 (of 10192), d = 37, its = 47
i = 7658 (of 10192), d = 4.32063, its = 18
i = 7659 (of 10192), d = 37, its = 49
i = 7660 (of 10192), d = 37, its = 53
i = 7661 (of 10192), d = 37, its = 52
i = 7662 (of 10192), d = 37, its = 42
i = 7663 (of 10192), d = 37, its = 37
i = 7664 (of 10192), d = 5.63494, its = 7
i = 7665 (of 10192), d = 37, its = 42
i = 7666 (of 10192), d = 6.96828, its = 5
i = 7667 (of 10192), d = 37, its = 37
i = 7668 (of 10192), d = 11.7169, its = 5
i = 7669 (of 10192), d = 37, its = 51
i = 7670 (of 10192), d = 37, its = 51
i = 7671 (of 10192), d = 2.08927, its = 21
i = 7672 (of 10192), d = 37, its = 52
i = 7673 (of 10192), d = 37, its = 51
i = 7674 (of 10192), d = 37, its = 50
i = 7675 (of 10192), d = 37, its = 58
i = 7676 (of 10192), d = 37, its = 40
i = 7677 (of 10192), d = 35.7294, its = 35
i = 7678 (of 10192), d = 1.88377, its = 17
i = 7679 (of 10192), d = 37, its = 51
i = 7680 (of 10192), d = 8.69039, its = 4
i = 7681 (of 10192), d = 7.23635, its = 4
i = 7682 (of 10192), d = 37, its = 41
i = 7683 (of 10192), d = 37, its = 55
i = 7684 (of 10192), d = 37, its = 37
i = 7685 (of 10192), d = 37, its = 53
i = 7686 (of 10192), d = 8.5557, its = 4
i = 7687 (of 10192), d = 37, its = 40
i = 7688 (of 10192), d = 37, its = 37
i = 7689 (of 10192), d = 6.56724, its = 5
i = 7690 (of 10192), d = 37, its = 52
i = 7691 (of 10192), d = 37, its = 37
i = 7692 (of 10192), d = 33.2941, its = 8
i = 7693 (of 10192), d = 7.48418, its = 4
i = 7694 (of 10192), d = 37, its = 41
i = 7695 (of 10192), d = 5.34423, its = 6
i = 7696 (of 10192), d = 8.7656, its = 4
i = 7697 (of 10192), d = 9.61389, its = 5
i = 7698 (of 10192), d = 37, its = 40
i = 7699 (of 10192), d = 37, its = 41
i = 7700 (of 10192), d = 37, its = 56
i = 7701 (of 10192), d = 7.72868, its = 3
i = 7702 (of 10192), d = 37, its = 57
i = 7703 (of 10192), d = 37, its = 57
i = 7704 (of 10192), d = 6.69287, its = 5
i = 7705 (of 10192), d = 4.66368, its = 20
i = 7706 (of 10192), d = 4.68257, its = 5
i = 7707 (of 10192), d = 37, its = 37
i = 7708 (of 10192), d = 3.99561, its = 27
i = 7709 (of 10192), d = 2.77467, its = 17
i = 7710 (of 10192), d = 2.61513, its = 17
i = 7711 (of 10192), d = 5.73898, its = 7
i = 7712 (of 10192), d = 3.58205, its = 25
i = 7713 (of 10192), d = 37, its = 54
i = 7714 (of 10192), d = 18.4656, its = 7
i = 7715 (of 10192), d = 8.37821, its = 4
i = 7716 (of 10192), d = 37, its = 51
i = 7717 (of 10192), d = 3.39678, its = 19
i = 7718 (of 10192), d = 37, its = 37
i = 7719 (of 10192), d = 37, its = 52
i = 7720 (of 10192), d = 2.1139, its = 20
i = 7721 (of 10192), d = 3.43326, its = 20
i = 7722 (of 10192), d = 37, its = 52
i = 7723 (of 10192), d = 37, its = 51
i = 7724 (of 10192), d = 3.52068, its = 18
i = 7725 (of 10192), d = 37, its = 49
i = 7726 (of 10192), d = 37, its = 37
i = 7727 (of 10192), d = 37, its = 49
i = 7728 (of 10192), d = 12.2976, its = 7
i = 7729 (of 10192), d = 37, its = 56
i = 7730 (of 10192), d = 37, its = 54
i = 7731 (of 10192), d = 37, its = 49
i = 7732 (of 10192), d = 37, its = 51
i = 7733 (of 10192), d = 8.80978, its = 4
i = 7734 (of 10192), d = 37, its = 37
i = 7735 (of 10192), d = 37, its = 37
i = 7736 (of 10192), d = 37, its = 44
i = 7737 (of 10192), d = 37, its = 53
i = 7738 (of 10192), d = 37, its = 56
i = 7739 (of 10192), d = 37, its = 54
i = 7740 (of 10192), d = 37, its = 56
i = 7741 (of 10192), d = 4.65524, its = 5
i = 7742 (of 10192), d = 2.36061, its = 19
i = 7743 (of 10192), d = 37, its = 40
i = 7744 (of 10192), d = 7.55536, its = 4
i = 7745 (of 10192), d = 37, its = 55
i = 7746 (of 10192), d = 37, its = 37
i = 7747 (of 10192), d = 37, its = 49
i = 7748 (of 10192), d = 37, its = 50
i = 7749 (of 10192), d = 10.7187, its = 5
i = 7750 (of 10192), d = 37, its = 53
i = 7751 (of 10192), d = 1.86066, its = 17
i = 7752 (of 10192), d = 37, its = 37
i = 7753 (of 10192), d = 37, its = 37
i = 7754 (of 10192), d = 37, its = 53
i = 7755 (of 10192), d = 2.86827, its = 21
i = 7756 (of 10192), d = 37, its = 48
i = 7757 (of 10192), d = 37, its = 40
i = 7758 (of 10192), d = 37, its = 54
i = 7759 (of 10192), d = 6.75768, its = 7
i = 7760 (of 10192), d = 37, its = 50
i = 7761 (of 10192), d = 9.44731, its = 6
i = 7762 (of 10192), d = 37, its = 38
i = 7763 (of 10192), d = 4.10994, its = 18
i = 7764 (of 10192), d = 5.57837, its = 7
i = 7765 (of 10192), d = 37, its = 55
i = 7766 (of 10192), d = 4.42082, its = 21
i = 7767 (of 10192), d = 37, its = 55
i = 7768 (of 10192), d = 37, its = 43
i = 7769 (of 10192), d = 37, its = 51
i = 7770 (of 10192), d = 37, its = 53
i = 7771 (of 10192), d = 37, its = 40
i = 7772 (of 10192), d = 3.41005, its = 22
i = 7773 (of 10192), d = 29.7019, its = 8
i = 7774 (of 10192), d = 1.93758, its = 19
i = 7775 (of 10192), d = 37, its = 53
i = 7776 (of 10192), d = 2.04318, its = 26
i = 7777 (of 10192), d = 4.30446, its = 20
i = 7778 (of 10192), d = 37, its = 49
i = 7779 (of 10192), d = 37, its = 52
i = 7780 (of 10192), d = 8.68205, its = 4
i = 7781 (of 10192), d = 37, its = 51
i = 7782 (of 10192), d = 37, its = 56
i = 7783 (of 10192), d = 2.734, its = 23
i = 7784 (of 10192), d = 37, its = 52
i = 7785 (of 10192), d = 37, its = 54
i = 7786 (of 10192), d = 37, its = 37
i = 7787 (of 10192), d = 37, its = 51
i = 7788 (of 10192), d = 37, its = 54
i = 7789 (of 10192), d = 37, its = 58
i = 7790 (of 10192), d = 37, its = 37
i = 7791 (of 10192), d = 37, its = 50
i = 7792 (of 10192), d = 37, its = 51
i = 7793 (of 10192), d = 37, its = 37
i = 7794 (of 10192), d = 37, its = 50
i = 7795 (of 10192), d = 37, its = 53
i = 7796 (of 10192), d = 37, its = 37
i = 7797 (of 10192), d = 37, its = 41
i = 7798 (of 10192), d = 37, its = 54
i = 7799 (of 10192), d = 37, its = 50
i = 7800 (of 10192), d = 3.15363, its = 26
i = 7801 (of 10192), d = 37, its = 53
i = 7802 (of 10192), d = 10.9855, its = 5
i = 7803 (of 10192), d = 37, its = 57
i = 7804 (of 10192), d = 37, its = 57
i = 7805 (of 10192), d = 14.327, its = 6
i = 7806 (of 10192), d = 37, its = 57
i = 7807 (of 10192), d = 37, its = 51
i = 7808 (of 10192), d = 37, its = 53
i = 7809 (of 10192), d = 37, its = 50
i = 7810 (of 10192), d = 37, its = 54
i = 7811 (of 10192), d = 37, its = 43
i = 7812 (of 10192), d = 37, its = 50
i = 7813 (of 10192), d = 2.65558, its = 17
i = 7814 (of 10192), d = 3.44373, its = 22
i = 7815 (of 10192), d = 8.56154, its = 4
i = 7816 (of 10192), d = 11.0945, its = 5
i = 7817 (of 10192), d = 37, its = 41
i = 7818 (of 10192), d = 1.58632, its = 18
i = 7819 (of 10192), d = 37, its = 54
i = 7820 (of 10192), d = 37, its = 37
i = 7821 (of 10192), d = 9.94759, its = 5
i = 7822 (of 10192), d = 37, its = 54
i = 7823 (of 10192), d = 37, its = 56
i = 7824 (of 10192), d = 37, its = 37
i = 7825 (of 10192), d = 10.9737, its = 7
i = 7826 (of 10192), d = 37, its = 52
i = 7827 (of 10192), d = 37, its = 55
i = 7828 (of 10192), d = 37, its = 54
i = 7829 (of 10192), d = 37, its = 54
i = 7830 (of 10192), d = 37, its = 37
i = 7831 (of 10192), d = 37, its = 48
i = 7832 (of 10192), d = 37, its = 53
i = 7833 (of 10192), d = 6.29828, its = 6
i = 7834 (of 10192), d = 37, its = 53
i = 7835 (of 10192), d = 37, its = 47
i = 7836 (of 10192), d = 37, its = 37
i = 7837 (of 10192), d = 37, its = 55
i = 7838 (of 10192), d = 37, its = 52
i = 7839 (of 10192), d = 37, its = 52
i = 7840 (of 10192), d = 6.18144, its = 6
i = 7841 (of 10192), d = 37, its = 39
i = 7842 (of 10192), d = 37, its = 40
i = 7843 (of 10192), d = 37, its = 53
i = 7844 (of 10192), d = 37, its = 37
i = 7845 (of 10192), d = 37, its = 52
i = 7846 (of 10192), d = 3.72315, its = 22
i = 7847 (of 10192), d = 37, its = 51
i = 7848 (of 10192), d = 37, its = 52
i = 7849 (of 10192), d = 7.72868, its = 3
i = 7850 (of 10192), d = 5.84815, its = 6
i = 7851 (of 10192), d = 37, its = 39
i = 7852 (of 10192), d = 8.20563, its = 3
i = 7853 (of 10192), d = 37, its = 37
i = 7854 (of 10192), d = 37, its = 51
i = 7855 (of 10192), d = 37, its = 50
i = 7856 (of 10192), d = 14.7895, its = 7
i = 7857 (of 10192), d = 37, its = 49
i = 7858 (of 10192), d = 37, its = 37
i = 7859 (of 10192), d = 9.37798, its = 4
i = 7860 (of 10192), d = 37, its = 54
i = 7861 (of 10192), d = 10.7962, its = 5
i = 7862 (of 10192), d = 37, its = 39
i = 7863 (of 10192), d = 8.26476, its = 3
i = 7864 (of 10192), d = 4.15622, its = 23
i = 7865 (of 10192), d = 37, its = 52
i = 7866 (of 10192), d = 5.33768, its = 6
i = 7867 (of 10192), d = 7.20954, its = 5
i = 7868 (of 10192), d = 37, its = 43
i = 7869 (of 10192), d = 37, its = 52
i = 7870 (of 10192), d = 37, its = 53
i = 7871 (of 10192), d = 37, its = 40
i = 7872 (of 10192), d = 37, its = 52
i = 7873 (of 10192), d = 37, its = 52
i = 7874 (of 10192), d = 2.69428, its = 17
i = 7875 (of 10192), d = 37, its = 54
i = 7876 (of 10192), d = 4.88726, its = 18
i = 7877 (of 10192), d = 33.4518, its = 9
i = 7878 (of 10192), d = 5.82467, its = 7
i = 7879 (of 10192), d = 37, its = 53
i = 7880 (of 10192), d = 6.28523, its = 6
i = 7881 (of 10192), d = 37, its = 55
i = 7882 (of 10192), d = 37, its = 42
i = 7883 (of 10192), d = 37, its = 52
i = 7884 (of 10192), d = 37, its = 48
i = 7885 (of 10192), d = 3.94036, its = 19
i = 7886 (of 10192), d = 37, its = 37
i = 7887 (of 10192), d = 37, its = 52
i = 7888 (of 10192), d = 37, its = 39
i = 7889 (of 10192), d = 37, its = 39
i = 7890 (of 10192), d = 37, its = 52
i = 7891 (of 10192), d = 4.23217, its = 18
i = 7892 (of 10192), d = 37, its = 50
i = 7893 (of 10192), d = 37, its = 54
i = 7894 (of 10192), d = 37, its = 52
i = 7895 (of 10192), d = 6.35949, its = 6
i = 7896 (of 10192), d = 37, its = 56
i = 7897 (of 10192), d = 37, its = 52
i = 7898 (of 10192), d = 37, its = 55
i = 7899 (of 10192), d = 11.7382, its = 6
i = 7900 (of 10192), d = 37, its = 55
i = 7901 (of 10192), d = 37, its = 37
i = 7902 (of 10192), d = 37, its = 38
i = 7903 (of 10192), d = 37, its = 50
i = 7904 (of 10192), d = 11.415, its = 5
i = 7905 (of 10192), d = 37, its = 55
i = 7906 (of 10192), d = 37, its = 37
i = 7907 (of 10192), d = 8.06159, its = 3
i = 7908 (of 10192), d = 37, its = 37
i = 7909 (of 10192), d = 37, its = 51
i = 7910 (of 10192), d = 7.12604, its = 5
i = 7911 (of 10192), d = 37, its = 51
i = 7912 (of 10192), d = 37, its = 54
i = 7913 (of 10192), d = 11.5402, its = 6
i = 7914 (of 10192), d = 37, its = 38
i = 7915 (of 10192), d = 4.69244, its = 21
i = 7916 (of 10192), d = 1.92479, its = 19
i = 7917 (of 10192), d = 37, its = 53
i = 7918 (of 10192), d = 37, its = 54
i = 7919 (of 10192), d = 37, its = 40
i = 7920 (of 10192), d = 37, its = 56
i = 7921 (of 10192), d = 37, its = 41
i = 7922 (of 10192), d = 11.0951, its = 5
i = 7923 (of 10192), d = 37, its = 55
i = 7924 (of 10192), d = 37, its = 53
i = 7925 (of 10192), d = 7.64944, its = 4
i = 7926 (of 10192), d = 37, its = 53
i = 7927 (of 10192), d = 37, its = 54
i = 7928 (of 10192), d = 4.72555, its = 7
i = 7929 (of 10192), d = 37, its = 53
i = 7930 (of 10192), d = 37, its = 55
i = 7931 (of 10192), d = 37, its = 54
i = 7932 (of 10192), d = 7.46157, its = 4
i = 7933 (of 10192), d = 37, its = 49
i = 7934 (of 10192), d = 37, its = 50
i = 7935 (of 10192), d = 6.35421, its = 6
i = 7936 (of 10192), d = 3.17662, its = 17
i = 7937 (of 10192), d = 37, its = 41
i = 7938 (of 10192), d = 2.58971, its = 28
i = 7939 (of 10192), d = 5.07335, its = 7
i = 7940 (of 10192), d = 4.29377, its = 7
i = 7941 (of 10192), d = 37, its = 40
i = 7942 (of 10192), d = 12.8536, its = 6
i = 7943 (of 10192), d = 37, its = 55
i = 7944 (of 10192), d = 37, its = 39
i = 7945 (of 10192), d = 37, its = 56
i = 7946 (of 10192), d = 7.62267, its = 4
i = 7947 (of 10192), d = 8.05775, its = 3
i = 7948 (of 10192), d = 2.66278, its = 19
i = 7949 (of 10192), d = 37, its = 53
i = 7950 (of 10192), d = 7.25909, its = 5
i = 7951 (of 10192), d = 6.8676, its = 6
i = 7952 (of 10192), d = 37, its = 53
i = 7953 (of 10192), d = 37, its = 56
i = 7954 (of 10192), d = 37, its = 55
i = 7955 (of 10192), d = 37, its = 40
i = 7956 (of 10192), d = 37, its = 37
i = 7957 (of 10192), d = 37, its = 54
i = 7958 (of 10192), d = 7.91817, its = 3
i = 7959 (of 10192), d = 37, its = 57
i = 7960 (of 10192), d = 37, its = 53
i = 7961 (of 10192), d = 37, its = 52
i = 7962 (of 10192), d = 37, its = 40
i = 7963 (of 10192), d = 19.8536, its = 7
i = 7964 (of 10192), d = 37, its = 57
i = 7965 (of 10192), d = 37, its = 37
i = 7966 (of 10192), d = 37, its = 54
i = 7967 (of 10192), d = 10.8386, its = 5
i = 7968 (of 10192), d = 37, its = 41
i = 7969 (of 10192), d = 10.2871, its = 5
i = 7970 (of 10192), d = 37, its = 50
i = 7971 (of 10192), d = 10.2926, its = 5
i = 7972 (of 10192), d = 37, its = 51
i = 7973 (of 10192), d = 16.3856, its = 7
i = 7974 (of 10192), d = 37, its = 51
i = 7975 (of 10192), d = 37, its = 41
i = 7976 (of 10192), d = 37, its = 42
i = 7977 (of 10192), d = 37, its = 52
i = 7978 (of 10192), d = 37, its = 43
i = 7979 (of 10192), d = 37, its = 54
i = 7980 (of 10192), d = 37, its = 57
i = 7981 (of 10192), d = 37, its = 50
i = 7982 (of 10192), d = 37, its = 40
i = 7983 (of 10192), d = 37, its = 41
i = 7984 (of 10192), d = 6.0087, its = 6
i = 7985 (of 10192), d = 37, its = 54
i = 7986 (of 10192), d = 37, its = 54
i = 7987 (of 10192), d = 37, its = 52
i = 7988 (of 10192), d = 37, its = 53
i = 7989 (of 10192), d = 37, its = 55
i = 7990 (of 10192), d = 37, its = 50
i = 7991 (of 10192), d = 2.87723, its = 30
i = 7992 (of 10192), d = 37, its = 41
i = 7993 (of 10192), d = 37, its = 37
i = 7994 (of 10192), d = 9.27317, its = 5
i = 7995 (of 10192), d = 37, its = 52
i = 7996 (of 10192), d = 37, its = 37
i = 7997 (of 10192), d = 2.27466, its = 19
i = 7998 (of 10192), d = 2.72316, its = 18
i = 7999 (of 10192), d = 37, its = 37
i = 8000 (of 10192), d = 37, its = 39
i = 8001 (of 10192), d = 37, its = 52
i = 8002 (of 10192), d = 37, its = 53
i = 8003 (of 10192), d = 37, its = 55
i = 8004 (of 10192), d = 3.37699, its = 27
i = 8005 (of 10192), d = 37, its = 39
i = 8006 (of 10192), d = 37, its = 56
i = 8007 (of 10192), d = 12.0779, its = 7
i = 8008 (of 10192), d = 6.14361, its = 5
i = 8009 (of 10192), d = 4.45656, its = 19
i = 8010 (of 10192), d = 3.54705, its = 19
i = 8011 (of 10192), d = 37, its = 53
i = 8012 (of 10192), d = 4.03786, its = 24
i = 8013 (of 10192), d = 37, its = 48
i = 8014 (of 10192), d = 37, its = 40
i = 8015 (of 10192), d = 6.80296, its = 6
i = 8016 (of 10192), d = 37, its = 55
i = 8017 (of 10192), d = 37, its = 51
i = 8018 (of 10192), d = 37, its = 50
i = 8019 (of 10192), d = 6.94867, its = 7
i = 8020 (of 10192), d = 37, its = 51
i = 8021 (of 10192), d = 37, its = 41
i = 8022 (of 10192), d = 37, its = 39
i = 8023 (of 10192), d = 37, its = 50
i = 8024 (of 10192), d = 12.7148, its = 6
i = 8025 (of 10192), d = 3.24605, its = 21
i = 8026 (of 10192), d = 9.02718, its = 5
i = 8027 (of 10192), d = 31.0548, its = 8
i = 8028 (of 10192), d = 37, its = 49
i = 8029 (of 10192), d = 4.86353, its = 18
i = 8030 (of 10192), d = 37, its = 39
i = 8031 (of 10192), d = 37, its = 39
i = 8032 (of 10192), d = 37, its = 42
i = 8033 (of 10192), d = 7.0457, its = 5
i = 8034 (of 10192), d = 37, its = 38
i = 8035 (of 10192), d = 37, its = 51
i = 8036 (of 10192), d = 37, its = 37
i = 8037 (of 10192), d = 37, its = 39
i = 8038 (of 10192), d = 37, its = 39
i = 8039 (of 10192), d = 6.66719, its = 6
i = 8040 (of 10192), d = 37, its = 37
i = 8041 (of 10192), d = 37, its = 49
i = 8042 (of 10192), d = 7.55422, its = 4
i = 8043 (of 10192), d = 37, its = 43
i = 8044 (of 10192), d = 37, its = 37
i = 8045 (of 10192), d = 37, its = 37
i = 8046 (of 10192), d = 37, its = 37
i = 8047 (of 10192), d = 37, its = 40
i = 8048 (of 10192), d = 1.06466, its = 28
i = 8049 (of 10192), d = 37, its = 54
i = 8050 (of 10192), d = 37, its = 53
i = 8051 (of 10192), d = 37, its = 56
i = 8052 (of 10192), d = 37, its = 55
i = 8053 (of 10192), d = 37, its = 53
i = 8054 (of 10192), d = 37, its = 37
i = 8055 (of 10192), d = 37, its = 53
i = 8056 (of 10192), d = 12.0579, its = 6
i = 8057 (of 10192), d = 37, its = 52
i = 8058 (of 10192), d = 10.2571, its = 5
i = 8059 (of 10192), d = 37, its = 53
i = 8060 (of 10192), d = 7.14018, its = 4
i = 8061 (of 10192), d = 37, its = 50
i = 8062 (of 10192), d = 37, its = 40
i = 8063 (of 10192), d = 11.4836, its = 5
i = 8064 (of 10192), d = 1.5137, its = 17
i = 8065 (of 10192), d = 37, its = 46
i = 8066 (of 10192), d = 37, its = 55
i = 8067 (of 10192), d = 9.48773, its = 5
i = 8068 (of 10192), d = 37, its = 56
i = 8069 (of 10192), d = 37, its = 37
i = 8070 (of 10192), d = 37, its = 51
i = 8071 (of 10192), d = 37, its = 37
i = 8072 (of 10192), d = 37, its = 51
i = 8073 (of 10192), d = 2.49429, its = 21
i = 8074 (of 10192), d = 20.8864, its = 9
i = 8075 (of 10192), d = 2.25042, its = 19
i = 8076 (of 10192), d = 9.50262, its = 4
i = 8077 (of 10192), d = 37, its = 48
i = 8078 (of 10192), d = 37, its = 37
i = 8079 (of 10192), d = 3.60995, its = 19
i = 8080 (of 10192), d = 37, its = 39
i = 8081 (of 10192), d = 37, its = 37
i = 8082 (of 10192), d = 37, its = 51
i = 8083 (of 10192), d = 2.82766, its = 20
i = 8084 (of 10192), d = 37, its = 51
i = 8085 (of 10192), d = 37, its = 52
i = 8086 (of 10192), d = 6.49479, its = 6
i = 8087 (of 10192), d = 37, its = 51
i = 8088 (of 10192), d = 37, its = 52
i = 8089 (of 10192), d = 3.91346, its = 19
i = 8090 (of 10192), d = 37, its = 37
i = 8091 (of 10192), d = 9.82409, its = 5
i = 8092 (of 10192), d = 37, its = 55
i = 8093 (of 10192), d = 37, its = 51
i = 8094 (of 10192), d = 37, its = 52
i = 8095 (of 10192), d = 37, its = 40
i = 8096 (of 10192), d = 7.81346, its = 3
i = 8097 (of 10192), d = 37, its = 54
i = 8098 (of 10192), d = 37, its = 51
i = 8099 (of 10192), d = 37, its = 37
i = 8100 (of 10192), d = 37, its = 55
i = 8101 (of 10192), d = 37, its = 51
i = 8102 (of 10192), d = 10.9871, its = 6
i = 8103 (of 10192), d = 37, its = 52
i = 8104 (of 10192), d = 37, its = 37
i = 8105 (of 10192), d = 7.7933, its = 4
i = 8106 (of 10192), d = 37, its = 53
i = 8107 (of 10192), d = 6.30697, its = 6
i = 8108 (of 10192), d = 37, its = 52
i = 8109 (of 10192), d = 1.80059, its = 18
i = 8110 (of 10192), d = 37, its = 56
i = 8111 (of 10192), d = 37, its = 53
i = 8112 (of 10192), d = 37, its = 50
i = 8113 (of 10192), d = 37, its = 37
i = 8114 (of 10192), d = 7.72303, its = 3
i = 8115 (of 10192), d = 37, its = 54
i = 8116 (of 10192), d = 7.03081, its = 5
i = 8117 (of 10192), d = 37, its = 50
i = 8118 (of 10192), d = 7.38071, its = 4
i = 8119 (of 10192), d = 37, its = 37
i = 8120 (of 10192), d = 37, its = 58
i = 8121 (of 10192), d = 37, its = 37
i = 8122 (of 10192), d = 4.11468, its = 19
i = 8123 (of 10192), d = 37, its = 37
i = 8124 (of 10192), d = 7.26147, its = 4
i = 8125 (of 10192), d = 1.86412, its = 17
i = 8126 (of 10192), d = 37, its = 51
i = 8127 (of 10192), d = 37, its = 52
i = 8128 (of 10192), d = 37, its = 37
i = 8129 (of 10192), d = 37, its = 52
i = 8130 (of 10192), d = 37, its = 37
i = 8131 (of 10192), d = 37, its = 47
i = 8132 (of 10192), d = 37, its = 40
i = 8133 (of 10192), d = 37, its = 53
i = 8134 (of 10192), d = 37, its = 52
i = 8135 (of 10192), d = 37, its = 55
i = 8136 (of 10192), d = 37, its = 41
i = 8137 (of 10192), d = 37, its = 37
i = 8138 (of 10192), d = 37, its = 42
i = 8139 (of 10192), d = 6.35949, its = 6
i = 8140 (of 10192), d = 37, its = 52
i = 8141 (of 10192), d = 37, its = 37
i = 8142 (of 10192), d = 37, its = 42
i = 8143 (of 10192), d = 37, its = 50
i = 8144 (of 10192), d = 2.25425, its = 18
i = 8145 (of 10192), d = 4.39474, its = 25
i = 8146 (of 10192), d = 37, its = 52
i = 8147 (of 10192), d = 37, its = 50
i = 8148 (of 10192), d = 2.58133, its = 27
i = 8149 (of 10192), d = 37, its = 48
i = 8150 (of 10192), d = 37, its = 38
i = 8151 (of 10192), d = 23.3345, its = 8
i = 8152 (of 10192), d = 37, its = 50
i = 8153 (of 10192), d = 37, its = 47
i = 8154 (of 10192), d = 37, its = 51
i = 8155 (of 10192), d = 37, its = 51
i = 8156 (of 10192), d = 37, its = 51
i = 8157 (of 10192), d = 37, its = 53
i = 8158 (of 10192), d = 37, its = 53
i = 8159 (of 10192), d = 37, its = 53
i = 8160 (of 10192), d = 37, its = 52
i = 8161 (of 10192), d = 37, its = 37
i = 8162 (of 10192), d = 37, its = 50
i = 8163 (of 10192), d = 37, its = 53
i = 8164 (of 10192), d = 37, its = 37
i = 8165 (of 10192), d = 37, its = 51
i = 8166 (of 10192), d = 6.89037, its = 5
i = 8167 (of 10192), d = 7.91305, its = 3
i = 8168 (of 10192), d = 37, its = 55
i = 8169 (of 10192), d = 37, its = 53
i = 8170 (of 10192), d = 2.78238, its = 17
i = 8171 (of 10192), d = 37, its = 53
i = 8172 (of 10192), d = 37, its = 53
i = 8173 (of 10192), d = 37, its = 53
i = 8174 (of 10192), d = 8.6827, its = 4
i = 8175 (of 10192), d = 11.9022, its = 6
i = 8176 (of 10192), d = 37, its = 51
i = 8177 (of 10192), d = 37, its = 52
i = 8178 (of 10192), d = 2.91167, its = 19
i = 8179 (of 10192), d = 37, its = 50
i = 8180 (of 10192), d = 37, its = 37
i = 8181 (of 10192), d = 37, its = 51
i = 8182 (of 10192), d = 37, its = 55
i = 8183 (of 10192), d = 5.87044, its = 7
i = 8184 (of 10192), d = 3.72982, its = 20
i = 8185 (of 10192), d = 37, its = 55
i = 8186 (of 10192), d = 12.6072, its = 7
i = 8187 (of 10192), d = 4.07561, its = 16
i = 8188 (of 10192), d = 2.66676, its = 23
i = 8189 (of 10192), d = 11.994, its = 7
i = 8190 (of 10192), d = 37, its = 51
i = 8191 (of 10192), d = 3.3789, its = 19
i = 8192 (of 10192), d = 37, its = 53
i = 8193 (of 10192), d = 37, its = 53
i = 8194 (of 10192), d = 37, its = 37
i = 8195 (of 10192), d = 37, its = 52
i = 8196 (of 10192), d = 37, its = 37
i = 8197 (of 10192), d = 37, its = 48
i = 8198 (of 10192), d = 37, its = 53
i = 8199 (of 10192), d = 8.68275, its = 4
i = 8200 (of 10192), d = 37, its = 51
i = 8201 (of 10192), d = 37, its = 58
i = 8202 (of 10192), d = 37, its = 58
i = 8203 (of 10192), d = 37, its = 50
i = 8204 (of 10192), d = 37, its = 40
i = 8205 (of 10192), d = 6.20278, its = 6
i = 8206 (of 10192), d = 37, its = 58
i = 8207 (of 10192), d = 37, its = 40
i = 8208 (of 10192), d = 7.12604, its = 5
i = 8209 (of 10192), d = 37, its = 41
i = 8210 (of 10192), d = 37, its = 53
i = 8211 (of 10192), d = 37, its = 47
i = 8212 (of 10192), d = 37, its = 55
i = 8213 (of 10192), d = 37, its = 56
i = 8214 (of 10192), d = 5.49368, its = 7
i = 8215 (of 10192), d = 12.7238, its = 7
i = 8216 (of 10192), d = 37, its = 37
i = 8217 (of 10192), d = 37, its = 55
i = 8218 (of 10192), d = 6.77261, its = 7
i = 8219 (of 10192), d = 37, its = 37
i = 8220 (of 10192), d = 5.43373, its = 6
i = 8221 (of 10192), d = 37, its = 54
i = 8222 (of 10192), d = 7.63197, its = 4
i = 8223 (of 10192), d = 37, its = 55
i = 8224 (of 10192), d = 37, its = 37
i = 8225 (of 10192), d = 37, its = 37
i = 8226 (of 10192), d = 37, its = 49
i = 8227 (of 10192), d = 37, its = 51
i = 8228 (of 10192), d = 37, its = 49
i = 8229 (of 10192), d = 37, its = 37
i = 8230 (of 10192), d = 37, its = 39
i = 8231 (of 10192), d = 37, its = 52
i = 8232 (of 10192), d = 37, its = 37
i = 8233 (of 10192), d = 31.6374, its = 8
i = 8234 (of 10192), d = 37, its = 37
i = 8235 (of 10192), d = 37, its = 52
i = 8236 (of 10192), d = 37, its = 51
i = 8237 (of 10192), d = 37, its = 42
i = 8238 (of 10192), d = 37, its = 51
i = 8239 (of 10192), d = 37, its = 53
i = 8240 (of 10192), d = 37, its = 43
i = 8241 (of 10192), d = 6.22939, its = 6
i = 8242 (of 10192), d = 13.9346, its = 7
i = 8243 (of 10192), d = 5.51097, its = 6
i = 8244 (of 10192), d = 37, its = 51
i = 8245 (of 10192), d = 2.92217, its = 18
i = 8246 (of 10192), d = 6.57586, its = 6
i = 8247 (of 10192), d = 37, its = 53
i = 8248 (of 10192), d = 1.5416, its = 18
i = 8249 (of 10192), d = 3.02406, its = 16
i = 8250 (of 10192), d = 37, its = 56
i = 8251 (of 10192), d = 37, its = 53
i = 8252 (of 10192), d = 37, its = 37
i = 8253 (of 10192), d = 37, its = 57
i = 8254 (of 10192), d = 37, its = 50
i = 8255 (of 10192), d = 8.48058, its = 4
i = 8256 (of 10192), d = 9.32079, its = 5
i = 8257 (of 10192), d = 7.40103, its = 4
i = 8258 (of 10192), d = 37, its = 37
i = 8259 (of 10192), d = 21.7315, its = 7
i = 8260 (of 10192), d = 3.09145, its = 25
i = 8261 (of 10192), d = 6.62864, its = 6
i = 8262 (of 10192), d = 37, its = 54
i = 8263 (of 10192), d = 37, its = 56
i = 8264 (of 10192), d = 37, its = 53
i = 8265 (of 10192), d = 9.11343, its = 5
i = 8266 (of 10192), d = 1.39303, its = 17
i = 8267 (of 10192), d = 3.36714, its = 23
i = 8268 (of 10192), d = 37, its = 53
i = 8269 (of 10192), d = 37, its = 50
i = 8270 (of 10192), d = 37, its = 52
i = 8271 (of 10192), d = 8.70307, its = 4
i = 8272 (of 10192), d = 37, its = 50
i = 8273 (of 10192), d = 37, its = 51
i = 8274 (of 10192), d = 37, its = 54
i = 8275 (of 10192), d = 11.642, its = 5
i = 8276 (of 10192), d = 37, its = 55
i = 8277 (of 10192), d = 37, its = 54
i = 8278 (of 10192), d = 37, its = 54
i = 8279 (of 10192), d = 37, its = 52
i = 8280 (of 10192), d = 3.57024, its = 26
i = 8281 (of 10192), d = 1.2857, its = 18
i = 8282 (of 10192), d = 37, its = 55
i = 8283 (of 10192), d = 37, its = 39
i = 8284 (of 10192), d = 37, its = 48
i = 8285 (of 10192), d = 37, its = 56
i = 8286 (of 10192), d = 37, its = 37
i = 8287 (of 10192), d = 37, its = 54
i = 8288 (of 10192), d = 2.8041, its = 21
i = 8289 (of 10192), d = 3.86456, its = 18
i = 8290 (of 10192), d = 3.31774, its = 22
i = 8291 (of 10192), d = 3.1639, its = 18
i = 8292 (of 10192), d = 37, its = 51
i = 8293 (of 10192), d = 37, its = 40
i = 8294 (of 10192), d = 37, its = 50
i = 8295 (of 10192), d = 1.93136, its = 20
i = 8296 (of 10192), d = 27.5084, its = 7
i = 8297 (of 10192), d = 6.3676, its = 6
i = 8298 (of 10192), d = 4.89652, its = 6
i = 8299 (of 10192), d = 9.98942, its = 5
i = 8300 (of 10192), d = 4.83026, its = 19
i = 8301 (of 10192), d = 2.64161, its = 19
i = 8302 (of 10192), d = 37, its = 52
i = 8303 (of 10192), d = 9.31367, its = 5
i = 8304 (of 10192), d = 37, its = 55
i = 8305 (of 10192), d = 37, its = 53
i = 8306 (of 10192), d = 37, its = 53
i = 8307 (of 10192), d = 6.15384, its = 6
i = 8308 (of 10192), d = 37, its = 52
i = 8309 (of 10192), d = 37, its = 53
i = 8310 (of 10192), d = 37, its = 52
i = 8311 (of 10192), d = 37, its = 37
i = 8312 (of 10192), d = 37, its = 37
i = 8313 (of 10192), d = 37, its = 37
i = 8314 (of 10192), d = 37, its = 57
i = 8315 (of 10192), d = 37, its = 39
i = 8316 (of 10192), d = 8.62489, its = 4
i = 8317 (of 10192), d = 37, its = 52
i = 8318 (of 10192), d = 37, its = 56
i = 8319 (of 10192), d = 37, its = 57
i = 8320 (of 10192), d = 37, its = 46
i = 8321 (of 10192), d = 11.174, its = 6
i = 8322 (of 10192), d = 37, its = 40
i = 8323 (of 10192), d = 3.27711, its = 16
i = 8324 (of 10192), d = 37, its = 41
i = 8325 (of 10192), d = 37, its = 53
i = 8326 (of 10192), d = 37, its = 50
i = 8327 (of 10192), d = 6.43564, its = 5
i = 8328 (of 10192), d = 37, its = 55
i = 8329 (of 10192), d = 1.91297, its = 19
i = 8330 (of 10192), d = 3.19482, its = 17
i = 8331 (of 10192), d = 37, its = 37
i = 8332 (of 10192), d = 6.26026, its = 6
i = 8333 (of 10192), d = 8.52667, its = 4
i = 8334 (of 10192), d = 10.4901, its = 5
i = 8335 (of 10192), d = 37, its = 53
i = 8336 (of 10192), d = 37, its = 54
i = 8337 (of 10192), d = 37, its = 52
i = 8338 (of 10192), d = 37, its = 56
i = 8339 (of 10192), d = 37, its = 53
i = 8340 (of 10192), d = 37, its = 43
i = 8341 (of 10192), d = 37, its = 53
i = 8342 (of 10192), d = 37, its = 51
i = 8343 (of 10192), d = 37, its = 55
i = 8344 (of 10192), d = 5.70774, its = 6
i = 8345 (of 10192), d = 3.10198, its = 20
i = 8346 (of 10192), d = 37, its = 37
i = 8347 (of 10192), d = 37, its = 57
i = 8348 (of 10192), d = 37, its = 52
i = 8349 (of 10192), d = 37, its = 49
i = 8350 (of 10192), d = 13.5277, its = 6
i = 8351 (of 10192), d = 37, its = 56
i = 8352 (of 10192), d = 6.33093, its = 6
i = 8353 (of 10192), d = 37, its = 55
i = 8354 (of 10192), d = 11.6599, its = 6
i = 8355 (of 10192), d = 8.72396, its = 4
i = 8356 (of 10192), d = 8.7746, its = 4
i = 8357 (of 10192), d = 37, its = 56
i = 8358 (of 10192), d = 37, its = 55
i = 8359 (of 10192), d = 37, its = 51
i = 8360 (of 10192), d = 10.3513, its = 5
i = 8361 (of 10192), d = 18.517, its = 7
i = 8362 (of 10192), d = 9.52983, its = 5
i = 8363 (of 10192), d = 37, its = 54
i = 8364 (of 10192), d = 7.98691, its = 2
i = 8365 (of 10192), d = 8.1191, its = 3
i = 8366 (of 10192), d = 3.80168, its = 17
i = 8367 (of 10192), d = 3.48653, its = 18
i = 8368 (of 10192), d = 37, its = 55
i = 8369 (of 10192), d = 1.37029, its = 19
i = 8370 (of 10192), d = 37, its = 40
i = 8371 (of 10192), d = 37, its = 53
i = 8372 (of 10192), d = 37, its = 49
i = 8373 (of 10192), d = 37, its = 37
i = 8374 (of 10192), d = 37, its = 37
i = 8375 (of 10192), d = 7.84405, its = 3
i = 8376 (of 10192), d = 6.93149, its = 6
i = 8377 (of 10192), d = 37, its = 50
i = 8378 (of 10192), d = 37, its = 52
i = 8379 (of 10192), d = 37, its = 55
i = 8380 (of 10192), d = 37, its = 44
i = 8381 (of 10192), d = 37, its = 37
i = 8382 (of 10192), d = 37, its = 54
i = 8383 (of 10192), d = 37, its = 54
i = 8384 (of 10192), d = 37, its = 37
i = 8385 (of 10192), d = 37, its = 56
i = 8386 (of 10192), d = 37, its = 37
i = 8387 (of 10192), d = 3.67941, its = 19
i = 8388 (of 10192), d = 37, its = 49
i = 8389 (of 10192), d = 37, its = 48
i = 8390 (of 10192), d = 37, its = 37
i = 8391 (of 10192), d = 37, its = 39
i = 8392 (of 10192), d = 37, its = 39
i = 8393 (of 10192), d = 37, its = 51
i = 8394 (of 10192), d = 37, its = 37
i = 8395 (of 10192), d = 37, its = 43
i = 8396 (of 10192), d = 4.24849, its = 17
i = 8397 (of 10192), d = 37, its = 50
i = 8398 (of 10192), d = 13.0465, its = 6
i = 8399 (of 10192), d = 37, its = 37
i = 8400 (of 10192), d = 37, its = 55
i = 8401 (of 10192), d = 3.05594, its = 22
i = 8402 (of 10192), d = 37, its = 55
i = 8403 (of 10192), d = 37, its = 47
i = 8404 (of 10192), d = 4.78179, its = 5
i = 8405 (of 10192), d = 4.54469, its = 17
i = 8406 (of 10192), d = 12.6253, its = 6
i = 8407 (of 10192), d = 37, its = 51
i = 8408 (of 10192), d = 37, its = 54
i = 8409 (of 10192), d = 7.9991, its = 2
i = 8410 (of 10192), d = 37, its = 50
i = 8411 (of 10192), d = 37, its = 40
i = 8412 (of 10192), d = 37, its = 40
i = 8413 (of 10192), d = 37, its = 53
i = 8414 (of 10192), d = 37, its = 47
i = 8415 (of 10192), d = 37, its = 40
i = 8416 (of 10192), d = 37, its = 55
i = 8417 (of 10192), d = 37, its = 56
i = 8418 (of 10192), d = 37, its = 53
i = 8419 (of 10192), d = 9.29638, its = 5
i = 8420 (of 10192), d = 1.93216, its = 21
i = 8421 (of 10192), d = 2.86995, its = 20
i = 8422 (of 10192), d = 37, its = 37
i = 8423 (of 10192), d = 37, its = 49
i = 8424 (of 10192), d = 37, its = 53
i = 8425 (of 10192), d = 7.72868, its = 3
i = 8426 (of 10192), d = 37, its = 40
i = 8427 (of 10192), d = 9.03719, its = 5
i = 8428 (of 10192), d = 9.6689, its = 4
i = 8429 (of 10192), d = 37, its = 52
i = 8430 (of 10192), d = 37, its = 55
i = 8431 (of 10192), d = 37, its = 37
i = 8432 (of 10192), d = 37, its = 54
i = 8433 (of 10192), d = 37, its = 51
i = 8434 (of 10192), d = 37, its = 40
i = 8435 (of 10192), d = 7.71034, its = 4
i = 8436 (of 10192), d = 37, its = 37
i = 8437 (of 10192), d = 37, its = 56
i = 8438 (of 10192), d = 37, its = 37
i = 8439 (of 10192), d = 37, its = 50
i = 8440 (of 10192), d = 37, its = 53
i = 8441 (of 10192), d = 37, its = 57
i = 8442 (of 10192), d = 37, its = 56
i = 8443 (of 10192), d = 37, its = 52
i = 8444 (of 10192), d = 37, its = 37
i = 8445 (of 10192), d = 2.76, its = 16
i = 8446 (of 10192), d = 37, its = 55
i = 8447 (of 10192), d = 37, its = 50
i = 8448 (of 10192), d = 37, its = 51
i = 8449 (of 10192), d = 37, its = 49
i = 8450 (of 10192), d = 1.26549, its = 18
i = 8451 (of 10192), d = 37, its = 42
i = 8452 (of 10192), d = 7.74717, its = 4
i = 8453 (of 10192), d = 37, its = 50
i = 8454 (of 10192), d = 12.6599, its = 6
i = 8455 (of 10192), d = 37, its = 52
i = 8456 (of 10192), d = 37, its = 54
i = 8457 (of 10192), d = 4.56857, its = 6
i = 8458 (of 10192), d = 5.90782, its = 6
i = 8459 (of 10192), d = 37, its = 51
i = 8460 (of 10192), d = 37, its = 37
i = 8461 (of 10192), d = 37, its = 57
i = 8462 (of 10192), d = 37, its = 52
i = 8463 (of 10192), d = 37, its = 37
i = 8464 (of 10192), d = 3.63244, its = 20
i = 8465 (of 10192), d = 14.1281, its = 6
i = 8466 (of 10192), d = 3.02214, its = 16
i = 8467 (of 10192), d = 37, its = 42
i = 8468 (of 10192), d = 5.85322, its = 7
i = 8469 (of 10192), d = 3.70199, its = 24
i = 8470 (of 10192), d = 4.3286, its = 19
i = 8471 (of 10192), d = 37, its = 43
i = 8472 (of 10192), d = 37, its = 51
i = 8473 (of 10192), d = 37, its = 52
i = 8474 (of 10192), d = 37, its = 50
i = 8475 (of 10192), d = 37, its = 37
i = 8476 (of 10192), d = 37, its = 46
i = 8477 (of 10192), d = 37, its = 55
i = 8478 (of 10192), d = 37, its = 54
i = 8479 (of 10192), d = 37, its = 37
i = 8480 (of 10192), d = 0.5, its = 45
i = 8481 (of 10192), d = 4.22436, its = 21
i = 8482 (of 10192), d = 37, its = 37
i = 8483 (of 10192), d = 37, its = 55
i = 8484 (of 10192), d = 9.2784, its = 4
i = 8485 (of 10192), d = 37, its = 39
i = 8486 (of 10192), d = 4.72797, its = 22
i = 8487 (of 10192), d = 37, its = 37
i = 8488 (of 10192), d = 37, its = 52
i = 8489 (of 10192), d = 37, its = 53
i = 8490 (of 10192), d = 37, its = 52
i = 8491 (of 10192), d = 37, its = 58
i = 8492 (of 10192), d = 16.6147, its = 6
i = 8493 (of 10192), d = 37, its = 50
i = 8494 (of 10192), d = 6.07285, its = 6
i = 8495 (of 10192), d = 8.73498, its = 4
i = 8496 (of 10192), d = 37, its = 55
i = 8497 (of 10192), d = 37, its = 52
i = 8498 (of 10192), d = 6.83062, its = 5
i = 8499 (of 10192), d = 37, its = 54
i = 8500 (of 10192), d = 8.20084, its = 3
i = 8501 (of 10192), d = 4.7993, its = 21
i = 8502 (of 10192), d = 37, its = 55
i = 8503 (of 10192), d = 6.56771, its = 5
i = 8504 (of 10192), d = 37, its = 37
i = 8505 (of 10192), d = 37, its = 51
i = 8506 (of 10192), d = 3.88577, its = 17
i = 8507 (of 10192), d = 37, its = 52
i = 8508 (of 10192), d = 37, its = 52
i = 8509 (of 10192), d = 11.9348, its = 7
i = 8510 (of 10192), d = 37, its = 55
i = 8511 (of 10192), d = 2.94643, its = 18
i = 8512 (of 10192), d = 3.94964, its = 19
i = 8513 (of 10192), d = 2.88022, its = 18
i = 8514 (of 10192), d = 3.03378, its = 17
i = 8515 (of 10192), d = 37, its = 51
i = 8516 (of 10192), d = 37, its = 54
i = 8517 (of 10192), d = 37, its = 56
i = 8518 (of 10192), d = 8.60703, its = 4
i = 8519 (of 10192), d = 37, its = 41
i = 8520 (of 10192), d = 37, its = 48
i = 8521 (of 10192), d = 37, its = 55
i = 8522 (of 10192), d = 37, its = 37
i = 8523 (of 10192), d = 37, its = 38
i = 8524 (of 10192), d = 37, its = 40
i = 8525 (of 10192), d = 37, its = 37
i = 8526 (of 10192), d = 37, its = 43
i = 8527 (of 10192), d = 7.84489, its = 3
i = 8528 (of 10192), d = 9.45836, its = 4
i = 8529 (of 10192), d = 3.93303, its = 18
i = 8530 (of 10192), d = 37, its = 53
i = 8531 (of 10192), d = 14.8454, its = 6
i = 8532 (of 10192), d = 3.07926, its = 18
i = 8533 (of 10192), d = 37, its = 51
i = 8534 (of 10192), d = 37, its = 53
i = 8535 (of 10192), d = 6.24453, its = 6
i = 8536 (of 10192), d = 37, its = 48
i = 8537 (of 10192), d = 1.60975, its = 19
i = 8538 (of 10192), d = 37, its = 52
i = 8539 (of 10192), d = 4.20821, its = 22
i = 8540 (of 10192), d = 37, its = 52
i = 8541 (of 10192), d = 37, its = 53
i = 8542 (of 10192), d = 37, its = 59
i = 8543 (of 10192), d = 3.81041, its = 24
i = 8544 (of 10192), d = 37, its = 48
i = 8545 (of 10192), d = 37, its = 40
i = 8546 (of 10192), d = 37, its = 52
i = 8547 (of 10192), d = 37, its = 51
i = 8548 (of 10192), d = 3.41005, its = 22
i = 8549 (of 10192), d = 37, its = 53
i = 8550 (of 10192), d = 37, its = 48
i = 8551 (of 10192), d = 37, its = 52
i = 8552 (of 10192), d = 12.9771, its = 7
i = 8553 (of 10192), d = 37, its = 37
i = 8554 (of 10192), d = 37, its = 37
i = 8555 (of 10192), d = 37, its = 52
i = 8556 (of 10192), d = 37, its = 53
i = 8557 (of 10192), d = 37, its = 40
i = 8558 (of 10192), d = 37, its = 56
i = 8559 (of 10192), d = 14.3889, its = 7
i = 8560 (of 10192), d = 37, its = 37
i = 8561 (of 10192), d = 5.30468, its = 6
i = 8562 (of 10192), d = 37, its = 37
i = 8563 (of 10192), d = 10.7375, its = 6
i = 8564 (of 10192), d = 37, its = 51
i = 8565 (of 10192), d = 37, its = 55
i = 8566 (of 10192), d = 5.89001, its = 7
i = 8567 (of 10192), d = 37, its = 52
i = 8568 (of 10192), d = 4.01285, its = 18
i = 8569 (of 10192), d = 37, its = 39
i = 8570 (of 10192), d = 37, its = 53
i = 8571 (of 10192), d = 10.5502, its = 6
i = 8572 (of 10192), d = 37, its = 50
i = 8573 (of 10192), d = 3.15938, its = 17
i = 8574 (of 10192), d = 8.94318, its = 4
i = 8575 (of 10192), d = 5.66697, its = 21
i = 8576 (of 10192), d = 37, its = 53
i = 8577 (of 10192), d = 37, its = 55
i = 8578 (of 10192), d = 37, its = 55
i = 8579 (of 10192), d = 37, its = 50
i = 8580 (of 10192), d = 9.3872, its = 5
i = 8581 (of 10192), d = 37, its = 54
i = 8582 (of 10192), d = 37, its = 38
i = 8583 (of 10192), d = 4.50974, its = 16
i = 8584 (of 10192), d = 8.56154, its = 4
i = 8585 (of 10192), d = 37, its = 50
i = 8586 (of 10192), d = 37, its = 51
i = 8587 (of 10192), d = 37, its = 51
i = 8588 (of 10192), d = 37, its = 52
i = 8589 (of 10192), d = 14.557, its = 6
i = 8590 (of 10192), d = 3.69333, its = 19
i = 8591 (of 10192), d = 37, its = 40
i = 8592 (of 10192), d = 6.03632, its = 6
i = 8593 (of 10192), d = 37, its = 56
i = 8594 (of 10192), d = 37, its = 38
i = 8595 (of 10192), d = 8.93651, its = 4
i = 8596 (of 10192), d = 2.82466, its = 20
i = 8597 (of 10192), d = 37, its = 54
i = 8598 (of 10192), d = 2.09021, its = 17
i = 8599 (of 10192), d = 18.9556, its = 8
i = 8600 (of 10192), d = 37, its = 51
i = 8601 (of 10192), d = 37, its = 41
i = 8602 (of 10192), d = 11.0344, its = 5
i = 8603 (of 10192), d = 37, its = 51
i = 8604 (of 10192), d = 37, its = 53
i = 8605 (of 10192), d = 37, its = 51
i = 8606 (of 10192), d = 37, its = 54
i = 8607 (of 10192), d = 8.84363, its = 4
i = 8608 (of 10192), d = 7.41192, its = 4
i = 8609 (of 10192), d = 37, its = 50
i = 8610 (of 10192), d = 37, its = 54
i = 8611 (of 10192), d = 37, its = 53
i = 8612 (of 10192), d = 37, its = 37
i = 8613 (of 10192), d = 37, its = 56
i = 8614 (of 10192), d = 37, its = 56
i = 8615 (of 10192), d = 5.77895, its = 6
i = 8616 (of 10192), d = 37, its = 54
i = 8617 (of 10192), d = 37, its = 51
i = 8618 (of 10192), d = 37, its = 54
i = 8619 (of 10192), d = 5.67822, its = 7
i = 8620 (of 10192), d = 37, its = 52
i = 8621 (of 10192), d = 37, its = 54
i = 8622 (of 10192), d = 3.52359, its = 18
i = 8623 (of 10192), d = 37, its = 56
i = 8624 (of 10192), d = 37, its = 37
i = 8625 (of 10192), d = 37, its = 37
i = 8626 (of 10192), d = 37, its = 56
i = 8627 (of 10192), d = 37, its = 59
i = 8628 (of 10192), d = 37, its = 53
i = 8629 (of 10192), d = 37, its = 52
i = 8630 (of 10192), d = 6.63596, its = 6
i = 8631 (of 10192), d = 8.54304, its = 4
i = 8632 (of 10192), d = 37, its = 52
i = 8633 (of 10192), d = 37, its = 54
i = 8634 (of 10192), d = 37, its = 37
i = 8635 (of 10192), d = 3.39015, its = 20
i = 8636 (of 10192), d = 10.5491, its = 5
i = 8637 (of 10192), d = 7.7474, its = 4
i = 8638 (of 10192), d = 7.16308, its = 5
i = 8639 (of 10192), d = 6.9555, its = 5
i = 8640 (of 10192), d = 5.30025, its = 7
i = 8641 (of 10192), d = 37, its = 43
i = 8642 (of 10192), d = 37, its = 52
i = 8643 (of 10192), d = 37, its = 50
i = 8644 (of 10192), d = 3.26422, its = 22
i = 8645 (of 10192), d = 37, its = 53
i = 8646 (of 10192), d = 37, its = 58
i = 8647 (of 10192), d = 37, its = 52
i = 8648 (of 10192), d = 7.61842, its = 4
i = 8649 (of 10192), d = 37, its = 38
i = 8650 (of 10192), d = 3.4125, its = 21
i = 8651 (of 10192), d = 4.19192, its = 19
i = 8652 (of 10192), d = 37, its = 49
i = 8653 (of 10192), d = 5.59814, its = 25
i = 8654 (of 10192), d = 7.78535, its = 3
i = 8655 (of 10192), d = 37, its = 37
i = 8656 (of 10192), d = 37, its = 37
i = 8657 (of 10192), d = 10.7517, its = 5
i = 8658 (of 10192), d = 37, its = 40
i = 8659 (of 10192), d = 37, its = 49
i = 8660 (of 10192), d = 37, its = 37
i = 8661 (of 10192), d = 10.8727, its = 6
i = 8662 (of 10192), d = 37, its = 38
i = 8663 (of 10192), d = 37, its = 56
i = 8664 (of 10192), d = 2.97147, its = 18
i = 8665 (of 10192), d = 37, its = 52
i = 8666 (of 10192), d = 37, its = 37
i = 8667 (of 10192), d = 7.58005, its = 4
i = 8668 (of 10192), d = 37, its = 57
i = 8669 (of 10192), d = 37, its = 51
i = 8670 (of 10192), d = 37, its = 54
i = 8671 (of 10192), d = 3.32047, its = 18
i = 8672 (of 10192), d = 8.15198, its = 3
i = 8673 (of 10192), d = 37, its = 40
i = 8674 (of 10192), d = 37, its = 40
i = 8675 (of 10192), d = 37, its = 53
i = 8676 (of 10192), d = 37, its = 52
i = 8677 (of 10192), d = 37, its = 40
i = 8678 (of 10192), d = 37, its = 50
i = 8679 (of 10192), d = 6.85223, its = 5
i = 8680 (of 10192), d = 37, its = 37
i = 8681 (of 10192), d = 2.24491, its = 20
i = 8682 (of 10192), d = 37, its = 55
i = 8683 (of 10192), d = 2.66033, its = 19
i = 8684 (of 10192), d = 37, its = 53
i = 8685 (of 10192), d = 37, its = 56
i = 8686 (of 10192), d = 37, its = 55
i = 8687 (of 10192), d = 10.6007, its = 5
i = 8688 (of 10192), d = 37, its = 37
i = 8689 (of 10192), d = 37, its = 48
i = 8690 (of 10192), d = 37, its = 49
i = 8691 (of 10192), d = 37, its = 51
i = 8692 (of 10192), d = 37, its = 54
i = 8693 (of 10192), d = 37, its = 37
i = 8694 (of 10192), d = 6.95046, its = 5
i = 8695 (of 10192), d = 2.63141, its = 27
i = 8696 (of 10192), d = 6.36157, its = 6
i = 8697 (of 10192), d = 37, its = 52
i = 8698 (of 10192), d = 37, its = 50
i = 8699 (of 10192), d = 37, its = 54
i = 8700 (of 10192), d = 7.26837, its = 4
i = 8701 (of 10192), d = 37, its = 51
i = 8702 (of 10192), d = 37, its = 53
i = 8703 (of 10192), d = 4.70734, its = 19
i = 8704 (of 10192), d = 37, its = 54
i = 8705 (of 10192), d = 37, its = 37
i = 8706 (of 10192), d = 37, its = 37
i = 8707 (of 10192), d = 37, its = 38
i = 8708 (of 10192), d = 7.53773, its = 4
i = 8709 (of 10192), d = 37, its = 55
i = 8710 (of 10192), d = 8.09484, its = 3
i = 8711 (of 10192), d = 37, its = 37
i = 8712 (of 10192), d = 37, its = 55
i = 8713 (of 10192), d = 37, its = 37
i = 8714 (of 10192), d = 11.0891, its = 6
i = 8715 (of 10192), d = 37, its = 50
i = 8716 (of 10192), d = 2.37349, its = 19
i = 8717 (of 10192), d = 37, its = 56
i = 8718 (of 10192), d = 37, its = 41
i = 8719 (of 10192), d = 2.45652, its = 17
i = 8720 (of 10192), d = 26.1784, its = 9
i = 8721 (of 10192), d = 37, its = 38
i = 8722 (of 10192), d = 37, its = 37
i = 8723 (of 10192), d = 37, its = 54
i = 8724 (of 10192), d = 37, its = 53
i = 8725 (of 10192), d = 37, its = 37
i = 8726 (of 10192), d = 6.11367, its = 6
i = 8727 (of 10192), d = 6.70815, its = 5
i = 8728 (of 10192), d = 37, its = 53
i = 8729 (of 10192), d = 6.21867, its = 6
i = 8730 (of 10192), d = 37, its = 57
i = 8731 (of 10192), d = 37, its = 37
i = 8732 (of 10192), d = 37, its = 37
i = 8733 (of 10192), d = 37, its = 58
i = 8734 (of 10192), d = 37, its = 55
i = 8735 (of 10192), d = 37, its = 37
i = 8736 (of 10192), d = 37, its = 54
i = 8737 (of 10192), d = 37, its = 37
i = 8738 (of 10192), d = 37, its = 37
i = 8739 (of 10192), d = 3.30322, its = 18
i = 8740 (of 10192), d = 2.56604, its = 20
i = 8741 (of 10192), d = 37, its = 54
i = 8742 (of 10192), d = 37, its = 37
i = 8743 (of 10192), d = 37, its = 45
i = 8744 (of 10192), d = 3.46836, its = 18
i = 8745 (of 10192), d = 14.3823, its = 6
i = 8746 (of 10192), d = 37, its = 39
i = 8747 (of 10192), d = 37, its = 42
i = 8748 (of 10192), d = 7.78074, its = 3
i = 8749 (of 10192), d = 37, its = 52
i = 8750 (of 10192), d = 1.81781, its = 17
i = 8751 (of 10192), d = 37, its = 53
i = 8752 (of 10192), d = 37, its = 40
i = 8753 (of 10192), d = 37, its = 44
i = 8754 (of 10192), d = 10.203, its = 5
i = 8755 (of 10192), d = 10.6081, its = 5
i = 8756 (of 10192), d = 37, its = 39
i = 8757 (of 10192), d = 37, its = 49
i = 8758 (of 10192), d = 37, its = 53
i = 8759 (of 10192), d = 3.18613, its = 17
i = 8760 (of 10192), d = 37, its = 59
i = 8761 (of 10192), d = 37, its = 54
i = 8762 (of 10192), d = 37, its = 37
i = 8763 (of 10192), d = 7.66315, its = 4
i = 8764 (of 10192), d = 37, its = 54
i = 8765 (of 10192), d = 37, its = 37
i = 8766 (of 10192), d = 37, its = 49
i = 8767 (of 10192), d = 6.4932, its = 6
i = 8768 (of 10192), d = 2.2262, its = 17
i = 8769 (of 10192), d = 37, its = 42
i = 8770 (of 10192), d = 11.6752, its = 5
i = 8771 (of 10192), d = 37, its = 52
i = 8772 (of 10192), d = 37, its = 60
i = 8773 (of 10192), d = 2.45597, its = 19
i = 8774 (of 10192), d = 37, its = 39
i = 8775 (of 10192), d = 37, its = 37
i = 8776 (of 10192), d = 37, its = 39
i = 8777 (of 10192), d = 37, its = 38
i = 8778 (of 10192), d = 3.01985, its = 27
i = 8779 (of 10192), d = 37, its = 37
i = 8780 (of 10192), d = 37, its = 54
i = 8781 (of 10192), d = 37, its = 49
i = 8782 (of 10192), d = 37, its = 54
i = 8783 (of 10192), d = 37, its = 38
i = 8784 (of 10192), d = 37, its = 57
i = 8785 (of 10192), d = 37, its = 40
i = 8786 (of 10192), d = 37, its = 55
i = 8787 (of 10192), d = 37, its = 50
i = 8788 (of 10192), d = 37, its = 54
i = 8789 (of 10192), d = 37, its = 44
i = 8790 (of 10192), d = 2.483, its = 17
i = 8791 (of 10192), d = 37, its = 38
i = 8792 (of 10192), d = 37, its = 57
i = 8793 (of 10192), d = 37, its = 50
i = 8794 (of 10192), d = 37, its = 59
i = 8795 (of 10192), d = 5.29291, its = 6
i = 8796 (of 10192), d = 37, its = 40
i = 8797 (of 10192), d = 21.3907, its = 7
i = 8798 (of 10192), d = 2.76602, its = 17
i = 8799 (of 10192), d = 37, its = 37
i = 8800 (of 10192), d = 37, its = 38
i = 8801 (of 10192), d = 37, its = 39
i = 8802 (of 10192), d = 2.47527, its = 19
i = 8803 (of 10192), d = 37, its = 57
i = 8804 (of 10192), d = 37, its = 46
i = 8805 (of 10192), d = 37, its = 55
i = 8806 (of 10192), d = 37, its = 54
i = 8807 (of 10192), d = 37, its = 51
i = 8808 (of 10192), d = 37, its = 53
i = 8809 (of 10192), d = 20.0046, its = 7
i = 8810 (of 10192), d = 37, its = 50
i = 8811 (of 10192), d = 37, its = 40
i = 8812 (of 10192), d = 37, its = 54
i = 8813 (of 10192), d = 4.68257, its = 5
i = 8814 (of 10192), d = 37, its = 54
i = 8815 (of 10192), d = 8.2756, its = 3
i = 8816 (of 10192), d = 37, its = 55
i = 8817 (of 10192), d = 37, its = 51
i = 8818 (of 10192), d = 37, its = 56
i = 8819 (of 10192), d = 37, its = 61
i = 8820 (of 10192), d = 37, its = 48
i = 8821 (of 10192), d = 37, its = 37
i = 8822 (of 10192), d = 37, its = 52
i = 8823 (of 10192), d = 8.94824, its = 5
i = 8824 (of 10192), d = 7.75384, its = 3
i = 8825 (of 10192), d = 6.23646, its = 5
i = 8826 (of 10192), d = 37, its = 41
i = 8827 (of 10192), d = 8.2771, its = 3
i = 8828 (of 10192), d = 6.90828, its = 5
i = 8829 (of 10192), d = 37, its = 55
i = 8830 (of 10192), d = 37, its = 43
i = 8831 (of 10192), d = 37, its = 37
i = 8832 (of 10192), d = 7.60856, its = 4
i = 8833 (of 10192), d = 2.80729, its = 24
i = 8834 (of 10192), d = 37, its = 55
i = 8835 (of 10192), d = 37, its = 55
i = 8836 (of 10192), d = 8.44667, its = 4
i = 8837 (of 10192), d = 37, its = 37
i = 8838 (of 10192), d = 37, its = 37
i = 8839 (of 10192), d = 37, its = 40
i = 8840 (of 10192), d = 8.43633, its = 4
i = 8841 (of 10192), d = 37, its = 39
i = 8842 (of 10192), d = 37, its = 56
i = 8843 (of 10192), d = 5.78252, its = 7
i = 8844 (of 10192), d = 37, its = 42
i = 8845 (of 10192), d = 3.05817, its = 18
i = 8846 (of 10192), d = 4.44595, its = 19
i = 8847 (of 10192), d = 6.28748, its = 6
i = 8848 (of 10192), d = 37, its = 37
i = 8849 (of 10192), d = 37, its = 50
i = 8850 (of 10192), d = 37, its = 37
i = 8851 (of 10192), d = 37, its = 52
i = 8852 (of 10192), d = 9.13893, its = 4
i = 8853 (of 10192), d = 37, its = 52
i = 8854 (of 10192), d = 37, its = 37
i = 8855 (of 10192), d = 4.07083, its = 17
i = 8856 (of 10192), d = 7.04617, its = 5
i = 8857 (of 10192), d = 37, its = 50
i = 8858 (of 10192), d = 6.35981, its = 6
i = 8859 (of 10192), d = 1.78465, its = 18
i = 8860 (of 10192), d = 37, its = 41
i = 8861 (of 10192), d = 37, its = 39
i = 8862 (of 10192), d = 37, its = 40
i = 8863 (of 10192), d = 2.07852, its = 17
i = 8864 (of 10192), d = 37, its = 55
i = 8865 (of 10192), d = 37, its = 50
i = 8866 (of 10192), d = 2.59262, its = 17
i = 8867 (of 10192), d = 37, its = 53
i = 8868 (of 10192), d = 37, its = 43
i = 8869 (of 10192), d = 37, its = 37
i = 8870 (of 10192), d = 37, its = 37
i = 8871 (of 10192), d = 37, its = 37
i = 8872 (of 10192), d = 9.84841, its = 5
i = 8873 (of 10192), d = 37, its = 53
i = 8874 (of 10192), d = 10.7513, its = 6
i = 8875 (of 10192), d = 1.47406, its = 17
i = 8876 (of 10192), d = 37, its = 37
i = 8877 (of 10192), d = 37, its = 53
i = 8878 (of 10192), d = 37, its = 54
i = 8879 (of 10192), d = 10.4977, its = 5
i = 8880 (of 10192), d = 37, its = 52
i = 8881 (of 10192), d = 37, its = 37
i = 8882 (of 10192), d = 37, its = 37
i = 8883 (of 10192), d = 37, its = 40
i = 8884 (of 10192), d = 1.38718, its = 19
i = 8885 (of 10192), d = 8.8201, its = 4
i = 8886 (of 10192), d = 37, its = 54
i = 8887 (of 10192), d = 1.78039, its = 25
i = 8888 (of 10192), d = 3.3607, its = 18
i = 8889 (of 10192), d = 37, its = 43
i = 8890 (of 10192), d = 37, its = 37
i = 8891 (of 10192), d = 37, its = 55
i = 8892 (of 10192), d = 2.41738, its = 22
i = 8893 (of 10192), d = 3.40423, its = 23
i = 8894 (of 10192), d = 37, its = 52
i = 8895 (of 10192), d = 37, its = 37
i = 8896 (of 10192), d = 3.20416, its = 18
i = 8897 (of 10192), d = 37, its = 50
i = 8898 (of 10192), d = 10.2757, its = 5
i = 8899 (of 10192), d = 37, its = 37
i = 8900 (of 10192), d = 6.51009, its = 6
i = 8901 (of 10192), d = 37, its = 37
i = 8902 (of 10192), d = 9.96472, its = 5
i = 8903 (of 10192), d = 2.87367, its = 29
i = 8904 (of 10192), d = 37, its = 51
i = 8905 (of 10192), d = 8.57967, its = 4
i = 8906 (of 10192), d = 1.86616, its = 17
i = 8907 (of 10192), d = 2.2145, its = 17
i = 8908 (of 10192), d = 19.6514, its = 8
i = 8909 (of 10192), d = 37, its = 57
i = 8910 (of 10192), d = 1.72745, its = 23
i = 8911 (of 10192), d = 37, its = 51
i = 8912 (of 10192), d = 37, its = 53
i = 8913 (of 10192), d = 37, its = 54
i = 8914 (of 10192), d = 37, its = 53
i = 8915 (of 10192), d = 37, its = 37
i = 8916 (of 10192), d = 2.13751, its = 17
i = 8917 (of 10192), d = 37, its = 39
i = 8918 (of 10192), d = 6.81154, its = 5
i = 8919 (of 10192), d = 3.03458, its = 17
i = 8920 (of 10192), d = 3.53287, its = 24
i = 8921 (of 10192), d = 37, its = 54
i = 8922 (of 10192), d = 37, its = 37
i = 8923 (of 10192), d = 37, its = 51
i = 8924 (of 10192), d = 37, its = 57
i = 8925 (of 10192), d = 12.4909, its = 6
i = 8926 (of 10192), d = 37, its = 37
i = 8927 (of 10192), d = 37, its = 52
i = 8928 (of 10192), d = 37, its = 37
i = 8929 (of 10192), d = 37, its = 37
i = 8930 (of 10192), d = 37, its = 37
i = 8931 (of 10192), d = 37, its = 52
i = 8932 (of 10192), d = 37, its = 50
i = 8933 (of 10192), d = 37, its = 53
i = 8934 (of 10192), d = 37, its = 55
i = 8935 (of 10192), d = 11.8299, its = 7
i = 8936 (of 10192), d = 37, its = 53
i = 8937 (of 10192), d = 9.38353, its = 5
i = 8938 (of 10192), d = 6.76172, its = 5
i = 8939 (of 10192), d = 7.78373, its = 3
i = 8940 (of 10192), d = 4.47724, its = 22
i = 8941 (of 10192), d = 37, its = 41
i = 8942 (of 10192), d = 5.46951, its = 6
i = 8943 (of 10192), d = 37, its = 56
i = 8944 (of 10192), d = 16.2081, its = 7
i = 8945 (of 10192), d = 37, its = 37
i = 8946 (of 10192), d = 37, its = 48
i = 8947 (of 10192), d = 37, its = 40
i = 8948 (of 10192), d = 37, its = 54
i = 8949 (of 10192), d = 37, its = 44
i = 8950 (of 10192), d = 37, its = 48
i = 8951 (of 10192), d = 37, its = 53
i = 8952 (of 10192), d = 8.50726, its = 4
i = 8953 (of 10192), d = 37, its = 37
i = 8954 (of 10192), d = 37, its = 51
i = 8955 (of 10192), d = 33.0338, its = 8
i = 8956 (of 10192), d = 37, its = 37
i = 8957 (of 10192), d = 37, its = 55
i = 8958 (of 10192), d = 37, its = 40
i = 8959 (of 10192), d = 37, its = 39
i = 8960 (of 10192), d = 37, its = 37
i = 8961 (of 10192), d = 37, its = 53
i = 8962 (of 10192), d = 37, its = 55
i = 8963 (of 10192), d = 37, its = 54
i = 8964 (of 10192), d = 37, its = 53
i = 8965 (of 10192), d = 37, its = 49
i = 8966 (of 10192), d = 37, its = 50
i = 8967 (of 10192), d = 37, its = 49
i = 8968 (of 10192), d = 37, its = 37
i = 8969 (of 10192), d = 7.16308, its = 5
i = 8970 (of 10192), d = 37, its = 53
i = 8971 (of 10192), d = 2.45284, its = 17
i = 8972 (of 10192), d = 37, its = 50
i = 8973 (of 10192), d = 37, its = 57
i = 8974 (of 10192), d = 37, its = 37
i = 8975 (of 10192), d = 37, its = 55
i = 8976 (of 10192), d = 37, its = 37
i = 8977 (of 10192), d = 11.0891, its = 6
i = 8978 (of 10192), d = 11.8179, its = 6
i = 8979 (of 10192), d = 37, its = 40
i = 8980 (of 10192), d = 37, its = 52
i = 8981 (of 10192), d = 37, its = 50
i = 8982 (of 10192), d = 5.01217, its = 6
i = 8983 (of 10192), d = 37, its = 56
i = 8984 (of 10192), d = 37, its = 56
i = 8985 (of 10192), d = 37, its = 53
i = 8986 (of 10192), d = 37, its = 54
i = 8987 (of 10192), d = 2.44679, its = 17
i = 8988 (of 10192), d = 37, its = 50
i = 8989 (of 10192), d = 7.77069, its = 3
i = 8990 (of 10192), d = 4.53911, its = 19
i = 8991 (of 10192), d = 37, its = 40
i = 8992 (of 10192), d = 9.96118, its = 5
i = 8993 (of 10192), d = 6.91865, its = 5
i = 8994 (of 10192), d = 14.5424, its = 8
i = 8995 (of 10192), d = 37, its = 53
i = 8996 (of 10192), d = 37, its = 54
i = 8997 (of 10192), d = 37, its = 55
i = 8998 (of 10192), d = 37, its = 54
i = 8999 (of 10192), d = 37, its = 48
i = 9000 (of 10192), d = 10.1208, its = 6
i = 9001 (of 10192), d = 37, its = 40
i = 9002 (of 10192), d = 37, its = 37
i = 9003 (of 10192), d = 37, its = 37
i = 9004 (of 10192), d = 37, its = 47
i = 9005 (of 10192), d = 37, its = 41
i = 9006 (of 10192), d = 37, its = 40
i = 9007 (of 10192), d = 4.7521, its = 6
i = 9008 (of 10192), d = 4.79607, its = 25
i = 9009 (of 10192), d = 37, its = 57
i = 9010 (of 10192), d = 13.644, its = 6
i = 9011 (of 10192), d = 37, its = 37
i = 9012 (of 10192), d = 9.66396, its = 5
i = 9013 (of 10192), d = 37, its = 22
i = 9014 (of 10192), d = 37, its = 37
i = 9015 (of 10192), d = 37, its = 37
i = 9016 (of 10192), d = 37, its = 51
i = 9017 (of 10192), d = 37, its = 50
i = 9018 (of 10192), d = 37, its = 40
i = 9019 (of 10192), d = 37, its = 37
i = 9020 (of 10192), d = 37, its = 37
i = 9021 (of 10192), d = 37, its = 51
i = 9022 (of 10192), d = 5.10211, its = 19
i = 9023 (of 10192), d = 9.88911, its = 5
i = 9024 (of 10192), d = 37, its = 51
i = 9025 (of 10192), d = 7.04317, its = 5
i = 9026 (of 10192), d = 8.0084, its = 2
i = 9027 (of 10192), d = 8.47561, its = 4
i = 9028 (of 10192), d = 11.5088, its = 8
i = 9029 (of 10192), d = 11.106, its = 6
i = 9030 (of 10192), d = 37, its = 37
i = 9031 (of 10192), d = 2.40383, its = 20
i = 9032 (of 10192), d = 37, its = 49
i = 9033 (of 10192), d = 37, its = 53
i = 9034 (of 10192), d = 37, its = 50
i = 9035 (of 10192), d = 37, its = 47
i = 9036 (of 10192), d = 37, its = 41
i = 9037 (of 10192), d = 37, its = 49
i = 9038 (of 10192), d = 37, its = 37
i = 9039 (of 10192), d = 11.6039, its = 6
i = 9040 (of 10192), d = 3.9699, its = 18
i = 9041 (of 10192), d = 37, its = 37
i = 9042 (of 10192), d = 1.49403, its = 17
i = 9043 (of 10192), d = 6.92058, its = 5
i = 9044 (of 10192), d = 37, its = 51
i = 9045 (of 10192), d = 5.24962, its = 6
i = 9046 (of 10192), d = 37, its = 53
i = 9047 (of 10192), d = 37, its = 53
i = 9048 (of 10192), d = 4.08079, its = 19
i = 9049 (of 10192), d = 37, its = 56
i = 9050 (of 10192), d = 37, its = 42
i = 9051 (of 10192), d = 10.6536, its = 5
i = 9052 (of 10192), d = 37, its = 40
i = 9053 (of 10192), d = 37, its = 53
i = 9054 (of 10192), d = 37, its = 48
i = 9055 (of 10192), d = 14.7566, its = 7
i = 9056 (of 10192), d = 13.5381, its = 6
i = 9057 (of 10192), d = 37, its = 56
i = 9058 (of 10192), d = 37, its = 55
i = 9059 (of 10192), d = 37, its = 41
i = 9060 (of 10192), d = 37, its = 52
i = 9061 (of 10192), d = 6.18708, its = 6
i = 9062 (of 10192), d = 37, its = 51
i = 9063 (of 10192), d = 37, its = 54
i = 9064 (of 10192), d = 37, its = 52
i = 9065 (of 10192), d = 37, its = 55
i = 9066 (of 10192), d = 2.25042, its = 19
i = 9067 (of 10192), d = 37, its = 37
i = 9068 (of 10192), d = 37, its = 51
i = 9069 (of 10192), d = 37, its = 51
i = 9070 (of 10192), d = 37, its = 52
i = 9071 (of 10192), d = 37, its = 48
i = 9072 (of 10192), d = 37, its = 40
i = 9073 (of 10192), d = 27.524, its = 8
i = 9074 (of 10192), d = 37, its = 52
i = 9075 (of 10192), d = 5.58998, its = 6
i = 9076 (of 10192), d = 37, its = 55
i = 9077 (of 10192), d = 37, its = 43
i = 9078 (of 10192), d = 3.04109, its = 21
i = 9079 (of 10192), d = 37, its = 40
i = 9080 (of 10192), d = 37, its = 55
i = 9081 (of 10192), d = 37, its = 52
i = 9082 (of 10192), d = 10.2872, its = 5
i = 9083 (of 10192), d = 37, its = 53
i = 9084 (of 10192), d = 37, its = 37
i = 9085 (of 10192), d = 37, its = 52
i = 9086 (of 10192), d = 37, its = 51
i = 9087 (of 10192), d = 37, its = 55
i = 9088 (of 10192), d = 37, its = 53
i = 9089 (of 10192), d = 9.02418, its = 4
i = 9090 (of 10192), d = 37, its = 49
i = 9091 (of 10192), d = 37, its = 48
i = 9092 (of 10192), d = 37, its = 55
i = 9093 (of 10192), d = 37, its = 57
i = 9094 (of 10192), d = 37, its = 37
i = 9095 (of 10192), d = 37, its = 51
i = 9096 (of 10192), d = 37, its = 40
i = 9097 (of 10192), d = 37, its = 38
i = 9098 (of 10192), d = 2.45652, its = 17
i = 9099 (of 10192), d = 37, its = 57
i = 9100 (of 10192), d = 37, its = 37
i = 9101 (of 10192), d = 1.14355, its = 22
i = 9102 (of 10192), d = 37, its = 40
i = 9103 (of 10192), d = 37, its = 53
i = 9104 (of 10192), d = 37, its = 53
i = 9105 (of 10192), d = 37, its = 40
i = 9106 (of 10192), d = 8.06159, its = 3
i = 9107 (of 10192), d = 37, its = 51
i = 9108 (of 10192), d = 5.97033, its = 7
i = 9109 (of 10192), d = 37, its = 58
i = 9110 (of 10192), d = 2.52053, its = 20
i = 9111 (of 10192), d = 3.75827, its = 18
i = 9112 (of 10192), d = 37, its = 59
i = 9113 (of 10192), d = 7.39305, its = 4
i = 9114 (of 10192), d = 1.75444, its = 24
i = 9115 (of 10192), d = 37, its = 37
i = 9116 (of 10192), d = 37, its = 50
i = 9117 (of 10192), d = 1.63614, its = 20
i = 9118 (of 10192), d = 37, its = 38
i = 9119 (of 10192), d = 37, its = 53
i = 9120 (of 10192), d = 4.39561, its = 19
i = 9121 (of 10192), d = 37, its = 37
i = 9122 (of 10192), d = 3.56242, its = 19
i = 9123 (of 10192), d = 37, its = 51
i = 9124 (of 10192), d = 2.42502, its = 19
i = 9125 (of 10192), d = 37, its = 55
i = 9126 (of 10192), d = 37, its = 41
i = 9127 (of 10192), d = 10.619, its = 6
i = 9128 (of 10192), d = 37, its = 53
i = 9129 (of 10192), d = 37, its = 48
i = 9130 (of 10192), d = 37, its = 53
i = 9131 (of 10192), d = 37, its = 51
i = 9132 (of 10192), d = 37, its = 53
i = 9133 (of 10192), d = 37, its = 52
i = 9134 (of 10192), d = 7.10697, its = 5
i = 9135 (of 10192), d = 37, its = 37
i = 9136 (of 10192), d = 37, its = 39
i = 9137 (of 10192), d = 37, its = 52
i = 9138 (of 10192), d = 37, its = 55
i = 9139 (of 10192), d = 37, its = 50
i = 9140 (of 10192), d = 14.2832, its = 6
i = 9141 (of 10192), d = 37, its = 55
i = 9142 (of 10192), d = 3.3937, its = 18
i = 9143 (of 10192), d = 37, its = 37
i = 9144 (of 10192), d = 2.53972, its = 17
i = 9145 (of 10192), d = 37, its = 53
i = 9146 (of 10192), d = 14.4016, its = 8
i = 9147 (of 10192), d = 37, its = 37
i = 9148 (of 10192), d = 10.6081, its = 5
i = 9149 (of 10192), d = 37, its = 52
i = 9150 (of 10192), d = 37, its = 51
i = 9151 (of 10192), d = 6.67436, its = 5
i = 9152 (of 10192), d = 37, its = 53
i = 9153 (of 10192), d = 37, its = 53
i = 9154 (of 10192), d = 37, its = 51
i = 9155 (of 10192), d = 37, its = 37
i = 9156 (of 10192), d = 37, its = 37
i = 9157 (of 10192), d = 37, its = 40
i = 9158 (of 10192), d = 2.20324, its = 27
i = 9159 (of 10192), d = 6.73214, its = 5
i = 9160 (of 10192), d = 4.73489, its = 7
i = 9161 (of 10192), d = 2.44811, its = 17
i = 9162 (of 10192), d = 37, its = 51
i = 9163 (of 10192), d = 37, its = 52
i = 9164 (of 10192), d = 37, its = 55
i = 9165 (of 10192), d = 37, its = 51
i = 9166 (of 10192), d = 7.47528, its = 4
i = 9167 (of 10192), d = 5.61427, its = 6
i = 9168 (of 10192), d = 11.1745, its = 5
i = 9169 (of 10192), d = 8.1191, its = 3
i = 9170 (of 10192), d = 8.44466, its = 4
i = 9171 (of 10192), d = 37, its = 51
i = 9172 (of 10192), d = 37, its = 51
i = 9173 (of 10192), d = 37, its = 41
i = 9174 (of 10192), d = 37, its = 37
i = 9175 (of 10192), d = 37, its = 51
i = 9176 (of 10192), d = 37, its = 59
i = 9177 (of 10192), d = 11.7872, its = 6
i = 9178 (of 10192), d = 6.14361, its = 5
i = 9179 (of 10192), d = 37, its = 53
i = 9180 (of 10192), d = 5.04891, its = 6
i = 9181 (of 10192), d = 37, its = 37
i = 9182 (of 10192), d = 8.4612, its = 4
i = 9183 (of 10192), d = 37, its = 37
i = 9184 (of 10192), d = 12.031, its = 5
i = 9185 (of 10192), d = 2.97749, its = 17
i = 9186 (of 10192), d = 10.5421, its = 5
i = 9187 (of 10192), d = 5.4787, its = 6
i = 9188 (of 10192), d = 37, its = 40
i = 9189 (of 10192), d = 37, its = 56
i = 9190 (of 10192), d = 37, its = 37
i = 9191 (of 10192), d = 6.97204, its = 6
i = 9192 (of 10192), d = 11.5803, its = 7
i = 9193 (of 10192), d = 37, its = 52
i = 9194 (of 10192), d = 37, its = 53
i = 9195 (of 10192), d = 6.25408, its = 6
i = 9196 (of 10192), d = 37, its = 39
i = 9197 (of 10192), d = 37, its = 37
i = 9198 (of 10192), d = 37, its = 53
i = 9199 (of 10192), d = 37, its = 50
i = 9200 (of 10192), d = 11.1744, its = 5
i = 9201 (of 10192), d = 37, its = 51
i = 9202 (of 10192), d = 4.18871, its = 19
i = 9203 (of 10192), d = 1.83154, its = 18
i = 9204 (of 10192), d = 8.37936, its = 4
i = 9205 (of 10192), d = 37, its = 54
i = 9206 (of 10192), d = 3.05124, its = 17
i = 9207 (of 10192), d = 37, its = 53
i = 9208 (of 10192), d = 37, its = 37
i = 9209 (of 10192), d = 37, its = 37
i = 9210 (of 10192), d = 37, its = 41
i = 9211 (of 10192), d = 37, its = 53
i = 9212 (of 10192), d = 37, its = 57
i = 9213 (of 10192), d = 12.0835, its = 6
i = 9214 (of 10192), d = 3.45996, its = 18
i = 9215 (of 10192), d = 37, its = 56
i = 9216 (of 10192), d = 5.03406, its = 8
i = 9217 (of 10192), d = 4.54469, its = 17
i = 9218 (of 10192), d = 37, its = 45
i = 9219 (of 10192), d = 5.74146, its = 6
i = 9220 (of 10192), d = 3.10822, its = 25
i = 9221 (of 10192), d = 37, its = 54
i = 9222 (of 10192), d = 37, its = 61
i = 9223 (of 10192), d = 37, its = 54
i = 9224 (of 10192), d = 6.26636, its = 6
i = 9225 (of 10192), d = 37, its = 54
i = 9226 (of 10192), d = 37, its = 37
i = 9227 (of 10192), d = 8.4261, its = 4
i = 9228 (of 10192), d = 37, its = 49
i = 9229 (of 10192), d = 9.38736, its = 5
i = 9230 (of 10192), d = 6.31086, its = 6
i = 9231 (of 10192), d = 37, its = 46
i = 9232 (of 10192), d = 3.47528, its = 23
i = 9233 (of 10192), d = 37, its = 37
i = 9234 (of 10192), d = 37, its = 37
i = 9235 (of 10192), d = 8.55483, its = 4
i = 9236 (of 10192), d = 1.9607, its = 18
i = 9237 (of 10192), d = 37, its = 52
i = 9238 (of 10192), d = 37, its = 54
i = 9239 (of 10192), d = 37, its = 37
i = 9240 (of 10192), d = 37, its = 52
i = 9241 (of 10192), d = 14.5956, its = 6
i = 9242 (of 10192), d = 37, its = 57
i = 9243 (of 10192), d = 37, its = 52
i = 9244 (of 10192), d = 9.17955, its = 4
i = 9245 (of 10192), d = 37, its = 52
i = 9246 (of 10192), d = 7.59774, its = 4
i = 9247 (of 10192), d = 3.21671, its = 25
i = 9248 (of 10192), d = 37, its = 52
i = 9249 (of 10192), d = 10.326, its = 5
i = 9250 (of 10192), d = 6.76172, its = 5
i = 9251 (of 10192), d = 37, its = 58
i = 9252 (of 10192), d = 37, its = 52
i = 9253 (of 10192), d = 37, its = 53
i = 9254 (of 10192), d = 8.98587, its = 4
i = 9255 (of 10192), d = 7.0206, its = 5
i = 9256 (of 10192), d = 37, its = 49
i = 9257 (of 10192), d = 37, its = 51
i = 9258 (of 10192), d = 37, its = 50
i = 9259 (of 10192), d = 2.88475, its = 19
i = 9260 (of 10192), d = 37, its = 56
i = 9261 (of 10192), d = 37, its = 39
i = 9262 (of 10192), d = 37, its = 40
i = 9263 (of 10192), d = 37, its = 37
i = 9264 (of 10192), d = 37, its = 49
i = 9265 (of 10192), d = 37, its = 56
i = 9266 (of 10192), d = 37, its = 55
i = 9267 (of 10192), d = 1.98159, its = 18
i = 9268 (of 10192), d = 37, its = 57
i = 9269 (of 10192), d = 37, its = 57
i = 9270 (of 10192), d = 3.97728, its = 19
i = 9271 (of 10192), d = 37, its = 53
i = 9272 (of 10192), d = 10.8031, its = 5
i = 9273 (of 10192), d = 37, its = 55
i = 9274 (of 10192), d = 37, its = 51
i = 9275 (of 10192), d = 37, its = 37
i = 9276 (of 10192), d = 37, its = 37
i = 9277 (of 10192), d = 37, its = 39
i = 9278 (of 10192), d = 37, its = 37
i = 9279 (of 10192), d = 37, its = 57
i = 9280 (of 10192), d = 37, its = 58
i = 9281 (of 10192), d = 37, its = 37
i = 9282 (of 10192), d = 7.82247, its = 3
i = 9283 (of 10192), d = 37, its = 53
i = 9284 (of 10192), d = 37, its = 37
i = 9285 (of 10192), d = 37, its = 37
i = 9286 (of 10192), d = 3.18495, its = 18
i = 9287 (of 10192), d = 14.6587, its = 6
i = 9288 (of 10192), d = 37, its = 52
i = 9289 (of 10192), d = 37, its = 51
i = 9290 (of 10192), d = 37, its = 53
i = 9291 (of 10192), d = 37, its = 53
i = 9292 (of 10192), d = 37, its = 52
i = 9293 (of 10192), d = 10.5382, its = 5
i = 9294 (of 10192), d = 7.94339, its = 3
i = 9295 (of 10192), d = 37, its = 50
i = 9296 (of 10192), d = 37, its = 54
i = 9297 (of 10192), d = 6.08219, its = 6
i = 9298 (of 10192), d = 3.15485, its = 16
i = 9299 (of 10192), d = 37, its = 53
i = 9300 (of 10192), d = 9.38421, its = 5
i = 9301 (of 10192), d = 4.19211, its = 23
i = 9302 (of 10192), d = 37, its = 54
i = 9303 (of 10192), d = 37, its = 53
i = 9304 (of 10192), d = 37, its = 37
i = 9305 (of 10192), d = 37, its = 55
i = 9306 (of 10192), d = 37, its = 52
i = 9307 (of 10192), d = 37, its = 37
i = 9308 (of 10192), d = 37, its = 49
i = 9309 (of 10192), d = 9.38576, its = 5
i = 9310 (of 10192), d = 37, its = 42
i = 9311 (of 10192), d = 37, its = 43
i = 9312 (of 10192), d = 8.22268, its = 3
i = 9313 (of 10192), d = 37, its = 37
i = 9314 (of 10192), d = 37, its = 51
i = 9315 (of 10192), d = 37, its = 50
i = 9316 (of 10192), d = 37, its = 50
i = 9317 (of 10192), d = 2.82363, its = 22
i = 9318 (of 10192), d = 5.56291, its = 6
i = 9319 (of 10192), d = 37, its = 54
i = 9320 (of 10192), d = 37, its = 54
i = 9321 (of 10192), d = 7.2304, its = 4
i = 9322 (of 10192), d = 37, its = 37
i = 9323 (of 10192), d = 37, its = 51
i = 9324 (of 10192), d = 2.64268, its = 19
i = 9325 (of 10192), d = 37, its = 52
i = 9326 (of 10192), d = 37, its = 41
i = 9327 (of 10192), d = 37, its = 37
i = 9328 (of 10192), d = 37, its = 40
i = 9329 (of 10192), d = 37, its = 37
i = 9330 (of 10192), d = 37, its = 40
i = 9331 (of 10192), d = 8.37876, its = 4
i = 9332 (of 10192), d = 37, its = 56
i = 9333 (of 10192), d = 7.00259, its = 7
i = 9334 (of 10192), d = 37, its = 37
i = 9335 (of 10192), d = 2.41349, its = 18
i = 9336 (of 10192), d = 37, its = 37
i = 9337 (of 10192), d = 37, its = 52
i = 9338 (of 10192), d = 2.88022, its = 18
i = 9339 (of 10192), d = 37, its = 41
i = 9340 (of 10192), d = 5.41114, its = 7
i = 9341 (of 10192), d = 37, its = 54
i = 9342 (of 10192), d = 37, its = 50
i = 9343 (of 10192), d = 37, its = 52
i = 9344 (of 10192), d = 4.31547, its = 21
i = 9345 (of 10192), d = 37, its = 40
i = 9346 (of 10192), d = 37, its = 37
i = 9347 (of 10192), d = 5.07996, its = 8
i = 9348 (of 10192), d = 37, its = 52
i = 9349 (of 10192), d = 8.97975, its = 4
i = 9350 (of 10192), d = 6.60594, its = 6
i = 9351 (of 10192), d = 37, its = 46
i = 9352 (of 10192), d = 37, its = 54
i = 9353 (of 10192), d = 8.69178, its = 4
i = 9354 (of 10192), d = 37, its = 56
i = 9355 (of 10192), d = 6.14238, its = 6
i = 9356 (of 10192), d = 8.75735, its = 4
i = 9357 (of 10192), d = 37, its = 52
i = 9358 (of 10192), d = 7.41179, its = 4
i = 9359 (of 10192), d = 3.14173, its = 16
i = 9360 (of 10192), d = 37, its = 54
i = 9361 (of 10192), d = 37, its = 37
i = 9362 (of 10192), d = 37, its = 51
i = 9363 (of 10192), d = 12.3407, its = 6
i = 9364 (of 10192), d = 37, its = 39
i = 9365 (of 10192), d = 37, its = 37
i = 9366 (of 10192), d = 37, its = 56
i = 9367 (of 10192), d = 6.58741, its = 6
i = 9368 (of 10192), d = 2.60652, its = 26
i = 9369 (of 10192), d = 37, its = 53
i = 9370 (of 10192), d = 37, its = 40
i = 9371 (of 10192), d = 37, its = 37
i = 9372 (of 10192), d = 37, its = 52
i = 9373 (of 10192), d = 37, its = 37
i = 9374 (of 10192), d = 37, its = 41
i = 9375 (of 10192), d = 37, its = 37
i = 9376 (of 10192), d = 37, its = 53
i = 9377 (of 10192), d = 37, its = 37
i = 9378 (of 10192), d = 37, its = 50
i = 9379 (of 10192), d = 37, its = 52
i = 9380 (of 10192), d = 37, its = 52
i = 9381 (of 10192), d = 4.17112, its = 17
i = 9382 (of 10192), d = 37, its = 57
i = 9383 (of 10192), d = 3.9389, its = 18
i = 9384 (of 10192), d = 37, its = 54
i = 9385 (of 10192), d = 37, its = 56
i = 9386 (of 10192), d = 37, its = 55
i = 9387 (of 10192), d = 37, its = 54
i = 9388 (of 10192), d = 37, its = 42
i = 9389 (of 10192), d = 37, its = 37
i = 9390 (of 10192), d = 37, its = 52
i = 9391 (of 10192), d = 5.4066, its = 20
i = 9392 (of 10192), d = 37, its = 38
i = 9393 (of 10192), d = 37, its = 53
i = 9394 (of 10192), d = 37, its = 57
i = 9395 (of 10192), d = 37, its = 55
i = 9396 (of 10192), d = 37, its = 39
i = 9397 (of 10192), d = 37, its = 56
i = 9398 (of 10192), d = 4.73027, its = 21
i = 9399 (of 10192), d = 37, its = 55
i = 9400 (of 10192), d = 6.9454, its = 5
i = 9401 (of 10192), d = 37, its = 54
i = 9402 (of 10192), d = 37, its = 42
i = 9403 (of 10192), d = 5.03406, its = 8
i = 9404 (of 10192), d = 37, its = 58
i = 9405 (of 10192), d = 7.79849, its = 3
i = 9406 (of 10192), d = 37, its = 53
i = 9407 (of 10192), d = 6.94479, its = 5
i = 9408 (of 10192), d = 37, its = 57
i = 9409 (of 10192), d = 37, its = 55
i = 9410 (of 10192), d = 37, its = 53
i = 9411 (of 10192), d = 37, its = 55
i = 9412 (of 10192), d = 37, its = 49
i = 9413 (of 10192), d = 37, its = 54
i = 9414 (of 10192), d = 37, its = 37
i = 9415 (of 10192), d = 37, its = 49
i = 9416 (of 10192), d = 9.53116, its = 5
i = 9417 (of 10192), d = 37, its = 52
i = 9418 (of 10192), d = 37, its = 54
i = 9419 (of 10192), d = 37, its = 37
i = 9420 (of 10192), d = 14.9393, its = 8
i = 9421 (of 10192), d = 10.2459, its = 5
i = 9422 (of 10192), d = 37, its = 55
i = 9423 (of 10192), d = 37, its = 37
i = 9424 (of 10192), d = 37, its = 58
i = 9425 (of 10192), d = 37, its = 37
i = 9426 (of 10192), d = 37, its = 37
i = 9427 (of 10192), d = 37, its = 39
i = 9428 (of 10192), d = 14.6587, its = 6
i = 9429 (of 10192), d = 37, its = 55
i = 9430 (of 10192), d = 6.75718, its = 5
i = 9431 (of 10192), d = 37, its = 52
i = 9432 (of 10192), d = 37, its = 37
i = 9433 (of 10192), d = 1.72031, its = 18
i = 9434 (of 10192), d = 37, its = 53
i = 9435 (of 10192), d = 37, its = 40
i = 9436 (of 10192), d = 19.1969, its = 7
i = 9437 (of 10192), d = 9.66557, its = 4
i = 9438 (of 10192), d = 37, its = 37
i = 9439 (of 10192), d = 4.7221, its = 19
i = 9440 (of 10192), d = 7.87273, its = 3
i = 9441 (of 10192), d = 37, its = 52
i = 9442 (of 10192), d = 37, its = 49
i = 9443 (of 10192), d = 37, its = 52
i = 9444 (of 10192), d = 37, its = 52
i = 9445 (of 10192), d = 37, its = 51
i = 9446 (of 10192), d = 7.78205, its = 4
i = 9447 (of 10192), d = 5.44914, its = 6
i = 9448 (of 10192), d = 37, its = 50
i = 9449 (of 10192), d = 37, its = 52
i = 9450 (of 10192), d = 16.0174, its = 7
i = 9451 (of 10192), d = 37, its = 54
i = 9452 (of 10192), d = 37, its = 51
i = 9453 (of 10192), d = 6.17256, its = 6
i = 9454 (of 10192), d = 10.1313, its = 5
i = 9455 (of 10192), d = 37, its = 53
i = 9456 (of 10192), d = 1.89055, its = 19
i = 9457 (of 10192), d = 9.66656, its = 5
i = 9458 (of 10192), d = 37, its = 52
i = 9459 (of 10192), d = 37, its = 37
i = 9460 (of 10192), d = 37, its = 53
i = 9461 (of 10192), d = 37, its = 53
i = 9462 (of 10192), d = 37, its = 51
i = 9463 (of 10192), d = 3.35022, its = 19
i = 9464 (of 10192), d = 5.56467, its = 7
i = 9465 (of 10192), d = 1.51194, its = 23
i = 9466 (of 10192), d = 10.0483, its = 5
i = 9467 (of 10192), d = 37, its = 47
i = 9468 (of 10192), d = 37, its = 53
i = 9469 (of 10192), d = 37, its = 37
i = 9470 (of 10192), d = 37, its = 37
i = 9471 (of 10192), d = 37, its = 51
i = 9472 (of 10192), d = 37, its = 52
i = 9473 (of 10192), d = 37, its = 51
i = 9474 (of 10192), d = 37, its = 55
i = 9475 (of 10192), d = 37, its = 53
i = 9476 (of 10192), d = 7.3774, its = 4
i = 9477 (of 10192), d = 37, its = 40
i = 9478 (of 10192), d = 37, its = 54
i = 9479 (of 10192), d = 2.01316, its = 17
i = 9480 (of 10192), d = 7.62127, its = 4
i = 9481 (of 10192), d = 37, its = 43
i = 9482 (of 10192), d = 10.1894, its = 5
i = 9483 (of 10192), d = 37, its = 51
i = 9484 (of 10192), d = 7.08604, its = 4
i = 9485 (of 10192), d = 8.34354, its = 4
i = 9486 (of 10192), d = 37, its = 53
i = 9487 (of 10192), d = 37, its = 56
i = 9488 (of 10192), d = 4.11238, its = 20
i = 9489 (of 10192), d = 1.59021, its = 18
i = 9490 (of 10192), d = 37, its = 50
i = 9491 (of 10192), d = 21.9039, its = 8
i = 9492 (of 10192), d = 37, its = 55
i = 9493 (of 10192), d = 37, its = 57
i = 9494 (of 10192), d = 37, its = 55
i = 9495 (of 10192), d = 37, its = 37
i = 9496 (of 10192), d = 14.9911, its = 7
i = 9497 (of 10192), d = 37, its = 49
i = 9498 (of 10192), d = 11.388, its = 7
i = 9499 (of 10192), d = 6.14471, its = 6
i = 9500 (of 10192), d = 37, its = 39
i = 9501 (of 10192), d = 37, its = 54
i = 9502 (of 10192), d = 8.35804, its = 4
i = 9503 (of 10192), d = 4.39634, its = 20
i = 9504 (of 10192), d = 37, its = 54
i = 9505 (of 10192), d = 5.62763, its = 6
i = 9506 (of 10192), d = 2.3071, its = 18
i = 9507 (of 10192), d = 37, its = 50
i = 9508 (of 10192), d = 7.35701, its = 4
i = 9509 (of 10192), d = 4.92194, its = 6
i = 9510 (of 10192), d = 3.20313, its = 19
i = 9511 (of 10192), d = 37, its = 52
i = 9512 (of 10192), d = 37, its = 37
i = 9513 (of 10192), d = 5.9208, its = 6
i = 9514 (of 10192), d = 37, its = 50
i = 9515 (of 10192), d = 37, its = 51
i = 9516 (of 10192), d = 17.9584, its = 7
i = 9517 (of 10192), d = 37, its = 56
i = 9518 (of 10192), d = 3.87375, its = 17
i = 9519 (of 10192), d = 37, its = 54
i = 9520 (of 10192), d = 37, its = 52
i = 9521 (of 10192), d = 37, its = 41
i = 9522 (of 10192), d = 8.96648, its = 4
i = 9523 (of 10192), d = 37, its = 37
i = 9524 (of 10192), d = 37, its = 53
i = 9525 (of 10192), d = 1.42551, its = 28
i = 9526 (of 10192), d = 37, its = 37
i = 9527 (of 10192), d = 37, its = 49
i = 9528 (of 10192), d = 37, its = 55
i = 9529 (of 10192), d = 37, its = 55
i = 9530 (of 10192), d = 37, its = 37
i = 9531 (of 10192), d = 37, its = 54
i = 9532 (of 10192), d = 37, its = 37
i = 9533 (of 10192), d = 37, its = 37
i = 9534 (of 10192), d = 8.74226, its = 5
i = 9535 (of 10192), d = 37, its = 53
i = 9536 (of 10192), d = 37, its = 53
i = 9537 (of 10192), d = 37, its = 54
i = 9538 (of 10192), d = 23.9381, its = 8
i = 9539 (of 10192), d = 37, its = 50
i = 9540 (of 10192), d = 37, its = 37
i = 9541 (of 10192), d = 9.70427, its = 7
i = 9542 (of 10192), d = 37, its = 53
i = 9543 (of 10192), d = 37, its = 53
i = 9544 (of 10192), d = 37, its = 52
i = 9545 (of 10192), d = 37, its = 52
i = 9546 (of 10192), d = 17.08, its = 8
i = 9547 (of 10192), d = 37, its = 52
i = 9548 (of 10192), d = 6.7624, its = 6
i = 9549 (of 10192), d = 5.33282, its = 6
i = 9550 (of 10192), d = 37, its = 51
i = 9551 (of 10192), d = 10.579, its = 5
i = 9552 (of 10192), d = 37, its = 37
i = 9553 (of 10192), d = 4.16074, its = 19
i = 9554 (of 10192), d = 7.775, its = 3
i = 9555 (of 10192), d = 37, its = 53
i = 9556 (of 10192), d = 37, its = 54
i = 9557 (of 10192), d = 37, its = 37
i = 9558 (of 10192), d = 37, its = 53
i = 9559 (of 10192), d = 4.4342, its = 21
i = 9560 (of 10192), d = 37, its = 55
i = 9561 (of 10192), d = 37, its = 55
i = 9562 (of 10192), d = 37, its = 49
i = 9563 (of 10192), d = 37, its = 37
i = 9564 (of 10192), d = 37, its = 37
i = 9565 (of 10192), d = 37, its = 53
i = 9566 (of 10192), d = 6.99687, its = 5
i = 9567 (of 10192), d = 37, its = 49
i = 9568 (of 10192), d = 37, its = 53
i = 9569 (of 10192), d = 10.269, its = 6
i = 9570 (of 10192), d = 37, its = 56
i = 9571 (of 10192), d = 3.10542, its = 18
i = 9572 (of 10192), d = 37, its = 43
i = 9573 (of 10192), d = 37, its = 54
i = 9574 (of 10192), d = 10.8189, its = 6
i = 9575 (of 10192), d = 37, its = 54
i = 9576 (of 10192), d = 37, its = 37
i = 9577 (of 10192), d = 4.15803, its = 18
i = 9578 (of 10192), d = 8.89207, its = 4
i = 9579 (of 10192), d = 37, its = 49
i = 9580 (of 10192), d = 3.75985, its = 23
i = 9581 (of 10192), d = 37, its = 50
i = 9582 (of 10192), d = 37, its = 53
i = 9583 (of 10192), d = 37, its = 40
i = 9584 (of 10192), d = 10.5832, its = 5
i = 9585 (of 10192), d = 37, its = 51
i = 9586 (of 10192), d = 37, its = 54
i = 9587 (of 10192), d = 37, its = 54
i = 9588 (of 10192), d = 37, its = 54
i = 9589 (of 10192), d = 37, its = 54
i = 9590 (of 10192), d = 37, its = 50
i = 9591 (of 10192), d = 37, its = 53
i = 9592 (of 10192), d = 37, its = 54
i = 9593 (of 10192), d = 28.3911, its = 8
i = 9594 (of 10192), d = 8.37442, its = 4
i = 9595 (of 10192), d = 37, its = 54
i = 9596 (of 10192), d = 37, its = 39
i = 9597 (of 10192), d = 12.9034, its = 6
i = 9598 (of 10192), d = 13.3822, its = 7
i = 9599 (of 10192), d = 37, its = 37
i = 9600 (of 10192), d = 37, its = 51
i = 9601 (of 10192), d = 37, its = 37
i = 9602 (of 10192), d = 8.08845, its = 3
i = 9603 (of 10192), d = 37, its = 39
i = 9604 (of 10192), d = 37, its = 55
i = 9605 (of 10192), d = 37, its = 43
i = 9606 (of 10192), d = 17.0623, its = 7
i = 9607 (of 10192), d = 37, its = 53
i = 9608 (of 10192), d = 37, its = 56
i = 9609 (of 10192), d = 37, its = 52
i = 9610 (of 10192), d = 1.24626, its = 19
i = 9611 (of 10192), d = 37, its = 53
i = 9612 (of 10192), d = 19.6315, its = 8
i = 9613 (of 10192), d = 37, its = 55
i = 9614 (of 10192), d = 37, its = 37
i = 9615 (of 10192), d = 37, its = 53
i = 9616 (of 10192), d = 3.43048, its = 19
i = 9617 (of 10192), d = 37, its = 51
i = 9618 (of 10192), d = 37, its = 37
i = 9619 (of 10192), d = 37, its = 49
i = 9620 (of 10192), d = 3.82775, its = 20
i = 9621 (of 10192), d = 37, its = 55
i = 9622 (of 10192), d = 37, its = 48
i = 9623 (of 10192), d = 0.828163, its = 20
i = 9624 (of 10192), d = 37, its = 53
i = 9625 (of 10192), d = 37, its = 37
i = 9626 (of 10192), d = 10.5657, its = 5
i = 9627 (of 10192), d = 37, its = 50
i = 9628 (of 10192), d = 37, its = 37
i = 9629 (of 10192), d = 37, its = 54
i = 9630 (of 10192), d = 2.42559, its = 21
i = 9631 (of 10192), d = 12.9271, its = 6
i = 9632 (of 10192), d = 7.73828, its = 3
i = 9633 (of 10192), d = 32.943, its = 9
i = 9634 (of 10192), d = 37, its = 53
i = 9635 (of 10192), d = 37, its = 39
i = 9636 (of 10192), d = 37, its = 51
i = 9637 (of 10192), d = 37, its = 37
i = 9638 (of 10192), d = 5.13564, its = 6
i = 9639 (of 10192), d = 37, its = 39
i = 9640 (of 10192), d = 3.59392, its = 19
i = 9641 (of 10192), d = 37, its = 51
i = 9642 (of 10192), d = 4.52468, its = 20
i = 9643 (of 10192), d = 37, its = 53
i = 9644 (of 10192), d = 7.45938, its = 4
i = 9645 (of 10192), d = 37, its = 59
i = 9646 (of 10192), d = 37, its = 55
i = 9647 (of 10192), d = 37, its = 54
i = 9648 (of 10192), d = 37, its = 53
i = 9649 (of 10192), d = 37, its = 54
i = 9650 (of 10192), d = 5.72592, its = 21
i = 9651 (of 10192), d = 37, its = 49
i = 9652 (of 10192), d = 37, its = 53
i = 9653 (of 10192), d = 37, its = 52
i = 9654 (of 10192), d = 37, its = 53
i = 9655 (of 10192), d = 15.007, its = 6
i = 9656 (of 10192), d = 12.7346, its = 6
i = 9657 (of 10192), d = 19.6851, its = 8
i = 9658 (of 10192), d = 37, its = 37
i = 9659 (of 10192), d = 6.30505, its = 6
i = 9660 (of 10192), d = 37, its = 53
i = 9661 (of 10192), d = 37, its = 56
i = 9662 (of 10192), d = 37, its = 37
i = 9663 (of 10192), d = 37, its = 54
i = 9664 (of 10192), d = 37, its = 54
i = 9665 (of 10192), d = 3.41061, its = 24
i = 9666 (of 10192), d = 37, its = 57
i = 9667 (of 10192), d = 3.34458, its = 19
i = 9668 (of 10192), d = 37, its = 54
i = 9669 (of 10192), d = 37, its = 37
i = 9670 (of 10192), d = 37, its = 53
i = 9671 (of 10192), d = 37, its = 50
i = 9672 (of 10192), d = 13.5685, its = 6
i = 9673 (of 10192), d = 22.0258, its = 7
i = 9674 (of 10192), d = 37, its = 41
i = 9675 (of 10192), d = 37, its = 37
i = 9676 (of 10192), d = 4.66368, its = 20
i = 9677 (of 10192), d = 9.7792, its = 6
i = 9678 (of 10192), d = 37, its = 50
i = 9679 (of 10192), d = 37, its = 50
i = 9680 (of 10192), d = 37, its = 37
i = 9681 (of 10192), d = 37, its = 58
i = 9682 (of 10192), d = 5.88377, its = 6
i = 9683 (of 10192), d = 37, its = 52
i = 9684 (of 10192), d = 5.39537, its = 6
i = 9685 (of 10192), d = 37, its = 54
i = 9686 (of 10192), d = 37, its = 53
i = 9687 (of 10192), d = 37, its = 51
i = 9688 (of 10192), d = 5.009, its = 6
i = 9689 (of 10192), d = 13.4846, its = 6
i = 9690 (of 10192), d = 37, its = 41
i = 9691 (of 10192), d = 37, its = 37
i = 9692 (of 10192), d = 37, its = 52
i = 9693 (of 10192), d = 37, its = 46
i = 9694 (of 10192), d = 2.94145, its = 17
i = 9695 (of 10192), d = 37, its = 40
i = 9696 (of 10192), d = 37, its = 53
i = 9697 (of 10192), d = 4.67126, its = 16
i = 9698 (of 10192), d = 1.80948, its = 17
i = 9699 (of 10192), d = 12.9752, its = 7
i = 9700 (of 10192), d = 37, its = 51
i = 9701 (of 10192), d = 3.76463, its = 18
i = 9702 (of 10192), d = 37, its = 50
i = 9703 (of 10192), d = 9.98721, its = 5
i = 9704 (of 10192), d = 37, its = 40
i = 9705 (of 10192), d = 2.87723, its = 30
i = 9706 (of 10192), d = 37, its = 57
i = 9707 (of 10192), d = 37, its = 51
i = 9708 (of 10192), d = 6.05027, its = 7
i = 9709 (of 10192), d = 37, its = 54
i = 9710 (of 10192), d = 37, its = 51
i = 9711 (of 10192), d = 37, its = 53
i = 9712 (of 10192), d = 37, its = 53
i = 9713 (of 10192), d = 37, its = 37
i = 9714 (of 10192), d = 37, its = 54
i = 9715 (of 10192), d = 18.8818, its = 7
i = 9716 (of 10192), d = 37, its = 51
i = 9717 (of 10192), d = 37, its = 37
i = 9718 (of 10192), d = 37, its = 51
i = 9719 (of 10192), d = 3.44349, its = 20
i = 9720 (of 10192), d = 37, its = 39
i = 9721 (of 10192), d = 13.0343, its = 8
i = 9722 (of 10192), d = 5.99068, its = 6
i = 9723 (of 10192), d = 5.71229, its = 6
i = 9724 (of 10192), d = 3.10666, its = 20
i = 9725 (of 10192), d = 37, its = 37
i = 9726 (of 10192), d = 3.22399, its = 18
i = 9727 (of 10192), d = 8.27438, its = 3
i = 9728 (of 10192), d = 37, its = 52
i = 9729 (of 10192), d = 37, its = 39
i = 9730 (of 10192), d = 37, its = 40
i = 9731 (of 10192), d = 15.7064, its = 7
i = 9732 (of 10192), d = 37, its = 38
i = 9733 (of 10192), d = 5.00661, its = 6
i = 9734 (of 10192), d = 37, its = 53
i = 9735 (of 10192), d = 6.03391, its = 7
i = 9736 (of 10192), d = 5.48656, its = 6
i = 9737 (of 10192), d = 2.483, its = 17
i = 9738 (of 10192), d = 37, its = 51
i = 9739 (of 10192), d = 37, its = 54
i = 9740 (of 10192), d = 37, its = 38
i = 9741 (of 10192), d = 37, its = 37
i = 9742 (of 10192), d = 5.42989, its = 6
i = 9743 (of 10192), d = 37, its = 40
i = 9744 (of 10192), d = 37, its = 37
i = 9745 (of 10192), d = 5.40862, its = 7
i = 9746 (of 10192), d = 3.16066, its = 18
i = 9747 (of 10192), d = 3.15897, its = 19
i = 9748 (of 10192), d = 37, its = 52
i = 9749 (of 10192), d = 37, its = 53
i = 9750 (of 10192), d = 37, its = 53
i = 9751 (of 10192), d = 3.24861, its = 17
i = 9752 (of 10192), d = 37, its = 54
i = 9753 (of 10192), d = 37, its = 53
i = 9754 (of 10192), d = 37, its = 57
i = 9755 (of 10192), d = 6.07796, its = 6
i = 9756 (of 10192), d = 37, its = 53
i = 9757 (of 10192), d = 37, its = 46
i = 9758 (of 10192), d = 34.4153, its = 8
i = 9759 (of 10192), d = 37, its = 53
i = 9760 (of 10192), d = 2.73325, its = 17
i = 9761 (of 10192), d = 37, its = 53
i = 9762 (of 10192), d = 11.8169, its = 6
i = 9763 (of 10192), d = 37, its = 54
i = 9764 (of 10192), d = 37, its = 53
i = 9765 (of 10192), d = 37, its = 37
i = 9766 (of 10192), d = 37, its = 55
i = 9767 (of 10192), d = 6.2548, its = 6
i = 9768 (of 10192), d = 3.26666, its = 18
i = 9769 (of 10192), d = 9.58495, its = 5
i = 9770 (of 10192), d = 7.80244, its = 3
i = 9771 (of 10192), d = 37, its = 45
i = 9772 (of 10192), d = 37, its = 51
i = 9773 (of 10192), d = 37, its = 37
i = 9774 (of 10192), d = 37, its = 60
i = 9775 (of 10192), d = 37, its = 51
i = 9776 (of 10192), d = 37, its = 51
i = 9777 (of 10192), d = 4.62166, its = 18
i = 9778 (of 10192), d = 37, its = 51
i = 9779 (of 10192), d = 9.63532, its = 4
i = 9780 (of 10192), d = 37, its = 49
i = 9781 (of 10192), d = 37, its = 48
i = 9782 (of 10192), d = 37, its = 54
i = 9783 (of 10192), d = 37, its = 53
i = 9784 (of 10192), d = 37, its = 37
i = 9785 (of 10192), d = 37, its = 39
i = 9786 (of 10192), d = 5.74009, its = 6
i = 9787 (of 10192), d = 6.27576, its = 6
i = 9788 (of 10192), d = 37, its = 40
i = 9789 (of 10192), d = 10.4576, its = 5
i = 9790 (of 10192), d = 37, its = 41
i = 9791 (of 10192), d = 37, its = 54
i = 9792 (of 10192), d = 2.9848, its = 17
i = 9793 (of 10192), d = 37, its = 53
i = 9794 (of 10192), d = 4.3043, its = 21
i = 9795 (of 10192), d = 10.9193, its = 5
i = 9796 (of 10192), d = 37, its = 52
i = 9797 (of 10192), d = 2.80325, its = 20
i = 9798 (of 10192), d = 37, its = 48
i = 9799 (of 10192), d = 37, its = 53
i = 9800 (of 10192), d = 37, its = 51
i = 9801 (of 10192), d = 15.8389, its = 8
i = 9802 (of 10192), d = 12.5455, its = 6
i = 9803 (of 10192), d = 6.87127, its = 5
i = 9804 (of 10192), d = 37, its = 56
i = 9805 (of 10192), d = 37, its = 53
i = 9806 (of 10192), d = 37, its = 37
i = 9807 (of 10192), d = 37, its = 37
i = 9808 (of 10192), d = 37, its = 54
i = 9809 (of 10192), d = 37, its = 54
i = 9810 (of 10192), d = 37, its = 53
i = 9811 (of 10192), d = 37, its = 37
i = 9812 (of 10192), d = 37, its = 40
i = 9813 (of 10192), d = 37, its = 48
i = 9814 (of 10192), d = 4.75685, its = 7
i = 9815 (of 10192), d = 37, its = 53
i = 9816 (of 10192), d = 13.6292, its = 6
i = 9817 (of 10192), d = 37, its = 37
i = 9818 (of 10192), d = 37, its = 53
i = 9819 (of 10192), d = 37, its = 54
i = 9820 (of 10192), d = 37, its = 52
i = 9821 (of 10192), d = 11.5608, its = 6
i = 9822 (of 10192), d = 37, its = 54
i = 9823 (of 10192), d = 37, its = 57
i = 9824 (of 10192), d = 37, its = 37
i = 9825 (of 10192), d = 9.62749, its = 5
i = 9826 (of 10192), d = 3.07237, its = 16
i = 9827 (of 10192), d = 37, its = 39
i = 9828 (of 10192), d = 15.3339, its = 7
i = 9829 (of 10192), d = 7.56386, its = 4
i = 9830 (of 10192), d = 37, its = 54
i = 9831 (of 10192), d = 37, its = 50
i = 9832 (of 10192), d = 37, its = 52
i = 9833 (of 10192), d = 7.49465, its = 4
i = 9834 (of 10192), d = 37, its = 48
i = 9835 (of 10192), d = 37, its = 55
i = 9836 (of 10192), d = 37, its = 55
i = 9837 (of 10192), d = 37, its = 55
i = 9838 (of 10192), d = 9.48773, its = 5
i = 9839 (of 10192), d = 8.37264, its = 4
i = 9840 (of 10192), d = 37, its = 37
i = 9841 (of 10192), d = 37, its = 52
i = 9842 (of 10192), d = 2.25538, its = 17
i = 9843 (of 10192), d = 11.4937, its = 6
i = 9844 (of 10192), d = 6.55069, its = 6
i = 9845 (of 10192), d = 37, its = 57
i = 9846 (of 10192), d = 12.3725, its = 6
i = 9847 (of 10192), d = 37, its = 51
i = 9848 (of 10192), d = 37, its = 40
i = 9849 (of 10192), d = 37, its = 39
i = 9850 (of 10192), d = 2.85429, its = 20
i = 9851 (of 10192), d = 5.74587, its = 6
i = 9852 (of 10192), d = 37, its = 40
i = 9853 (of 10192), d = 6.77261, its = 7
i = 9854 (of 10192), d = 37, its = 37
i = 9855 (of 10192), d = 37, its = 37
i = 9856 (of 10192), d = 37, its = 54
i = 9857 (of 10192), d = 37, its = 57
i = 9858 (of 10192), d = 37, its = 41
i = 9859 (of 10192), d = 3.39293, its = 29
i = 9860 (of 10192), d = 6.66719, its = 6
i = 9861 (of 10192), d = 5.71125, its = 6
i = 9862 (of 10192), d = 37, its = 53
i = 9863 (of 10192), d = 37, its = 39
i = 9864 (of 10192), d = 37, its = 52
i = 9865 (of 10192), d = 1.6575, its = 19
i = 9866 (of 10192), d = 37, its = 37
i = 9867 (of 10192), d = 5.75386, its = 6
i = 9868 (of 10192), d = 37, its = 55
i = 9869 (of 10192), d = 37, its = 54
i = 9870 (of 10192), d = 2.03803, its = 26
i = 9871 (of 10192), d = 1.91195, its = 17
i = 9872 (of 10192), d = 37, its = 57
i = 9873 (of 10192), d = 37, its = 55
i = 9874 (of 10192), d = 4.34098, its = 19
i = 9875 (of 10192), d = 2.25209, its = 21
i = 9876 (of 10192), d = 3.49481, its = 21
i = 9877 (of 10192), d = 5.16754, its = 6
i = 9878 (of 10192), d = 37, its = 52
i = 9879 (of 10192), d = 37, its = 53
i = 9880 (of 10192), d = 37, its = 52
i = 9881 (of 10192), d = 37, its = 53
i = 9882 (of 10192), d = 37, its = 57
i = 9883 (of 10192), d = 10.7911, its = 5
i = 9884 (of 10192), d = 2.31708, its = 27
i = 9885 (of 10192), d = 37, its = 40
i = 9886 (of 10192), d = 9.49202, its = 5
i = 9887 (of 10192), d = 37, its = 53
i = 9888 (of 10192), d = 37, its = 55
i = 9889 (of 10192), d = 37, its = 47
i = 9890 (of 10192), d = 4.60366, its = 17
i = 9891 (of 10192), d = 37, its = 37
i = 9892 (of 10192), d = 10.0879, its = 5
i = 9893 (of 10192), d = 6.64276, its = 5
i = 9894 (of 10192), d = 14.825, its = 7
i = 9895 (of 10192), d = 2.57642, its = 18
i = 9896 (of 10192), d = 6.34356, its = 5
i = 9897 (of 10192), d = 9.41549, its = 5
i = 9898 (of 10192), d = 4.91886, its = 6
i = 9899 (of 10192), d = 37, its = 37
i = 9900 (of 10192), d = 9.96118, its = 5
i = 9901 (of 10192), d = 10.9671, its = 5
i = 9902 (of 10192), d = 12.5091, its = 6
i = 9903 (of 10192), d = 37, its = 37
i = 9904 (of 10192), d = 6.45339, its = 5
i = 9905 (of 10192), d = 37, its = 37
i = 9906 (of 10192), d = 15.2871, its = 7
i = 9907 (of 10192), d = 37, its = 47
i = 9908 (of 10192), d = 37, its = 53
i = 9909 (of 10192), d = 37, its = 51
i = 9910 (of 10192), d = 7.0696, its = 5
i = 9911 (of 10192), d = 37, its = 51
i = 9912 (of 10192), d = 37, its = 53
i = 9913 (of 10192), d = 37, its = 53
i = 9914 (of 10192), d = 37, its = 55
i = 9915 (of 10192), d = 37, its = 55
i = 9916 (of 10192), d = 37, its = 52
i = 9917 (of 10192), d = 37, its = 37
i = 9918 (of 10192), d = 37, its = 37
i = 9919 (of 10192), d = 37, its = 39
i = 9920 (of 10192), d = 37, its = 57
i = 9921 (of 10192), d = 10.3709, its = 5
i = 9922 (of 10192), d = 37, its = 37
i = 9923 (of 10192), d = 14.9977, its = 6
i = 9924 (of 10192), d = 37, its = 43
i = 9925 (of 10192), d = 37, its = 55
i = 9926 (of 10192), d = 9.95474, its = 5
i = 9927 (of 10192), d = 37, its = 37
i = 9928 (of 10192), d = 37, its = 53
i = 9929 (of 10192), d = 37, its = 55
i = 9930 (of 10192), d = 3.45602, its = 19
i = 9931 (of 10192), d = 37, its = 37
i = 9932 (of 10192), d = 37, its = 55
i = 9933 (of 10192), d = 3.40161, its = 24
i = 9934 (of 10192), d = 37, its = 37
i = 9935 (of 10192), d = 2.76532, its = 17
i = 9936 (of 10192), d = 2.64435, its = 17
i = 9937 (of 10192), d = 37, its = 37
i = 9938 (of 10192), d = 37, its = 51
i = 9939 (of 10192), d = 37, its = 54
i = 9940 (of 10192), d = 37, its = 41
i = 9941 (of 10192), d = 37, its = 52
i = 9942 (of 10192), d = 37, its = 37
i = 9943 (of 10192), d = 6.60873, its = 6
i = 9944 (of 10192), d = 37, its = 53
i = 9945 (of 10192), d = 37, its = 54
i = 9946 (of 10192), d = 37, its = 52
i = 9947 (of 10192), d = 4.08521, its = 20
i = 9948 (of 10192), d = 37, its = 53
i = 9949 (of 10192), d = 2.56497, its = 17
i = 9950 (of 10192), d = 37, its = 51
i = 9951 (of 10192), d = 1.24257, its = 23
i = 9952 (of 10192), d = 37, its = 50
i = 9953 (of 10192), d = 37, its = 56
i = 9954 (of 10192), d = 7.22149, its = 4
i = 9955 (of 10192), d = 4.22265, its = 16
i = 9956 (of 10192), d = 37, its = 53
i = 9957 (of 10192), d = 37, its = 39
i = 9958 (of 10192), d = 3.65549, its = 18
i = 9959 (of 10192), d = 10.454, its = 6
i = 9960 (of 10192), d = 37, its = 37
i = 9961 (of 10192), d = 37, its = 37
i = 9962 (of 10192), d = 37, its = 49
i = 9963 (of 10192), d = 37, its = 53
i = 9964 (of 10192), d = 37, its = 55
i = 9965 (of 10192), d = 13.0974, its = 6
i = 9966 (of 10192), d = 37, its = 53
i = 9967 (of 10192), d = 37, its = 52
i = 9968 (of 10192), d = 37, its = 37
i = 9969 (of 10192), d = 29.5734, its = 8
i = 9970 (of 10192), d = 37, its = 55
i = 9971 (of 10192), d = 37, its = 49
i = 9972 (of 10192), d = 37, its = 38
i = 9973 (of 10192), d = 4.8532, its = 18
i = 9974 (of 10192), d = 37, its = 50
i = 9975 (of 10192), d = 37, its = 37
i = 9976 (of 10192), d = 7.16802, its = 5
i = 9977 (of 10192), d = 37, its = 58
i = 9978 (of 10192), d = 37, its = 50
i = 9979 (of 10192), d = 37, its = 52
i = 9980 (of 10192), d = 37, its = 51
i = 9981 (of 10192), d = 37, its = 39
i = 9982 (of 10192), d = 8.31807, its = 3
i = 9983 (of 10192), d = 37, its = 57
i = 9984 (of 10192), d = 2.15189, its = 17
i = 9985 (of 10192), d = 37, its = 51
i = 9986 (of 10192), d = 37, its = 41
i = 9987 (of 10192), d = 2.88733, its = 19
i = 9988 (of 10192), d = 37, its = 52
i = 9989 (of 10192), d = 37, its = 49
i = 9990 (of 10192), d = 37, its = 57
i = 9991 (of 10192), d = 37, its = 52
i = 9992 (of 10192), d = 37, its = 49
i = 9993 (of 10192), d = 37, its = 55
i = 9994 (of 10192), d = 37, its = 42
i = 9995 (of 10192), d = 8.68594, its = 4
i = 9996 (of 10192), d = 37, its = 55
i = 9997 (of 10192), d = 37, its = 50
i = 9998 (of 10192), d = 37, its = 53
i = 9999 (of 10192), d = 37, its = 37
i = 10000 (of 10192), d = 7.17033, its = 6
i = 10001 (of 10192), d = 37, its = 52
i = 10002 (of 10192), d = 37, its = 54
i = 10003 (of 10192), d = 5.04633, its = 7
i = 10004 (of 10192), d = 19.4825, its = 7
i = 10005 (of 10192), d = 37, its = 58
i = 10006 (of 10192), d = 37, its = 53
i = 10007 (of 10192), d = 5.51294, its = 6
i = 10008 (of 10192), d = 37, its = 56
i = 10009 (of 10192), d = 37, its = 37
i = 10010 (of 10192), d = 37, its = 37
i = 10011 (of 10192), d = 3.40017, its = 18
i = 10012 (of 10192), d = 37, its = 51
i = 10013 (of 10192), d = 7.95716, its = 3
i = 10014 (of 10192), d = 37, its = 46
i = 10015 (of 10192), d = 3.88607, its = 21
i = 10016 (of 10192), d = 37, its = 52
i = 10017 (of 10192), d = 37, its = 50
i = 10018 (of 10192), d = 37, its = 37
i = 10019 (of 10192), d = 37, its = 41
i = 10020 (of 10192), d = 37, its = 56
i = 10021 (of 10192), d = 6.34312, its = 7
i = 10022 (of 10192), d = 4.58791, its = 16
i = 10023 (of 10192), d = 37, its = 54
i = 10024 (of 10192), d = 37, its = 53
i = 10025 (of 10192), d = 37, its = 37
i = 10026 (of 10192), d = 37, its = 37
i = 10027 (of 10192), d = 37, its = 48
i = 10028 (of 10192), d = 37, its = 51
i = 10029 (of 10192), d = 9.02418, its = 4
i = 10030 (of 10192), d = 8.76253, its = 4
i = 10031 (of 10192), d = 8.8423, its = 5
i = 10032 (of 10192), d = 16.2034, its = 7
i = 10033 (of 10192), d = 37, its = 54
i = 10034 (of 10192), d = 37, its = 57
i = 10035 (of 10192), d = 37, its = 37
i = 10036 (of 10192), d = 37, its = 52
i = 10037 (of 10192), d = 37, its = 57
i = 10038 (of 10192), d = 6.61914, its = 5
i = 10039 (of 10192), d = 10.1487, its = 5
i = 10040 (of 10192), d = 9.01459, its = 4
i = 10041 (of 10192), d = 20.6519, its = 8
i = 10042 (of 10192), d = 37, its = 42
i = 10043 (of 10192), d = 37, its = 48
i = 10044 (of 10192), d = 8.69771, its = 4
i = 10045 (of 10192), d = 3.50847, its = 19
i = 10046 (of 10192), d = 37, its = 39
i = 10047 (of 10192), d = 37, its = 51
i = 10048 (of 10192), d = 37, its = 53
i = 10049 (of 10192), d = 37, its = 53
i = 10050 (of 10192), d = 37, its = 55
i = 10051 (of 10192), d = 37, its = 51
i = 10052 (of 10192), d = 37, its = 55
i = 10053 (of 10192), d = 3.11839, its = 18
i = 10054 (of 10192), d = 2.44463, its = 18
i = 10055 (of 10192), d = 37, its = 37
i = 10056 (of 10192), d = 2.82585, its = 20
i = 10057 (of 10192), d = 10.8617, its = 5
i = 10058 (of 10192), d = 37, its = 39
i = 10059 (of 10192), d = 8.72963, its = 4
i = 10060 (of 10192), d = 37, its = 39
i = 10061 (of 10192), d = 37, its = 37
i = 10062 (of 10192), d = 6.94495, its = 5
i = 10063 (of 10192), d = 12.1265, its = 6
i = 10064 (of 10192), d = 37, its = 52
i = 10065 (of 10192), d = 37, its = 37
i = 10066 (of 10192), d = 37, its = 37
i = 10067 (of 10192), d = 4.4342, its = 21
i = 10068 (of 10192), d = 37, its = 54
i = 10069 (of 10192), d = 37, its = 56
i = 10070 (of 10192), d = 37, its = 37
i = 10071 (of 10192), d = 8.69844, its = 4
i = 10072 (of 10192), d = 5.78122, its = 6
i = 10073 (of 10192), d = 37, its = 56
i = 10074 (of 10192), d = 10.6122, its = 5
i = 10075 (of 10192), d = 8.46203, its = 4
i = 10076 (of 10192), d = 37, its = 56
i = 10077 (of 10192), d = 37, its = 55
i = 10078 (of 10192), d = 37, its = 51
i = 10079 (of 10192), d = 37, its = 44
i = 10080 (of 10192), d = 37, its = 54
i = 10081 (of 10192), d = 37, its = 38
i = 10082 (of 10192), d = 37, its = 54
i = 10083 (of 10192), d = 37, its = 56
i = 10084 (of 10192), d = 8.34354, its = 4
i = 10085 (of 10192), d = 37, its = 49
i = 10086 (of 10192), d = 37, its = 51
i = 10087 (of 10192), d = 37, its = 37
i = 10088 (of 10192), d = 37, its = 49
i = 10089 (of 10192), d = 5.79752, its = 7
i = 10090 (of 10192), d = 37, its = 57
i = 10091 (of 10192), d = 3.29909, its = 18
i = 10092 (of 10192), d = 37, its = 53
i = 10093 (of 10192), d = 9.83438, its = 5
i = 10094 (of 10192), d = 37, its = 54
i = 10095 (of 10192), d = 37, its = 48
i = 10096 (of 10192), d = 37, its = 55
i = 10097 (of 10192), d = 37, its = 54
i = 10098 (of 10192), d = 37, its = 37
i = 10099 (of 10192), d = 37, its = 55
i = 10100 (of 10192), d = 37, its = 49
i = 10101 (of 10192), d = 3.11242, its = 17
i = 10102 (of 10192), d = 37, its = 54
i = 10103 (of 10192), d = 7.16852, its = 4
i = 10104 (of 10192), d = 37, its = 39
i = 10105 (of 10192), d = 37, its = 54
i = 10106 (of 10192), d = 2.42646, its = 20
i = 10107 (of 10192), d = 37, its = 52
i = 10108 (of 10192), d = 37, its = 54
i = 10109 (of 10192), d = 9.76245, its = 6
i = 10110 (of 10192), d = 37, its = 50
i = 10111 (of 10192), d = 37, its = 54
i = 10112 (of 10192), d = 9.41549, its = 5
i = 10113 (of 10192), d = 2.45886, its = 17
i = 10114 (of 10192), d = 37, its = 54
i = 10115 (of 10192), d = 2.43658, its = 19
i = 10116 (of 10192), d = 37, its = 42
i = 10117 (of 10192), d = 37, its = 52
i = 10118 (of 10192), d = 4.32063, its = 18
i = 10119 (of 10192), d = 8.9776, its = 4
i = 10120 (of 10192), d = 37, its = 55
i = 10121 (of 10192), d = 37, its = 41
i = 10122 (of 10192), d = 4.85169, its = 19
i = 10123 (of 10192), d = 37, its = 37
i = 10124 (of 10192), d = 37, its = 40
i = 10125 (of 10192), d = 37, its = 55
i = 10126 (of 10192), d = 37, its = 37
i = 10127 (of 10192), d = 37, its = 54
i = 10128 (of 10192), d = 37, its = 55
i = 10129 (of 10192), d = 37, its = 39
i = 10130 (of 10192), d = 1.59301, its = 19
i = 10131 (of 10192), d = 37, its = 37
i = 10132 (of 10192), d = 37, its = 53
i = 10133 (of 10192), d = 3.44591, its = 19
i = 10134 (of 10192), d = 37, its = 43
i = 10135 (of 10192), d = 8.25965, its = 3
i = 10136 (of 10192), d = 37, its = 51
i = 10137 (of 10192), d = 37, its = 54
i = 10138 (of 10192), d = 37, its = 54
i = 10139 (of 10192), d = 9.99033, its = 5
i = 10140 (of 10192), d = 37, its = 53
i = 10141 (of 10192), d = 3.31764, its = 22
i = 10142 (of 10192), d = 11.6617, its = 6
i = 10143 (of 10192), d = 37, its = 37
i = 10144 (of 10192), d = 37, its = 38
i = 10145 (of 10192), d = 37, its = 37
i = 10146 (of 10192), d = 37, its = 38
i = 10147 (of 10192), d = 1.74452, its = 17
i = 10148 (of 10192), d = 2.12991, its = 22
i = 10149 (of 10192), d = 4.46066, its = 17
i = 10150 (of 10192), d = 37, its = 55
i = 10151 (of 10192), d = 37, its = 57
i = 10152 (of 10192), d = 37, its = 52
i = 10153 (of 10192), d = 37, its = 51
i = 10154 (of 10192), d = 4.60157, its = 17
i = 10155 (of 10192), d = 37, its = 51
i = 10156 (of 10192), d = 37, its = 53
i = 10157 (of 10192), d = 37, its = 54
i = 10158 (of 10192), d = 37, its = 57
i = 10159 (of 10192), d = 12.1295, its = 5
i = 10160 (of 10192), d = 10.7243, its = 5
i = 10161 (of 10192), d = 1.75356, its = 17
i = 10162 (of 10192), d = 11.6731, its = 7
i = 10163 (of 10192), d = 37, its = 51
i = 10164 (of 10192), d = 37, its = 52
i = 10165 (of 10192), d = 37, its = 52
i = 10166 (of 10192), d = 37, its = 37
i = 10167 (of 10192), d = 37, its = 53
i = 10168 (of 10192), d = 10.8322, its = 5
i = 10169 (of 10192), d = 37, its = 37
i = 10170 (of 10192), d = 37, its = 49
i = 10171 (of 10192), d = 37, its = 52
i = 10172 (of 10192), d = 37, its = 50
i = 10173 (of 10192), d = 37, its = 51
i = 10174 (of 10192), d = 37, its = 54
i = 10175 (of 10192), d = 37, its = 54
i = 10176 (of 10192), d = 37, its = 51
i = 10177 (of 10192), d = 37, its = 53
i = 10178 (of 10192), d = 37, its = 37
i = 10179 (of 10192), d = 37, its = 37
i = 10180 (of 10192), d = 37, its = 55
i = 10181 (of 10192), d = 9.29637, its = 4
i = 10182 (of 10192), d = 37, its = 53
i = 10183 (of 10192), d = 2.57383, its = 19
i = 10184 (of 10192), d = 9.65234, its = 5
i = 10185 (of 10192), d = 12.9478, its = 5
i = 10186 (of 10192), d = 37, its = 50
i = 10187 (of 10192), d = 8.22219, its = 3
i = 10188 (of 10192), d = 37, its = 53
i = 10189 (of 10192), d = 2.62387, its = 17
i = 10190 (of 10192), d = 37, its = 52
i = 10191 (of 10192), d = 37, its = 49
i = 10192 (of 10192), d = 37, its = 37
[1] "Fri Feb 09 04:00:18 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 04:00:24 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 04:00:29 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 04:00:34 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 04:00:40 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Fri Feb 09 04:00:52 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=14; decay=3.63
# weights:  169
initial  value 427140.096518 
iter  10 value 140615.721868
iter  20 value 52533.139346
iter  30 value 37272.130200
iter  40 value 35646.332228
iter  50 value 34954.360035
iter  60 value 34667.931995
iter  70 value 34370.971616
iter  80 value 33975.057215
iter  90 value 33042.194486
iter 100 value 32405.312993
final  value 32405.312993 
stopped after 100 iterations
# weights:  169
initial  value 392747.747366 
iter  10 value 42353.453310
iter  20 value 27357.651561
iter  30 value 25340.401630
iter  40 value 24835.579630
iter  50 value 24702.963363
iter  60 value 24621.705390
iter  70 value 24454.318199
iter  80 value 23960.596452
iter  90 value 23631.246806
iter 100 value 23463.021790
final  value 23463.021790 
stopped after 100 iterations
# weights:  169
initial  value 399255.324246 
iter  10 value 59159.270059
iter  20 value 31688.729453
iter  30 value 29246.205160
iter  40 value 28790.285859
iter  50 value 28665.092980
iter  60 value 28594.489084
iter  70 value 28496.117069
iter  80 value 27824.491805
iter  90 value 27155.087797
iter 100 value 26635.708820
final  value 26635.708820 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=1.02; time: 1.3 min
[Tune-x] 2: size=7; decay=0.00565
# weights:  85
initial  value 410449.825065 
iter  10 value 93605.136420
iter  20 value 43290.588721
iter  30 value 33510.195243
iter  40 value 31579.799647
iter  50 value 30301.398354
iter  60 value 28577.606071
iter  70 value 26158.564803
iter  80 value 23958.397250
iter  90 value 22064.806579
iter 100 value 21092.158972
final  value 21092.158972 
stopped after 100 iterations
# weights:  85
initial  value 436579.048901 
iter  10 value 98899.439605
iter  20 value 44986.634393
iter  30 value 39159.469032
iter  40 value 36711.931922
iter  50 value 34848.748121
iter  60 value 32851.415728
iter  70 value 29929.879373
iter  80 value 28694.916141
iter  90 value 25996.236892
iter 100 value 22253.705846
final  value 22253.705846 
stopped after 100 iterations
# weights:  85
initial  value 403187.562880 
iter  10 value 50183.371515
iter  20 value 30370.538901
iter  30 value 26782.324635
iter  40 value 25164.416965
iter  50 value 24349.155566
iter  60 value 23255.864848
iter  70 value 22255.156147
iter  80 value 21470.468586
iter  90 value 20884.759797
iter 100 value 20543.106803
final  value 20543.106803 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=1.02; time: 0.6 min
[Tune-x] 3: size=5; decay=1.03
# weights:  61
initial  value 394606.974934 
iter  10 value 107343.241401
iter  20 value 70403.574848
iter  30 value 64088.889700
iter  40 value 59226.531075
iter  50 value 55454.221433
iter  60 value 47610.351661
iter  70 value 29926.727039
iter  80 value 25114.797458
iter  90 value 23177.628943
iter 100 value 22262.771238
final  value 22262.771238 
stopped after 100 iterations
# weights:  61
initial  value 423325.485278 
iter  10 value 82433.748442
iter  20 value 59373.377475
iter  30 value 55190.666863
iter  40 value 53714.536219
iter  50 value 52889.934388
iter  60 value 45237.693647
iter  70 value 34123.711693
iter  80 value 24801.369651
iter  90 value 22126.648372
iter 100 value 21589.044396
final  value 21589.044396 
stopped after 100 iterations
# weights:  61
initial  value 406183.302918 
iter  10 value 117799.717317
iter  20 value 64892.373162
iter  30 value 58021.661036
iter  40 value 55573.569124
iter  50 value 52864.214790
iter  60 value 44299.167450
iter  70 value 32370.254929
iter  80 value 28107.588899
iter  90 value 26170.429092
iter 100 value 25608.233020
final  value 25608.233020 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=1.05; time: 0.4 min
[Tune-x] 4: size=2; decay=0.253
# weights:  25
initial  value 397285.858200 
iter  10 value 163845.497549
iter  20 value 144908.124452
iter  30 value 68851.774865
iter  40 value 48197.528997
iter  50 value 37373.952005
iter  60 value 36444.642650
iter  70 value 36315.541234
iter  80 value 36169.526417
iter  90 value 35995.305929
iter 100 value 35923.739826
final  value 35923.739826 
stopped after 100 iterations
# weights:  25
initial  value 399394.961816 
iter  10 value 177335.337284
iter  20 value 166317.437379
iter  30 value 144804.298668
iter  40 value 133374.410543
iter  50 value 123590.370740
iter  60 value 119520.316349
iter  70 value 118337.279901
iter  80 value 117396.568121
iter  90 value 112492.453375
iter 100 value 103189.198597
final  value 103189.198597 
stopped after 100 iterations
# weights:  25
initial  value 412148.148064 
iter  10 value 291199.498925
iter  20 value 192222.915146
iter  30 value 182632.122072
iter  40 value 152626.940708
iter  50 value 135318.214734
iter  60 value 128134.944047
iter  70 value 117674.264935
iter  80 value 110102.104528
iter  90 value 68548.207495
iter 100 value 67690.581751
final  value 67690.581751 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=1.84; time: 0.2 min
[Tune-x] 5: size=14; decay=0.0447
# weights:  169
initial  value 390867.731391 
iter  10 value 36936.258913
iter  20 value 23019.404299
iter  30 value 21366.539948
iter  40 value 21125.990836
iter  50 value 20899.603946
iter  60 value 20662.839787
iter  70 value 20525.547618
iter  80 value 20361.102495
iter  90 value 20281.812776
iter 100 value 20228.357133
final  value 20228.357133 
stopped after 100 iterations
# weights:  169
initial  value 424968.729384 
iter  10 value 60261.954954
iter  20 value 31205.809244
iter  30 value 23368.408726
iter  40 value 22305.115667
iter  50 value 21562.793627
iter  60 value 20940.113954
iter  70 value 20700.813232
iter  80 value 20600.462562
iter  90 value 20524.494978
iter 100 value 20433.668160
final  value 20433.668160 
stopped after 100 iterations
# weights:  169
initial  value 401996.855196 
iter  10 value 51427.937319
iter  20 value 25685.087655
iter  30 value 23325.193231
iter  40 value 22062.048378
iter  50 value 21262.553119
iter  60 value 20770.334024
iter  70 value 20624.316494
iter  80 value 20526.701607
iter  90 value 20462.868463
iter 100 value 20434.426903
final  value 20434.426903 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.01; time: 1.3 min
[Tune-x] 6: size=17; decay=0.274
# weights:  205
initial  value 363517.638054 
iter  10 value 35827.857478
iter  20 value 24210.483057
iter  30 value 21653.568098
iter  40 value 20848.070669
iter  50 value 20637.133105
iter  60 value 20565.680943
iter  70 value 20515.608623
iter  80 value 20486.676897
iter  90 value 20443.797868
iter 100 value 20425.035205
final  value 20425.035205 
stopped after 100 iterations
# weights:  205
initial  value 434133.452963 
iter  10 value 59256.754399
iter  20 value 27667.303400
iter  30 value 23714.090188
iter  40 value 22354.763319
iter  50 value 21722.400366
iter  60 value 21462.415109
iter  70 value 21260.538859
iter  80 value 21158.544178
iter  90 value 21067.063785
iter 100 value 20989.934880
final  value 20989.934880 
stopped after 100 iterations
# weights:  205
initial  value 439263.174185 
iter  10 value 31673.490970
iter  20 value 22644.256797
iter  30 value 21133.039531
iter  40 value 20771.516956
iter  50 value 20579.685466
iter  60 value 20504.623615
iter  70 value 20458.999878
iter  80 value 20425.046079
iter  90 value 20400.504439
iter 100 value 20374.659577
final  value 20374.659577 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=   1; time: 1.6 min
[Tune-x] 7: size=5; decay=1.35
# weights:  61
initial  value 400526.107535 
iter  10 value 87827.643411
iter  20 value 48709.466146
iter  30 value 45106.059945
iter  40 value 43318.448723
iter  50 value 41834.669278
iter  60 value 30982.996778
iter  70 value 23667.655818
iter  80 value 21903.869067
iter  90 value 21550.796223
iter 100 value 21451.462553
final  value 21451.462553 
stopped after 100 iterations
# weights:  61
initial  value 385714.591859 
iter  10 value 101630.711396
iter  20 value 60999.812826
iter  30 value 48766.072188
iter  40 value 45854.254763
iter  50 value 42102.499309
iter  60 value 33385.973651
iter  70 value 27427.462986
iter  80 value 24571.195233
iter  90 value 23247.467211
iter 100 value 22450.055423
final  value 22450.055423 
stopped after 100 iterations
# weights:  61
initial  value 377993.359154 
iter  10 value 95805.692390
iter  20 value 53830.517719
iter  30 value 46499.851855
iter  40 value 40295.847489
iter  50 value 37177.365819
iter  60 value 26151.500405
iter  70 value 23213.775878
iter  80 value 22407.745456
iter  90 value 21970.048048
iter 100 value 21690.582510
final  value 21690.582510 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 8: size=7; decay=2.65e-05
# weights:  85
initial  value 404154.252365 
iter  10 value 70496.477387
iter  20 value 33575.922422
iter  30 value 28677.472454
iter  40 value 27192.218440
iter  50 value 26267.585354
iter  60 value 25709.051878
iter  70 value 24795.181461
iter  80 value 24019.803944
iter  90 value 22133.667098
iter 100 value 20763.055585
final  value 20763.055585 
stopped after 100 iterations
# weights:  85
initial  value 402622.344360 
iter  10 value 63260.402250
iter  20 value 38308.676363
iter  30 value 34053.025767
iter  40 value 31410.798146
iter  50 value 29920.359755
iter  60 value 28222.477038
iter  70 value 26983.459884
iter  80 value 25612.759070
iter  90 value 22199.083282
iter 100 value 20736.212820
final  value 20736.212820 
stopped after 100 iterations
# weights:  85
initial  value 408238.338168 
iter  10 value 59874.803663
iter  20 value 33473.526874
iter  30 value 26520.225116
iter  40 value 24653.333615
iter  50 value 23717.491262
iter  60 value 23264.424419
iter  70 value 22260.233894
iter  80 value 21039.359634
iter  90 value 20476.618170
iter 100 value 20375.989218
final  value 20375.989218 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 9: size=16; decay=0.00826
# weights:  193
initial  value 422581.771461 
iter  10 value 39849.018937
iter  20 value 23665.407104
iter  30 value 22355.017594
iter  40 value 21291.133354
iter  50 value 20805.243085
iter  60 value 20463.444653
iter  70 value 20345.313844
iter  80 value 20276.726623
iter  90 value 20203.035019
iter 100 value 20163.413352
final  value 20163.413352 
stopped after 100 iterations
# weights:  193
initial  value 450503.046015 
iter  10 value 57374.916715
iter  20 value 24432.485412
iter  30 value 21509.081989
iter  40 value 20922.744615
iter  50 value 20451.235623
iter  60 value 20333.524286
iter  70 value 20255.015229
iter  80 value 20148.023500
iter  90 value 20104.627334
iter 100 value 20042.881094
final  value 20042.881094 
stopped after 100 iterations
# weights:  193
initial  value 400343.315085 
iter  10 value 39925.513971
iter  20 value 26428.978197
iter  30 value 23239.245956
iter  40 value 21687.377788
iter  50 value 21057.045353
iter  60 value 20817.904879
iter  70 value 20642.747010
iter  80 value 20531.670916
iter  90 value 20457.559487
iter 100 value 20397.470073
final  value 20397.470073 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=   1; time: 1.5 min
[Tune-x] 10: size=6; decay=1.77
# weights:  73
initial  value 444543.745886 
iter  10 value 87275.576973
iter  20 value 62523.913183
iter  30 value 56153.902616
iter  40 value 54096.985331
iter  50 value 52489.354554
iter  60 value 50030.719874
iter  70 value 43542.375780
iter  80 value 31287.077973
iter  90 value 26035.461139
iter 100 value 24127.551384
final  value 24127.551384 
stopped after 100 iterations
# weights:  73
initial  value 395580.330240 
iter  10 value 90702.006630
iter  20 value 43821.033522
iter  30 value 38114.866012
iter  40 value 36088.839661
iter  50 value 34789.794603
iter  60 value 31280.639782
iter  70 value 28384.220604
iter  80 value 24924.821943
iter  90 value 22781.990821
iter 100 value 22004.999217
final  value 22004.999217 
stopped after 100 iterations
# weights:  73
initial  value 401027.149715 
iter  10 value 94096.680835
iter  20 value 64943.336245
iter  30 value 55872.349416
iter  40 value 53254.798702
iter  50 value 51019.871424
iter  60 value 42530.021666
iter  70 value 30322.246560
iter  80 value 25134.237718
iter  90 value 22752.218028
iter 100 value 22186.319134
final  value 22186.319134 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=1.02; time: 0.5 min
[Tune-x] 11: size=19; decay=0.000425
# weights:  229
initial  value 450368.464755 
iter  10 value 67825.044181
iter  20 value 24400.399090
iter  30 value 21458.706443
iter  40 value 20894.746043
iter  50 value 20460.817327
iter  60 value 20290.441364
iter  70 value 20198.305384
iter  80 value 20172.345993
iter  90 value 20139.258885
iter 100 value 20103.828823
final  value 20103.828823 
stopped after 100 iterations
# weights:  229
initial  value 349381.843257 
iter  10 value 57787.719494
iter  20 value 26575.267996
iter  30 value 22570.917972
iter  40 value 21661.414559
iter  50 value 20950.766158
iter  60 value 20449.664071
iter  70 value 20259.967123
iter  80 value 20170.492991
iter  90 value 20120.721674
iter 100 value 20070.717333
final  value 20070.717333 
stopped after 100 iterations
# weights:  229
initial  value 378396.124569 
iter  10 value 48790.099087
iter  20 value 28041.576477
iter  30 value 22991.013649
iter  40 value 21362.966470
iter  50 value 20905.003971
iter  60 value 20603.044861
iter  70 value 20469.937559
iter  80 value 20395.161538
iter  90 value 20339.628350
iter 100 value 20309.608646
final  value 20309.608646 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=   1; time: 1.8 min
[Tune-x] 12: size=2; decay=0.000219
# weights:  25
initial  value 406403.968568 
iter  10 value 121440.053598
iter  20 value 86606.895048
iter  30 value 82572.165240
iter  40 value 77594.038276
iter  50 value 33910.502539
iter  60 value 25999.345786
iter  70 value 22557.800888
iter  80 value 22119.850191
iter  90 value 21519.459757
iter 100 value 20423.910992
final  value 20423.910992 
stopped after 100 iterations
# weights:  25
initial  value 398146.076328 
iter  10 value 224468.785232
iter  20 value 201140.254021
iter  30 value 129341.382674
iter  40 value 93157.775538
iter  50 value 75848.817311
iter  60 value 67105.076027
iter  70 value 65700.099920
iter  80 value 64997.572321
iter  90 value 64382.095404
iter 100 value 63640.850881
final  value 63640.850881 
stopped after 100 iterations
# weights:  25
initial  value 407522.586972 
iter  10 value 185238.430930
iter  20 value 182699.858708
iter  30 value 156450.316915
iter  40 value 131399.722454
iter  50 value 121550.667007
iter  60 value 118610.517701
iter  70 value 118408.064788
iter  80 value 117346.939548
iter  90 value 117190.462912
iter 100 value 117103.263617
final  value 117103.263617 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.82; time: 0.2 min
[Tune-x] 13: size=14; decay=0.0969
# weights:  169
initial  value 441176.585587 
iter  10 value 45476.031197
iter  20 value 25320.956569
iter  30 value 23174.330988
iter  40 value 22245.569179
iter  50 value 21365.423343
iter  60 value 21069.320423
iter  70 value 20909.973556
iter  80 value 20772.415623
iter  90 value 20638.613780
iter 100 value 20520.147571
final  value 20520.147571 
stopped after 100 iterations
# weights:  169
initial  value 402515.418512 
iter  10 value 49942.268473
iter  20 value 28956.081264
iter  30 value 25912.280056
iter  40 value 24524.635740
iter  50 value 23489.490347
iter  60 value 22588.790244
iter  70 value 22151.406245
iter  80 value 21616.595074
iter  90 value 21105.254623
iter 100 value 20848.920845
final  value 20848.920845 
stopped after 100 iterations
# weights:  169
initial  value 411961.482916 
iter  10 value 45704.474841
iter  20 value 26745.520649
iter  30 value 23190.719888
iter  40 value 21365.125452
iter  50 value 20962.473395
iter  60 value 20831.684548
iter  70 value 20749.180464
iter  80 value 20631.923993
iter  90 value 20554.353745
iter 100 value 20495.909216
final  value 20495.909216 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=1.01; time: 1.3 min
[Tune-x] 14: size=17; decay=0.214
# weights:  205
initial  value 414232.767659 
iter  10 value 64962.037972
iter  20 value 24084.158643
iter  30 value 22069.547499
iter  40 value 21259.602626
iter  50 value 20929.270870
iter  60 value 20749.186556
iter  70 value 20607.635296
iter  80 value 20551.580780
iter  90 value 20492.887219
iter 100 value 20429.543635
final  value 20429.543635 
stopped after 100 iterations
# weights:  205
initial  value 455140.720941 
iter  10 value 48767.775694
iter  20 value 29558.556453
iter  30 value 23360.961113
iter  40 value 21158.279139
iter  50 value 20632.466403
iter  60 value 20473.131984
iter  70 value 20386.691188
iter  80 value 20344.041308
iter  90 value 20308.903862
iter 100 value 20280.238669
final  value 20280.238669 
stopped after 100 iterations
# weights:  205
initial  value 401309.489168 
iter  10 value 42216.549451
iter  20 value 22909.097444
iter  30 value 21349.978819
iter  40 value 20890.518536
iter  50 value 20725.707718
iter  60 value 20552.275828
iter  70 value 20503.553000
iter  80 value 20485.971803
iter  90 value 20466.285912
iter 100 value 20438.196368
final  value 20438.196368 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=   1; time: 1.6 min
[Tune-x] 15: size=5; decay=0.0492
# weights:  61
initial  value 403020.769504 
iter  10 value 81512.584401
iter  20 value 55835.802891
iter  30 value 45697.490739
iter  40 value 39734.776999
iter  50 value 35299.660153
iter  60 value 29607.192130
iter  70 value 23457.462546
iter  80 value 21680.454554
iter  90 value 21181.546126
iter 100 value 20843.807271
final  value 20843.807271 
stopped after 100 iterations
# weights:  61
initial  value 401543.099070 
iter  10 value 109333.399704
iter  20 value 69686.374625
iter  30 value 64786.756763
iter  40 value 58480.348545
iter  50 value 53408.200035
iter  60 value 47810.712728
iter  70 value 41830.806570
iter  80 value 35112.212167
iter  90 value 31342.546829
iter 100 value 30935.814493
final  value 30935.814493 
stopped after 100 iterations
# weights:  61
initial  value 407001.187907 
iter  10 value 100011.070696
iter  20 value 59194.329068
iter  30 value 49946.683819
iter  40 value 48005.501389
iter  50 value 45366.042599
iter  60 value 36418.402712
iter  70 value 27149.701384
iter  80 value 22143.355550
iter  90 value 21208.160367
iter 100 value 20995.373647
final  value 20995.373647 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=1.09; time: 0.4 min
[Tune-x] 16: size=20; decay=0.233
# weights:  241
initial  value 441627.675347 
iter  10 value 47378.685206
iter  20 value 24768.458483
iter  30 value 22206.123063
iter  40 value 21386.133319
iter  50 value 20908.791215
iter  60 value 20738.146880
iter  70 value 20621.894321
iter  80 value 20543.200745
iter  90 value 20492.838758
iter 100 value 20450.460290
final  value 20450.460290 
stopped after 100 iterations
# weights:  241
initial  value 452068.133788 
iter  10 value 55572.981879
iter  20 value 22817.033397
iter  30 value 20821.504795
iter  40 value 20416.551594
iter  50 value 20252.794017
iter  60 value 20193.657010
iter  70 value 20158.629281
iter  80 value 20144.789697
iter  90 value 20132.982895
iter 100 value 20114.635023
final  value 20114.635023 
stopped after 100 iterations
# weights:  241
initial  value 403374.964391 
iter  10 value 36303.950762
iter  20 value 22269.657592
iter  30 value 20935.682316
iter  40 value 20618.584522
iter  50 value 20505.941511
iter  60 value 20448.723490
iter  70 value 20418.174759
iter  80 value 20380.782144
iter  90 value 20367.306189
iter 100 value 20351.056018
final  value 20351.056018 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=   1; time: 2.0 min
[Tune-x] 17: size=11; decay=2.3e-05
# weights:  133
initial  value 400550.039742 
iter  10 value 36617.758456
iter  20 value 25120.370920
iter  30 value 22367.852776
iter  40 value 21306.936121
iter  50 value 20990.036044
iter  60 value 20858.016784
iter  70 value 20755.918734
iter  80 value 20617.911276
iter  90 value 20498.032150
iter 100 value 20366.886238
final  value 20366.886238 
stopped after 100 iterations
# weights:  133
initial  value 409496.326807 
iter  10 value 52665.761509
iter  20 value 31270.841877
iter  30 value 25111.713473
iter  40 value 22301.525614
iter  50 value 21400.868087
iter  60 value 20902.416839
iter  70 value 20733.838768
iter  80 value 20606.599958
iter  90 value 20428.824962
iter 100 value 20269.063928
final  value 20269.063928 
stopped after 100 iterations
# weights:  133
initial  value 416547.374340 
iter  10 value 70442.184014
iter  20 value 31116.427165
iter  30 value 26718.138850
iter  40 value 24957.927574
iter  50 value 23802.442315
iter  60 value 22999.691718
iter  70 value 22271.108346
iter  80 value 21991.330456
iter  90 value 21705.223935
iter 100 value 21463.179600
final  value 21463.179600 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=1.01; time: 1.0 min
[Tune-x] 18: size=9; decay=0.554
# weights:  109
initial  value 413017.058763 
iter  10 value 59855.002319
iter  20 value 29279.824156
iter  30 value 26813.044885
iter  40 value 26118.699172
iter  50 value 24943.502932
iter  60 value 24274.477508
iter  70 value 23800.657996
iter  80 value 23218.674306
iter  90 value 22465.952165
iter 100 value 21924.883667
final  value 21924.883667 
stopped after 100 iterations
# weights:  109
initial  value 451328.732355 
iter  10 value 63069.375211
iter  20 value 26868.052012
iter  30 value 23987.670711
iter  40 value 22958.954317
iter  50 value 22303.777018
iter  60 value 21602.622788
iter  70 value 21292.314667
iter  80 value 21133.140339
iter  90 value 20906.907206
iter 100 value 20790.920367
final  value 20790.920367 
stopped after 100 iterations
# weights:  109
initial  value 409840.696450 
iter  10 value 74786.830362
iter  20 value 42033.139861
iter  30 value 30632.819251
iter  40 value 26432.890908
iter  50 value 24795.761472
iter  60 value 23730.848467
iter  70 value 23335.694378
iter  80 value 23134.223191
iter  90 value 22722.983526
iter 100 value 22106.373943
final  value 22106.373943 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=1.01; time: 0.8 min
[Tune-x] 19: size=3; decay=1.46
# weights:  37
initial  value 406481.320051 
iter  10 value 131226.426549
iter  20 value 104566.058555
iter  30 value 102753.466021
iter  40 value 87105.486953
iter  50 value 55079.107754
iter  60 value 42099.529298
iter  70 value 33259.479787
iter  80 value 31610.963834
iter  90 value 29693.102725
iter 100 value 28928.953269
final  value 28928.953269 
stopped after 100 iterations
# weights:  37
initial  value 402033.014955 
iter  10 value 188676.801184
iter  20 value 149628.601724
iter  30 value 147690.412003
iter  40 value 94240.888831
iter  50 value 27614.372934
iter  60 value 22914.849895
iter  70 value 21985.605559
iter  80 value 21864.772834
iter  90 value 21792.408912
iter 100 value 21665.382441
final  value 21665.382441 
stopped after 100 iterations
# weights:  37
initial  value 388790.117042 
iter  10 value 73304.645759
iter  20 value 66970.117753
iter  30 value 65944.253908
iter  40 value 52832.455681
iter  50 value 42462.401786
iter  60 value 39255.385110
iter  70 value 37459.104182
iter  80 value 37115.011967
iter  90 value 36696.474679
iter 100 value 36444.871339
final  value 36444.871339 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=1.18; time: 0.3 min
[Tune-x] 20: size=6; decay=6.87e-05
# weights:  73
initial  value 399180.551104 
iter  10 value 42753.809274
iter  20 value 25495.321094
iter  30 value 23869.971532
iter  40 value 23499.291170
iter  50 value 23202.563752
iter  60 value 23059.484478
iter  70 value 22091.318137
iter  80 value 21222.197291
iter  90 value 20388.622279
iter 100 value 20207.175686
final  value 20207.175686 
stopped after 100 iterations
# weights:  73
initial  value 413803.244672 
iter  10 value 69009.161626
iter  20 value 40584.541369
iter  30 value 34082.013677
iter  40 value 29638.604907
iter  50 value 26829.055316
iter  60 value 25577.419240
iter  70 value 23164.640911
iter  80 value 20940.325285
iter  90 value 20446.493067
iter 100 value 20261.220047
final  value 20261.220047 
stopped after 100 iterations
# weights:  73
initial  value 406439.868332 
iter  10 value 87717.019527
iter  20 value 44691.796904
iter  30 value 41282.102943
iter  40 value 35309.412635
iter  50 value 32795.055081
iter  60 value 30206.634283
iter  70 value 27682.064176
iter  80 value 22101.180741
iter  90 value 21110.851112
iter 100 value 20862.643891
final  value 20862.643891 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=   1; time: 0.5 min
[Tune] Result: size=17; decay=0.214 : rmse.test.rmse=   1
# weights:  205
initial  value 649786.997886 
iter  10 value 70298.733508
iter  20 value 35892.389983
iter  30 value 32445.536220
iter  40 value 31596.577351
iter  50 value 30977.626736
iter  60 value 30706.735285
iter  70 value 30600.377400
iter  80 value 30549.315084
iter  90 value 30518.593717
iter 100 value 30494.556936
final  value 30494.556936 
stopped after 100 iterations
[1] "Fri Feb 09 04:19:53 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 239 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 158
 number of selected nodes                               : 19 
[1] "Fri Feb 09 04:23:33 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 04:23:39 2018"
Loading required package: penalized
Loading required package: survival

Attaching package: 'survival'

The following object is masked from 'package:caret':

    cluster

Welcome to penalized. For extended examples, see vignette("penalized").
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 04:24:04 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.randomForestSRC: Error : cannot allocate vector of size 81.0 Mb

[1] "Fri Feb 09 04:26:34 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3 1 to 10   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=7; min.node.size=10
[Tune-y] 1: rmse.test.rmse=1.04; time: 3.6 min
[Tune-x] 2: mtry=4; min.node.size=5
[Tune-y] 2: rmse.test.rmse=1.04; time: 3.0 min
[Tune-x] 3: mtry=3; min.node.size=9
[Tune-y] 3: rmse.test.rmse=1.06; time: 2.1 min
[Tune-x] 4: mtry=1; min.node.size=8
[Tune-y] 4: rmse.test.rmse=2.84; time: 0.6 min
[Tune-x] 5: mtry=7; min.node.size=7
[Tune-y] 5: rmse.test.rmse=1.06; time: 3.8 min
[Tune-x] 6: mtry=9; min.node.size=8
[Tune-y] 6: rmse.test.rmse=1.06; time: 4.3 min
[Tune-x] 7: mtry=3; min.node.size=9
[Tune-y] 7: rmse.test.rmse=1.06; time: 2.0 min
[Tune-x] 8: mtry=4; min.node.size=1
[Tune-y] 8: rmse.test.rmse=1.05; time: 3.2 min
[Tune-x] 9: mtry=8; min.node.size=5
[Tune-y] 9: rmse.test.rmse=1.08; time: 5.9 min
[Tune-x] 10: mtry=3; min.node.size=9
[Tune-y] 10: rmse.test.rmse=1.06; time: 2.1 min
[Tune-x] 11: mtry=10; min.node.size=3
[Tune-y] 11: rmse.test.rmse=1.11; time: 7.8 min
[Tune-x] 12: mtry=1; min.node.size=3
[Tune-y] 12: rmse.test.rmse=2.85; time: 0.6 min
[Tune-x] 13: mtry=7; min.node.size=7
[Tune-y] 13: rmse.test.rmse=1.06; time: 4.3 min
[Tune-x] 14: mtry=9; min.node.size=8
[Tune-y] 14: rmse.test.rmse=1.06; time: 4.7 min
[Tune-x] 15: mtry=3; min.node.size=7
[Tune-y] 15: rmse.test.rmse=1.06; time: 2.5 min
[Tune-x] 16: mtry=10; min.node.size=8
[Tune-y] 16: rmse.test.rmse=1.07; time: 5.6 min
[Tune-x] 17: mtry=6; min.node.size=1
[Tune-y] 17: rmse.test.rmse= 1.1; time: 7.3 min
[Tune-x] 18: mtry=5; min.node.size=8
[Tune-y] 18: rmse.test.rmse=1.04; time: 3.3 min
[Tune-x] 19: mtry=2; min.node.size=9
[Tune-y] 19: rmse.test.rmse=1.41; time: 1.5 min
[Tune-x] 20: mtry=3; min.node.size=2
[Tune-y] 20: rmse.test.rmse=1.06; time: 4.0 min
[Tune] Result: mtry=5; min.node.size=8 : rmse.test.rmse=1.04
[1] "Fri Feb 09 05:41:43 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Fri Feb 09 05:43:28 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.0903; maxdepth=28; minbucket=19; minsplit=26
[Tune-y] 1: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 2: cp=0.00398; maxdepth=26; minbucket=8; minsplit=38
[Tune-y] 2: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 3: cp=0.118; maxdepth=20; minbucket=42; minsplit=39
[Tune-y] 3: rmse.test.rmse=2.59; time: 0.0 min
[Tune-x] 4: cp=0.00503; maxdepth=26; minbucket=19; minsplit=8
[Tune-y] 4: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 5: cp=0.209; maxdepth=16; minbucket=16; minsplit=45
[Tune-y] 5: rmse.test.rmse=3.24; time: 0.0 min
[Tune-x] 6: cp=0.549; maxdepth=10; minbucket=9; minsplit=15
[Tune-y] 6: rmse.test.rmse=4.39; time: 0.0 min
[Tune-x] 7: cp=0.0925; maxdepth=21; minbucket=43; minsplit=38
[Tune-y] 7: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 8: cp=0.0042; maxdepth=20; minbucket=49; minsplit=38
[Tune-y] 8: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 9: cp=0.0324; maxdepth=4; minbucket=23; minsplit=41
[Tune-y] 9: rmse.test.rmse= 2.1; time: 0.0 min
[Tune-x] 10: cp=0.0023; maxdepth=27; minbucket=18; minsplit=11
[Tune-y] 10: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 11: cp=0.826; maxdepth=15; minbucket=38; minsplit=41
[Tune-y] 11: rmse.test.rmse=4.39; time: 0.0 min
[Tune-x] 12: cp=0.00812; maxdepth=5; minbucket=35; minsplit=27
[Tune-y] 12: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 13: cp=0.0516; maxdepth=20; minbucket=40; minsplit=10
[Tune-y] 13: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 14: cp=0.00591; maxdepth=28; minbucket=39; minsplit=10
[Tune-y] 14: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 15: cp=0.00566; maxdepth=18; minbucket=50; minsplit=45
[Tune-y] 15: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 16: cp=0.0047; maxdepth=22; minbucket=46; minsplit=8
[Tune-y] 16: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 17: cp=0.179; maxdepth=15; minbucket=50; minsplit=15
[Tune-y] 17: rmse.test.rmse=3.24; time: 0.1 min
[Tune-x] 18: cp=0.1; maxdepth=15; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 19: cp=0.00566; maxdepth=20; minbucket=47; minsplit=49
[Tune-y] 19: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 20: cp=0.00461; maxdepth=26; minbucket=9; minsplit=33
[Tune-y] 20: rmse.test.rmse= 1.3; time: 0.1 min
[Tune] Result: cp=0.0047; maxdepth=22; minbucket=46; minsplit=8 : rmse.test.rmse= 1.3
[1] "Fri Feb 09 05:44:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 05:44:33 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.rvm: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 05:44:39 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.67600 0.22800 0.07660 0.02580 0.00868
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 8 
Runtime: 12.76068 mins 

 Values of predicted responses: 
   index             3 
   lambda      0.07661 
    Y 1          2.482 
    Y 2           -6.1 
    Y 3         0.2173 
    Y 4         -4.324 
    Y 5          3.109 
[1] "Fri Feb 09 05:57:31 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=501; max_depth=10; eta=0.186; gamma=4.59; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.305
[Tune-y] 1: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 2: nrounds=812; max_depth=7; eta=0.365; gamma=8.11; colsample_bytree=0.596; min_child_weight=4.73; subsample=0.891
[Tune-y] 2: rmse.test.rmse=1.01; time: 0.8 min
[Tune-x] 3: nrounds=71; max_depth=1; eta=0.465; gamma=4.86; colsample_bytree=0.402; min_child_weight=17.5; subsample=0.935
[Tune-y] 3: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 4: nrounds=51; max_depth=1; eta=0.135; gamma=6.57; colsample_bytree=0.566; min_child_weight=16.7; subsample=0.791
[Tune-y] 4: rmse.test.rmse= 2.6; time: 0.0 min
[Tune-x] 5: nrounds=35; max_depth=7; eta=0.582; gamma=7.28; colsample_bytree=0.502; min_child_weight=1.21; subsample=0.559
[Tune-y] 5: rmse.test.rmse=1.04; time: 0.0 min
[Tune-x] 6: nrounds=1.14e+03; max_depth=2; eta=0.517; gamma=2.98; colsample_bytree=0.356; min_child_weight=19.4; subsample=0.588
[Tune-y] 6: rmse.test.rmse=   1; time: 0.4 min
[Tune-x] 7: nrounds=776; max_depth=8; eta=0.184; gamma=0.767; colsample_bytree=0.567; min_child_weight=9.71; subsample=0.679
[Tune-y] 7: rmse.test.rmse=1.07; time: 0.9 min
[Tune-x] 8: nrounds=437; max_depth=8; eta=0.0727; gamma=2.6; colsample_bytree=0.658; min_child_weight=15; subsample=0.343
[Tune-y] 8: rmse.test.rmse=1.03; time: 0.5 min
[Tune-x] 9: nrounds=46; max_depth=6; eta=0.588; gamma=8.82; colsample_bytree=0.391; min_child_weight=13.8; subsample=0.934
[Tune-y] 9: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 10: nrounds=17; max_depth=8; eta=0.277; gamma=9.98; colsample_bytree=0.389; min_child_weight=13.4; subsample=0.578
[Tune-y] 10: rmse.test.rmse=2.08; time: 0.0 min
[Tune-x] 11: nrounds=35; max_depth=9; eta=0.153; gamma=6.28; colsample_bytree=0.666; min_child_weight=19.4; subsample=0.418
[Tune-y] 11: rmse.test.rmse=1.17; time: 0.1 min
[Tune-x] 12: nrounds=1.64e+03; max_depth=1; eta=0.366; gamma=6.87; colsample_bytree=0.641; min_child_weight=17.8; subsample=0.291
[Tune-y] 12: rmse.test.rmse=2.39; time: 0.4 min
[Tune-x] 13: nrounds=1.01e+03; max_depth=6; eta=0.254; gamma=5.25; colsample_bytree=0.33; min_child_weight=8.31; subsample=0.813
[Tune-y] 13: rmse.test.rmse=   1; time: 0.6 min
[Tune-x] 14: nrounds=95; max_depth=3; eta=0.0119; gamma=5.53; colsample_bytree=0.337; min_child_weight=12; subsample=0.938
[Tune-y] 14: rmse.test.rmse=3.44; time: 0.0 min
[Tune-x] 15: nrounds=236; max_depth=3; eta=0.0305; gamma=9.18; colsample_bytree=0.547; min_child_weight=10.5; subsample=0.362
[Tune-y] 15: rmse.test.rmse=1.23; time: 0.1 min
[Tune-x] 16: nrounds=245; max_depth=8; eta=0.513; gamma=1.21; colsample_bytree=0.671; min_child_weight=18.7; subsample=0.336
[Tune-y] 16: rmse.test.rmse=1.22; time: 0.3 min
[Tune-x] 17: nrounds=26; max_depth=3; eta=0.377; gamma=8.57; colsample_bytree=0.392; min_child_weight=18.9; subsample=0.627
[Tune-y] 17: rmse.test.rmse=1.66; time: 0.0 min
[Tune-x] 18: nrounds=457; max_depth=7; eta=0.6; gamma=1.96; colsample_bytree=0.364; min_child_weight=18.4; subsample=0.794
[Tune-y] 18: rmse.test.rmse=1.01; time: 0.3 min
[Tune-x] 19: nrounds=3.71e+03; max_depth=9; eta=0.371; gamma=9.29; colsample_bytree=0.385; min_child_weight=4.32; subsample=0.707
[Tune-y] 19: rmse.test.rmse=1.01; time: 2.6 min
[Tune-x] 20: nrounds=23; max_depth=2; eta=0.171; gamma=4.85; colsample_bytree=0.374; min_child_weight=3.12; subsample=0.786
[Tune-y] 20: rmse.test.rmse=2.34; time: 0.1 min
[Tune] Result: nrounds=1.14e+03; max_depth=2; eta=0.517; gamma=2.98; colsample_bytree=0.356; min_child_weight=19.4; subsample=0.588 : rmse.test.rmse=   1
[1] "Fri Feb 09 06:05:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 06:05:36 2018"
<simpleError: cannot allocate vector of size 233.2 Mb>
Error in randomForestSRC::rfsrc(getTaskFormula(task), data = getTaskData(task),  : 
  An error has occurred in the grow algorithm.  Please turn trace on for further analysis.
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
**GROW** @depth 0: [4,0.498015], n=(20300,10276)
**GROW** @depth 1: [2,0], n=(3401,6875)
**GROW** @depth 2: [2,0.502101], n=(6873,6574)
**GROW** @depth 2: [3,0.499714], n=(4577,2298)
**PRUNE** @depth 2: [4,0.498015]
**GROW** @depth 3: [7,0.499375], n=(4399,2321)
**GROW** @depth 4: [4,0], n=(1451,2948)
**GROW** @depth 3: [3,0], n=(3467,3336)
**GROW** @depth 4: [1,0], n=(1679,1657)
**GROW** @depth 5: [7,0.499375], n=(1117,562)
**GROW** @depth 5: [3,0], n=(1468,1480)
**PRUNE** @depth 5: [3,0]
**GROW** @depth 4: [6,0.499917], n=(1558,763)
**PRUNE** @depth 5: [7,0]
**PRUNE** @depth 4: [4,0]
**PRUNE** @depth 4: [3,0]
**GROW** @depth 5: [3,0], n=(563,553)
**GROW** @depth 1: [7,0.499375], n=(6884,3370)
**GROW** @depth 2: [1,0], n=(1672,1698)
**PRUNE** @depth 5: [7,0.499375]
**GROW** @depth 3: [4,0.498015], n=(4471,2336)
**GROW** @depth 2: [5,0], n=(2262,4622)
**GROW** @depth 3: [3,0.499714], n=(1506,756)
**GROW** @depth 4: [5,0.498895], n=(765,379)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(2288,1105,6676,3457,4471,2336,2231,765,379,2244,540,619,3465)
**PRUNE** @depth 3: [3,0.499714]
**GROW** @depth 3: [4,0.498015], n=(2307,1150)
**GROW** @depth 4: [3,0.499714], n=(1485,750)
**GROW** @depth 5: [3,0], n=(363,348)
**GROW** @depth 4: [2,0], n=(2307,2323)
**GROW** @depth 5: [4,0.498015], n=(409,210)
**GROW** @depth 2: [6,0.499917], n=(2327,1138)
**GROW** @depth 5: [6,0.499917], n=(380,371)
**GROW** @depth 3: [1,0], n=(1165,1162)
**GROW** @depth 4: [4,0.498015], n=(751,411)
**GROW** @depth 5: [6,0.499917], n=(718,773)
**GROW** @depth 4: [6,0], n=(575,590)
**GROW** @depth 5: [4,0.498015], n=(372,168)
**GROW** @depth 4: [4,0.498015], n=(705,390)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(3393,6676,2307,2323,2327,711,732,769,347,748,1133,380,371,718,773,2244,372,168,409,210,575,590,751,411,1138)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [5,0.498895], n=(382,193)
**PRUNE** @depth 4: [4,0.498015]
**GROW** @depth 5: [3,0], n=(400,369)
**GROW** @depth 6: [4,0], n=(188,184)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(3393,6676,2307,1150,3500,711,732,400,369,347,748,1133,380,371,718,773,2244,188,184,168,409,210,382,771,392,198,1722)
**GROW** @depth 6: [6,0.499917], n=(514,234)
**PRUNE** @depth 6: [6,0.499917]
**GROW** @depth 6: [5,0.498895], n=(373,345)
**GROW** @depth 4: [2,0], n=(1127,1117)
**GROW** @depth 5: [6,0], n=(193,199)
**GROW** @depth 3: [4,0.498015], n=(1130,592)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(3393,6676,2307,1150,3500,363,348,748,753,347,748,1133,380,371,373,394,724,1127,1117,188,184,168,409,210,382,771,193,199,198,1130,592)
**GROW** @depth 4: [5,0.498895], n=(1493,760)
**GROW** @depth 6: [4,0.498015], n=(389,398)
**GROW** @depth 5: [5,0], n=(762,745)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(1682,3343,1140,554,1691,186,173,377,380,171,376,743,1510,759,759,704,790,1467,2296,2312,365,360,363,380,389,398,743,762,745,384,382,414,2267,1211)
**GROW** @depth 6: [4,0.498015], n=(199,181)
**GROW** @depth 5: [4,0.498015], n=(1538,758)
**GROW** @depth 6: [6,0.499917], n=(391,371)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(1682,3343,1140,554,1691,186,173,377,199,181,171,376,743,1510,759,759,704,790,1467,1538,758,2312,365,360,363,380,389,398,743,391,371,745,384,382,414,2267,1211)
**GROW** @depth 5: [3,0.499714], n=(751,389)
**GROW** @depth 5: [4,0.498015], n=(177,199)
**GROW** @depth 7: [7,0.499375], n=(388,371)
**PRUNE** @depth 5: [4,0.498015]
**GROW** @depth 4: [2,0.502101], n=(839,372)
**GROW** @depth 7: [7,0.499375], n=(379,380)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(1682,3343,751,366,577,1691,186,173,377,199,181,171,376,743,1510,379,380,388,371,704,790,1467,1538,758,2312,365,360,363,380,389,398,743,391,371,745,384,382,414,2388,718,372)
Grow: 14.2%, Prune: 2.786%, Change: 1.946%, Swap: 10.99%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 06:21:00 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 06:21:04 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.blm: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 06:23:14 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Number of parameters (weights and biases) to estimate: 24 
Nguyen-Widrow method
Scaling factor= 0.7000159 
gamma= 23.9922 	 alpha= 0.6775 	 beta= 14.8505 
[1] "Fri Feb 09 06:23:27 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:23:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576
r=2000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576
r=4000 d=[0]; mh=1 n=30576
r=5000 d=[0]; mh=1 n=30576
Grow: 0%, 

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.btlm: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 06:34:23 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:34:32 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:34:44 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:34:50 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:34:57 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Loading required package: MASS
[1] "Fri Feb 09 06:35:02 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:35:09 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:35:14 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.gausspr: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 06:35:22 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=51; interaction.depth=2; shrinkage=0.202; n.minobsinnode=5
[Tune-y] 1: rmse.test.rmse=0.0882; time: 0.1 min
[Tune-x] 2: n.trees=21; interaction.depth=2; shrinkage=0.0996; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 3: n.trees=13; interaction.depth=4; shrinkage=0.237; n.minobsinnode=13
[Tune-y] 3: rmse.test.rmse=0.0896; time: 0.0 min
[Tune-x] 4: n.trees=15; interaction.depth=2; shrinkage=0.159; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 5: n.trees=818; interaction.depth=6; shrinkage=0.342; n.minobsinnode=11
[Tune-y] 5: rmse.test.rmse=0.0672; time: 1.5 min
[Tune-x] 6: n.trees=62; interaction.depth=7; shrinkage=0.0733; n.minobsinnode=17
[Tune-y] 6: rmse.test.rmse=0.0677; time: 0.1 min
[Tune-x] 7: n.trees=711; interaction.depth=7; shrinkage=0.452; n.minobsinnode=15
[Tune-y] 7: rmse.test.rmse=0.0684; time: 1.6 min
[Tune-x] 8: n.trees=891; interaction.depth=1; shrinkage=0.325; n.minobsinnode=12
[Tune-y] 8: rmse.test.rmse=0.155; time: 0.5 min
[Tune-x] 9: n.trees=13; interaction.depth=6; shrinkage=0.102; n.minobsinnode=11
[Tune-y] 9: rmse.test.rmse=0.12; time: 0.2 min
[Tune-x] 10: n.trees=953; interaction.depth=4; shrinkage=0.464; n.minobsinnode=23
[Tune-y] 10: rmse.test.rmse=0.0666; time: 1.4 min
[Tune-x] 11: n.trees=498; interaction.depth=5; shrinkage=0.546; n.minobsinnode=9
[Tune-y] 11: rmse.test.rmse=0.0671; time: 0.9 min
[Tune-x] 12: n.trees=50; interaction.depth=1; shrinkage=0.0975; n.minobsinnode=5
[Tune-y] 12: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 13: n.trees=185; interaction.depth=5; shrinkage=0.462; n.minobsinnode=16
[Tune-y] 13: rmse.test.rmse=0.0661; time: 0.3 min
[Tune-x] 14: n.trees=11; interaction.depth=4; shrinkage=0.0689; n.minobsinnode=21
[Tune-y] 14: rmse.test.rmse=0.175; time: 0.0 min
[Tune-x] 15: n.trees=20; interaction.depth=8; shrinkage=0.183; n.minobsinnode=15
[Tune-y] 15: rmse.test.rmse=0.0681; time: 0.1 min
[Tune-x] 16: n.trees=317; interaction.depth=10; shrinkage=0.0353; n.minobsinnode=24
[Tune-y] 16: rmse.test.rmse=0.0651; time: 0.9 min
[Tune-x] 17: n.trees=53; interaction.depth=10; shrinkage=0.464; n.minobsinnode=20
[Tune-y] 17: rmse.test.rmse=0.0659; time: 0.2 min
[Tune-x] 18: n.trees=23; interaction.depth=6; shrinkage=0.595; n.minobsinnode=9
[Tune-y] 18: rmse.test.rmse=0.0662; time: 0.1 min
[Tune-x] 19: n.trees=23; interaction.depth=9; shrinkage=0.0996; n.minobsinnode=9
[Tune-y] 19: rmse.test.rmse=0.0784; time: 0.1 min
[Tune-x] 20: n.trees=11; interaction.depth=8; shrinkage=0.365; n.minobsinnode=13
[Tune-y] 20: rmse.test.rmse=0.0669; time: 0.0 min
[Tune] Result: n.trees=317; interaction.depth=10; shrinkage=0.0353; n.minobsinnode=24 : rmse.test.rmse=0.0651
[1] "Fri Feb 09 06:43:53 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 06:43:59 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.355; lambda=0.00324
[Tune-y] 1: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 2: alpha=0.336; lambda=0.00107
[Tune-y] 2: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 3: alpha=0.158; lambda=0.00322
[Tune-y] 3: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 4: alpha=0.165; lambda=0.0343
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: alpha=0.0508; lambda=0.0336
[Tune-y] 5: rmse.test.rmse=0.158; time: 0.0 min
[Tune-x] 6: alpha=0.394; lambda=0.0411
[Tune-y] 6: rmse.test.rmse=0.165; time: 0.0 min
[Tune-x] 7: alpha=0.093; lambda=0.00542
[Tune-y] 7: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 8: alpha=0.263; lambda=0.0278
[Tune-y] 8: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 9: alpha=0.956; lambda=0.113
[Tune-y] 9: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 10: alpha=0.569; lambda=0.0171
[Tune-y] 10: rmse.test.rmse=0.158; time: 0.0 min
[Tune-x] 11: alpha=0.395; lambda=0.333
[Tune-y] 11: rmse.test.rmse=0.263; time: 0.0 min
[Tune-x] 12: alpha=0.121; lambda=0.227
[Tune-y] 12: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 13: alpha=0.926; lambda=0.218
[Tune-y] 13: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 14: alpha=0.754; lambda=0.0767
[Tune-y] 14: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 15: alpha=0.975; lambda=0.00102
[Tune-y] 15: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 16: alpha=0.541; lambda=0.0247
[Tune-y] 16: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 17: alpha=0.0568; lambda=0.145
[Tune-y] 17: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 18: alpha=0.168; lambda=0.0179
[Tune-y] 18: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 19: alpha=0.99; lambda=0.0269
[Tune-y] 19: rmse.test.rmse=0.17; time: 0.0 min
[Tune-x] 20: alpha=0.773; lambda=2.49
[Tune-y] 20: rmse.test.rmse=0.289; time: 0.0 min
[Tune] Result: alpha=0.336; lambda=0.00107 : rmse.test.rmse=0.155
[1] "Fri Feb 09 06:44:14 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |============================                                          |  40%  |                                                                              |==========================================                            |  60%  |                                                                              |========================================================              |  80%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 06:45:14 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========                                                           |  16%  |                                                                              |======================================                                |  54%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 06:45:29 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 06:45:40 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |===========                                                           |  16%  |                                                                              |===============                                                       |  22%  |                                                                              |==================                                                    |  26%  |                                                                              |=====================                                                 |  30%  |                                                                              |========================                                              |  34%  |                                                                              |===========================                                           |  38%  |                                                                              |===============================                                       |  44%  |                                                                              |==================================                                    |  48%  |                                                                              |======================================                                |  54%  |                                                                              |==========================================                            |  60%  |                                                                              |==============================================                        |  66%  |                                                                              |=================================================                     |  70%  |                                                                              |=====================================================                 |  76%  |                                                                              |=========================================================             |  82%  |                                                                              |============================================================          |  86%  |                                                                              |================================================================      |  92%  |                                                                              |===================================================================== |  98%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 06:46:17 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 06:47:21 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
i = 1 (of 10192), d = 3.74586, its = 7
i = 2 (of 10192), d = 10, its = 55
i = 3 (of 10192), d = 2.70984, its = 6
i = 4 (of 10192), d = 10, its = 37
i = 5 (of 10192), d = 10, its = 54
i = 6 (of 10192), d = 10, its = 54
i = 7 (of 10192), d = 10, its = 54
i = 8 (of 10192), d = 10, its = 40
i = 9 (of 10192), d = 10, its = 56
i = 10 (of 10192), d = 10, its = 50
i = 11 (of 10192), d = 2.53583, its = 5
i = 12 (of 10192), d = 10, its = 42
i = 13 (of 10192), d = 2.10315, its = 4
i = 14 (of 10192), d = 10, its = 51
i = 15 (of 10192), d = 10, its = 53
i = 16 (of 10192), d = 10, its = 37
i = 17 (of 10192), d = 10, its = 55
i = 18 (of 10192), d = 10, its = 37
i = 19 (of 10192), d = 10, its = 53
i = 20 (of 10192), d = 10, its = 53
i = 21 (of 10192), d = 1.25403, its = 7
i = 22 (of 10192), d = 3.13686, its = 6
i = 23 (of 10192), d = 10, its = 57
i = 24 (of 10192), d = 1.37922, its = 7
i = 25 (of 10192), d = 10, its = 42
i = 26 (of 10192), d = 10, its = 50
i = 27 (of 10192), d = 10, its = 53
i = 28 (of 10192), d = 1.83767, its = 4
i = 29 (of 10192), d = 10, its = 43
i = 30 (of 10192), d = 0.856756, its = 20
i = 31 (of 10192), d = 10, its = 52
i = 32 (of 10192), d = 10, its = 54
i = 33 (of 10192), d = 10, its = 53
i = 34 (of 10192), d = 10, its = 53
i = 35 (of 10192), d = 2.71573, its = 5
i = 36 (of 10192), d = 10, its = 37
i = 37 (of 10192), d = 10, its = 49
i = 38 (of 10192), d = 10, its = 52
i = 39 (of 10192), d = 10, its = 54
i = 40 (of 10192), d = 10, its = 40
i = 41 (of 10192), d = 10, its = 54
i = 42 (of 10192), d = 10, its = 53
i = 43 (of 10192), d = 2.66179, its = 6
i = 44 (of 10192), d = 10, its = 56
i = 45 (of 10192), d = 1.77137, its = 5
i = 46 (of 10192), d = 10, its = 50
i = 47 (of 10192), d = 10, its = 52
i = 48 (of 10192), d = 10, its = 42
i = 49 (of 10192), d = 1.93377, its = 3
i = 50 (of 10192), d = 10, its = 54
i = 51 (of 10192), d = 10, its = 53
i = 52 (of 10192), d = 10, its = 54
i = 53 (of 10192), d = 10, its = 49
i = 54 (of 10192), d = 2.145, its = 4
i = 55 (of 10192), d = 10, its = 51
i = 56 (of 10192), d = 10, its = 51
i = 57 (of 10192), d = 1.8477, its = 4
i = 58 (of 10192), d = 10, its = 53
i = 59 (of 10192), d = 10, its = 56
i = 60 (of 10192), d = 10, its = 52
i = 61 (of 10192), d = 10, its = 56
i = 62 (of 10192), d = 1.15668, its = 23
i = 63 (of 10192), d = 10, its = 55
i = 64 (of 10192), d = 10, its = 54
i = 65 (of 10192), d = 10, its = 41
i = 66 (of 10192), d = 10, its = 51
i = 67 (of 10192), d = 2.45181, its = 5
i = 68 (of 10192), d = 3.20589, its = 6
i = 69 (of 10192), d = 10, its = 53
i = 70 (of 10192), d = 10, its = 53
i = 71 (of 10192), d = 4.30358, its = 7
i = 72 (of 10192), d = 10, its = 38
i = 73 (of 10192), d = 10, its = 50
i = 74 (of 10192), d = 10, its = 56
i = 75 (of 10192), d = 6.97427, its = 8
i = 76 (of 10192), d = 10, its = 49
i = 77 (of 10192), d = 1.59564, its = 6
i = 78 (of 10192), d = 4.09223, its = 7
i = 79 (of 10192), d = 10, its = 51
i = 80 (of 10192), d = 2.30721, its = 5
i = 81 (of 10192), d = 10, its = 39
i = 82 (of 10192), d = 1.81188, its = 4
i = 83 (of 10192), d = 10, its = 54
i = 84 (of 10192), d = 10, its = 58
i = 85 (of 10192), d = 0.983175, its = 17
i = 86 (of 10192), d = 1.64372, its = 6
i = 87 (of 10192), d = 2.28558, its = 5
i = 88 (of 10192), d = 10, its = 41
i = 89 (of 10192), d = 10, its = 53
i = 90 (of 10192), d = 1.12424, its = 20
i = 91 (of 10192), d = 10, its = 37
i = 92 (of 10192), d = 10, its = 37
i = 93 (of 10192), d = 2.47275, its = 5
i = 94 (of 10192), d = 2.97357, its = 6
i = 95 (of 10192), d = 10, its = 49
i = 96 (of 10192), d = 1.94853, its = 4
i = 97 (of 10192), d = 7.26475, its = 11
i = 98 (of 10192), d = 10, its = 53
i = 99 (of 10192), d = 10, its = 53
i = 100 (of 10192), d = 10, its = 52
i = 101 (of 10192), d = 10, its = 52
i = 102 (of 10192), d = 0.786847, its = 17
i = 103 (of 10192), d = 10, its = 50
i = 104 (of 10192), d = 10, its = 52
i = 105 (of 10192), d = 10, its = 37
i = 106 (of 10192), d = 10, its = 39
i = 107 (of 10192), d = 2.60418, its = 7
i = 108 (of 10192), d = 10, its = 49
i = 109 (of 10192), d = 1.60618, its = 6
i = 110 (of 10192), d = 2.04202, its = 3
i = 111 (of 10192), d = 10, its = 51
i = 112 (of 10192), d = 10, its = 47
i = 113 (of 10192), d = 10, its = 52
i = 114 (of 10192), d = 10, its = 51
i = 115 (of 10192), d = 10, its = 50
i = 116 (of 10192), d = 10, its = 39
i = 117 (of 10192), d = 10, its = 55
i = 118 (of 10192), d = 10, its = 57
i = 119 (of 10192), d = 10, its = 50
i = 120 (of 10192), d = 10, its = 40
i = 121 (of 10192), d = 10, its = 50
i = 122 (of 10192), d = 10, its = 52
i = 123 (of 10192), d = 10, its = 55
i = 124 (of 10192), d = 10, its = 40
i = 125 (of 10192), d = 10, its = 43
i = 126 (of 10192), d = 2.56895, its = 5
i = 127 (of 10192), d = 10, its = 45
i = 128 (of 10192), d = 2.52721, its = 5
i = 129 (of 10192), d = 10, its = 53
i = 130 (of 10192), d = 10, its = 56
i = 131 (of 10192), d = 10, its = 54
i = 132 (of 10192), d = 10, its = 49
i = 133 (of 10192), d = 1.14633, its = 21
i = 134 (of 10192), d = 10, its = 51
i = 135 (of 10192), d = 10, its = 52
i = 136 (of 10192), d = 8.02637, its = 8
i = 137 (of 10192), d = 10, its = 49
i = 138 (of 10192), d = 10, its = 51
i = 139 (of 10192), d = 10, its = 58
i = 140 (of 10192), d = 10, its = 52
i = 141 (of 10192), d = 10, its = 52
i = 142 (of 10192), d = 10, its = 49
i = 143 (of 10192), d = 2.48773, its = 5
i = 144 (of 10192), d = 10, its = 60
i = 145 (of 10192), d = 10, its = 55
i = 146 (of 10192), d = 5.67677, its = 7
i = 147 (of 10192), d = 2.32384, its = 5
i = 148 (of 10192), d = 10, its = 58
i = 149 (of 10192), d = 10, its = 39
i = 150 (of 10192), d = 1.84673, its = 4
i = 151 (of 10192), d = 4.46602, its = 7
i = 152 (of 10192), d = 10, its = 58
i = 153 (of 10192), d = 4.27508, its = 6
i = 154 (of 10192), d = 4.69036, its = 7
i = 155 (of 10192), d = 10, its = 53
i = 156 (of 10192), d = 10, its = 40
i = 157 (of 10192), d = 10, its = 54
i = 158 (of 10192), d = 10, its = 37
i = 159 (of 10192), d = 10, its = 50
i = 160 (of 10192), d = 10, its = 52
i = 161 (of 10192), d = 10, its = 51
i = 162 (of 10192), d = 2.56303, its = 5
i = 163 (of 10192), d = 3.64826, its = 9
i = 164 (of 10192), d = 3.95667, its = 8
i = 165 (of 10192), d = 3.39117, its = 7
i = 166 (of 10192), d = 5.52961, its = 9
i = 167 (of 10192), d = 10, its = 55
i = 168 (of 10192), d = 10, its = 51
i = 169 (of 10192), d = 10, its = 54
i = 170 (of 10192), d = 10, its = 39
i = 171 (of 10192), d = 10, its = 37
i = 172 (of 10192), d = 10, its = 57
i = 173 (of 10192), d = 10, its = 37
i = 174 (of 10192), d = 2.00695, its = 3
i = 175 (of 10192), d = 10, its = 55
i = 176 (of 10192), d = 10, its = 51
i = 177 (of 10192), d = 2.54851, its = 5
i = 178 (of 10192), d = 1.93111, its = 4
i = 179 (of 10192), d = 0.912792, its = 20
i = 180 (of 10192), d = 10, its = 53
i = 181 (of 10192), d = 10, its = 37
i = 182 (of 10192), d = 10, its = 37
i = 183 (of 10192), d = 2.83356, its = 6
i = 184 (of 10192), d = 10, its = 51
i = 185 (of 10192), d = 2.05933, its = 3
i = 186 (of 10192), d = 1.23649, its = 19
i = 187 (of 10192), d = 10, its = 40
i = 188 (of 10192), d = 10, its = 41
i = 189 (of 10192), d = 2.56895, its = 5
i = 190 (of 10192), d = 2.56411, its = 5
i = 191 (of 10192), d = 10, its = 52
i = 192 (of 10192), d = 10, its = 55
i = 193 (of 10192), d = 10, its = 55
i = 194 (of 10192), d = 10, its = 51
i = 195 (of 10192), d = 1.5764, its = 6
i = 196 (of 10192), d = 10, its = 50
i = 197 (of 10192), d = 10, its = 49
i = 198 (of 10192), d = 10, its = 49
i = 199 (of 10192), d = 10, its = 56
i = 200 (of 10192), d = 10, its = 52
i = 201 (of 10192), d = 10, its = 51
i = 202 (of 10192), d = 3.28358, its = 6
i = 203 (of 10192), d = 2.14694, its = 4
i = 204 (of 10192), d = 10, its = 40
i = 205 (of 10192), d = 1.75848, its = 5
i = 206 (of 10192), d = 1.59471, its = 6
i = 207 (of 10192), d = 10, its = 55
i = 208 (of 10192), d = 10, its = 51
i = 209 (of 10192), d = 1.89943, its = 4
i = 210 (of 10192), d = 3.31165, its = 9
i = 211 (of 10192), d = 10, its = 37
i = 212 (of 10192), d = 10, its = 50
i = 213 (of 10192), d = 10, its = 37
i = 214 (of 10192), d = 0.968686, its = 17
i = 215 (of 10192), d = 10, its = 55
i = 216 (of 10192), d = 4.60497, its = 9
i = 217 (of 10192), d = 10, its = 52
i = 218 (of 10192), d = 10, its = 37
i = 219 (of 10192), d = 10, its = 56
i = 220 (of 10192), d = 10, its = 56
i = 221 (of 10192), d = 1.6019, its = 6
i = 222 (of 10192), d = 10, its = 52
i = 223 (of 10192), d = 10, its = 54
i = 224 (of 10192), d = 10, its = 37
i = 225 (of 10192), d = 10, its = 37
i = 226 (of 10192), d = 2.22704, its = 4
i = 227 (of 10192), d = 10, its = 49
i = 228 (of 10192), d = 3.05143, its = 6
i = 229 (of 10192), d = 10, its = 55
i = 230 (of 10192), d = 10, its = 51
i = 231 (of 10192), d = 3.8386, its = 7
i = 232 (of 10192), d = 10, its = 48
i = 233 (of 10192), d = 4.85731, its = 7
i = 234 (of 10192), d = 10, its = 51
i = 235 (of 10192), d = 10, its = 51
i = 236 (of 10192), d = 1.21035, its = 19
i = 237 (of 10192), d = 1.83475, its = 4
i = 238 (of 10192), d = 10, its = 57
i = 239 (of 10192), d = 10, its = 37
i = 240 (of 10192), d = 2.60915, its = 5
i = 241 (of 10192), d = 10, its = 51
i = 242 (of 10192), d = 10, its = 57
i = 243 (of 10192), d = 2.38179, its = 5
i = 244 (of 10192), d = 10, its = 52
i = 245 (of 10192), d = 10, its = 55
i = 246 (of 10192), d = 4.27464, its = 9
i = 247 (of 10192), d = 1.29304, its = 24
i = 248 (of 10192), d = 10, its = 37
i = 249 (of 10192), d = 6.36388, its = 9
i = 250 (of 10192), d = 10, its = 49
i = 251 (of 10192), d = 10, its = 37
i = 252 (of 10192), d = 2.37566, its = 5
i = 253 (of 10192), d = 10, its = 56
i = 254 (of 10192), d = 10, its = 37
i = 255 (of 10192), d = 10, its = 50
i = 256 (of 10192), d = 10, its = 41
i = 257 (of 10192), d = 1.99146, its = 3
i = 258 (of 10192), d = 1.32021, its = 7
i = 259 (of 10192), d = 10, its = 40
i = 260 (of 10192), d = 10, its = 49
i = 261 (of 10192), d = 10, its = 52
i = 262 (of 10192), d = 7.76919, its = 23
i = 263 (of 10192), d = 10, its = 39
i = 264 (of 10192), d = 1.88299, its = 4
i = 265 (of 10192), d = 10, its = 51
i = 266 (of 10192), d = 10, its = 51
i = 267 (of 10192), d = 10, its = 48
i = 268 (of 10192), d = 10, its = 52
i = 269 (of 10192), d = 10, its = 52
i = 270 (of 10192), d = 10, its = 54
i = 271 (of 10192), d = 10, its = 45
i = 272 (of 10192), d = 10, its = 52
i = 273 (of 10192), d = 10, its = 41
i = 274 (of 10192), d = 10, its = 52
i = 275 (of 10192), d = 10, its = 55
i = 276 (of 10192), d = 2.8616, its = 6
i = 277 (of 10192), d = 1.65869, its = 6
i = 278 (of 10192), d = 10, its = 53
i = 279 (of 10192), d = 10, its = 37
i = 280 (of 10192), d = 2.18959, its = 4
i = 281 (of 10192), d = 10, its = 52
i = 282 (of 10192), d = 10, its = 51
i = 283 (of 10192), d = 10, its = 51
i = 284 (of 10192), d = 10, its = 56
i = 285 (of 10192), d = 1.36541, its = 22
i = 286 (of 10192), d = 10, its = 51
i = 287 (of 10192), d = 10, its = 56
i = 288 (of 10192), d = 10, its = 53
i = 289 (of 10192), d = 10, its = 50
i = 290 (of 10192), d = 10, its = 37
i = 291 (of 10192), d = 10, its = 55
i = 292 (of 10192), d = 10, its = 37
i = 293 (of 10192), d = 1.76128, its = 4
i = 294 (of 10192), d = 10, its = 49
i = 295 (of 10192), d = 3.03565, its = 6
i = 296 (of 10192), d = 10, its = 52
i = 297 (of 10192), d = 10, its = 37
i = 298 (of 10192), d = 10, its = 50
i = 299 (of 10192), d = 10, its = 40
i = 300 (of 10192), d = 10, its = 46
i = 301 (of 10192), d = 10, its = 57
i = 302 (of 10192), d = 10, its = 53
i = 303 (of 10192), d = 1.57531, its = 6
i = 304 (of 10192), d = 10, its = 55
i = 305 (of 10192), d = 2.05624, its = 3
i = 306 (of 10192), d = 10, its = 55
i = 307 (of 10192), d = 10, its = 56
i = 308 (of 10192), d = 1.63514, its = 6
i = 309 (of 10192), d = 10, its = 56
i = 310 (of 10192), d = 10, its = 39
i = 311 (of 10192), d = 10, its = 53
i = 312 (of 10192), d = 0.980813, its = 19
i = 313 (of 10192), d = 10, its = 37
i = 314 (of 10192), d = 10, its = 52
i = 315 (of 10192), d = 4.33437, its = 7
i = 316 (of 10192), d = 10, its = 41
i = 317 (of 10192), d = 10, its = 41
i = 318 (of 10192), d = 10, its = 39
i = 319 (of 10192), d = 1.47903, its = 7
i = 320 (of 10192), d = 10, its = 37
i = 321 (of 10192), d = 10, its = 55
i = 322 (of 10192), d = 10, its = 49
i = 323 (of 10192), d = 10, its = 47
i = 324 (of 10192), d = 10, its = 37
i = 325 (of 10192), d = 10, its = 51
i = 326 (of 10192), d = 2.4437, its = 5
i = 327 (of 10192), d = 10, its = 56
i = 328 (of 10192), d = 10, its = 55
i = 329 (of 10192), d = 10, its = 52
i = 330 (of 10192), d = 10, its = 53
i = 331 (of 10192), d = 10, its = 57
i = 332 (of 10192), d = 10, its = 56
i = 333 (of 10192), d = 2.57735, its = 5
i = 334 (of 10192), d = 10, its = 49
i = 335 (of 10192), d = 2.09481, its = 4
i = 336 (of 10192), d = 10, its = 49
i = 337 (of 10192), d = 10, its = 52
i = 338 (of 10192), d = 10, its = 50
i = 339 (of 10192), d = 10, its = 55
i = 340 (of 10192), d = 10, its = 52
i = 341 (of 10192), d = 10, its = 53
i = 342 (of 10192), d = 10, its = 43
i = 343 (of 10192), d = 10, its = 37
i = 344 (of 10192), d = 2.60852, its = 5
i = 345 (of 10192), d = 10, its = 51
i = 346 (of 10192), d = 10, its = 52
i = 347 (of 10192), d = 10, its = 54
i = 348 (of 10192), d = 10, its = 51
i = 349 (of 10192), d = 10, its = 57
i = 350 (of 10192), d = 10, its = 54
i = 351 (of 10192), d = 10, its = 37
i = 352 (of 10192), d = 10, its = 52
i = 353 (of 10192), d = 1.17971, its = 20
i = 354 (of 10192), d = 10, its = 55
i = 355 (of 10192), d = 1.84465, its = 4
i = 356 (of 10192), d = 10, its = 54
i = 357 (of 10192), d = 10, its = 54
i = 358 (of 10192), d = 4.22785, its = 7
i = 359 (of 10192), d = 10, its = 56
i = 360 (of 10192), d = 10, its = 50
i = 361 (of 10192), d = 1.03935, its = 21
i = 362 (of 10192), d = 10, its = 41
i = 363 (of 10192), d = 10, its = 50
i = 364 (of 10192), d = 10, its = 37
i = 365 (of 10192), d = 2.48137, its = 5
i = 366 (of 10192), d = 2.21847, its = 4
i = 367 (of 10192), d = 4.87134, its = 7
i = 368 (of 10192), d = 10, its = 54
i = 369 (of 10192), d = 10, its = 37
i = 370 (of 10192), d = 10, its = 53
i = 371 (of 10192), d = 10, its = 50
i = 372 (of 10192), d = 10, its = 58
i = 373 (of 10192), d = 3.18797, its = 6
i = 374 (of 10192), d = 10, its = 43
i = 375 (of 10192), d = 10, its = 39
i = 376 (of 10192), d = 2.13604, its = 4
i = 377 (of 10192), d = 2.43279, its = 5
i = 378 (of 10192), d = 10, its = 47
i = 379 (of 10192), d = 2.50374, its = 5
i = 380 (of 10192), d = 10, its = 54
i = 381 (of 10192), d = 2.94322, its = 6
i = 382 (of 10192), d = 10, its = 55
i = 383 (of 10192), d = 0.937444, its = 24
i = 384 (of 10192), d = 10, its = 54
i = 385 (of 10192), d = 10, its = 54
i = 386 (of 10192), d = 10, its = 53
i = 387 (of 10192), d = 2.31106, its = 5
i = 388 (of 10192), d = 7.195, its = 18
i = 389 (of 10192), d = 10, its = 53
i = 390 (of 10192), d = 10, its = 50
i = 391 (of 10192), d = 10, its = 53
i = 392 (of 10192), d = 10, its = 37
i = 393 (of 10192), d = 1.72006, its = 6
i = 394 (of 10192), d = 2.8624, its = 6
i = 395 (of 10192), d = 10, its = 42
i = 396 (of 10192), d = 2.63006, its = 5
i = 397 (of 10192), d = 10, its = 37
i = 398 (of 10192), d = 10, its = 48
i = 399 (of 10192), d = 2.41641, its = 5
i = 400 (of 10192), d = 2.30741, its = 5
i = 401 (of 10192), d = 10, its = 40
i = 402 (of 10192), d = 10, its = 55
i = 403 (of 10192), d = 1.76589, its = 5
i = 404 (of 10192), d = 10, its = 50
i = 405 (of 10192), d = 10, its = 48
i = 406 (of 10192), d = 1.79616, its = 4
i = 407 (of 10192), d = 1.99381, its = 2
i = 408 (of 10192), d = 1.49343, its = 6
i = 409 (of 10192), d = 2.85781, its = 6
i = 410 (of 10192), d = 10, its = 52
i = 411 (of 10192), d = 1.90823, its = 4
i = 412 (of 10192), d = 10, its = 40
i = 413 (of 10192), d = 10, its = 54
i = 414 (of 10192), d = 10, its = 52
i = 415 (of 10192), d = 10, its = 54
i = 416 (of 10192), d = 10, its = 50
i = 417 (of 10192), d = 10, its = 49
i = 418 (of 10192), d = 10, its = 39
i = 419 (of 10192), d = 10, its = 53
i = 420 (of 10192), d = 10, its = 50
i = 421 (of 10192), d = 10, its = 41
i = 422 (of 10192), d = 10, its = 56
i = 423 (of 10192), d = 10, its = 54
i = 424 (of 10192), d = 10, its = 53
i = 425 (of 10192), d = 10, its = 37
i = 426 (of 10192), d = 2.48192, its = 5
i = 427 (of 10192), d = 1.05714, its = 18
i = 428 (of 10192), d = 10, its = 50
i = 429 (of 10192), d = 10, its = 37
i = 430 (of 10192), d = 2.15625, its = 4
i = 431 (of 10192), d = 10, its = 54
i = 432 (of 10192), d = 1.31719, its = 7
i = 433 (of 10192), d = 10, its = 57
i = 434 (of 10192), d = 10, its = 54
i = 435 (of 10192), d = 1.74541, its = 5
i = 436 (of 10192), d = 10, its = 51
i = 437 (of 10192), d = 2.24262, its = 4
i = 438 (of 10192), d = 2.11776, its = 4
i = 439 (of 10192), d = 1.54976, its = 6
i = 440 (of 10192), d = 10, its = 55
i = 441 (of 10192), d = 10, its = 51
i = 442 (of 10192), d = 10, its = 50
i = 443 (of 10192), d = 10, its = 51
i = 444 (of 10192), d = 10, its = 55
i = 445 (of 10192), d = 10, its = 58
i = 446 (of 10192), d = 2.29021, its = 4
i = 447 (of 10192), d = 10, its = 37
i = 448 (of 10192), d = 10, its = 37
i = 449 (of 10192), d = 10, its = 41
i = 450 (of 10192), d = 2.6661, its = 5
i = 451 (of 10192), d = 10, its = 40
i = 452 (of 10192), d = 10, its = 48
i = 453 (of 10192), d = 10, its = 53
i = 454 (of 10192), d = 10, its = 56
i = 455 (of 10192), d = 10, its = 53
i = 456 (of 10192), d = 10, its = 53
i = 457 (of 10192), d = 0.699799, its = 17
i = 458 (of 10192), d = 10, its = 57
i = 459 (of 10192), d = 10, its = 53
i = 460 (of 10192), d = 10, its = 53
i = 461 (of 10192), d = 10, its = 58
i = 462 (of 10192), d = 7.21635, its = 8
i = 463 (of 10192), d = 10, its = 37
i = 464 (of 10192), d = 10, its = 37
i = 465 (of 10192), d = 10, its = 37
i = 466 (of 10192), d = 0.926408, its = 20
i = 467 (of 10192), d = 10, its = 41
i = 468 (of 10192), d = 1.81447, its = 4
i = 469 (of 10192), d = 1.42203, its = 8
i = 470 (of 10192), d = 10, its = 37
i = 471 (of 10192), d = 2.48535, its = 5
i = 472 (of 10192), d = 10, its = 37
i = 473 (of 10192), d = 10, its = 56
i = 474 (of 10192), d = 10, its = 52
i = 475 (of 10192), d = 0.982883, its = 18
i = 476 (of 10192), d = 10, its = 55
i = 477 (of 10192), d = 1.76051, its = 5
i = 478 (of 10192), d = 10, its = 50
i = 479 (of 10192), d = 10, its = 56
i = 480 (of 10192), d = 1.58043, its = 6
i = 481 (of 10192), d = 10, its = 51
i = 482 (of 10192), d = 7.57728, its = 8
i = 483 (of 10192), d = 2.85809, its = 6
i = 484 (of 10192), d = 10, its = 48
i = 485 (of 10192), d = 10, its = 52
i = 486 (of 10192), d = 10, its = 54
i = 487 (of 10192), d = 10, its = 37
i = 488 (of 10192), d = 10, its = 49
i = 489 (of 10192), d = 10, its = 41
i = 490 (of 10192), d = 10, its = 49
i = 491 (of 10192), d = 10, its = 54
i = 492 (of 10192), d = 10, its = 53
i = 493 (of 10192), d = 10, its = 54
i = 494 (of 10192), d = 1.83404, its = 4
i = 495 (of 10192), d = 2.79298, its = 5
i = 496 (of 10192), d = 1.9965, its = 2
i = 497 (of 10192), d = 10, its = 54
i = 498 (of 10192), d = 10, its = 54
i = 499 (of 10192), d = 2.05686, its = 3
i = 500 (of 10192), d = 10, its = 55
i = 501 (of 10192), d = 10, its = 52
i = 502 (of 10192), d = 10, its = 50
i = 503 (of 10192), d = 10, its = 37
i = 504 (of 10192), d = 10, its = 40
i = 505 (of 10192), d = 10, its = 43
i = 506 (of 10192), d = 10, its = 54
i = 507 (of 10192), d = 1.54784, its = 6
i = 508 (of 10192), d = 10, its = 52
i = 509 (of 10192), d = 3.47035, its = 7
i = 510 (of 10192), d = 0.844782, its = 16
i = 511 (of 10192), d = 10, its = 39
i = 512 (of 10192), d = 1.7642, its = 5
i = 513 (of 10192), d = 10, its = 45
i = 514 (of 10192), d = 10, its = 51
i = 515 (of 10192), d = 10, its = 56
i = 516 (of 10192), d = 10, its = 39
i = 517 (of 10192), d = 10, its = 40
i = 518 (of 10192), d = 10, its = 37
i = 519 (of 10192), d = 10, its = 37
i = 520 (of 10192), d = 1.53494, its = 6
i = 521 (of 10192), d = 10, its = 48
i = 522 (of 10192), d = 10, its = 37
i = 523 (of 10192), d = 8.25144, its = 8
i = 524 (of 10192), d = 10, its = 55
i = 525 (of 10192), d = 10, its = 37
i = 526 (of 10192), d = 1.90696, its = 4
i = 527 (of 10192), d = 1.7427, its = 5
i = 528 (of 10192), d = 10, its = 57
i = 529 (of 10192), d = 10, its = 52
i = 530 (of 10192), d = 3.71165, its = 6
i = 531 (of 10192), d = 10, its = 54
i = 532 (of 10192), d = 10, its = 55
i = 533 (of 10192), d = 3.52094, its = 7
i = 534 (of 10192), d = 10, its = 37
i = 535 (of 10192), d = 1.07439, its = 19
i = 536 (of 10192), d = 10, its = 52
i = 537 (of 10192), d = 10, its = 50
i = 538 (of 10192), d = 2.0872, its = 4
i = 539 (of 10192), d = 10, its = 40
i = 540 (of 10192), d = 2.31227, its = 5
i = 541 (of 10192), d = 1.59958, its = 6
i = 542 (of 10192), d = 10, its = 52
i = 543 (of 10192), d = 10, its = 49
i = 544 (of 10192), d = 10, its = 50
i = 545 (of 10192), d = 10, its = 54
i = 546 (of 10192), d = 1.72208, its = 5
i = 547 (of 10192), d = 10, its = 48
i = 548 (of 10192), d = 1.44616, its = 8
i = 549 (of 10192), d = 10, its = 55
i = 550 (of 10192), d = 10, its = 41
i = 551 (of 10192), d = 10, its = 57
i = 552 (of 10192), d = 10, its = 40
i = 553 (of 10192), d = 10, its = 53
i = 554 (of 10192), d = 3.60719, its = 6
i = 555 (of 10192), d = 10, its = 40
i = 556 (of 10192), d = 10, its = 54
i = 557 (of 10192), d = 10, its = 49
i = 558 (of 10192), d = 10, its = 39
i = 559 (of 10192), d = 10, its = 37
i = 560 (of 10192), d = 10, its = 37
i = 561 (of 10192), d = 10, its = 43
i = 562 (of 10192), d = 1.12059, its = 18
i = 563 (of 10192), d = 10, its = 38
i = 564 (of 10192), d = 10, its = 55
i = 565 (of 10192), d = 4.52353, its = 7
i = 566 (of 10192), d = 10, its = 56
i = 567 (of 10192), d = 10, its = 53
i = 568 (of 10192), d = 1.79616, its = 4
i = 569 (of 10192), d = 10, its = 37
i = 570 (of 10192), d = 10, its = 41
i = 571 (of 10192), d = 10, its = 50
i = 572 (of 10192), d = 10, its = 52
i = 573 (of 10192), d = 10, its = 50
i = 574 (of 10192), d = 10, its = 54
i = 575 (of 10192), d = 10, its = 56
i = 576 (of 10192), d = 2.4578, its = 5
i = 577 (of 10192), d = 2.41126, its = 5
i = 578 (of 10192), d = 10, its = 50
i = 579 (of 10192), d = 1.82799, its = 4
i = 580 (of 10192), d = 10, its = 40
i = 581 (of 10192), d = 10, its = 53
i = 582 (of 10192), d = 10, its = 56
i = 583 (of 10192), d = 10, its = 52
i = 584 (of 10192), d = 10, its = 57
i = 585 (of 10192), d = 10, its = 37
i = 586 (of 10192), d = 0.769144, its = 18
i = 587 (of 10192), d = 10, its = 48
i = 588 (of 10192), d = 10, its = 51
i = 589 (of 10192), d = 10, its = 51
i = 590 (of 10192), d = 1.52772, its = 6
i = 591 (of 10192), d = 10, its = 57
i = 592 (of 10192), d = 10, its = 41
i = 593 (of 10192), d = 10, its = 51
i = 594 (of 10192), d = 10, its = 37
i = 595 (of 10192), d = 10, its = 52
i = 596 (of 10192), d = 10, its = 57
i = 597 (of 10192), d = 10, its = 37
i = 598 (of 10192), d = 10, its = 59
i = 599 (of 10192), d = 10, its = 37
i = 600 (of 10192), d = 1.6682, its = 7
i = 601 (of 10192), d = 10, its = 55
i = 602 (of 10192), d = 10, its = 59
i = 603 (of 10192), d = 10, its = 55
i = 604 (of 10192), d = 10, its = 44
i = 605 (of 10192), d = 10, its = 39
i = 606 (of 10192), d = 10, its = 55
i = 607 (of 10192), d = 0.754628, its = 18
i = 608 (of 10192), d = 10, its = 37
i = 609 (of 10192), d = 10, its = 39
i = 610 (of 10192), d = 10, its = 41
i = 611 (of 10192), d = 0.950593, its = 17
i = 612 (of 10192), d = 3.09567, its = 7
i = 613 (of 10192), d = 10, its = 53
i = 614 (of 10192), d = 10, its = 47
i = 615 (of 10192), d = 0.717074, its = 20
i = 616 (of 10192), d = 10, its = 37
i = 617 (of 10192), d = 1.2668, its = 6
i = 618 (of 10192), d = 10, its = 50
i = 619 (of 10192), d = 10, its = 50
i = 620 (of 10192), d = 1.68639, its = 5
i = 621 (of 10192), d = 10, its = 37
i = 622 (of 10192), d = 10, its = 55
i = 623 (of 10192), d = 10, its = 37
i = 624 (of 10192), d = 2.15528, its = 4
i = 625 (of 10192), d = 10, its = 53
i = 626 (of 10192), d = 10, its = 50
i = 627 (of 10192), d = 10, its = 52
i = 628 (of 10192), d = 1.64003, its = 6
i = 629 (of 10192), d = 10, its = 44
i = 630 (of 10192), d = 1.63833, its = 7
i = 631 (of 10192), d = 2.37225, its = 4
i = 632 (of 10192), d = 10, its = 51
i = 633 (of 10192), d = 10, its = 41
i = 634 (of 10192), d = 10, its = 51
i = 635 (of 10192), d = 2.13714, its = 4
i = 636 (of 10192), d = 10, its = 37
i = 637 (of 10192), d = 10, its = 48
i = 638 (of 10192), d = 1.55117, its = 6
i = 639 (of 10192), d = 10, its = 55
i = 640 (of 10192), d = 10, its = 53
i = 641 (of 10192), d = 10, its = 44
i = 642 (of 10192), d = 10, its = 40
i = 643 (of 10192), d = 10, its = 40
i = 644 (of 10192), d = 10, its = 40
i = 645 (of 10192), d = 2.59176, its = 5
i = 646 (of 10192), d = 10, its = 55
i = 647 (of 10192), d = 0.936399, its = 17
i = 648 (of 10192), d = 1.2048, its = 26
i = 649 (of 10192), d = 10, its = 40
i = 650 (of 10192), d = 2.27759, its = 5
i = 651 (of 10192), d = 1.75456, its = 5
i = 652 (of 10192), d = 10, its = 51
i = 653 (of 10192), d = 2.46258, its = 5
i = 654 (of 10192), d = 10, its = 55
i = 655 (of 10192), d = 2.3969, its = 5
i = 656 (of 10192), d = 1.50469, its = 7
i = 657 (of 10192), d = 10, its = 41
i = 658 (of 10192), d = 1.43893, its = 7
i = 659 (of 10192), d = 10, its = 39
i = 660 (of 10192), d = 2.32158, its = 5
i = 661 (of 10192), d = 2.0355, its = 3
i = 662 (of 10192), d = 10, its = 51
i = 663 (of 10192), d = 3.43345, its = 7
i = 664 (of 10192), d = 10, its = 55
i = 665 (of 10192), d = 2.06635, its = 4
i = 666 (of 10192), d = 1.85538, its = 4
i = 667 (of 10192), d = 10, its = 37
i = 668 (of 10192), d = 10, its = 55
i = 669 (of 10192), d = 10, its = 50
i = 670 (of 10192), d = 10, its = 37
i = 671 (of 10192), d = 2.03863, its = 3
i = 672 (of 10192), d = 2.04941, its = 4
i = 673 (of 10192), d = 10, its = 52
i = 674 (of 10192), d = 4.25065, its = 7
i = 675 (of 10192), d = 10, its = 56
i = 676 (of 10192), d = 10, its = 51
i = 677 (of 10192), d = 10, its = 52
i = 678 (of 10192), d = 10, its = 50
i = 679 (of 10192), d = 10, its = 51
i = 680 (of 10192), d = 1.41191, its = 7
i = 681 (of 10192), d = 1.51788, its = 7
i = 682 (of 10192), d = 10, its = 53
i = 683 (of 10192), d = 10, its = 56
i = 684 (of 10192), d = 6.5345, its = 8
i = 685 (of 10192), d = 2.04773, its = 3
i = 686 (of 10192), d = 2.03668, its = 3
i = 687 (of 10192), d = 10, its = 57
i = 688 (of 10192), d = 10, its = 54
i = 689 (of 10192), d = 2.4513, its = 5
i = 690 (of 10192), d = 1.52269, its = 7
i = 691 (of 10192), d = 10, its = 51
i = 692 (of 10192), d = 10, its = 52
i = 693 (of 10192), d = 0.915601, its = 19
i = 694 (of 10192), d = 4.56106, its = 7
i = 695 (of 10192), d = 10, its = 37
i = 696 (of 10192), d = 10, its = 56
i = 697 (of 10192), d = 10, its = 55
i = 698 (of 10192), d = 10, its = 51
i = 699 (of 10192), d = 10, its = 50
i = 700 (of 10192), d = 10, its = 39
i = 701 (of 10192), d = 10, its = 49
i = 702 (of 10192), d = 10, its = 52
i = 703 (of 10192), d = 4.93764, its = 7
i = 704 (of 10192), d = 2.81898, its = 5
i = 705 (of 10192), d = 10, its = 37
i = 706 (of 10192), d = 10, its = 55
i = 707 (of 10192), d = 10, its = 37
i = 708 (of 10192), d = 10, its = 37
i = 709 (of 10192), d = 1.76695, its = 5
i = 710 (of 10192), d = 10, its = 37
i = 711 (of 10192), d = 0.967625, its = 21
i = 712 (of 10192), d = 2.52544, its = 6
i = 713 (of 10192), d = 10, its = 40
i = 714 (of 10192), d = 1.74333, its = 5
i = 715 (of 10192), d = 10, its = 52
i = 716 (of 10192), d = 10, its = 49
i = 717 (of 10192), d = 10, its = 54
i = 718 (of 10192), d = 10, its = 40
i = 719 (of 10192), d = 10, its = 52
i = 720 (of 10192), d = 10, its = 52
i = 721 (of 10192), d = 10, its = 56
i = 722 (of 10192), d = 10, its = 48
i = 723 (of 10192), d = 1.95619, its = 3
i = 724 (of 10192), d = 10, its = 55
i = 725 (of 10192), d = 1.95127, its = 3
i = 726 (of 10192), d = 10, its = 50
i = 727 (of 10192), d = 10, its = 48
i = 728 (of 10192), d = 10, its = 40
i = 729 (of 10192), d = 10, its = 55
i = 730 (of 10192), d = 10, its = 37
i = 731 (of 10192), d = 10, its = 46
i = 732 (of 10192), d = 2.68337, its = 5
i = 733 (of 10192), d = 10, its = 55
i = 734 (of 10192), d = 2.36401, its = 5
i = 735 (of 10192), d = 10, its = 51
i = 736 (of 10192), d = 1.58253, its = 6
i = 737 (of 10192), d = 10, its = 51
i = 738 (of 10192), d = 10, its = 53
i = 739 (of 10192), d = 10, its = 55
i = 740 (of 10192), d = 3.71076, its = 7
i = 741 (of 10192), d = 1.56081, its = 6
i = 742 (of 10192), d = 10, its = 37
i = 743 (of 10192), d = 10, its = 52
i = 744 (of 10192), d = 10, its = 39
i = 745 (of 10192), d = 10, its = 49
i = 746 (of 10192), d = 10, its = 40
i = 747 (of 10192), d = 3.58617, its = 8
i = 748 (of 10192), d = 1.10965, its = 17
i = 749 (of 10192), d = 10, its = 40
i = 750 (of 10192), d = 10, its = 37
i = 751 (of 10192), d = 2.21185, its = 4
i = 752 (of 10192), d = 10, its = 41
i = 753 (of 10192), d = 10, its = 41
i = 754 (of 10192), d = 10, its = 53
i = 755 (of 10192), d = 1.79606, its = 4
i = 756 (of 10192), d = 1.34181, its = 8
i = 757 (of 10192), d = 10, its = 45
i = 758 (of 10192), d = 1.37922, its = 7
i = 759 (of 10192), d = 1.561, its = 6
i = 760 (of 10192), d = 10, its = 56
i = 761 (of 10192), d = 1.8798, its = 4
i = 762 (of 10192), d = 10, its = 54
i = 763 (of 10192), d = 10, its = 53
i = 764 (of 10192), d = 4.7001, its = 8
i = 765 (of 10192), d = 10, its = 41
i = 766 (of 10192), d = 10, its = 54
i = 767 (of 10192), d = 10, its = 52
i = 768 (of 10192), d = 10, its = 48
i = 769 (of 10192), d = 10, its = 53
i = 770 (of 10192), d = 1.843, its = 4
i = 771 (of 10192), d = 1.58496, its = 6
i = 772 (of 10192), d = 10, its = 55
i = 773 (of 10192), d = 1.59956, its = 7
i = 774 (of 10192), d = 2.74035, its = 5
i = 775 (of 10192), d = 10, its = 54
i = 776 (of 10192), d = 10, its = 42
i = 777 (of 10192), d = 2.55471, its = 6
i = 778 (of 10192), d = 1.1826, its = 17
i = 779 (of 10192), d = 10, its = 59
i = 780 (of 10192), d = 10, its = 51
i = 781 (of 10192), d = 3.57933, its = 6
i = 782 (of 10192), d = 1.02363, its = 19
i = 783 (of 10192), d = 10, its = 45
i = 784 (of 10192), d = 10, its = 37
i = 785 (of 10192), d = 10, its = 54
i = 786 (of 10192), d = 1.34958, its = 6
i = 787 (of 10192), d = 0.738252, its = 21
i = 788 (of 10192), d = 2.74811, its = 5
i = 789 (of 10192), d = 10, its = 38
i = 790 (of 10192), d = 3.28668, its = 6
i = 791 (of 10192), d = 10, its = 37
i = 792 (of 10192), d = 10, its = 51
i = 793 (of 10192), d = 10, its = 37
i = 794 (of 10192), d = 10, its = 51
i = 795 (of 10192), d = 2.02232, its = 3
i = 796 (of 10192), d = 10, its = 51
i = 797 (of 10192), d = 10, its = 40
i = 798 (of 10192), d = 10, its = 53
i = 799 (of 10192), d = 1.88433, its = 4
i = 800 (of 10192), d = 10, its = 37
i = 801 (of 10192), d = 0.757353, its = 18
i = 802 (of 10192), d = 10, its = 55
i = 803 (of 10192), d = 10, its = 53
i = 804 (of 10192), d = 0.772465, its = 18
i = 805 (of 10192), d = 10, its = 52
i = 806 (of 10192), d = 10, its = 52
i = 807 (of 10192), d = 10, its = 55
i = 808 (of 10192), d = 2.00759, its = 3
i = 809 (of 10192), d = 10, its = 37
i = 810 (of 10192), d = 10, its = 37
i = 811 (of 10192), d = 10, its = 37
i = 812 (of 10192), d = 1.68628, its = 6
i = 813 (of 10192), d = 10, its = 55
i = 814 (of 10192), d = 4.89339, its = 8
i = 815 (of 10192), d = 2.48646, its = 5
i = 816 (of 10192), d = 10, its = 37
i = 817 (of 10192), d = 10, its = 49
i = 818 (of 10192), d = 1.17511, its = 6
i = 819 (of 10192), d = 10, its = 37
i = 820 (of 10192), d = 10, its = 51
i = 821 (of 10192), d = 10, its = 37
i = 822 (of 10192), d = 10, its = 40
i = 823 (of 10192), d = 10, its = 56
i = 824 (of 10192), d = 0.823123, its = 17
i = 825 (of 10192), d = 2.04461, its = 4
i = 826 (of 10192), d = 10, its = 51
i = 827 (of 10192), d = 10, its = 52
i = 828 (of 10192), d = 0.941979, its = 21
i = 829 (of 10192), d = 10, its = 54
i = 830 (of 10192), d = 10, its = 37
i = 831 (of 10192), d = 1.21516, its = 16
i = 832 (of 10192), d = 2.40739, its = 5
i = 833 (of 10192), d = 3.17318, its = 6
i = 834 (of 10192), d = 1.2236, its = 6
i = 835 (of 10192), d = 10, its = 52
i = 836 (of 10192), d = 10, its = 56
i = 837 (of 10192), d = 10, its = 41
i = 838 (of 10192), d = 10, its = 40
i = 839 (of 10192), d = 0.995355, its = 18
i = 840 (of 10192), d = 10, its = 49
i = 841 (of 10192), d = 7.99628, its = 8
i = 842 (of 10192), d = 1.61253, its = 6
i = 843 (of 10192), d = 10, its = 37
i = 844 (of 10192), d = 2.82653, its = 5
i = 845 (of 10192), d = 4.40592, its = 7
i = 846 (of 10192), d = 10, its = 40
i = 847 (of 10192), d = 10, its = 37
i = 848 (of 10192), d = 10, its = 50
i = 849 (of 10192), d = 1.48156, its = 7
i = 850 (of 10192), d = 10, its = 52
i = 851 (of 10192), d = 2.4207, its = 5
i = 852 (of 10192), d = 10, its = 40
i = 853 (of 10192), d = 10, its = 54
i = 854 (of 10192), d = 10, its = 58
i = 855 (of 10192), d = 10, its = 51
i = 856 (of 10192), d = 10, its = 50
i = 857 (of 10192), d = 4.91162, its = 7
i = 858 (of 10192), d = 10, its = 52
i = 859 (of 10192), d = 10, its = 53
i = 860 (of 10192), d = 10, its = 51
i = 861 (of 10192), d = 10, its = 49
i = 862 (of 10192), d = 10, its = 49
i = 863 (of 10192), d = 10, its = 56
i = 864 (of 10192), d = 10, its = 53
i = 865 (of 10192), d = 10, its = 55
i = 866 (of 10192), d = 10, its = 51
i = 867 (of 10192), d = 1.77357, its = 4
i = 868 (of 10192), d = 10, its = 55
i = 869 (of 10192), d = 10, its = 50
i = 870 (of 10192), d = 10, its = 53
i = 871 (of 10192), d = 10, its = 55
i = 872 (of 10192), d = 4.07017, its = 9
i = 873 (of 10192), d = 10, its = 47
i = 874 (of 10192), d = 10, its = 43
i = 875 (of 10192), d = 1.94565, its = 4
i = 876 (of 10192), d = 0.869545, its = 17
i = 877 (of 10192), d = 10, its = 39
i = 878 (of 10192), d = 10, its = 39
i = 879 (of 10192), d = 10, its = 39
i = 880 (of 10192), d = 2.36737, its = 5
i = 881 (of 10192), d = 10, its = 54
i = 882 (of 10192), d = 1.12099, its = 17
i = 883 (of 10192), d = 10, its = 53
i = 884 (of 10192), d = 10, its = 56
i = 885 (of 10192), d = 2.38744, its = 5
i = 886 (of 10192), d = 10, its = 54
i = 887 (of 10192), d = 1.69584, its = 5
i = 888 (of 10192), d = 10, its = 59
i = 889 (of 10192), d = 3.33144, its = 6
i = 890 (of 10192), d = 10, its = 56
i = 891 (of 10192), d = 2.73997, its = 5
i = 892 (of 10192), d = 10, its = 52
i = 893 (of 10192), d = 10, its = 54
i = 894 (of 10192), d = 10, its = 43
i = 895 (of 10192), d = 10, its = 57
i = 896 (of 10192), d = 10, its = 40
i = 897 (of 10192), d = 2.59519, its = 5
i = 898 (of 10192), d = 10, its = 50
i = 899 (of 10192), d = 10, its = 49
i = 900 (of 10192), d = 2.12267, its = 4
i = 901 (of 10192), d = 10, its = 40
i = 902 (of 10192), d = 2.90438, its = 5
i = 903 (of 10192), d = 10, its = 51
i = 904 (of 10192), d = 10, its = 50
i = 905 (of 10192), d = 10, its = 55
i = 906 (of 10192), d = 2.47286, its = 5
i = 907 (of 10192), d = 1.51927, its = 7
i = 908 (of 10192), d = 2.219, its = 4
i = 909 (of 10192), d = 10, its = 47
i = 910 (of 10192), d = 10, its = 42
i = 911 (of 10192), d = 10, its = 53
i = 912 (of 10192), d = 10, its = 49
i = 913 (of 10192), d = 10, its = 52
i = 914 (of 10192), d = 10, its = 49
i = 915 (of 10192), d = 10, its = 41
i = 916 (of 10192), d = 10, its = 37
i = 917 (of 10192), d = 1.97848, its = 3
i = 918 (of 10192), d = 0.953514, its = 18
i = 919 (of 10192), d = 10, its = 37
i = 920 (of 10192), d = 1.48594, its = 7
i = 921 (of 10192), d = 10, its = 37
i = 922 (of 10192), d = 0.669998, its = 21
i = 923 (of 10192), d = 1.50969, its = 6
i = 924 (of 10192), d = 1.76393, its = 5
i = 925 (of 10192), d = 10, its = 39
i = 926 (of 10192), d = 10, its = 52
i = 927 (of 10192), d = 10, its = 51
i = 928 (of 10192), d = 10, its = 60
i = 929 (of 10192), d = 3.83511, its = 7
i = 930 (of 10192), d = 10, its = 56
i = 931 (of 10192), d = 2.24372, its = 4
i = 932 (of 10192), d = 10, its = 39
i = 933 (of 10192), d = 10, its = 37
i = 934 (of 10192), d = 1.43044, its = 6
i = 935 (of 10192), d = 1.80398, its = 5
i = 936 (of 10192), d = 0.939746, its = 17
i = 937 (of 10192), d = 10, its = 53
i = 938 (of 10192), d = 10, its = 20
i = 939 (of 10192), d = 2.20496, its = 4
i = 940 (of 10192), d = 10, its = 37
i = 941 (of 10192), d = 10, its = 55
i = 942 (of 10192), d = 10, its = 37
i = 943 (of 10192), d = 10, its = 52
i = 944 (of 10192), d = 10, its = 54
i = 945 (of 10192), d = 10, its = 54
i = 946 (of 10192), d = 10, its = 51
i = 947 (of 10192), d = 10, its = 51
i = 948 (of 10192), d = 10, its = 37
i = 949 (of 10192), d = 10, its = 50
i = 950 (of 10192), d = 10, its = 50
i = 951 (of 10192), d = 10, its = 40
i = 952 (of 10192), d = 10, its = 51
i = 953 (of 10192), d = 10, its = 52
i = 954 (of 10192), d = 10, its = 49
i = 955 (of 10192), d = 0.975581, its = 19
i = 956 (of 10192), d = 10, its = 55
i = 957 (of 10192), d = 10, its = 37
i = 958 (of 10192), d = 10, its = 40
i = 959 (of 10192), d = 10, its = 53
i = 960 (of 10192), d = 2.9726, its = 6
i = 961 (of 10192), d = 0.983007, its = 24
i = 962 (of 10192), d = 10, its = 53
i = 963 (of 10192), d = 10, its = 37
i = 964 (of 10192), d = 10, its = 52
i = 965 (of 10192), d = 10, its = 53
i = 966 (of 10192), d = 10, its = 58
i = 967 (of 10192), d = 10, its = 57
i = 968 (of 10192), d = 3.15859, its = 6
i = 969 (of 10192), d = 10, its = 51
i = 970 (of 10192), d = 10, its = 41
i = 971 (of 10192), d = 1.38653, its = 7
i = 972 (of 10192), d = 10, its = 52
i = 973 (of 10192), d = 10, its = 37
i = 974 (of 10192), d = 2.14753, its = 4
i = 975 (of 10192), d = 10, its = 52
i = 976 (of 10192), d = 10, its = 37
i = 977 (of 10192), d = 10, its = 37
i = 978 (of 10192), d = 1.86736, its = 4
i = 979 (of 10192), d = 10, its = 40
i = 980 (of 10192), d = 1.70673, its = 5
i = 981 (of 10192), d = 10, its = 52
i = 982 (of 10192), d = 10, its = 50
i = 983 (of 10192), d = 1.65014, its = 6
i = 984 (of 10192), d = 10, its = 52
i = 985 (of 10192), d = 10, its = 39
i = 986 (of 10192), d = 10, its = 52
i = 987 (of 10192), d = 10, its = 37
i = 988 (of 10192), d = 0.97787, its = 17
i = 989 (of 10192), d = 10, its = 59
i = 990 (of 10192), d = 10, its = 55
i = 991 (of 10192), d = 10, its = 55
i = 992 (of 10192), d = 10, its = 37
i = 993 (of 10192), d = 10, its = 49
i = 994 (of 10192), d = 0.89865, its = 21
i = 995 (of 10192), d = 10, its = 37
i = 996 (of 10192), d = 10, its = 52
i = 997 (of 10192), d = 1.3121, its = 19
i = 998 (of 10192), d = 10, its = 37
i = 999 (of 10192), d = 10, its = 43
i = 1000 (of 10192), d = 10, its = 39
i = 1001 (of 10192), d = 10, its = 52
i = 1002 (of 10192), d = 10, its = 54
i = 1003 (of 10192), d = 3.01951, its = 5
i = 1004 (of 10192), d = 10, its = 40
i = 1005 (of 10192), d = 2.71573, its = 5
i = 1006 (of 10192), d = 10, its = 42
i = 1007 (of 10192), d = 10, its = 37
i = 1008 (of 10192), d = 10, its = 50
i = 1009 (of 10192), d = 10, its = 50
i = 1010 (of 10192), d = 10, its = 41
i = 1011 (of 10192), d = 10, its = 37
i = 1012 (of 10192), d = 1.989, its = 3
i = 1013 (of 10192), d = 10, its = 51
i = 1014 (of 10192), d = 10, its = 44
i = 1015 (of 10192), d = 2.27007, its = 4
i = 1016 (of 10192), d = 10, its = 49
i = 1017 (of 10192), d = 10, its = 51
i = 1018 (of 10192), d = 10, its = 53
i = 1019 (of 10192), d = 10, its = 58
i = 1020 (of 10192), d = 10, its = 51
i = 1021 (of 10192), d = 10, its = 51
i = 1022 (of 10192), d = 10, its = 52
i = 1023 (of 10192), d = 10, its = 53
i = 1024 (of 10192), d = 10, its = 52
i = 1025 (of 10192), d = 10, its = 49
i = 1026 (of 10192), d = 10, its = 56
i = 1027 (of 10192), d = 10, its = 50
i = 1028 (of 10192), d = 1.87199, its = 4
i = 1029 (of 10192), d = 1.5381, its = 7
i = 1030 (of 10192), d = 10, its = 37
i = 1031 (of 10192), d = 0.645016, its = 17
i = 1032 (of 10192), d = 10, its = 37
i = 1033 (of 10192), d = 10, its = 56
i = 1034 (of 10192), d = 10, its = 52
i = 1035 (of 10192), d = 10, its = 37
i = 1036 (of 10192), d = 10, its = 40
i = 1037 (of 10192), d = 2.05515, its = 4
i = 1038 (of 10192), d = 10, its = 51
i = 1039 (of 10192), d = 10, its = 54
i = 1040 (of 10192), d = 2.98335, its = 5
i = 1041 (of 10192), d = 10, its = 54
i = 1042 (of 10192), d = 2.55663, its = 5
i = 1043 (of 10192), d = 10, its = 37
i = 1044 (of 10192), d = 10, its = 54
i = 1045 (of 10192), d = 10, its = 53
i = 1046 (of 10192), d = 1.96707, its = 3
i = 1047 (of 10192), d = 10, its = 56
i = 1048 (of 10192), d = 1.87258, its = 4
i = 1049 (of 10192), d = 10, its = 53
i = 1050 (of 10192), d = 10, its = 49
i = 1051 (of 10192), d = 1.87162, its = 4
i = 1052 (of 10192), d = 10, its = 37
i = 1053 (of 10192), d = 1.65156, its = 6
i = 1054 (of 10192), d = 1.13535, its = 18
i = 1055 (of 10192), d = 10, its = 37
i = 1056 (of 10192), d = 10, its = 54
i = 1057 (of 10192), d = 10, its = 51
i = 1058 (of 10192), d = 1.33997, its = 6
i = 1059 (of 10192), d = 10, its = 51
i = 1060 (of 10192), d = 10, its = 42
i = 1061 (of 10192), d = 10, its = 54
i = 1062 (of 10192), d = 10, its = 50
i = 1063 (of 10192), d = 10, its = 51
i = 1064 (of 10192), d = 10, its = 55
i = 1065 (of 10192), d = 1.65301, its = 5
i = 1066 (of 10192), d = 1.52811, its = 7
i = 1067 (of 10192), d = 10, its = 52
i = 1068 (of 10192), d = 10, its = 59
i = 1069 (of 10192), d = 10, its = 53
i = 1070 (of 10192), d = 10, its = 51
i = 1071 (of 10192), d = 2.48014, its = 5
i = 1072 (of 10192), d = 10, its = 49
i = 1073 (of 10192), d = 1.16083, its = 22
i = 1074 (of 10192), d = 10, its = 40
i = 1075 (of 10192), d = 10, its = 54
i = 1076 (of 10192), d = 10, its = 53
i = 1077 (of 10192), d = 10, its = 37
i = 1078 (of 10192), d = 10, its = 52
i = 1079 (of 10192), d = 1.99856, its = 2
i = 1080 (of 10192), d = 10, its = 51
i = 1081 (of 10192), d = 10, its = 49
i = 1082 (of 10192), d = 10, its = 51
i = 1083 (of 10192), d = 3.38843, its = 6
i = 1084 (of 10192), d = 1.35399, its = 6
i = 1085 (of 10192), d = 10, its = 51
i = 1086 (of 10192), d = 10, its = 53
i = 1087 (of 10192), d = 10, its = 53
i = 1088 (of 10192), d = 10, its = 52
i = 1089 (of 10192), d = 10, its = 51
i = 1090 (of 10192), d = 10, its = 37
i = 1091 (of 10192), d = 10, its = 51
i = 1092 (of 10192), d = 10, its = 53
i = 1093 (of 10192), d = 4.47425, its = 8
i = 1094 (of 10192), d = 10, its = 55
i = 1095 (of 10192), d = 10, its = 54
i = 1096 (of 10192), d = 10, its = 37
i = 1097 (of 10192), d = 10, its = 55
i = 1098 (of 10192), d = 1.48567, its = 6
i = 1099 (of 10192), d = 10, its = 53
i = 1100 (of 10192), d = 10, its = 54
i = 1101 (of 10192), d = 10, its = 49
i = 1102 (of 10192), d = 3.69474, its = 7
i = 1103 (of 10192), d = 10, its = 54
i = 1104 (of 10192), d = 10, its = 37
i = 1105 (of 10192), d = 1.70131, its = 5
i = 1106 (of 10192), d = 10, its = 56
i = 1107 (of 10192), d = 2.09071, its = 4
i = 1108 (of 10192), d = 10, its = 58
i = 1109 (of 10192), d = 10, its = 52
i = 1110 (of 10192), d = 10, its = 54
i = 1111 (of 10192), d = 10, its = 37
i = 1112 (of 10192), d = 10, its = 42
i = 1113 (of 10192), d = 3.11173, its = 6
i = 1114 (of 10192), d = 1.81213, its = 5
i = 1115 (of 10192), d = 10, its = 50
i = 1116 (of 10192), d = 2.82173, its = 6
i = 1117 (of 10192), d = 1.49007, its = 7
i = 1118 (of 10192), d = 2.03999, its = 3
i = 1119 (of 10192), d = 10, its = 42
i = 1120 (of 10192), d = 1.3568, its = 7
i = 1121 (of 10192), d = 10, its = 54
i = 1122 (of 10192), d = 10, its = 54
i = 1123 (of 10192), d = 10, its = 48
i = 1124 (of 10192), d = 10, its = 38
i = 1125 (of 10192), d = 1.2699, its = 6
i = 1126 (of 10192), d = 10, its = 54
i = 1127 (of 10192), d = 10, its = 40
i = 1128 (of 10192), d = 1.23757, its = 6
i = 1129 (of 10192), d = 1.67515, its = 5
i = 1130 (of 10192), d = 1.87606, its = 4
i = 1131 (of 10192), d = 10, its = 54
i = 1132 (of 10192), d = 10, its = 48
i = 1133 (of 10192), d = 10, its = 50
i = 1134 (of 10192), d = 10, its = 52
i = 1135 (of 10192), d = 1.55771, its = 6
i = 1136 (of 10192), d = 10, its = 52
i = 1137 (of 10192), d = 2.28352, its = 5
i = 1138 (of 10192), d = 10, its = 54
i = 1139 (of 10192), d = 10, its = 55
i = 1140 (of 10192), d = 10, its = 50
i = 1141 (of 10192), d = 1.29555, its = 8
i = 1142 (of 10192), d = 10, its = 54
i = 1143 (of 10192), d = 1.63238, its = 6
i = 1144 (of 10192), d = 10, its = 53
i = 1145 (of 10192), d = 0.757353, its = 18
i = 1146 (of 10192), d = 10, its = 54
i = 1147 (of 10192), d = 10, its = 41
i = 1148 (of 10192), d = 10, its = 55
i = 1149 (of 10192), d = 10, its = 51
i = 1150 (of 10192), d = 2.17867, its = 4
i = 1151 (of 10192), d = 10, its = 55
i = 1152 (of 10192), d = 10, its = 37
i = 1153 (of 10192), d = 2.31126, its = 5
i = 1154 (of 10192), d = 10, its = 53
i = 1155 (of 10192), d = 10, its = 39
i = 1156 (of 10192), d = 10, its = 55
i = 1157 (of 10192), d = 10, its = 52
i = 1158 (of 10192), d = 1.94787, its = 3
i = 1159 (of 10192), d = 10, its = 55
i = 1160 (of 10192), d = 10, its = 53
i = 1161 (of 10192), d = 1.05706, its = 20
i = 1162 (of 10192), d = 10, its = 38
i = 1163 (of 10192), d = 3.77633, its = 6
i = 1164 (of 10192), d = 0.845927, its = 19
i = 1165 (of 10192), d = 1.61074, its = 7
i = 1166 (of 10192), d = 1.38656, its = 6
i = 1167 (of 10192), d = 10, its = 40
i = 1168 (of 10192), d = 2.32231, its = 5
i = 1169 (of 10192), d = 10, its = 48
i = 1170 (of 10192), d = 10, its = 37
i = 1171 (of 10192), d = 1.92027, its = 4
i = 1172 (of 10192), d = 10, its = 48
i = 1173 (of 10192), d = 1.36356, its = 7
i = 1174 (of 10192), d = 3.82099, its = 8
i = 1175 (of 10192), d = 2.98054, its = 6
i = 1176 (of 10192), d = 10, its = 37
i = 1177 (of 10192), d = 4.71428, its = 7
i = 1178 (of 10192), d = 1.02005, its = 22
i = 1179 (of 10192), d = 10, its = 52
i = 1180 (of 10192), d = 3.08669, its = 6
i = 1181 (of 10192), d = 10, its = 37
i = 1182 (of 10192), d = 10, its = 54
i = 1183 (of 10192), d = 10, its = 39
i = 1184 (of 10192), d = 10, its = 54
i = 1185 (of 10192), d = 10, its = 37
i = 1186 (of 10192), d = 10, its = 52
i = 1187 (of 10192), d = 10, its = 41
i = 1188 (of 10192), d = 10, its = 53
i = 1189 (of 10192), d = 10, its = 53
i = 1190 (of 10192), d = 1.90371, its = 4
i = 1191 (of 10192), d = 10, its = 54
i = 1192 (of 10192), d = 10, its = 55
i = 1193 (of 10192), d = 10, its = 52
i = 1194 (of 10192), d = 10, its = 52
i = 1195 (of 10192), d = 10, its = 59
i = 1196 (of 10192), d = 10, its = 52
i = 1197 (of 10192), d = 1.99168, its = 3
i = 1198 (of 10192), d = 2.55591, its = 5
i = 1199 (of 10192), d = 1.76993, its = 5
i = 1200 (of 10192), d = 10, its = 37
i = 1201 (of 10192), d = 1.06659, its = 20
i = 1202 (of 10192), d = 10, its = 50
i = 1203 (of 10192), d = 10, its = 55
i = 1204 (of 10192), d = 1.35483, its = 6
i = 1205 (of 10192), d = 10, its = 55
i = 1206 (of 10192), d = 10, its = 54
i = 1207 (of 10192), d = 10, its = 50
i = 1208 (of 10192), d = 10, its = 37
i = 1209 (of 10192), d = 2.73515, its = 6
i = 1210 (of 10192), d = 10, its = 53
i = 1211 (of 10192), d = 10, its = 55
i = 1212 (of 10192), d = 10, its = 37
i = 1213 (of 10192), d = 10, its = 47
i = 1214 (of 10192), d = 10, its = 39
i = 1215 (of 10192), d = 2.25842, its = 4
i = 1216 (of 10192), d = 10, its = 37
i = 1217 (of 10192), d = 10, its = 52
i = 1218 (of 10192), d = 1.67317, its = 6
i = 1219 (of 10192), d = 10, its = 40
i = 1220 (of 10192), d = 10, its = 37
i = 1221 (of 10192), d = 10, its = 53
i = 1222 (of 10192), d = 10, its = 39
i = 1223 (of 10192), d = 10, its = 48
i = 1224 (of 10192), d = 10, its = 54
i = 1225 (of 10192), d = 10, its = 39
i = 1226 (of 10192), d = 1.42273, its = 8
i = 1227 (of 10192), d = 10, its = 56
i = 1228 (of 10192), d = 10, its = 54
i = 1229 (of 10192), d = 10, its = 42
i = 1230 (of 10192), d = 10, its = 51
i = 1231 (of 10192), d = 10, its = 51
i = 1232 (of 10192), d = 4.63002, its = 7
i = 1233 (of 10192), d = 1.98414, its = 3
i = 1234 (of 10192), d = 10, its = 39
i = 1235 (of 10192), d = 10, its = 57
i = 1236 (of 10192), d = 1.63896, its = 5
i = 1237 (of 10192), d = 10, its = 39
i = 1238 (of 10192), d = 2.23107, its = 4
i = 1239 (of 10192), d = 2.90341, its = 6
i = 1240 (of 10192), d = 10, its = 53
i = 1241 (of 10192), d = 10, its = 53
i = 1242 (of 10192), d = 10, its = 53
i = 1243 (of 10192), d = 1.62523, its = 6
i = 1244 (of 10192), d = 10, its = 17
i = 1245 (of 10192), d = 1.21935, its = 6
i = 1246 (of 10192), d = 10, its = 53
i = 1247 (of 10192), d = 10, its = 53
i = 1248 (of 10192), d = 10, its = 37
i = 1249 (of 10192), d = 10, its = 52
i = 1250 (of 10192), d = 10, its = 50
i = 1251 (of 10192), d = 10, its = 52
i = 1252 (of 10192), d = 0.918774, its = 24
i = 1253 (of 10192), d = 10, its = 40
i = 1254 (of 10192), d = 10, its = 52
i = 1255 (of 10192), d = 10, its = 41
i = 1256 (of 10192), d = 10, its = 58
i = 1257 (of 10192), d = 10, its = 56
i = 1258 (of 10192), d = 10, its = 53
i = 1259 (of 10192), d = 1.34659, its = 19
i = 1260 (of 10192), d = 10, its = 40
i = 1261 (of 10192), d = 3.75862, its = 6
i = 1262 (of 10192), d = 0.985973, its = 23
i = 1263 (of 10192), d = 10, its = 37
i = 1264 (of 10192), d = 10, its = 48
i = 1265 (of 10192), d = 10, its = 52
i = 1266 (of 10192), d = 10, its = 52
i = 1267 (of 10192), d = 1.47786, its = 7
i = 1268 (of 10192), d = 10, its = 53
i = 1269 (of 10192), d = 10, its = 51
i = 1270 (of 10192), d = 10, its = 51
i = 1271 (of 10192), d = 2.98881, its = 6
i = 1272 (of 10192), d = 10, its = 37
i = 1273 (of 10192), d = 2.02393, its = 3
i = 1274 (of 10192), d = 10, its = 53
i = 1275 (of 10192), d = 10, its = 40
i = 1276 (of 10192), d = 10, its = 58
i = 1277 (of 10192), d = 10, its = 55
i = 1278 (of 10192), d = 10, its = 41
i = 1279 (of 10192), d = 10, its = 51
i = 1280 (of 10192), d = 2.12578, its = 4
i = 1281 (of 10192), d = 0.818583, its = 17
i = 1282 (of 10192), d = 2.57357, its = 7
i = 1283 (of 10192), d = 10, its = 54
i = 1284 (of 10192), d = 10, its = 56
i = 1285 (of 10192), d = 10, its = 37
i = 1286 (of 10192), d = 2.16134, its = 4
i = 1287 (of 10192), d = 1.92182, its = 4
i = 1288 (of 10192), d = 10, its = 53
i = 1289 (of 10192), d = 1.02658, its = 20
i = 1290 (of 10192), d = 1.55322, its = 6
i = 1291 (of 10192), d = 10, its = 55
i = 1292 (of 10192), d = 3.21238, its = 6
i = 1293 (of 10192), d = 10, its = 40
i = 1294 (of 10192), d = 10, its = 51
i = 1295 (of 10192), d = 2.24238, its = 4
i = 1296 (of 10192), d = 10, its = 51
i = 1297 (of 10192), d = 1.98928, its = 3
i = 1298 (of 10192), d = 10, its = 37
i = 1299 (of 10192), d = 10, its = 41
i = 1300 (of 10192), d = 10, its = 53
i = 1301 (of 10192), d = 3.45719, its = 7
i = 1302 (of 10192), d = 10, its = 56
i = 1303 (of 10192), d = 10, its = 52
i = 1304 (of 10192), d = 10, its = 38
i = 1305 (of 10192), d = 10, its = 37
i = 1306 (of 10192), d = 10, its = 52
i = 1307 (of 10192), d = 10, its = 37
i = 1308 (of 10192), d = 2.15517, its = 4
i = 1309 (of 10192), d = 2.12845, its = 4
i = 1310 (of 10192), d = 10, its = 54
i = 1311 (of 10192), d = 10, its = 40
i = 1312 (of 10192), d = 10, its = 56
i = 1313 (of 10192), d = 10, its = 37
i = 1314 (of 10192), d = 1.80037, its = 5
i = 1315 (of 10192), d = 3.9903, its = 6
i = 1316 (of 10192), d = 10, its = 50
i = 1317 (of 10192), d = 10, its = 57
i = 1318 (of 10192), d = 10, its = 39
i = 1319 (of 10192), d = 10, its = 40
i = 1320 (of 10192), d = 10, its = 37
i = 1321 (of 10192), d = 1.95205, its = 3
i = 1322 (of 10192), d = 2.00143, its = 2
i = 1323 (of 10192), d = 2.38458, its = 5
i = 1324 (of 10192), d = 2.40187, its = 5
i = 1325 (of 10192), d = 10, its = 53
i = 1326 (of 10192), d = 1.03928, its = 16
i = 1327 (of 10192), d = 10, its = 40
i = 1328 (of 10192), d = 2.87452, its = 5
i = 1329 (of 10192), d = 10, its = 53
i = 1330 (of 10192), d = 2.51519, its = 5
i = 1331 (of 10192), d = 10, its = 55
i = 1332 (of 10192), d = 10, its = 52
i = 1333 (of 10192), d = 10, its = 53
i = 1334 (of 10192), d = 10, its = 57
i = 1335 (of 10192), d = 10, its = 52
i = 1336 (of 10192), d = 4.15836, its = 6
i = 1337 (of 10192), d = 10, its = 53
i = 1338 (of 10192), d = 10, its = 56
i = 1339 (of 10192), d = 10, its = 37
i = 1340 (of 10192), d = 10, its = 52
i = 1341 (of 10192), d = 10, its = 40
i = 1342 (of 10192), d = 0.974277, its = 26
i = 1343 (of 10192), d = 0.519982, its = 18
i = 1344 (of 10192), d = 10, its = 58
i = 1345 (of 10192), d = 10, its = 37
i = 1346 (of 10192), d = 0.897313, its = 22
i = 1347 (of 10192), d = 10, its = 51
i = 1348 (of 10192), d = 1.03828, its = 20
i = 1349 (of 10192), d = 10, its = 37
i = 1350 (of 10192), d = 10, its = 53
i = 1351 (of 10192), d = 10, its = 53
i = 1352 (of 10192), d = 10, its = 54
i = 1353 (of 10192), d = 1.95754, its = 3
i = 1354 (of 10192), d = 2.16608, its = 4
i = 1355 (of 10192), d = 10, its = 39
i = 1356 (of 10192), d = 2.61977, its = 6
i = 1357 (of 10192), d = 10, its = 50
i = 1358 (of 10192), d = 1.91908, its = 4
i = 1359 (of 10192), d = 10, its = 37
i = 1360 (of 10192), d = 10, its = 37
i = 1361 (of 10192), d = 10, its = 52
i = 1362 (of 10192), d = 1.31898, its = 6
i = 1363 (of 10192), d = 10, its = 41
i = 1364 (of 10192), d = 10, its = 52
i = 1365 (of 10192), d = 2.27275, its = 5
i = 1366 (of 10192), d = 10, its = 40
i = 1367 (of 10192), d = 10, its = 57
i = 1368 (of 10192), d = 10, its = 52
i = 1369 (of 10192), d = 1.56925, its = 6
i = 1370 (of 10192), d = 10, its = 51
i = 1371 (of 10192), d = 10, its = 41
i = 1372 (of 10192), d = 10, its = 41
i = 1373 (of 10192), d = 10, its = 53
i = 1374 (of 10192), d = 10, its = 50
i = 1375 (of 10192), d = 1.49007, its = 7
i = 1376 (of 10192), d = 2.63983, its = 5
i = 1377 (of 10192), d = 10, its = 54
i = 1378 (of 10192), d = 10, its = 55
i = 1379 (of 10192), d = 10, its = 37
i = 1380 (of 10192), d = 10, its = 37
i = 1381 (of 10192), d = 10, its = 37
i = 1382 (of 10192), d = 3.87513, its = 6
i = 1383 (of 10192), d = 10, its = 39
i = 1384 (of 10192), d = 10, its = 37
i = 1385 (of 10192), d = 10, its = 37
i = 1386 (of 10192), d = 10, its = 37
i = 1387 (of 10192), d = 10, its = 54
i = 1388 (of 10192), d = 10, its = 55
i = 1389 (of 10192), d = 1.73796, its = 5
i = 1390 (of 10192), d = 10, its = 55
i = 1391 (of 10192), d = 10, its = 37
i = 1392 (of 10192), d = 10, its = 55
i = 1393 (of 10192), d = 10, its = 54
i = 1394 (of 10192), d = 10, its = 52
i = 1395 (of 10192), d = 1.84014, its = 4
i = 1396 (of 10192), d = 10, its = 53
i = 1397 (of 10192), d = 10, its = 37
i = 1398 (of 10192), d = 2.02176, its = 3
i = 1399 (of 10192), d = 10, its = 54
i = 1400 (of 10192), d = 10, its = 55
i = 1401 (of 10192), d = 2.53072, its = 5
i = 1402 (of 10192), d = 10, its = 51
i = 1403 (of 10192), d = 10, its = 49
i = 1404 (of 10192), d = 10, its = 50
i = 1405 (of 10192), d = 10, its = 54
i = 1406 (of 10192), d = 10, its = 48
i = 1407 (of 10192), d = 10, its = 42
i = 1408 (of 10192), d = 10, its = 53
i = 1409 (of 10192), d = 10, its = 56
i = 1410 (of 10192), d = 10, its = 49
i = 1411 (of 10192), d = 10, its = 49
i = 1412 (of 10192), d = 10, its = 55
i = 1413 (of 10192), d = 2.65455, its = 5
i = 1414 (of 10192), d = 10, its = 52
i = 1415 (of 10192), d = 10, its = 52
i = 1416 (of 10192), d = 10, its = 54
i = 1417 (of 10192), d = 10, its = 39
i = 1418 (of 10192), d = 2.39174, its = 5
i = 1419 (of 10192), d = 10, its = 54
i = 1420 (of 10192), d = 10, its = 53
i = 1421 (of 10192), d = 10, its = 54
i = 1422 (of 10192), d = 10, its = 52
i = 1423 (of 10192), d = 10, its = 54
i = 1424 (of 10192), d = 10, its = 39
i = 1425 (of 10192), d = 10, its = 37
i = 1426 (of 10192), d = 10, its = 37
i = 1427 (of 10192), d = 10, its = 54
i = 1428 (of 10192), d = 10, its = 56
i = 1429 (of 10192), d = 10, its = 56
i = 1430 (of 10192), d = 10, its = 39
i = 1431 (of 10192), d = 1.75766, its = 5
i = 1432 (of 10192), d = 10, its = 37
i = 1433 (of 10192), d = 10, its = 53
i = 1434 (of 10192), d = 4.24639, its = 7
i = 1435 (of 10192), d = 10, its = 53
i = 1436 (of 10192), d = 10, its = 37
i = 1437 (of 10192), d = 10, its = 52
i = 1438 (of 10192), d = 10, its = 49
i = 1439 (of 10192), d = 10, its = 50
i = 1440 (of 10192), d = 10, its = 52
i = 1441 (of 10192), d = 10, its = 52
i = 1442 (of 10192), d = 2.31174, its = 5
i = 1443 (of 10192), d = 10, its = 49
i = 1444 (of 10192), d = 0.900124, its = 18
i = 1445 (of 10192), d = 2.27439, its = 4
i = 1446 (of 10192), d = 10, its = 55
i = 1447 (of 10192), d = 10, its = 40
i = 1448 (of 10192), d = 10, its = 52
i = 1449 (of 10192), d = 10, its = 54
i = 1450 (of 10192), d = 10, its = 54
i = 1451 (of 10192), d = 10, its = 54
i = 1452 (of 10192), d = 10, its = 50
i = 1453 (of 10192), d = 10, its = 37
i = 1454 (of 10192), d = 2.90582, its = 6
i = 1455 (of 10192), d = 10, its = 51
i = 1456 (of 10192), d = 2.65045, its = 5
i = 1457 (of 10192), d = 2.03953, its = 3
i = 1458 (of 10192), d = 2.9978, its = 6
i = 1459 (of 10192), d = 10, its = 44
i = 1460 (of 10192), d = 10, its = 37
i = 1461 (of 10192), d = 10, its = 37
i = 1462 (of 10192), d = 10, its = 55
i = 1463 (of 10192), d = 10, its = 50
i = 1464 (of 10192), d = 10, its = 42
i = 1465 (of 10192), d = 10, its = 37
i = 1466 (of 10192), d = 10, its = 50
i = 1467 (of 10192), d = 1.71073, its = 5
i = 1468 (of 10192), d = 10, its = 51
i = 1469 (of 10192), d = 10, its = 54
i = 1470 (of 10192), d = 0.65932, its = 25
i = 1471 (of 10192), d = 3.47163, its = 6
i = 1472 (of 10192), d = 10, its = 38
i = 1473 (of 10192), d = 1.12958, its = 20
i = 1474 (of 10192), d = 10, its = 48
i = 1475 (of 10192), d = 10, its = 51
i = 1476 (of 10192), d = 10, its = 37
i = 1477 (of 10192), d = 10, its = 54
i = 1478 (of 10192), d = 10, its = 58
i = 1479 (of 10192), d = 1.08605, its = 8
i = 1480 (of 10192), d = 10, its = 37
i = 1481 (of 10192), d = 10, its = 56
i = 1482 (of 10192), d = 0.646971, its = 17
i = 1483 (of 10192), d = 10, its = 37
i = 1484 (of 10192), d = 3.85737, its = 7
i = 1485 (of 10192), d = 10, its = 49
i = 1486 (of 10192), d = 5.28763, its = 9
i = 1487 (of 10192), d = 3.72653, its = 7
i = 1488 (of 10192), d = 10, its = 39
i = 1489 (of 10192), d = 10, its = 54
i = 1490 (of 10192), d = 10, its = 44
i = 1491 (of 10192), d = 10, its = 40
i = 1492 (of 10192), d = 10, its = 44
i = 1493 (of 10192), d = 1.92302, its = 4
i = 1494 (of 10192), d = 2.69436, its = 6
i = 1495 (of 10192), d = 0.803152, its = 21
i = 1496 (of 10192), d = 10, its = 53
i = 1497 (of 10192), d = 10, its = 49
i = 1498 (of 10192), d = 0.856756, its = 20
i = 1499 (of 10192), d = 10, its = 56
i = 1500 (of 10192), d = 10, its = 45
i = 1501 (of 10192), d = 10, its = 52
i = 1502 (of 10192), d = 10, its = 52
i = 1503 (of 10192), d = 10, its = 53
i = 1504 (of 10192), d = 10, its = 53
i = 1505 (of 10192), d = 10, its = 51
i = 1506 (of 10192), d = 10, its = 37
i = 1507 (of 10192), d = 2.22354, its = 4
i = 1508 (of 10192), d = 1.77284, its = 5
i = 1509 (of 10192), d = 10, its = 37
i = 1510 (of 10192), d = 10, its = 51
i = 1511 (of 10192), d = 10, its = 50
i = 1512 (of 10192), d = 10, its = 46
i = 1513 (of 10192), d = 10, its = 54
i = 1514 (of 10192), d = 10, its = 45
i = 1515 (of 10192), d = 10, its = 53
i = 1516 (of 10192), d = 10, its = 37
i = 1517 (of 10192), d = 10, its = 37
i = 1518 (of 10192), d = 1.08812, its = 6
i = 1519 (of 10192), d = 1.57283, its = 7
i = 1520 (of 10192), d = 1.5708, its = 7
i = 1521 (of 10192), d = 2.54827, its = 6
i = 1522 (of 10192), d = 10, its = 37
i = 1523 (of 10192), d = 10, its = 44
i = 1524 (of 10192), d = 2.74892, its = 5
i = 1525 (of 10192), d = 10, its = 49
i = 1526 (of 10192), d = 3.27015, its = 7
i = 1527 (of 10192), d = 10, its = 49
i = 1528 (of 10192), d = 10, its = 53
i = 1529 (of 10192), d = 10, its = 37
i = 1530 (of 10192), d = 10, its = 48
i = 1531 (of 10192), d = 10, its = 51
i = 1532 (of 10192), d = 10, its = 37
i = 1533 (of 10192), d = 1.25256, its = 21
i = 1534 (of 10192), d = 10, its = 40
i = 1535 (of 10192), d = 2.31067, its = 4
i = 1536 (of 10192), d = 10, its = 53
i = 1537 (of 10192), d = 10, its = 38
i = 1538 (of 10192), d = 1.53945, its = 6
i = 1539 (of 10192), d = 10, its = 57
i = 1540 (of 10192), d = 1.01089, its = 19
i = 1541 (of 10192), d = 10, its = 49
i = 1542 (of 10192), d = 10, its = 40
i = 1543 (of 10192), d = 10, its = 39
i = 1544 (of 10192), d = 2.58517, its = 5
i = 1545 (of 10192), d = 2.08359, its = 4
i = 1546 (of 10192), d = 10, its = 41
i = 1547 (of 10192), d = 10, its = 51
i = 1548 (of 10192), d = 10, its = 45
i = 1549 (of 10192), d = 10, its = 53
i = 1550 (of 10192), d = 10, its = 55
i = 1551 (of 10192), d = 2.42688, its = 5
i = 1552 (of 10192), d = 10, its = 52
i = 1553 (of 10192), d = 10, its = 57
i = 1554 (of 10192), d = 2.96947, its = 6
i = 1555 (of 10192), d = 0.872266, its = 20
i = 1556 (of 10192), d = 10, its = 54
i = 1557 (of 10192), d = 10, its = 49
i = 1558 (of 10192), d = 1.61903, its = 6
i = 1559 (of 10192), d = 10, its = 56
i = 1560 (of 10192), d = 10, its = 40
i = 1561 (of 10192), d = 10, its = 40
i = 1562 (of 10192), d = 10, its = 50
i = 1563 (of 10192), d = 10, its = 37
i = 1564 (of 10192), d = 10, its = 37
i = 1565 (of 10192), d = 10, its = 38
i = 1566 (of 10192), d = 1.03533, its = 20
i = 1567 (of 10192), d = 10, its = 49
i = 1568 (of 10192), d = 2.03963, its = 3
i = 1569 (of 10192), d = 0.887773, its = 18
i = 1570 (of 10192), d = 10, its = 51
i = 1571 (of 10192), d = 10, its = 38
i = 1572 (of 10192), d = 10, its = 37
i = 1573 (of 10192), d = 10, its = 53
i = 1574 (of 10192), d = 10, its = 50
i = 1575 (of 10192), d = 10, its = 37
i = 1576 (of 10192), d = 10, its = 43
i = 1577 (of 10192), d = 10, its = 56
i = 1578 (of 10192), d = 10, its = 41
i = 1579 (of 10192), d = 10, its = 37
i = 1580 (of 10192), d = 2.71038, its = 6
i = 1581 (of 10192), d = 10, its = 55
i = 1582 (of 10192), d = 10, its = 40
i = 1583 (of 10192), d = 10, its = 40
i = 1584 (of 10192), d = 10, its = 55
i = 1585 (of 10192), d = 10, its = 50
i = 1586 (of 10192), d = 10, its = 49
i = 1587 (of 10192), d = 2.34989, its = 5
i = 1588 (of 10192), d = 2.57524, its = 5
i = 1589 (of 10192), d = 10, its = 50
i = 1590 (of 10192), d = 10, its = 41
i = 1591 (of 10192), d = 1.58599, its = 6
i = 1592 (of 10192), d = 10, its = 37
i = 1593 (of 10192), d = 1.18895, its = 20
i = 1594 (of 10192), d = 10, its = 54
i = 1595 (of 10192), d = 10, its = 37
i = 1596 (of 10192), d = 2.14186, its = 4
i = 1597 (of 10192), d = 10, its = 39
i = 1598 (of 10192), d = 1.03878, its = 20
i = 1599 (of 10192), d = 2.70452, its = 5
i = 1600 (of 10192), d = 10, its = 57
i = 1601 (of 10192), d = 1.80104, its = 4
i = 1602 (of 10192), d = 10, its = 52
i = 1603 (of 10192), d = 10, its = 40
i = 1604 (of 10192), d = 1.23757, its = 6
i = 1605 (of 10192), d = 10, its = 54
i = 1606 (of 10192), d = 1.43972, its = 6
i = 1607 (of 10192), d = 10, its = 54
i = 1608 (of 10192), d = 10, its = 54
i = 1609 (of 10192), d = 10, its = 37
i = 1610 (of 10192), d = 1.51916, its = 6
i = 1611 (of 10192), d = 3.09691, its = 6
i = 1612 (of 10192), d = 10, its = 52
i = 1613 (of 10192), d = 10, its = 53
i = 1614 (of 10192), d = 2.00799, its = 3
i = 1615 (of 10192), d = 10, its = 57
i = 1616 (of 10192), d = 4.05073, its = 7
i = 1617 (of 10192), d = 10, its = 41
i = 1618 (of 10192), d = 2.94109, its = 6
i = 1619 (of 10192), d = 2.19858, its = 4
i = 1620 (of 10192), d = 10, its = 37
i = 1621 (of 10192), d = 10, its = 41
i = 1622 (of 10192), d = 0.787976, its = 19
i = 1623 (of 10192), d = 10, its = 52
i = 1624 (of 10192), d = 10, its = 41
i = 1625 (of 10192), d = 10, its = 54
i = 1626 (of 10192), d = 10, its = 51
i = 1627 (of 10192), d = 10, its = 40
i = 1628 (of 10192), d = 10, its = 49
i = 1629 (of 10192), d = 10, its = 37
i = 1630 (of 10192), d = 2.08144, its = 4
i = 1631 (of 10192), d = 10, its = 37
i = 1632 (of 10192), d = 2.96113, its = 6
i = 1633 (of 10192), d = 10, its = 37
i = 1634 (of 10192), d = 10, its = 48
i = 1635 (of 10192), d = 10, its = 60
i = 1636 (of 10192), d = 8.99205, its = 8
i = 1637 (of 10192), d = 10, its = 50
i = 1638 (of 10192), d = 2.08117, its = 4
i = 1639 (of 10192), d = 10, its = 50
i = 1640 (of 10192), d = 2.55625, its = 5
i = 1641 (of 10192), d = 6.3653, its = 8
i = 1642 (of 10192), d = 10, its = 40
i = 1643 (of 10192), d = 10, its = 56
i = 1644 (of 10192), d = 10, its = 51
i = 1645 (of 10192), d = 10, its = 48
i = 1646 (of 10192), d = 2.02447, its = 3
i = 1647 (of 10192), d = 10, its = 37
i = 1648 (of 10192), d = 10, its = 52
i = 1649 (of 10192), d = 1.52912, its = 6
i = 1650 (of 10192), d = 0.646971, its = 17
i = 1651 (of 10192), d = 10, its = 37
i = 1652 (of 10192), d = 10, its = 51
i = 1653 (of 10192), d = 0.936912, its = 25
i = 1654 (of 10192), d = 10, its = 49
i = 1655 (of 10192), d = 10, its = 56
i = 1656 (of 10192), d = 1.5671, its = 6
i = 1657 (of 10192), d = 10, its = 50
i = 1658 (of 10192), d = 3.89031, its = 7
i = 1659 (of 10192), d = 10, its = 51
i = 1660 (of 10192), d = 10, its = 55
i = 1661 (of 10192), d = 1.97702, its = 3
i = 1662 (of 10192), d = 10, its = 53
i = 1663 (of 10192), d = 10, its = 41
i = 1664 (of 10192), d = 1.67475, its = 7
i = 1665 (of 10192), d = 10, its = 54
i = 1666 (of 10192), d = 10, its = 50
i = 1667 (of 10192), d = 10, its = 37
i = 1668 (of 10192), d = 10, its = 53
i = 1669 (of 10192), d = 10, its = 37
i = 1670 (of 10192), d = 2.21012, its = 4
i = 1671 (of 10192), d = 10, its = 52
i = 1672 (of 10192), d = 10, its = 39
i = 1673 (of 10192), d = 10, its = 51
i = 1674 (of 10192), d = 10, its = 57
i = 1675 (of 10192), d = 10, its = 52
i = 1676 (of 10192), d = 10, its = 40
i = 1677 (of 10192), d = 1.12918, its = 24
i = 1678 (of 10192), d = 10, its = 57
i = 1679 (of 10192), d = 2.33346, its = 5
i = 1680 (of 10192), d = 1.62274, its = 6
i = 1681 (of 10192), d = 2.62215, its = 5
i = 1682 (of 10192), d = 2.65933, its = 5
i = 1683 (of 10192), d = 10, its = 37
i = 1684 (of 10192), d = 10, its = 55
i = 1685 (of 10192), d = 10, its = 56
i = 1686 (of 10192), d = 10, its = 37
i = 1687 (of 10192), d = 10, its = 50
i = 1688 (of 10192), d = 10, its = 59
i = 1689 (of 10192), d = 10, its = 54
i = 1690 (of 10192), d = 10, its = 55
i = 1691 (of 10192), d = 10, its = 53
i = 1692 (of 10192), d = 1.33802, its = 7
i = 1693 (of 10192), d = 1.81491, its = 4
i = 1694 (of 10192), d = 10, its = 52
i = 1695 (of 10192), d = 2.59957, its = 5
i = 1696 (of 10192), d = 1.39803, its = 6
i = 1697 (of 10192), d = 1.73657, its = 5
i = 1698 (of 10192), d = 1.11391, its = 18
i = 1699 (of 10192), d = 2.56292, its = 6
i = 1700 (of 10192), d = 0.861975, its = 18
i = 1701 (of 10192), d = 10, its = 52
i = 1702 (of 10192), d = 10, its = 54
i = 1703 (of 10192), d = 10, its = 54
i = 1704 (of 10192), d = 2.86445, its = 5
i = 1705 (of 10192), d = 1.31391, its = 7
i = 1706 (of 10192), d = 2.16285, its = 5
i = 1707 (of 10192), d = 6.5598, its = 8
i = 1708 (of 10192), d = 10, its = 37
i = 1709 (of 10192), d = 1.45384, its = 6
i = 1710 (of 10192), d = 3.3761, its = 6
i = 1711 (of 10192), d = 10, its = 54
i = 1712 (of 10192), d = 10, its = 40
i = 1713 (of 10192), d = 1.54858, its = 6
i = 1714 (of 10192), d = 10, its = 52
i = 1715 (of 10192), d = 10, its = 40
i = 1716 (of 10192), d = 10, its = 38
i = 1717 (of 10192), d = 10, its = 56
i = 1718 (of 10192), d = 10, its = 39
i = 1719 (of 10192), d = 10, its = 37
i = 1720 (of 10192), d = 1.17954, its = 23
i = 1721 (of 10192), d = 10, its = 57
i = 1722 (of 10192), d = 10, its = 56
i = 1723 (of 10192), d = 10, its = 51
i = 1724 (of 10192), d = 10, its = 55
i = 1725 (of 10192), d = 10, its = 53
i = 1726 (of 10192), d = 10, its = 54
i = 1727 (of 10192), d = 10, its = 56
i = 1728 (of 10192), d = 10, its = 55
i = 1729 (of 10192), d = 1.12959, its = 17
i = 1730 (of 10192), d = 10, its = 37
i = 1731 (of 10192), d = 10, its = 54
i = 1732 (of 10192), d = 2.69106, its = 6
i = 1733 (of 10192), d = 10, its = 59
i = 1734 (of 10192), d = 10, its = 49
i = 1735 (of 10192), d = 0.928766, its = 20
i = 1736 (of 10192), d = 1.00242, its = 19
i = 1737 (of 10192), d = 1.45641, its = 6
i = 1738 (of 10192), d = 10, its = 53
i = 1739 (of 10192), d = 10, its = 53
i = 1740 (of 10192), d = 10, its = 59
i = 1741 (of 10192), d = 10, its = 43
i = 1742 (of 10192), d = 3.73215, its = 7
i = 1743 (of 10192), d = 10, its = 52
i = 1744 (of 10192), d = 10, its = 39
i = 1745 (of 10192), d = 1.22368, its = 6
i = 1746 (of 10192), d = 10, its = 54
i = 1747 (of 10192), d = 10, its = 48
i = 1748 (of 10192), d = 1.41072, its = 19
i = 1749 (of 10192), d = 10, its = 51
i = 1750 (of 10192), d = 10, its = 54
i = 1751 (of 10192), d = 10, its = 50
i = 1752 (of 10192), d = 10, its = 50
i = 1753 (of 10192), d = 1.37627, its = 7
i = 1754 (of 10192), d = 10, its = 37
i = 1755 (of 10192), d = 2.21121, its = 4
i = 1756 (of 10192), d = 10, its = 55
i = 1757 (of 10192), d = 10, its = 50
i = 1758 (of 10192), d = 3.93441, its = 8
i = 1759 (of 10192), d = 10, its = 51
i = 1760 (of 10192), d = 10, its = 40
i = 1761 (of 10192), d = 2.56292, its = 6
i = 1762 (of 10192), d = 10, its = 50
i = 1763 (of 10192), d = 10, its = 48
i = 1764 (of 10192), d = 10, its = 51
i = 1765 (of 10192), d = 10, its = 37
i = 1766 (of 10192), d = 10, its = 56
i = 1767 (of 10192), d = 10, its = 54
i = 1768 (of 10192), d = 10, its = 56
i = 1769 (of 10192), d = 6.65624, its = 8
i = 1770 (of 10192), d = 10, its = 37
i = 1771 (of 10192), d = 10, its = 37
i = 1772 (of 10192), d = 1.60906, its = 6
i = 1773 (of 10192), d = 5.54971, its = 7
i = 1774 (of 10192), d = 10, its = 56
i = 1775 (of 10192), d = 10, its = 55
i = 1776 (of 10192), d = 10, its = 37
i = 1777 (of 10192), d = 10, its = 40
i = 1778 (of 10192), d = 10, its = 41
i = 1779 (of 10192), d = 1.80197, its = 5
i = 1780 (of 10192), d = 10, its = 54
i = 1781 (of 10192), d = 1.13248, its = 6
i = 1782 (of 10192), d = 10, its = 54
i = 1783 (of 10192), d = 10, its = 49
i = 1784 (of 10192), d = 10, its = 53
i = 1785 (of 10192), d = 10, its = 55
i = 1786 (of 10192), d = 1.26793, its = 6
i = 1787 (of 10192), d = 10, its = 48
i = 1788 (of 10192), d = 8.33864, its = 10
i = 1789 (of 10192), d = 10, its = 52
i = 1790 (of 10192), d = 10, its = 52
i = 1791 (of 10192), d = 1.85766, its = 4
i = 1792 (of 10192), d = 3.65963, its = 6
i = 1793 (of 10192), d = 10, its = 37
i = 1794 (of 10192), d = 10, its = 54
i = 1795 (of 10192), d = 1.35603, its = 20
i = 1796 (of 10192), d = 10, its = 52
i = 1797 (of 10192), d = 2.1854, its = 4
i = 1798 (of 10192), d = 10, its = 41
i = 1799 (of 10192), d = 10, its = 55
i = 1800 (of 10192), d = 10, its = 51
i = 1801 (of 10192), d = 10, its = 50
i = 1802 (of 10192), d = 2.31487, its = 4
i = 1803 (of 10192), d = 10, its = 49
i = 1804 (of 10192), d = 1.58904, its = 7
i = 1805 (of 10192), d = 10, its = 54
i = 1806 (of 10192), d = 10, its = 39
i = 1807 (of 10192), d = 2.21127, its = 4
i = 1808 (of 10192), d = 1.21968, its = 18
i = 1809 (of 10192), d = 2.12483, its = 4
i = 1810 (of 10192), d = 10, its = 53
i = 1811 (of 10192), d = 0.855948, its = 20
i = 1812 (of 10192), d = 10, its = 54
i = 1813 (of 10192), d = 2.36218, its = 4
i = 1814 (of 10192), d = 0.860607, its = 16
i = 1815 (of 10192), d = 0.664629, its = 18
i = 1816 (of 10192), d = 10, its = 49
i = 1817 (of 10192), d = 10, its = 55
i = 1818 (of 10192), d = 10, its = 53
i = 1819 (of 10192), d = 2.51757, its = 5
i = 1820 (of 10192), d = 10, its = 40
i = 1821 (of 10192), d = 10, its = 37
i = 1822 (of 10192), d = 10, its = 55
i = 1823 (of 10192), d = 10, its = 50
i = 1824 (of 10192), d = 10, its = 56
i = 1825 (of 10192), d = 10, its = 55
i = 1826 (of 10192), d = 10, its = 40
i = 1827 (of 10192), d = 10, its = 52
i = 1828 (of 10192), d = 10, its = 50
i = 1829 (of 10192), d = 10, its = 53
i = 1830 (of 10192), d = 10, its = 50
i = 1831 (of 10192), d = 10, its = 55
i = 1832 (of 10192), d = 1.49782, its = 7
i = 1833 (of 10192), d = 10, its = 54
i = 1834 (of 10192), d = 10, its = 52
i = 1835 (of 10192), d = 1.90028, its = 4
i = 1836 (of 10192), d = 2.52064, its = 5
i = 1837 (of 10192), d = 2.16038, its = 4
i = 1838 (of 10192), d = 2.62531, its = 6
i = 1839 (of 10192), d = 10, its = 37
i = 1840 (of 10192), d = 10, its = 54
i = 1841 (of 10192), d = 2.42866, its = 5
i = 1842 (of 10192), d = 10, its = 51
i = 1843 (of 10192), d = 10, its = 55
i = 1844 (of 10192), d = 1.6411, its = 6
i = 1845 (of 10192), d = 10, its = 54
i = 1846 (of 10192), d = 10, its = 50
i = 1847 (of 10192), d = 10, its = 37
i = 1848 (of 10192), d = 10, its = 39
i = 1849 (of 10192), d = 2.92014, its = 7
i = 1850 (of 10192), d = 1.0052, its = 19
i = 1851 (of 10192), d = 1.656, its = 6
i = 1852 (of 10192), d = 1.50335, its = 8
i = 1853 (of 10192), d = 10, its = 58
i = 1854 (of 10192), d = 0.970587, its = 19
i = 1855 (of 10192), d = 10, its = 51
i = 1856 (of 10192), d = 10, its = 37
i = 1857 (of 10192), d = 10, its = 54
i = 1858 (of 10192), d = 3.4213, its = 8
i = 1859 (of 10192), d = 7.42877, its = 8
i = 1860 (of 10192), d = 10, its = 55
i = 1861 (of 10192), d = 10, its = 53
i = 1862 (of 10192), d = 10, its = 59
i = 1863 (of 10192), d = 1.58729, its = 6
i = 1864 (of 10192), d = 10, its = 56
i = 1865 (of 10192), d = 10, its = 41
i = 1866 (of 10192), d = 10, its = 49
i = 1867 (of 10192), d = 10, its = 53
i = 1868 (of 10192), d = 10, its = 42
i = 1869 (of 10192), d = 2.77384, its = 5
i = 1870 (of 10192), d = 10, its = 37
i = 1871 (of 10192), d = 1.74642, its = 5
i = 1872 (of 10192), d = 10, its = 37
i = 1873 (of 10192), d = 10, its = 50
i = 1874 (of 10192), d = 10, its = 37
i = 1875 (of 10192), d = 10, its = 57
i = 1876 (of 10192), d = 10, its = 54
i = 1877 (of 10192), d = 2.62692, its = 6
i = 1878 (of 10192), d = 10, its = 47
i = 1879 (of 10192), d = 10, its = 57
i = 1880 (of 10192), d = 10, its = 37
i = 1881 (of 10192), d = 5.14123, its = 8
i = 1882 (of 10192), d = 10, its = 58
i = 1883 (of 10192), d = 2.89674, its = 5
i = 1884 (of 10192), d = 1.90077, its = 4
i = 1885 (of 10192), d = 10, its = 39
i = 1886 (of 10192), d = 10, its = 54
i = 1887 (of 10192), d = 10, its = 37
i = 1888 (of 10192), d = 10, its = 40
i = 1889 (of 10192), d = 3.09323, its = 6
i = 1890 (of 10192), d = 2.08522, its = 4
i = 1891 (of 10192), d = 10, its = 40
i = 1892 (of 10192), d = 10, its = 52
i = 1893 (of 10192), d = 1.4485, its = 6
i = 1894 (of 10192), d = 10, its = 56
i = 1895 (of 10192), d = 10, its = 40
i = 1896 (of 10192), d = 10, its = 51
i = 1897 (of 10192), d = 2.47286, its = 5
i = 1898 (of 10192), d = 1.9853, its = 3
i = 1899 (of 10192), d = 2.12813, its = 4
i = 1900 (of 10192), d = 10, its = 51
i = 1901 (of 10192), d = 10, its = 56
i = 1902 (of 10192), d = 10, its = 37
i = 1903 (of 10192), d = 1.96715, its = 3
i = 1904 (of 10192), d = 10, its = 50
i = 1905 (of 10192), d = 10, its = 53
i = 1906 (of 10192), d = 10, its = 40
i = 1907 (of 10192), d = 10, its = 53
i = 1908 (of 10192), d = 10, its = 51
i = 1909 (of 10192), d = 10, its = 53
i = 1910 (of 10192), d = 10, its = 56
i = 1911 (of 10192), d = 10, its = 52
i = 1912 (of 10192), d = 10, its = 53
i = 1913 (of 10192), d = 10, its = 37
i = 1914 (of 10192), d = 1.50861, its = 6
i = 1915 (of 10192), d = 10, its = 50
i = 1916 (of 10192), d = 10, its = 55
i = 1917 (of 10192), d = 1.88299, its = 4
i = 1918 (of 10192), d = 2.05079, its = 3
i = 1919 (of 10192), d = 6.64424, its = 8
i = 1920 (of 10192), d = 10, its = 56
i = 1921 (of 10192), d = 2.04133, its = 3
i = 1922 (of 10192), d = 1.67794, its = 5
i = 1923 (of 10192), d = 10, its = 51
i = 1924 (of 10192), d = 10, its = 52
i = 1925 (of 10192), d = 10, its = 43
i = 1926 (of 10192), d = 10, its = 55
i = 1927 (of 10192), d = 10, its = 39
i = 1928 (of 10192), d = 1.28914, its = 29
i = 1929 (of 10192), d = 1.89802, its = 4
i = 1930 (of 10192), d = 2.36577, its = 5
i = 1931 (of 10192), d = 3.44621, its = 6
i = 1932 (of 10192), d = 10, its = 40
i = 1933 (of 10192), d = 10, its = 53
i = 1934 (of 10192), d = 2.12265, its = 4
i = 1935 (of 10192), d = 10, its = 57
i = 1936 (of 10192), d = 10, its = 56
i = 1937 (of 10192), d = 10, its = 37
i = 1938 (of 10192), d = 2.72018, its = 6
i = 1939 (of 10192), d = 10, its = 37
i = 1940 (of 10192), d = 10, its = 37
i = 1941 (of 10192), d = 10, its = 54
i = 1942 (of 10192), d = 10, its = 52
i = 1943 (of 10192), d = 10, its = 54
i = 1944 (of 10192), d = 2.2541, its = 5
i = 1945 (of 10192), d = 10, its = 38
i = 1946 (of 10192), d = 6.10383, its = 8
i = 1947 (of 10192), d = 3.84086, its = 7
i = 1948 (of 10192), d = 10, its = 57
i = 1949 (of 10192), d = 10, its = 45
i = 1950 (of 10192), d = 1.50925, its = 6
i = 1951 (of 10192), d = 10, its = 37
i = 1952 (of 10192), d = 10, its = 37
i = 1953 (of 10192), d = 1.75913, its = 6
i = 1954 (of 10192), d = 2.09833, its = 4
i = 1955 (of 10192), d = 10, its = 55
i = 1956 (of 10192), d = 10, its = 51
i = 1957 (of 10192), d = 10, its = 59
i = 1958 (of 10192), d = 10, its = 37
i = 1959 (of 10192), d = 10, its = 42
i = 1960 (of 10192), d = 10, its = 57
i = 1961 (of 10192), d = 10, its = 40
i = 1962 (of 10192), d = 3.01757, its = 6
i = 1963 (of 10192), d = 3.25798, its = 6
i = 1964 (of 10192), d = 10, its = 39
i = 1965 (of 10192), d = 10, its = 59
i = 1966 (of 10192), d = 10, its = 45
i = 1967 (of 10192), d = 10, its = 37
i = 1968 (of 10192), d = 10, its = 49
i = 1969 (of 10192), d = 10, its = 41
i = 1970 (of 10192), d = 10, its = 49
i = 1971 (of 10192), d = 10, its = 51
i = 1972 (of 10192), d = 10, its = 56
i = 1973 (of 10192), d = 10, its = 50
i = 1974 (of 10192), d = 6.53313, its = 8
i = 1975 (of 10192), d = 10, its = 54
i = 1976 (of 10192), d = 3.33234, its = 6
i = 1977 (of 10192), d = 3.27138, its = 6
i = 1978 (of 10192), d = 10, its = 47
i = 1979 (of 10192), d = 1.89019, its = 4
i = 1980 (of 10192), d = 10, its = 54
i = 1981 (of 10192), d = 10, its = 54
i = 1982 (of 10192), d = 10, its = 52
i = 1983 (of 10192), d = 10, its = 41
i = 1984 (of 10192), d = 1.47346, its = 6
i = 1985 (of 10192), d = 10, its = 50
i = 1986 (of 10192), d = 3.42476, its = 7
i = 1987 (of 10192), d = 0.995649, its = 23
i = 1988 (of 10192), d = 10, its = 52
i = 1989 (of 10192), d = 10, its = 53
i = 1990 (of 10192), d = 1.79988, its = 4
i = 1991 (of 10192), d = 10, its = 37
i = 1992 (of 10192), d = 2.69106, its = 6
i = 1993 (of 10192), d = 10, its = 52
i = 1994 (of 10192), d = 1.88785, its = 4
i = 1995 (of 10192), d = 10, its = 37
i = 1996 (of 10192), d = 8.41231, its = 22
i = 1997 (of 10192), d = 10, its = 53
i = 1998 (of 10192), d = 10, its = 53
i = 1999 (of 10192), d = 10, its = 53
i = 2000 (of 10192), d = 10, its = 38
i = 2001 (of 10192), d = 10, its = 41
i = 2002 (of 10192), d = 10, its = 50
i = 2003 (of 10192), d = 10, its = 41
i = 2004 (of 10192), d = 4.17851, its = 8
i = 2005 (of 10192), d = 1.59419, its = 7
i = 2006 (of 10192), d = 0.626822, its = 22
i = 2007 (of 10192), d = 10, its = 37
i = 2008 (of 10192), d = 1.66878, its = 5
i = 2009 (of 10192), d = 10, its = 49
i = 2010 (of 10192), d = 10, its = 51
i = 2011 (of 10192), d = 10, its = 57
i = 2012 (of 10192), d = 2.09262, its = 4
i = 2013 (of 10192), d = 10, its = 53
i = 2014 (of 10192), d = 2.1407, its = 4
i = 2015 (of 10192), d = 10, its = 54
i = 2016 (of 10192), d = 10, its = 52
i = 2017 (of 10192), d = 10, its = 52
i = 2018 (of 10192), d = 10, its = 56
i = 2019 (of 10192), d = 3.70256, its = 8
i = 2020 (of 10192), d = 0.860752, its = 25
i = 2021 (of 10192), d = 10, its = 51
i = 2022 (of 10192), d = 2.76187, its = 5
i = 2023 (of 10192), d = 10, its = 54
i = 2024 (of 10192), d = 1.16083, its = 22
i = 2025 (of 10192), d = 8.41806, its = 9
i = 2026 (of 10192), d = 10, its = 52
i = 2027 (of 10192), d = 1.91702, its = 4
i = 2028 (of 10192), d = 10, its = 43
i = 2029 (of 10192), d = 1.55752, its = 6
i = 2030 (of 10192), d = 3.03678, its = 5
i = 2031 (of 10192), d = 10, its = 43
i = 2032 (of 10192), d = 2.04645, its = 3
i = 2033 (of 10192), d = 10, its = 39
i = 2034 (of 10192), d = 10, its = 55
i = 2035 (of 10192), d = 1.39868, its = 7
i = 2036 (of 10192), d = 10, its = 51
i = 2037 (of 10192), d = 10, its = 53
i = 2038 (of 10192), d = 10, its = 52
i = 2039 (of 10192), d = 0.822008, its = 18
i = 2040 (of 10192), d = 10, its = 52
i = 2041 (of 10192), d = 10, its = 40
i = 2042 (of 10192), d = 10, its = 53
i = 2043 (of 10192), d = 1.01043, its = 27
i = 2044 (of 10192), d = 10, its = 53
i = 2045 (of 10192), d = 10, its = 54
i = 2046 (of 10192), d = 0.632984, its = 18
i = 2047 (of 10192), d = 10, its = 41
i = 2048 (of 10192), d = 1.24264, its = 19
i = 2049 (of 10192), d = 1.37063, its = 6
i = 2050 (of 10192), d = 0.781886, its = 22
i = 2051 (of 10192), d = 10, its = 41
i = 2052 (of 10192), d = 10, its = 40
i = 2053 (of 10192), d = 10, its = 38
i = 2054 (of 10192), d = 10, its = 37
i = 2055 (of 10192), d = 10, its = 55
i = 2056 (of 10192), d = 10, its = 43
i = 2057 (of 10192), d = 10, its = 53
i = 2058 (of 10192), d = 4.32397, its = 7
i = 2059 (of 10192), d = 10, its = 53
i = 2060 (of 10192), d = 10, its = 55
i = 2061 (of 10192), d = 8.93842, its = 17
i = 2062 (of 10192), d = 10, its = 37
i = 2063 (of 10192), d = 10, its = 53
i = 2064 (of 10192), d = 10, its = 56
i = 2065 (of 10192), d = 10, its = 50
i = 2066 (of 10192), d = 10, its = 41
i = 2067 (of 10192), d = 2.69014, its = 6
i = 2068 (of 10192), d = 1.65156, its = 6
i = 2069 (of 10192), d = 1.89962, its = 4
i = 2070 (of 10192), d = 5.56335, its = 8
i = 2071 (of 10192), d = 10, its = 52
i = 2072 (of 10192), d = 10, its = 57
i = 2073 (of 10192), d = 10, its = 37
i = 2074 (of 10192), d = 1.03654, its = 20
i = 2075 (of 10192), d = 10, its = 41
i = 2076 (of 10192), d = 10, its = 54
i = 2077 (of 10192), d = 1.78668, its = 5
i = 2078 (of 10192), d = 10, its = 58
i = 2079 (of 10192), d = 10, its = 56
i = 2080 (of 10192), d = 10, its = 51
i = 2081 (of 10192), d = 10, its = 37
i = 2082 (of 10192), d = 1.49121, its = 6
i = 2083 (of 10192), d = 10, its = 52
i = 2084 (of 10192), d = 10, its = 37
i = 2085 (of 10192), d = 1.77176, its = 5
i = 2086 (of 10192), d = 10, its = 49
i = 2087 (of 10192), d = 10, its = 40
i = 2088 (of 10192), d = 10, its = 37
i = 2089 (of 10192), d = 10, its = 37
i = 2090 (of 10192), d = 1.85177, its = 4
i = 2091 (of 10192), d = 10, its = 46
i = 2092 (of 10192), d = 1.70729, its = 5
i = 2093 (of 10192), d = 10, its = 52
i = 2094 (of 10192), d = 2.79931, its = 6
i = 2095 (of 10192), d = 10, its = 41
i = 2096 (of 10192), d = 10, its = 54
i = 2097 (of 10192), d = 10, its = 48
i = 2098 (of 10192), d = 1.00827, its = 19
i = 2099 (of 10192), d = 2.57037, its = 6
i = 2100 (of 10192), d = 10, its = 38
i = 2101 (of 10192), d = 2.24638, its = 5
i = 2102 (of 10192), d = 2.15851, its = 4
i = 2103 (of 10192), d = 10, its = 37
i = 2104 (of 10192), d = 10, its = 54
i = 2105 (of 10192), d = 3.67883, its = 8
i = 2106 (of 10192), d = 2.37348, its = 5
i = 2107 (of 10192), d = 10, its = 39
i = 2108 (of 10192), d = 1.41783, its = 6
i = 2109 (of 10192), d = 10, its = 44
i = 2110 (of 10192), d = 10, its = 37
i = 2111 (of 10192), d = 10, its = 43
i = 2112 (of 10192), d = 10, its = 52
i = 2113 (of 10192), d = 1.47556, its = 6
i = 2114 (of 10192), d = 0.573889, its = 17
i = 2115 (of 10192), d = 10, its = 39
i = 2116 (of 10192), d = 10, its = 56
i = 2117 (of 10192), d = 10, its = 37
i = 2118 (of 10192), d = 10, its = 48
i = 2119 (of 10192), d = 10, its = 39
i = 2120 (of 10192), d = 10, its = 50
i = 2121 (of 10192), d = 10, its = 51
i = 2122 (of 10192), d = 1.39801, its = 7
i = 2123 (of 10192), d = 10, its = 37
i = 2124 (of 10192), d = 10, its = 56
i = 2125 (of 10192), d = 10, its = 53
i = 2126 (of 10192), d = 1.80996, its = 4
i = 2127 (of 10192), d = 10, its = 53
i = 2128 (of 10192), d = 1.00223, its = 18
i = 2129 (of 10192), d = 10, its = 49
i = 2130 (of 10192), d = 1.83892, its = 4
i = 2131 (of 10192), d = 1.59215, its = 6
i = 2132 (of 10192), d = 2.20325, its = 4
i = 2133 (of 10192), d = 10, its = 58
i = 2134 (of 10192), d = 10, its = 37
i = 2135 (of 10192), d = 1.57465, its = 6
i = 2136 (of 10192), d = 3.8861, its = 7
i = 2137 (of 10192), d = 7.97091, its = 9
i = 2138 (of 10192), d = 10, its = 55
i = 2139 (of 10192), d = 10, its = 52
i = 2140 (of 10192), d = 10, its = 39
i = 2141 (of 10192), d = 10, its = 53
i = 2142 (of 10192), d = 10, its = 52
i = 2143 (of 10192), d = 10, its = 57
i = 2144 (of 10192), d = 10, its = 55
i = 2145 (of 10192), d = 10, its = 37
i = 2146 (of 10192), d = 10, its = 39
i = 2147 (of 10192), d = 10, its = 49
i = 2148 (of 10192), d = 10, its = 48
i = 2149 (of 10192), d = 10, its = 37
i = 2150 (of 10192), d = 1.60487, its = 6
i = 2151 (of 10192), d = 10, its = 50
i = 2152 (of 10192), d = 10, its = 55
i = 2153 (of 10192), d = 10, its = 50
i = 2154 (of 10192), d = 2.39475, its = 5
i = 2155 (of 10192), d = 1.80776, its = 4
i = 2156 (of 10192), d = 10, its = 54
i = 2157 (of 10192), d = 10, its = 39
i = 2158 (of 10192), d = 10, its = 53
i = 2159 (of 10192), d = 10, its = 49
i = 2160 (of 10192), d = 10, its = 56
i = 2161 (of 10192), d = 10, its = 55
i = 2162 (of 10192), d = 10, its = 54
i = 2163 (of 10192), d = 0.632984, its = 18
i = 2164 (of 10192), d = 10, its = 52
i = 2165 (of 10192), d = 10, its = 54
i = 2166 (of 10192), d = 2.19143, its = 4
i = 2167 (of 10192), d = 2.24905, its = 5
i = 2168 (of 10192), d = 10, its = 50
i = 2169 (of 10192), d = 10, its = 52
i = 2170 (of 10192), d = 10, its = 37
i = 2171 (of 10192), d = 10, its = 37
i = 2172 (of 10192), d = 10, its = 51
i = 2173 (of 10192), d = 2.01957, its = 3
i = 2174 (of 10192), d = 1.00827, its = 19
i = 2175 (of 10192), d = 10, its = 51
i = 2176 (of 10192), d = 1.59277, its = 6
i = 2177 (of 10192), d = 10, its = 50
i = 2178 (of 10192), d = 10, its = 54
i = 2179 (of 10192), d = 3.07898, its = 6
i = 2180 (of 10192), d = 10, its = 37
i = 2181 (of 10192), d = 10, its = 39
i = 2182 (of 10192), d = 2.42197, its = 5
i = 2183 (of 10192), d = 1.43474, its = 6
i = 2184 (of 10192), d = 10, its = 54
i = 2185 (of 10192), d = 10, its = 52
i = 2186 (of 10192), d = 10, its = 49
i = 2187 (of 10192), d = 10, its = 53
i = 2188 (of 10192), d = 10, its = 54
i = 2189 (of 10192), d = 10, its = 48
i = 2190 (of 10192), d = 10, its = 40
i = 2191 (of 10192), d = 0.701895, its = 27
i = 2192 (of 10192), d = 1.50058, its = 7
i = 2193 (of 10192), d = 10, its = 37
i = 2194 (of 10192), d = 10, its = 52
i = 2195 (of 10192), d = 10, its = 37
i = 2196 (of 10192), d = 10, its = 58
i = 2197 (of 10192), d = 10, its = 40
i = 2198 (of 10192), d = 10, its = 53
i = 2199 (of 10192), d = 10, its = 56
i = 2200 (of 10192), d = 2.41208, its = 5
i = 2201 (of 10192), d = 1.65504, its = 6
i = 2202 (of 10192), d = 2.93488, its = 6
i = 2203 (of 10192), d = 10, its = 37
i = 2204 (of 10192), d = 2.11676, its = 4
i = 2205 (of 10192), d = 10, its = 51
i = 2206 (of 10192), d = 10, its = 50
i = 2207 (of 10192), d = 2.35086, its = 5
i = 2208 (of 10192), d = 10, its = 51
i = 2209 (of 10192), d = 2.05933, its = 3
i = 2210 (of 10192), d = 10, its = 52
i = 2211 (of 10192), d = 10, its = 41
i = 2212 (of 10192), d = 10, its = 42
i = 2213 (of 10192), d = 2.17858, its = 4
i = 2214 (of 10192), d = 0.707839, its = 18
i = 2215 (of 10192), d = 10, its = 41
i = 2216 (of 10192), d = 10, its = 39
i = 2217 (of 10192), d = 10, its = 48
i = 2218 (of 10192), d = 10, its = 37
i = 2219 (of 10192), d = 10, its = 53
i = 2220 (of 10192), d = 10, its = 55
i = 2221 (of 10192), d = 10, its = 51
i = 2222 (of 10192), d = 10, its = 52
i = 2223 (of 10192), d = 10, its = 51
i = 2224 (of 10192), d = 10, its = 41
i = 2225 (of 10192), d = 10, its = 56
i = 2226 (of 10192), d = 10, its = 50
i = 2227 (of 10192), d = 10, its = 42
i = 2228 (of 10192), d = 10, its = 44
i = 2229 (of 10192), d = 10, its = 57
i = 2230 (of 10192), d = 10, its = 50
i = 2231 (of 10192), d = 10, its = 50
i = 2232 (of 10192), d = 10, its = 53
i = 2233 (of 10192), d = 10, its = 52
i = 2234 (of 10192), d = 10, its = 55
i = 2235 (of 10192), d = 10, its = 54
i = 2236 (of 10192), d = 10, its = 37
i = 2237 (of 10192), d = 10, its = 53
i = 2238 (of 10192), d = 10, its = 55
i = 2239 (of 10192), d = 1.52214, its = 6
i = 2240 (of 10192), d = 10, its = 42
i = 2241 (of 10192), d = 1.7132, its = 5
i = 2242 (of 10192), d = 10, its = 46
i = 2243 (of 10192), d = 10, its = 37
i = 2244 (of 10192), d = 10, its = 39
i = 2245 (of 10192), d = 10, its = 54
i = 2246 (of 10192), d = 10, its = 50
i = 2247 (of 10192), d = 10, its = 54
i = 2248 (of 10192), d = 10, its = 55
i = 2249 (of 10192), d = 10, its = 52
i = 2250 (of 10192), d = 1.96891, its = 3
i = 2251 (of 10192), d = 10, its = 51
i = 2252 (of 10192), d = 3.33529, its = 6
i = 2253 (of 10192), d = 10, its = 52
i = 2254 (of 10192), d = 1.7287, its = 5
i = 2255 (of 10192), d = 1.54061, its = 7
i = 2256 (of 10192), d = 10, its = 37
i = 2257 (of 10192), d = 1.08605, its = 8
i = 2258 (of 10192), d = 10, its = 56
i = 2259 (of 10192), d = 1.07707, its = 17
i = 2260 (of 10192), d = 7.96864, its = 9
i = 2261 (of 10192), d = 10, its = 37
i = 2262 (of 10192), d = 10, its = 55
i = 2263 (of 10192), d = 10, its = 52
i = 2264 (of 10192), d = 10, its = 51
i = 2265 (of 10192), d = 10, its = 54
i = 2266 (of 10192), d = 10, its = 53
i = 2267 (of 10192), d = 2.82053, its = 6
i = 2268 (of 10192), d = 10, its = 37
i = 2269 (of 10192), d = 10, its = 40
i = 2270 (of 10192), d = 10, its = 54
i = 2271 (of 10192), d = 10, its = 37
i = 2272 (of 10192), d = 10, its = 54
i = 2273 (of 10192), d = 10, its = 55
i = 2274 (of 10192), d = 10, its = 52
i = 2275 (of 10192), d = 2.66942, its = 5
i = 2276 (of 10192), d = 10, its = 51
i = 2277 (of 10192), d = 10, its = 37
i = 2278 (of 10192), d = 1.86309, its = 4
i = 2279 (of 10192), d = 2.05677, its = 3
i = 2280 (of 10192), d = 2.24204, its = 4
i = 2281 (of 10192), d = 10, its = 56
i = 2282 (of 10192), d = 1.48196, its = 7
i = 2283 (of 10192), d = 10, its = 39
i = 2284 (of 10192), d = 10, its = 37
i = 2285 (of 10192), d = 10, its = 54
i = 2286 (of 10192), d = 3.03288, its = 6
i = 2287 (of 10192), d = 10, its = 50
i = 2288 (of 10192), d = 1.53525, its = 7
i = 2289 (of 10192), d = 10, its = 43
i = 2290 (of 10192), d = 10, its = 53
i = 2291 (of 10192), d = 10, its = 40
i = 2292 (of 10192), d = 10, its = 54
i = 2293 (of 10192), d = 1.39033, its = 7
i = 2294 (of 10192), d = 1.34107, its = 20
i = 2295 (of 10192), d = 10, its = 38
i = 2296 (of 10192), d = 10, its = 52
i = 2297 (of 10192), d = 10, its = 59
i = 2298 (of 10192), d = 10, its = 53
i = 2299 (of 10192), d = 2.05709, its = 3
i = 2300 (of 10192), d = 2.78431, its = 5
i = 2301 (of 10192), d = 1.46133, its = 8
i = 2302 (of 10192), d = 10, its = 52
i = 2303 (of 10192), d = 10, its = 53
i = 2304 (of 10192), d = 10, its = 41
i = 2305 (of 10192), d = 10, its = 41
i = 2306 (of 10192), d = 10, its = 53
i = 2307 (of 10192), d = 10, its = 55
i = 2308 (of 10192), d = 1.04422, its = 19
i = 2309 (of 10192), d = 10, its = 53
i = 2310 (of 10192), d = 10, its = 37
i = 2311 (of 10192), d = 10, its = 39
i = 2312 (of 10192), d = 10, its = 53
i = 2313 (of 10192), d = 10, its = 54
i = 2314 (of 10192), d = 10, its = 40
i = 2315 (of 10192), d = 10, its = 52
i = 2316 (of 10192), d = 10, its = 38
i = 2317 (of 10192), d = 10, its = 38
i = 2318 (of 10192), d = 3.15859, its = 6
i = 2319 (of 10192), d = 10, its = 40
i = 2320 (of 10192), d = 10, its = 40
i = 2321 (of 10192), d = 1.88273, its = 5
i = 2322 (of 10192), d = 1.45206, its = 8
i = 2323 (of 10192), d = 10, its = 54
i = 2324 (of 10192), d = 10, its = 54
i = 2325 (of 10192), d = 10, its = 40
i = 2326 (of 10192), d = 10, its = 46
i = 2327 (of 10192), d = 4.68719, its = 7
i = 2328 (of 10192), d = 10, its = 38
i = 2329 (of 10192), d = 10, its = 37
i = 2330 (of 10192), d = 0.918951, its = 21
i = 2331 (of 10192), d = 10, its = 53
i = 2332 (of 10192), d = 10, its = 37
i = 2333 (of 10192), d = 10, its = 51
i = 2334 (of 10192), d = 10, its = 53
i = 2335 (of 10192), d = 2.51354, its = 6
i = 2336 (of 10192), d = 10, its = 49
i = 2337 (of 10192), d = 2.06985, its = 4
i = 2338 (of 10192), d = 10, its = 48
i = 2339 (of 10192), d = 10, its = 55
i = 2340 (of 10192), d = 10, its = 41
i = 2341 (of 10192), d = 10, its = 50
i = 2342 (of 10192), d = 10, its = 54
i = 2343 (of 10192), d = 2.0742, its = 4
i = 2344 (of 10192), d = 10, its = 56
i = 2345 (of 10192), d = 10, its = 52
i = 2346 (of 10192), d = 10, its = 41
i = 2347 (of 10192), d = 10, its = 37
i = 2348 (of 10192), d = 1.70376, its = 5
i = 2349 (of 10192), d = 2.04358, its = 3
i = 2350 (of 10192), d = 10, its = 55
i = 2351 (of 10192), d = 10, its = 53
i = 2352 (of 10192), d = 2.78958, its = 5
i = 2353 (of 10192), d = 10, its = 50
i = 2354 (of 10192), d = 10, its = 53
i = 2355 (of 10192), d = 5.99613, its = 8
i = 2356 (of 10192), d = 10, its = 37
i = 2357 (of 10192), d = 10, its = 40
i = 2358 (of 10192), d = 10, its = 37
i = 2359 (of 10192), d = 2.07948, its = 4
i = 2360 (of 10192), d = 2.29607, its = 5
i = 2361 (of 10192), d = 10, its = 52
i = 2362 (of 10192), d = 10, its = 51
i = 2363 (of 10192), d = 10, its = 37
i = 2364 (of 10192), d = 10, its = 53
i = 2365 (of 10192), d = 2.06589, its = 3
i = 2366 (of 10192), d = 10, its = 52
i = 2367 (of 10192), d = 2.21494, its = 4
i = 2368 (of 10192), d = 10, its = 50
i = 2369 (of 10192), d = 10, its = 51
i = 2370 (of 10192), d = 10, its = 53
i = 2371 (of 10192), d = 3.01864, its = 6
i = 2372 (of 10192), d = 10, its = 54
i = 2373 (of 10192), d = 5.12902, its = 8
i = 2374 (of 10192), d = 10, its = 50
i = 2375 (of 10192), d = 10, its = 52
i = 2376 (of 10192), d = 10, its = 53
i = 2377 (of 10192), d = 10, its = 57
i = 2378 (of 10192), d = 8.35383, its = 20
i = 2379 (of 10192), d = 2.84771, its = 5
i = 2380 (of 10192), d = 10, its = 53
i = 2381 (of 10192), d = 10, its = 40
i = 2382 (of 10192), d = 10, its = 50
i = 2383 (of 10192), d = 0.836122, its = 26
i = 2384 (of 10192), d = 0.81733, its = 19
i = 2385 (of 10192), d = 1.43474, its = 6
i = 2386 (of 10192), d = 10, its = 49
i = 2387 (of 10192), d = 10, its = 54
i = 2388 (of 10192), d = 10, its = 55
i = 2389 (of 10192), d = 2.01293, its = 3
i = 2390 (of 10192), d = 2.26184, its = 5
i = 2391 (of 10192), d = 10, its = 50
i = 2392 (of 10192), d = 10, its = 52
i = 2393 (of 10192), d = 10, its = 52
i = 2394 (of 10192), d = 10, its = 56
i = 2395 (of 10192), d = 10, its = 37
i = 2396 (of 10192), d = 10, its = 49
i = 2397 (of 10192), d = 10, its = 40
i = 2398 (of 10192), d = 1.68598, its = 5
i = 2399 (of 10192), d = 2.50079, its = 6
i = 2400 (of 10192), d = 10, its = 52
i = 2401 (of 10192), d = 10, its = 37
i = 2402 (of 10192), d = 10, its = 37
i = 2403 (of 10192), d = 10, its = 54
i = 2404 (of 10192), d = 10, its = 52
i = 2405 (of 10192), d = 0.945239, its = 22
i = 2406 (of 10192), d = 10, its = 50
i = 2407 (of 10192), d = 7.36572, its = 8
i = 2408 (of 10192), d = 10, its = 57
i = 2409 (of 10192), d = 10, its = 55
i = 2410 (of 10192), d = 10, its = 54
i = 2411 (of 10192), d = 10, its = 56
i = 2412 (of 10192), d = 10, its = 58
i = 2413 (of 10192), d = 10, its = 37
i = 2414 (of 10192), d = 3.41478, its = 6
i = 2415 (of 10192), d = 2.49196, its = 5
i = 2416 (of 10192), d = 10, its = 52
i = 2417 (of 10192), d = 10, its = 49
i = 2418 (of 10192), d = 10, its = 37
i = 2419 (of 10192), d = 10, its = 57
i = 2420 (of 10192), d = 10, its = 46
i = 2421 (of 10192), d = 10, its = 52
i = 2422 (of 10192), d = 3.40622, its = 7
i = 2423 (of 10192), d = 10, its = 50
i = 2424 (of 10192), d = 2.27712, its = 4
i = 2425 (of 10192), d = 10, its = 53
i = 2426 (of 10192), d = 2.0987, its = 4
i = 2427 (of 10192), d = 2.95679, its = 7
i = 2428 (of 10192), d = 6.12558, its = 9
i = 2429 (of 10192), d = 10, its = 56
i = 2430 (of 10192), d = 0.983417, its = 25
i = 2431 (of 10192), d = 10, its = 51
i = 2432 (of 10192), d = 10, its = 57
i = 2433 (of 10192), d = 10, its = 49
i = 2434 (of 10192), d = 10, its = 37
i = 2435 (of 10192), d = 10, its = 41
i = 2436 (of 10192), d = 1.56742, its = 6
i = 2437 (of 10192), d = 2.5341, its = 5
i = 2438 (of 10192), d = 10, its = 37
i = 2439 (of 10192), d = 10, its = 50
i = 2440 (of 10192), d = 10, its = 52
i = 2441 (of 10192), d = 8.54982, its = 8
i = 2442 (of 10192), d = 1.62883, its = 6
i = 2443 (of 10192), d = 1.98969, its = 3
i = 2444 (of 10192), d = 10, its = 62
i = 2445 (of 10192), d = 7.6039, its = 8
i = 2446 (of 10192), d = 10, its = 51
i = 2447 (of 10192), d = 10, its = 48
i = 2448 (of 10192), d = 6.76505, its = 8
i = 2449 (of 10192), d = 2.11001, its = 4
i = 2450 (of 10192), d = 10, its = 51
i = 2451 (of 10192), d = 0.782189, its = 18
i = 2452 (of 10192), d = 10, its = 52
i = 2453 (of 10192), d = 10, its = 56
i = 2454 (of 10192), d = 1.59442, its = 6
i = 2455 (of 10192), d = 10, its = 54
i = 2456 (of 10192), d = 0.954053, its = 27
i = 2457 (of 10192), d = 2.52807, its = 5
i = 2458 (of 10192), d = 10, its = 54
i = 2459 (of 10192), d = 10, its = 50
i = 2460 (of 10192), d = 1.20368, its = 18
i = 2461 (of 10192), d = 10, its = 37
i = 2462 (of 10192), d = 10, its = 51
i = 2463 (of 10192), d = 10, its = 50
i = 2464 (of 10192), d = 10, its = 53
i = 2465 (of 10192), d = 3.2562, its = 7
i = 2466 (of 10192), d = 10, its = 50
i = 2467 (of 10192), d = 10, its = 50
i = 2468 (of 10192), d = 10, its = 53
i = 2469 (of 10192), d = 10, its = 55
i = 2470 (of 10192), d = 2.58156, its = 5
i = 2471 (of 10192), d = 1.85683, its = 4
i = 2472 (of 10192), d = 1.81158, its = 4
i = 2473 (of 10192), d = 10, its = 51
i = 2474 (of 10192), d = 0.918407, its = 20
i = 2475 (of 10192), d = 10, its = 50
i = 2476 (of 10192), d = 10, its = 37
i = 2477 (of 10192), d = 10, its = 41
i = 2478 (of 10192), d = 10, its = 57
i = 2479 (of 10192), d = 10, its = 49
i = 2480 (of 10192), d = 10, its = 53
i = 2481 (of 10192), d = 1.58471, its = 7
i = 2482 (of 10192), d = 10, its = 52
i = 2483 (of 10192), d = 2.72481, its = 6
i = 2484 (of 10192), d = 10, its = 37
i = 2485 (of 10192), d = 10, its = 51
i = 2486 (of 10192), d = 10, its = 53
i = 2487 (of 10192), d = 1.58311, its = 7
i = 2488 (of 10192), d = 10, its = 57
i = 2489 (of 10192), d = 10, its = 37
i = 2490 (of 10192), d = 10, its = 54
i = 2491 (of 10192), d = 10, its = 39
i = 2492 (of 10192), d = 3.31813, its = 7
i = 2493 (of 10192), d = 10, its = 37
i = 2494 (of 10192), d = 10, its = 37
i = 2495 (of 10192), d = 1.65439, its = 5
i = 2496 (of 10192), d = 10, its = 42
i = 2497 (of 10192), d = 2.10976, its = 4
i = 2498 (of 10192), d = 10, its = 53
i = 2499 (of 10192), d = 1.72324, its = 5
i = 2500 (of 10192), d = 10, its = 54
i = 2501 (of 10192), d = 10, its = 37
i = 2502 (of 10192), d = 10, its = 49
i = 2503 (of 10192), d = 10, its = 53
i = 2504 (of 10192), d = 10, its = 40
i = 2505 (of 10192), d = 3.69829, its = 7
i = 2506 (of 10192), d = 10, its = 54
i = 2507 (of 10192), d = 1.0658, its = 20
i = 2508 (of 10192), d = 2.56784, its = 6
i = 2509 (of 10192), d = 10, its = 55
i = 2510 (of 10192), d = 10, its = 53
i = 2511 (of 10192), d = 2.26444, its = 4
i = 2512 (of 10192), d = 3.6177, its = 6
i = 2513 (of 10192), d = 10, its = 56
i = 2514 (of 10192), d = 10, its = 56
i = 2515 (of 10192), d = 2.61497, its = 6
i = 2516 (of 10192), d = 10, its = 54
i = 2517 (of 10192), d = 10, its = 52
i = 2518 (of 10192), d = 10, its = 53
i = 2519 (of 10192), d = 10, its = 55
i = 2520 (of 10192), d = 10, its = 37
i = 2521 (of 10192), d = 10, its = 53
i = 2522 (of 10192), d = 10, its = 56
i = 2523 (of 10192), d = 2.66689, its = 6
i = 2524 (of 10192), d = 10, its = 41
i = 2525 (of 10192), d = 1.5571, its = 7
i = 2526 (of 10192), d = 10, its = 40
i = 2527 (of 10192), d = 10, its = 52
i = 2528 (of 10192), d = 10, its = 51
i = 2529 (of 10192), d = 10, its = 52
i = 2530 (of 10192), d = 10, its = 49
i = 2531 (of 10192), d = 10, its = 50
i = 2532 (of 10192), d = 10, its = 48
i = 2533 (of 10192), d = 10, its = 37
i = 2534 (of 10192), d = 10, its = 37
i = 2535 (of 10192), d = 10, its = 55
i = 2536 (of 10192), d = 10, its = 54
i = 2537 (of 10192), d = 10, its = 37
i = 2538 (of 10192), d = 1.79645, its = 5
i = 2539 (of 10192), d = 10, its = 57
i = 2540 (of 10192), d = 10, its = 50
i = 2541 (of 10192), d = 2.88015, its = 6
i = 2542 (of 10192), d = 10, its = 52
i = 2543 (of 10192), d = 10, its = 55
i = 2544 (of 10192), d = 10, its = 55
i = 2545 (of 10192), d = 10, its = 39
i = 2546 (of 10192), d = 2.70984, its = 6
i = 2547 (of 10192), d = 1.95782, its = 3
i = 2548 (of 10192), d = 10, its = 51
i = 2549 (of 10192), d = 10, its = 54
i = 2550 (of 10192), d = 10, its = 53
i = 2551 (of 10192), d = 10, its = 39
i = 2552 (of 10192), d = 4.89108, its = 9
i = 2553 (of 10192), d = 10, its = 56
i = 2554 (of 10192), d = 10, its = 54
i = 2555 (of 10192), d = 3.02231, its = 6
i = 2556 (of 10192), d = 10, its = 40
i = 2557 (of 10192), d = 10, its = 55
i = 2558 (of 10192), d = 10, its = 51
i = 2559 (of 10192), d = 10, its = 52
i = 2560 (of 10192), d = 10, its = 51
i = 2561 (of 10192), d = 10, its = 37
i = 2562 (of 10192), d = 1.83335, its = 5
i = 2563 (of 10192), d = 10, its = 53
i = 2564 (of 10192), d = 10, its = 53
i = 2565 (of 10192), d = 10, its = 52
i = 2566 (of 10192), d = 1.37255, its = 7
i = 2567 (of 10192), d = 10, its = 39
i = 2568 (of 10192), d = 10, its = 37
i = 2569 (of 10192), d = 10, its = 52
i = 2570 (of 10192), d = 1.30026, its = 7
i = 2571 (of 10192), d = 10, its = 39
i = 2572 (of 10192), d = 10, its = 38
i = 2573 (of 10192), d = 10, its = 39
i = 2574 (of 10192), d = 10, its = 37
i = 2575 (of 10192), d = 10, its = 52
i = 2576 (of 10192), d = 10, its = 53
i = 2577 (of 10192), d = 10, its = 41
i = 2578 (of 10192), d = 10, its = 50
i = 2579 (of 10192), d = 3.34519, its = 6
i = 2580 (of 10192), d = 2.29785, its = 5
i = 2581 (of 10192), d = 1.89277, its = 4
i = 2582 (of 10192), d = 10, its = 55
i = 2583 (of 10192), d = 4.55593, its = 7
i = 2584 (of 10192), d = 10, its = 50
i = 2585 (of 10192), d = 10, its = 37
i = 2586 (of 10192), d = 10, its = 52
i = 2587 (of 10192), d = 10, its = 50
i = 2588 (of 10192), d = 10, its = 51
i = 2589 (of 10192), d = 10, its = 39
i = 2590 (of 10192), d = 10, its = 52
i = 2591 (of 10192), d = 10, its = 54
i = 2592 (of 10192), d = 10, its = 51
i = 2593 (of 10192), d = 2.24174, its = 4
i = 2594 (of 10192), d = 10, its = 37
i = 2595 (of 10192), d = 10, its = 55
i = 2596 (of 10192), d = 10, its = 58
i = 2597 (of 10192), d = 1.81287, its = 4
i = 2598 (of 10192), d = 2.227, its = 4
i = 2599 (of 10192), d = 10, its = 41
i = 2600 (of 10192), d = 10, its = 50
i = 2601 (of 10192), d = 10, its = 53
i = 2602 (of 10192), d = 2.44, its = 5
i = 2603 (of 10192), d = 2.29303, its = 4
i = 2604 (of 10192), d = 10, its = 55
i = 2605 (of 10192), d = 10, its = 37
i = 2606 (of 10192), d = 2.55343, its = 5
i = 2607 (of 10192), d = 10, its = 40
i = 2608 (of 10192), d = 1.70288, its = 5
i = 2609 (of 10192), d = 10, its = 52
i = 2610 (of 10192), d = 10, its = 51
i = 2611 (of 10192), d = 10, its = 52
i = 2612 (of 10192), d = 2.37655, its = 6
i = 2613 (of 10192), d = 10, its = 37
i = 2614 (of 10192), d = 10, its = 37
i = 2615 (of 10192), d = 2.81646, its = 5
i = 2616 (of 10192), d = 10, its = 40
i = 2617 (of 10192), d = 10, its = 37
i = 2618 (of 10192), d = 4.32549, its = 6
i = 2619 (of 10192), d = 10, its = 55
i = 2620 (of 10192), d = 10, its = 55
i = 2621 (of 10192), d = 1.59784, its = 6
i = 2622 (of 10192), d = 10, its = 43
i = 2623 (of 10192), d = 2.45531, its = 5
i = 2624 (of 10192), d = 10, its = 50
i = 2625 (of 10192), d = 10, its = 50
i = 2626 (of 10192), d = 1.50902, its = 8
i = 2627 (of 10192), d = 10, its = 55
i = 2628 (of 10192), d = 2.39362, its = 5
i = 2629 (of 10192), d = 9.64217, its = 33
i = 2630 (of 10192), d = 2.02009, its = 3
i = 2631 (of 10192), d = 10, its = 40
i = 2632 (of 10192), d = 10, its = 57
i = 2633 (of 10192), d = 3.59279, its = 7
i = 2634 (of 10192), d = 10, its = 52
i = 2635 (of 10192), d = 10, its = 55
i = 2636 (of 10192), d = 0.947043, its = 19
i = 2637 (of 10192), d = 10, its = 37
i = 2638 (of 10192), d = 10, its = 37
i = 2639 (of 10192), d = 10, its = 54
i = 2640 (of 10192), d = 1.50936, its = 6
i = 2641 (of 10192), d = 2.70907, its = 5
i = 2642 (of 10192), d = 10, its = 49
i = 2643 (of 10192), d = 10, its = 52
i = 2644 (of 10192), d = 2.41808, its = 5
i = 2645 (of 10192), d = 10, its = 37
i = 2646 (of 10192), d = 2.95683, its = 6
i = 2647 (of 10192), d = 10, its = 51
i = 2648 (of 10192), d = 3.95402, its = 7
i = 2649 (of 10192), d = 10, its = 51
i = 2650 (of 10192), d = 1.58027, its = 7
i = 2651 (of 10192), d = 10, its = 51
i = 2652 (of 10192), d = 10, its = 56
i = 2653 (of 10192), d = 2.26613, its = 4
i = 2654 (of 10192), d = 10, its = 52
i = 2655 (of 10192), d = 10, its = 38
i = 2656 (of 10192), d = 10, its = 37
i = 2657 (of 10192), d = 10, its = 48
i = 2658 (of 10192), d = 2.5155, its = 5
i = 2659 (of 10192), d = 10, its = 54
i = 2660 (of 10192), d = 0.831321, its = 17
i = 2661 (of 10192), d = 10, its = 51
i = 2662 (of 10192), d = 1.49914, its = 6
i = 2663 (of 10192), d = 10, its = 56
i = 2664 (of 10192), d = 10, its = 52
i = 2665 (of 10192), d = 10, its = 54
i = 2666 (of 10192), d = 10, its = 51
i = 2667 (of 10192), d = 10, its = 44
i = 2668 (of 10192), d = 10, its = 55
i = 2669 (of 10192), d = 10, its = 50
i = 2670 (of 10192), d = 2.6558, its = 6
i = 2671 (of 10192), d = 2.28568, its = 5
i = 2672 (of 10192), d = 1.43264, its = 6
i = 2673 (of 10192), d = 10, its = 55
i = 2674 (of 10192), d = 10, its = 40
i = 2675 (of 10192), d = 4.28697, its = 8
i = 2676 (of 10192), d = 1.82511, its = 4
i = 2677 (of 10192), d = 10, its = 50
i = 2678 (of 10192), d = 1.05493, its = 18
i = 2679 (of 10192), d = 10, its = 52
i = 2680 (of 10192), d = 10, its = 55
i = 2681 (of 10192), d = 10, its = 51
i = 2682 (of 10192), d = 10, its = 41
i = 2683 (of 10192), d = 10, its = 39
i = 2684 (of 10192), d = 10, its = 37
i = 2685 (of 10192), d = 10, its = 56
i = 2686 (of 10192), d = 10, its = 37
i = 2687 (of 10192), d = 10, its = 52
i = 2688 (of 10192), d = 10, its = 40
i = 2689 (of 10192), d = 10, its = 56
i = 2690 (of 10192), d = 10, its = 52
i = 2691 (of 10192), d = 1.71964, its = 5
i = 2692 (of 10192), d = 10, its = 55
i = 2693 (of 10192), d = 10, its = 47
i = 2694 (of 10192), d = 0.640742, its = 18
i = 2695 (of 10192), d = 1.29458, its = 18
i = 2696 (of 10192), d = 10, its = 54
i = 2697 (of 10192), d = 2.81707, its = 6
i = 2698 (of 10192), d = 0.743509, its = 17
i = 2699 (of 10192), d = 10, its = 56
i = 2700 (of 10192), d = 10, its = 50
i = 2701 (of 10192), d = 10, its = 52
i = 2702 (of 10192), d = 10, its = 54
i = 2703 (of 10192), d = 10, its = 49
i = 2704 (of 10192), d = 10, its = 40
i = 2705 (of 10192), d = 10, its = 37
i = 2706 (of 10192), d = 10, its = 37
i = 2707 (of 10192), d = 10, its = 37
i = 2708 (of 10192), d = 2.59683, its = 5
i = 2709 (of 10192), d = 1.91945, its = 4
i = 2710 (of 10192), d = 10, its = 54
i = 2711 (of 10192), d = 10, its = 52
i = 2712 (of 10192), d = 10, its = 55
i = 2713 (of 10192), d = 10, its = 56
i = 2714 (of 10192), d = 10, its = 37
i = 2715 (of 10192), d = 10, its = 54
i = 2716 (of 10192), d = 10, its = 54
i = 2717 (of 10192), d = 2.50282, its = 5
i = 2718 (of 10192), d = 10, its = 40
i = 2719 (of 10192), d = 0.59501, its = 17
i = 2720 (of 10192), d = 10, its = 51
i = 2721 (of 10192), d = 10, its = 50
i = 2722 (of 10192), d = 10, its = 49
i = 2723 (of 10192), d = 10, its = 57
i = 2724 (of 10192), d = 1.6982, its = 5
i = 2725 (of 10192), d = 10, its = 58
i = 2726 (of 10192), d = 10, its = 53
i = 2727 (of 10192), d = 10, its = 55
i = 2728 (of 10192), d = 2.28329, its = 4
i = 2729 (of 10192), d = 10, its = 37
i = 2730 (of 10192), d = 1.55261, its = 7
i = 2731 (of 10192), d = 0.866498, its = 18
i = 2732 (of 10192), d = 10, its = 37
i = 2733 (of 10192), d = 10, its = 53
i = 2734 (of 10192), d = 10, its = 38
i = 2735 (of 10192), d = 10, its = 39
i = 2736 (of 10192), d = 1.28944, its = 5
i = 2737 (of 10192), d = 10, its = 39
i = 2738 (of 10192), d = 10, its = 37
i = 2739 (of 10192), d = 2.66148, its = 6
i = 2740 (of 10192), d = 10, its = 41
i = 2741 (of 10192), d = 10, its = 53
i = 2742 (of 10192), d = 10, its = 51
i = 2743 (of 10192), d = 2.60523, its = 5
i = 2744 (of 10192), d = 10, its = 59
i = 2745 (of 10192), d = 2.32658, its = 4
i = 2746 (of 10192), d = 10, its = 51
i = 2747 (of 10192), d = 10, its = 40
i = 2748 (of 10192), d = 1.35479, its = 6
i = 2749 (of 10192), d = 10, its = 53
i = 2750 (of 10192), d = 1.96045, its = 3
i = 2751 (of 10192), d = 10, its = 53
i = 2752 (of 10192), d = 10, its = 40
i = 2753 (of 10192), d = 1.67652, its = 5
i = 2754 (of 10192), d = 10, its = 53
i = 2755 (of 10192), d = 10, its = 40
i = 2756 (of 10192), d = 2.23428, its = 4
i = 2757 (of 10192), d = 10, its = 51
i = 2758 (of 10192), d = 10, its = 55
i = 2759 (of 10192), d = 1.53852, its = 7
i = 2760 (of 10192), d = 10, its = 41
i = 2761 (of 10192), d = 2.49293, its = 5
i = 2762 (of 10192), d = 10, its = 50
i = 2763 (of 10192), d = 10, its = 54
i = 2764 (of 10192), d = 3.13966, its = 6
i = 2765 (of 10192), d = 3.93122, its = 8
i = 2766 (of 10192), d = 10, its = 37
i = 2767 (of 10192), d = 10, its = 37
i = 2768 (of 10192), d = 1.5834, its = 6
i = 2769 (of 10192), d = 1.83047, its = 5
i = 2770 (of 10192), d = 10, its = 54
i = 2771 (of 10192), d = 10, its = 37
i = 2772 (of 10192), d = 0.699986, its = 25
i = 2773 (of 10192), d = 10, its = 43
i = 2774 (of 10192), d = 10, its = 54
i = 2775 (of 10192), d = 10, its = 37
i = 2776 (of 10192), d = 10, its = 53
i = 2777 (of 10192), d = 10, its = 55
i = 2778 (of 10192), d = 3.34986, its = 7
i = 2779 (of 10192), d = 10, its = 37
i = 2780 (of 10192), d = 2.24688, its = 4
i = 2781 (of 10192), d = 1.42218, its = 7
i = 2782 (of 10192), d = 10, its = 52
i = 2783 (of 10192), d = 1.02527, its = 17
i = 2784 (of 10192), d = 10, its = 39
i = 2785 (of 10192), d = 10, its = 51
i = 2786 (of 10192), d = 5.99094, its = 8
i = 2787 (of 10192), d = 10, its = 49
i = 2788 (of 10192), d = 2.31066, its = 4
i = 2789 (of 10192), d = 10, its = 37
i = 2790 (of 10192), d = 10, its = 37
i = 2791 (of 10192), d = 10, its = 37
i = 2792 (of 10192), d = 3.00315, its = 6
i = 2793 (of 10192), d = 10, its = 37
i = 2794 (of 10192), d = 2.95667, its = 6
i = 2795 (of 10192), d = 3.83669, its = 7
i = 2796 (of 10192), d = 10, its = 37
i = 2797 (of 10192), d = 1.90568, its = 4
i = 2798 (of 10192), d = 10, its = 37
i = 2799 (of 10192), d = 10, its = 43
i = 2800 (of 10192), d = 10, its = 50
i = 2801 (of 10192), d = 10, its = 41
i = 2802 (of 10192), d = 1.80067, its = 5
i = 2803 (of 10192), d = 1.61846, its = 5
i = 2804 (of 10192), d = 10, its = 41
i = 2805 (of 10192), d = 10, its = 37
i = 2806 (of 10192), d = 1.19532, its = 21
i = 2807 (of 10192), d = 10, its = 52
i = 2808 (of 10192), d = 10, its = 39
i = 2809 (of 10192), d = 1.06734, its = 18
i = 2810 (of 10192), d = 10, its = 53
i = 2811 (of 10192), d = 1.81993, its = 4
i = 2812 (of 10192), d = 10, its = 54
i = 2813 (of 10192), d = 10, its = 40
i = 2814 (of 10192), d = 10, its = 37
i = 2815 (of 10192), d = 10, its = 52
i = 2816 (of 10192), d = 10, its = 54
i = 2817 (of 10192), d = 10, its = 52
i = 2818 (of 10192), d = 10, its = 52
i = 2819 (of 10192), d = 1.60925, its = 6
i = 2820 (of 10192), d = 10, its = 53
i = 2821 (of 10192), d = 3.28927, its = 7
i = 2822 (of 10192), d = 10, its = 52
i = 2823 (of 10192), d = 10, its = 37
i = 2824 (of 10192), d = 1.34273, its = 8
i = 2825 (of 10192), d = 10, its = 49
i = 2826 (of 10192), d = 10, its = 50
i = 2827 (of 10192), d = 1.80197, its = 5
i = 2828 (of 10192), d = 10, its = 50
i = 2829 (of 10192), d = 10, its = 37
i = 2830 (of 10192), d = 10, its = 51
i = 2831 (of 10192), d = 3.05812, its = 6
i = 2832 (of 10192), d = 10, its = 37
i = 2833 (of 10192), d = 10, its = 41
i = 2834 (of 10192), d = 10, its = 39
i = 2835 (of 10192), d = 10, its = 41
i = 2836 (of 10192), d = 2.26709, its = 5
i = 2837 (of 10192), d = 3.99248, its = 7
i = 2838 (of 10192), d = 10, its = 37
i = 2839 (of 10192), d = 2.9978, its = 6
i = 2840 (of 10192), d = 2.45531, its = 5
i = 2841 (of 10192), d = 10, its = 53
i = 2842 (of 10192), d = 0.678416, its = 22
i = 2843 (of 10192), d = 10, its = 52
i = 2844 (of 10192), d = 10, its = 52
i = 2845 (of 10192), d = 10, its = 51
i = 2846 (of 10192), d = 10, its = 57
i = 2847 (of 10192), d = 10, its = 51
i = 2848 (of 10192), d = 10, its = 51
i = 2849 (of 10192), d = 2.76045, its = 5
i = 2850 (of 10192), d = 10, its = 58
i = 2851 (of 10192), d = 1.89668, its = 4
i = 2852 (of 10192), d = 1.16474, its = 24
i = 2853 (of 10192), d = 1.84399, its = 4
i = 2854 (of 10192), d = 10, its = 52
i = 2855 (of 10192), d = 10, its = 55
i = 2856 (of 10192), d = 10, its = 51
i = 2857 (of 10192), d = 10, its = 37
i = 2858 (of 10192), d = 10, its = 48
i = 2859 (of 10192), d = 9.64328, its = 8
i = 2860 (of 10192), d = 4.06675, its = 7
i = 2861 (of 10192), d = 2.42381, its = 5
i = 2862 (of 10192), d = 10, its = 40
i = 2863 (of 10192), d = 1.93166, its = 4
i = 2864 (of 10192), d = 3.11631, its = 6
i = 2865 (of 10192), d = 10, its = 49
i = 2866 (of 10192), d = 10, its = 37
i = 2867 (of 10192), d = 1.01461, its = 16
i = 2868 (of 10192), d = 0.818583, its = 17
i = 2869 (of 10192), d = 10, its = 53
i = 2870 (of 10192), d = 0.793555, its = 18
i = 2871 (of 10192), d = 10, its = 44
i = 2872 (of 10192), d = 10, its = 51
i = 2873 (of 10192), d = 2.20873, its = 4
i = 2874 (of 10192), d = 10, its = 40
i = 2875 (of 10192), d = 10, its = 52
i = 2876 (of 10192), d = 1.83651, its = 6
i = 2877 (of 10192), d = 10, its = 49
i = 2878 (of 10192), d = 10, its = 52
i = 2879 (of 10192), d = 10, its = 51
i = 2880 (of 10192), d = 10, its = 39
i = 2881 (of 10192), d = 10, its = 58
i = 2882 (of 10192), d = 10, its = 51
i = 2883 (of 10192), d = 10, its = 44
i = 2884 (of 10192), d = 10, its = 37
i = 2885 (of 10192), d = 10, its = 40
i = 2886 (of 10192), d = 0.933848, its = 25
i = 2887 (of 10192), d = 10, its = 54
i = 2888 (of 10192), d = 10, its = 51
i = 2889 (of 10192), d = 10, its = 37
i = 2890 (of 10192), d = 0.871782, its = 17
i = 2891 (of 10192), d = 10, its = 37
i = 2892 (of 10192), d = 10, its = 37
i = 2893 (of 10192), d = 1.73155, its = 5
i = 2894 (of 10192), d = 2.0718, its = 3
i = 2895 (of 10192), d = 10, its = 54
i = 2896 (of 10192), d = 10, its = 50
i = 2897 (of 10192), d = 10, its = 40
i = 2898 (of 10192), d = 3.99251, its = 7
i = 2899 (of 10192), d = 10, its = 51
i = 2900 (of 10192), d = 1.75748, its = 5
i = 2901 (of 10192), d = 1.04537, its = 17
i = 2902 (of 10192), d = 10, its = 53
i = 2903 (of 10192), d = 10, its = 48
i = 2904 (of 10192), d = 10, its = 52
i = 2905 (of 10192), d = 1.44469, its = 7
i = 2906 (of 10192), d = 2.4686, its = 5
i = 2907 (of 10192), d = 10, its = 44
i = 2908 (of 10192), d = 2.05057, its = 3
i = 2909 (of 10192), d = 0.734646, its = 18
i = 2910 (of 10192), d = 10, its = 41
i = 2911 (of 10192), d = 10, its = 37
i = 2912 (of 10192), d = 10, its = 52
i = 2913 (of 10192), d = 10, its = 56
i = 2914 (of 10192), d = 10, its = 41
i = 2915 (of 10192), d = 10, its = 54
i = 2916 (of 10192), d = 10, its = 51
i = 2917 (of 10192), d = 10, its = 37
i = 2918 (of 10192), d = 10, its = 56
i = 2919 (of 10192), d = 2.06075, its = 4
i = 2920 (of 10192), d = 1.28482, its = 7
i = 2921 (of 10192), d = 10, its = 54
i = 2922 (of 10192), d = 1.43569, its = 6
i = 2923 (of 10192), d = 10, its = 38
i = 2924 (of 10192), d = 2.14063, its = 4
i = 2925 (of 10192), d = 10, its = 56
i = 2926 (of 10192), d = 10, its = 53
i = 2927 (of 10192), d = 10, its = 53
i = 2928 (of 10192), d = 10, its = 56
i = 2929 (of 10192), d = 10, its = 40
i = 2930 (of 10192), d = 10, its = 40
i = 2931 (of 10192), d = 3.48632, its = 6
i = 2932 (of 10192), d = 2.06848, its = 4
i = 2933 (of 10192), d = 10, its = 50
i = 2934 (of 10192), d = 10, its = 49
i = 2935 (of 10192), d = 2.09038, its = 4
i = 2936 (of 10192), d = 3.10895, its = 6
i = 2937 (of 10192), d = 10, its = 55
i = 2938 (of 10192), d = 9.01629, its = 7
i = 2939 (of 10192), d = 10, its = 41
i = 2940 (of 10192), d = 10, its = 37
i = 2941 (of 10192), d = 10, its = 55
i = 2942 (of 10192), d = 10, its = 53
i = 2943 (of 10192), d = 10, its = 51
i = 2944 (of 10192), d = 10, its = 37
i = 2945 (of 10192), d = 10, its = 54
i = 2946 (of 10192), d = 10, its = 55
i = 2947 (of 10192), d = 2.04729, its = 3
i = 2948 (of 10192), d = 10, its = 50
i = 2949 (of 10192), d = 10, its = 50
i = 2950 (of 10192), d = 10, its = 52
i = 2951 (of 10192), d = 1.92623, its = 4
i = 2952 (of 10192), d = 10, its = 53
i = 2953 (of 10192), d = 10, its = 51
i = 2954 (of 10192), d = 10, its = 51
i = 2955 (of 10192), d = 10, its = 55
i = 2956 (of 10192), d = 1.85647, its = 4
i = 2957 (of 10192), d = 10, its = 37
i = 2958 (of 10192), d = 3.07172, its = 6
i = 2959 (of 10192), d = 10, its = 52
i = 2960 (of 10192), d = 10, its = 53
i = 2961 (of 10192), d = 10, its = 50
i = 2962 (of 10192), d = 2.05448, its = 3
i = 2963 (of 10192), d = 2.56589, its = 5
i = 2964 (of 10192), d = 10, its = 51
i = 2965 (of 10192), d = 2.09766, its = 4
i = 2966 (of 10192), d = 1.35375, its = 7
i = 2967 (of 10192), d = 10, its = 47
i = 2968 (of 10192), d = 1.13296, its = 20
i = 2969 (of 10192), d = 9.3548, its = 9
i = 2970 (of 10192), d = 1.65899, its = 5
i = 2971 (of 10192), d = 10, its = 51
i = 2972 (of 10192), d = 2.15625, its = 4
i = 2973 (of 10192), d = 10, its = 54
i = 2974 (of 10192), d = 4.59272, its = 8
i = 2975 (of 10192), d = 2.2904, its = 5
i = 2976 (of 10192), d = 2.89128, its = 7
i = 2977 (of 10192), d = 1.54571, its = 7
i = 2978 (of 10192), d = 10, its = 55
i = 2979 (of 10192), d = 10, its = 40
i = 2980 (of 10192), d = 10, its = 37
i = 2981 (of 10192), d = 10, its = 53
i = 2982 (of 10192), d = 4.55167, its = 7
i = 2983 (of 10192), d = 1.24997, its = 7
i = 2984 (of 10192), d = 10, its = 52
i = 2985 (of 10192), d = 10, its = 52
i = 2986 (of 10192), d = 1.37506, its = 6
i = 2987 (of 10192), d = 10, its = 53
i = 2988 (of 10192), d = 10, its = 54
i = 2989 (of 10192), d = 1.65021, its = 6
i = 2990 (of 10192), d = 10, its = 37
i = 2991 (of 10192), d = 0.519982, its = 18
i = 2992 (of 10192), d = 10, its = 51
i = 2993 (of 10192), d = 10, its = 54
i = 2994 (of 10192), d = 10, its = 49
i = 2995 (of 10192), d = 10, its = 40
i = 2996 (of 10192), d = 10, its = 56
i = 2997 (of 10192), d = 10, its = 44
i = 2998 (of 10192), d = 10, its = 52
i = 2999 (of 10192), d = 10, its = 50
i = 3000 (of 10192), d = 5.01326, its = 7
i = 3001 (of 10192), d = 8.99366, its = 8
i = 3002 (of 10192), d = 10, its = 41
i = 3003 (of 10192), d = 10, its = 50
i = 3004 (of 10192), d = 10, its = 53
i = 3005 (of 10192), d = 3.63928, its = 7
i = 3006 (of 10192), d = 1.81515, its = 5
i = 3007 (of 10192), d = 10, its = 50
i = 3008 (of 10192), d = 10, its = 50
i = 3009 (of 10192), d = 10, its = 39
i = 3010 (of 10192), d = 1.48147, its = 7
i = 3011 (of 10192), d = 10, its = 53
i = 3012 (of 10192), d = 10, its = 51
i = 3013 (of 10192), d = 10, its = 50
i = 3014 (of 10192), d = 10, its = 53
i = 3015 (of 10192), d = 0.833972, its = 17
i = 3016 (of 10192), d = 10, its = 51
i = 3017 (of 10192), d = 1.14496, its = 22
i = 3018 (of 10192), d = 10, its = 38
i = 3019 (of 10192), d = 10, its = 53
i = 3020 (of 10192), d = 10, its = 55
i = 3021 (of 10192), d = 1.72137, its = 6
i = 3022 (of 10192), d = 10, its = 52
i = 3023 (of 10192), d = 10, its = 56
i = 3024 (of 10192), d = 10, its = 40
i = 3025 (of 10192), d = 10, its = 54
i = 3026 (of 10192), d = 10, its = 37
i = 3027 (of 10192), d = 10, its = 50
i = 3028 (of 10192), d = 10, its = 55
i = 3029 (of 10192), d = 2.94896, its = 6
i = 3030 (of 10192), d = 10, its = 54
i = 3031 (of 10192), d = 10, its = 56
i = 3032 (of 10192), d = 10, its = 37
i = 3033 (of 10192), d = 10, its = 52
i = 3034 (of 10192), d = 2.32161, its = 4
i = 3035 (of 10192), d = 10, its = 52
i = 3036 (of 10192), d = 10, its = 50
i = 3037 (of 10192), d = 10, its = 41
i = 3038 (of 10192), d = 0.985662, its = 20
i = 3039 (of 10192), d = 1.70154, its = 7
i = 3040 (of 10192), d = 2.02654, its = 3
i = 3041 (of 10192), d = 10, its = 54
i = 3042 (of 10192), d = 1.8085, its = 7
i = 3043 (of 10192), d = 3.76726, its = 7
i = 3044 (of 10192), d = 10, its = 53
i = 3045 (of 10192), d = 1.62091, its = 6
i = 3046 (of 10192), d = 1.7675, its = 5
i = 3047 (of 10192), d = 10, its = 53
i = 3048 (of 10192), d = 10, its = 51
i = 3049 (of 10192), d = 10, its = 50
i = 3050 (of 10192), d = 1.9467, its = 4
i = 3051 (of 10192), d = 10, its = 37
i = 3052 (of 10192), d = 10, its = 46
i = 3053 (of 10192), d = 0.429545, its = 19
i = 3054 (of 10192), d = 2.37792, its = 4
i = 3055 (of 10192), d = 10, its = 51
i = 3056 (of 10192), d = 10, its = 37
i = 3057 (of 10192), d = 1.40225, its = 6
i = 3058 (of 10192), d = 10, its = 49
i = 3059 (of 10192), d = 10, its = 51
i = 3060 (of 10192), d = 10, its = 39
i = 3061 (of 10192), d = 10, its = 51
i = 3062 (of 10192), d = 2.44469, its = 5
i = 3063 (of 10192), d = 5.9879, its = 9
i = 3064 (of 10192), d = 2.46229, its = 5
i = 3065 (of 10192), d = 10, its = 50
i = 3066 (of 10192), d = 10, its = 53
i = 3067 (of 10192), d = 10, its = 40
i = 3068 (of 10192), d = 10, its = 37
i = 3069 (of 10192), d = 1.65784, its = 6
i = 3070 (of 10192), d = 1.64821, its = 6
i = 3071 (of 10192), d = 10, its = 37
i = 3072 (of 10192), d = 2.2835, its = 5
i = 3073 (of 10192), d = 10, its = 53
i = 3074 (of 10192), d = 10, its = 37
i = 3075 (of 10192), d = 10, its = 51
i = 3076 (of 10192), d = 2.06964, its = 4
i = 3077 (of 10192), d = 1.08835, its = 19
i = 3078 (of 10192), d = 10, its = 51
i = 3079 (of 10192), d = 0.928726, its = 20
i = 3080 (of 10192), d = 10, its = 37
i = 3081 (of 10192), d = 1.80774, its = 5
i = 3082 (of 10192), d = 10, its = 52
i = 3083 (of 10192), d = 1.89025, its = 4
i = 3084 (of 10192), d = 10, its = 40
i = 3085 (of 10192), d = 0.988443, its = 19
i = 3086 (of 10192), d = 2.36409, its = 5
i = 3087 (of 10192), d = 10, its = 48
i = 3088 (of 10192), d = 10, its = 55
i = 3089 (of 10192), d = 10, its = 37
i = 3090 (of 10192), d = 10, its = 52
i = 3091 (of 10192), d = 10, its = 52
i = 3092 (of 10192), d = 10, its = 56
i = 3093 (of 10192), d = 10, its = 52
i = 3094 (of 10192), d = 10, its = 50
i = 3095 (of 10192), d = 1.80184, its = 5
i = 3096 (of 10192), d = 10, its = 51
i = 3097 (of 10192), d = 10, its = 55
i = 3098 (of 10192), d = 10, its = 51
i = 3099 (of 10192), d = 10, its = 44
i = 3100 (of 10192), d = 1.26272, its = 6
i = 3101 (of 10192), d = 1.56547, its = 6
i = 3102 (of 10192), d = 10, its = 39
i = 3103 (of 10192), d = 10, its = 51
i = 3104 (of 10192), d = 10, its = 55
i = 3105 (of 10192), d = 10, its = 37
i = 3106 (of 10192), d = 10, its = 53
i = 3107 (of 10192), d = 10, its = 55
i = 3108 (of 10192), d = 10, its = 48
i = 3109 (of 10192), d = 10, its = 55
i = 3110 (of 10192), d = 2.23962, its = 4
i = 3111 (of 10192), d = 10, its = 41
i = 3112 (of 10192), d = 10, its = 39
i = 3113 (of 10192), d = 10, its = 57
i = 3114 (of 10192), d = 2.05928, its = 4
i = 3115 (of 10192), d = 10, its = 47
i = 3116 (of 10192), d = 10, its = 54
i = 3117 (of 10192), d = 2.08413, its = 4
i = 3118 (of 10192), d = 1.82514, its = 4
i = 3119 (of 10192), d = 10, its = 45
i = 3120 (of 10192), d = 4.99455, its = 8
i = 3121 (of 10192), d = 4.35036, its = 8
i = 3122 (of 10192), d = 2.01522, its = 3
i = 3123 (of 10192), d = 10, its = 37
i = 3124 (of 10192), d = 10, its = 54
i = 3125 (of 10192), d = 10, its = 50
i = 3126 (of 10192), d = 10, its = 37
i = 3127 (of 10192), d = 0.661188, its = 16
i = 3128 (of 10192), d = 10, its = 52
i = 3129 (of 10192), d = 5.20993, its = 7
i = 3130 (of 10192), d = 1.03828, its = 20
i = 3131 (of 10192), d = 1.63912, its = 7
i = 3132 (of 10192), d = 1.33932, its = 7
i = 3133 (of 10192), d = 10, its = 49
i = 3134 (of 10192), d = 10, its = 55
i = 3135 (of 10192), d = 10, its = 52
i = 3136 (of 10192), d = 10, its = 52
i = 3137 (of 10192), d = 10, its = 53
i = 3138 (of 10192), d = 10, its = 37
i = 3139 (of 10192), d = 10, its = 58
i = 3140 (of 10192), d = 4.2339, its = 7
i = 3141 (of 10192), d = 10, its = 53
i = 3142 (of 10192), d = 10, its = 54
i = 3143 (of 10192), d = 10, its = 41
i = 3144 (of 10192), d = 2.51065, its = 5
i = 3145 (of 10192), d = 10, its = 52
i = 3146 (of 10192), d = 10, its = 53
i = 3147 (of 10192), d = 1.76843, its = 5
i = 3148 (of 10192), d = 10, its = 37
i = 3149 (of 10192), d = 2.73475, its = 5
i = 3150 (of 10192), d = 2.79544, its = 5
i = 3151 (of 10192), d = 10, its = 52
i = 3152 (of 10192), d = 10, its = 37
i = 3153 (of 10192), d = 2.34335, its = 5
i = 3154 (of 10192), d = 0.701145, its = 18
i = 3155 (of 10192), d = 10, its = 41
i = 3156 (of 10192), d = 10, its = 37
i = 3157 (of 10192), d = 10, its = 52
i = 3158 (of 10192), d = 10, its = 53
i = 3159 (of 10192), d = 0.910089, its = 18
i = 3160 (of 10192), d = 10, its = 41
i = 3161 (of 10192), d = 10, its = 37
i = 3162 (of 10192), d = 10, its = 50
i = 3163 (of 10192), d = 10, its = 40
i = 3164 (of 10192), d = 3.06138, its = 6
i = 3165 (of 10192), d = 10, its = 52
i = 3166 (of 10192), d = 10, its = 58
i = 3167 (of 10192), d = 10, its = 43
i = 3168 (of 10192), d = 10, its = 37
i = 3169 (of 10192), d = 10, its = 53
i = 3170 (of 10192), d = 10, its = 50
i = 3171 (of 10192), d = 0.894892, its = 22
i = 3172 (of 10192), d = 10, its = 52
i = 3173 (of 10192), d = 10, its = 52
i = 3174 (of 10192), d = 10, its = 55
i = 3175 (of 10192), d = 2.76045, its = 5
i = 3176 (of 10192), d = 10, its = 54
i = 3177 (of 10192), d = 10, its = 51
i = 3178 (of 10192), d = 10, its = 55
i = 3179 (of 10192), d = 10, its = 37
i = 3180 (of 10192), d = 1.95466, its = 3
i = 3181 (of 10192), d = 3.08565, its = 7
i = 3182 (of 10192), d = 1.39246, its = 7
i = 3183 (of 10192), d = 10, its = 54
i = 3184 (of 10192), d = 10, its = 52
i = 3185 (of 10192), d = 10, its = 52
i = 3186 (of 10192), d = 10, its = 51
i = 3187 (of 10192), d = 3.52889, its = 6
i = 3188 (of 10192), d = 10, its = 39
i = 3189 (of 10192), d = 10, its = 51
i = 3190 (of 10192), d = 10, its = 52
i = 3191 (of 10192), d = 10, its = 37
i = 3192 (of 10192), d = 10, its = 55
i = 3193 (of 10192), d = 10, its = 53
i = 3194 (of 10192), d = 10, its = 52
i = 3195 (of 10192), d = 10, its = 55
i = 3196 (of 10192), d = 10, its = 40
i = 3197 (of 10192), d = 3.72102, its = 6
i = 3198 (of 10192), d = 10, its = 51
i = 3199 (of 10192), d = 2.16959, its = 4
i = 3200 (of 10192), d = 10, its = 55
i = 3201 (of 10192), d = 10, its = 38
i = 3202 (of 10192), d = 10, its = 53
i = 3203 (of 10192), d = 2.01344, its = 3
i = 3204 (of 10192), d = 1.35147, its = 6
i = 3205 (of 10192), d = 10, its = 39
i = 3206 (of 10192), d = 10, its = 50
i = 3207 (of 10192), d = 10, its = 37
i = 3208 (of 10192), d = 10, its = 55
i = 3209 (of 10192), d = 10, its = 54
i = 3210 (of 10192), d = 10, its = 40
i = 3211 (of 10192), d = 1.9531, its = 3
i = 3212 (of 10192), d = 2.07411, its = 4
i = 3213 (of 10192), d = 10, its = 40
i = 3214 (of 10192), d = 2.46625, its = 5
i = 3215 (of 10192), d = 10, its = 37
i = 3216 (of 10192), d = 10, its = 52
i = 3217 (of 10192), d = 3.41019, its = 6
i = 3218 (of 10192), d = 10, its = 52
i = 3219 (of 10192), d = 10, its = 56
i = 3220 (of 10192), d = 10, its = 56
i = 3221 (of 10192), d = 10, its = 53
i = 3222 (of 10192), d = 3.06855, its = 5
i = 3223 (of 10192), d = 10, its = 47
i = 3224 (of 10192), d = 10, its = 55
i = 3225 (of 10192), d = 10, its = 59
i = 3226 (of 10192), d = 10, its = 37
i = 3227 (of 10192), d = 10, its = 41
i = 3228 (of 10192), d = 10, its = 52
i = 3229 (of 10192), d = 10, its = 54
i = 3230 (of 10192), d = 10, its = 37
i = 3231 (of 10192), d = 10, its = 51
i = 3232 (of 10192), d = 10, its = 53
i = 3233 (of 10192), d = 10, its = 51
i = 3234 (of 10192), d = 10, its = 52
i = 3235 (of 10192), d = 10, its = 50
i = 3236 (of 10192), d = 10, its = 55
i = 3237 (of 10192), d = 0.916623, its = 19
i = 3238 (of 10192), d = 10, its = 53
i = 3239 (of 10192), d = 1.93235, its = 4
i = 3240 (of 10192), d = 10, its = 53
i = 3241 (of 10192), d = 10, its = 55
i = 3242 (of 10192), d = 1.11532, its = 22
i = 3243 (of 10192), d = 10, its = 37
i = 3244 (of 10192), d = 1.59847, its = 6
i = 3245 (of 10192), d = 4.96185, its = 8
i = 3246 (of 10192), d = 3.00696, its = 5
i = 3247 (of 10192), d = 3.20533, its = 6
i = 3248 (of 10192), d = 2.33554, its = 5
i = 3249 (of 10192), d = 10, its = 56
i = 3250 (of 10192), d = 10, its = 59
i = 3251 (of 10192), d = 10, its = 41
i = 3252 (of 10192), d = 1.46664, its = 8
i = 3253 (of 10192), d = 10, its = 51
i = 3254 (of 10192), d = 10, its = 56
i = 3255 (of 10192), d = 10, its = 52
i = 3256 (of 10192), d = 2.67752, its = 5
i = 3257 (of 10192), d = 10, its = 41
i = 3258 (of 10192), d = 10, its = 54
i = 3259 (of 10192), d = 10, its = 57
i = 3260 (of 10192), d = 10, its = 38
i = 3261 (of 10192), d = 10, its = 56
i = 3262 (of 10192), d = 10, its = 54
i = 3263 (of 10192), d = 4.0362, its = 7
i = 3264 (of 10192), d = 4.40685, its = 7
i = 3265 (of 10192), d = 10, its = 37
i = 3266 (of 10192), d = 10, its = 51
i = 3267 (of 10192), d = 2.18657, its = 4
i = 3268 (of 10192), d = 1.0065, its = 21
i = 3269 (of 10192), d = 10, its = 55
i = 3270 (of 10192), d = 10, its = 40
i = 3271 (of 10192), d = 10, its = 55
i = 3272 (of 10192), d = 1.73656, its = 5
i = 3273 (of 10192), d = 10, its = 53
i = 3274 (of 10192), d = 10, its = 51
i = 3275 (of 10192), d = 2.24638, its = 4
i = 3276 (of 10192), d = 10, its = 57
i = 3277 (of 10192), d = 10, its = 53
i = 3278 (of 10192), d = 1.07245, its = 19
i = 3279 (of 10192), d = 0.735564, its = 16
i = 3280 (of 10192), d = 10, its = 54
i = 3281 (of 10192), d = 10, its = 52
i = 3282 (of 10192), d = 10, its = 56
i = 3283 (of 10192), d = 2.28809, its = 4
i = 3284 (of 10192), d = 10, its = 54
i = 3285 (of 10192), d = 10, its = 53
i = 3286 (of 10192), d = 0.967191, its = 16
i = 3287 (of 10192), d = 6.41489, its = 7
i = 3288 (of 10192), d = 10, its = 51
i = 3289 (of 10192), d = 1.41585, its = 6
i = 3290 (of 10192), d = 10, its = 37
i = 3291 (of 10192), d = 1.60456, its = 6
i = 3292 (of 10192), d = 3.42376, its = 6
i = 3293 (of 10192), d = 10, its = 39
i = 3294 (of 10192), d = 10, its = 39
i = 3295 (of 10192), d = 10, its = 40
i = 3296 (of 10192), d = 10, its = 47
i = 3297 (of 10192), d = 0.753255, its = 18
i = 3298 (of 10192), d = 1.47859, its = 6
i = 3299 (of 10192), d = 10, its = 37
i = 3300 (of 10192), d = 2.1467, its = 4
i = 3301 (of 10192), d = 10, its = 37
i = 3302 (of 10192), d = 2.13599, its = 4
i = 3303 (of 10192), d = 2.51373, its = 5
i = 3304 (of 10192), d = 10, its = 49
i = 3305 (of 10192), d = 10, its = 43
i = 3306 (of 10192), d = 2.00457, its = 3
i = 3307 (of 10192), d = 10, its = 41
i = 3308 (of 10192), d = 2.81113, its = 6
i = 3309 (of 10192), d = 2.28199, its = 5
i = 3310 (of 10192), d = 2.30433, its = 5
i = 3311 (of 10192), d = 1.82986, its = 4
i = 3312 (of 10192), d = 7.57728, its = 8
i = 3313 (of 10192), d = 10, its = 39
i = 3314 (of 10192), d = 10, its = 51
i = 3315 (of 10192), d = 1.71307, its = 5
i = 3316 (of 10192), d = 2.28347, its = 5
i = 3317 (of 10192), d = 10, its = 37
i = 3318 (of 10192), d = 10, its = 52
i = 3319 (of 10192), d = 10, its = 54
i = 3320 (of 10192), d = 10, its = 57
i = 3321 (of 10192), d = 10, its = 37
i = 3322 (of 10192), d = 10, its = 52
i = 3323 (of 10192), d = 0.742605, its = 16
i = 3324 (of 10192), d = 10, its = 55
i = 3325 (of 10192), d = 10, its = 49
i = 3326 (of 10192), d = 10, its = 52
i = 3327 (of 10192), d = 1.16245, its = 20
i = 3328 (of 10192), d = 10, its = 52
i = 3329 (of 10192), d = 2.52312, its = 5
i = 3330 (of 10192), d = 10, its = 55
i = 3331 (of 10192), d = 10, its = 52
i = 3332 (of 10192), d = 10, its = 37
i = 3333 (of 10192), d = 10, its = 54
i = 3334 (of 10192), d = 10, its = 54
i = 3335 (of 10192), d = 10, its = 53
i = 3336 (of 10192), d = 1.95021, its = 3
i = 3337 (of 10192), d = 3.28373, its = 6
i = 3338 (of 10192), d = 10, its = 54
i = 3339 (of 10192), d = 10, its = 47
i = 3340 (of 10192), d = 10, its = 56
i = 3341 (of 10192), d = 10, its = 51
i = 3342 (of 10192), d = 10, its = 42
i = 3343 (of 10192), d = 10, its = 50
i = 3344 (of 10192), d = 10, its = 54
i = 3345 (of 10192), d = 10, its = 41
i = 3346 (of 10192), d = 10, its = 52
i = 3347 (of 10192), d = 10, its = 49
i = 3348 (of 10192), d = 2.02706, its = 3
i = 3349 (of 10192), d = 10, its = 53
i = 3350 (of 10192), d = 2.04006, its = 3
i = 3351 (of 10192), d = 1.86595, its = 4
i = 3352 (of 10192), d = 10, its = 37
i = 3353 (of 10192), d = 10, its = 37
i = 3354 (of 10192), d = 10, its = 55
i = 3355 (of 10192), d = 2.10114, its = 4
i = 3356 (of 10192), d = 0.970013, its = 19
i = 3357 (of 10192), d = 10, its = 54
i = 3358 (of 10192), d = 10, its = 37
i = 3359 (of 10192), d = 10, its = 37
i = 3360 (of 10192), d = 10, its = 37
i = 3361 (of 10192), d = 2.02201, its = 3
i = 3362 (of 10192), d = 10, its = 52
i = 3363 (of 10192), d = 10, its = 52
i = 3364 (of 10192), d = 10, its = 37
i = 3365 (of 10192), d = 3.00804, its = 6
i = 3366 (of 10192), d = 10, its = 49
i = 3367 (of 10192), d = 10, its = 54
i = 3368 (of 10192), d = 10, its = 59
i = 3369 (of 10192), d = 10, its = 53
i = 3370 (of 10192), d = 10, its = 47
i = 3371 (of 10192), d = 10, its = 41
i = 3372 (of 10192), d = 10, its = 52
i = 3373 (of 10192), d = 10, its = 52
i = 3374 (of 10192), d = 10, its = 56
i = 3375 (of 10192), d = 10, its = 50
i = 3376 (of 10192), d = 10, its = 50
i = 3377 (of 10192), d = 10, its = 42
i = 3378 (of 10192), d = 10, its = 37
i = 3379 (of 10192), d = 10, its = 53
i = 3380 (of 10192), d = 10, its = 53
i = 3381 (of 10192), d = 10, its = 56
i = 3382 (of 10192), d = 10, its = 53
i = 3383 (of 10192), d = 2.83461, its = 6
i = 3384 (of 10192), d = 1.47468, its = 8
i = 3385 (of 10192), d = 5.56091, its = 8
i = 3386 (of 10192), d = 10, its = 50
i = 3387 (of 10192), d = 10, its = 37
i = 3388 (of 10192), d = 10, its = 43
i = 3389 (of 10192), d = 10, its = 54
i = 3390 (of 10192), d = 2.04186, its = 3
i = 3391 (of 10192), d = 10, its = 52
i = 3392 (of 10192), d = 10, its = 53
i = 3393 (of 10192), d = 10, its = 39
i = 3394 (of 10192), d = 10, its = 37
i = 3395 (of 10192), d = 3.0429, its = 6
i = 3396 (of 10192), d = 10, its = 41
i = 3397 (of 10192), d = 10, its = 51
i = 3398 (of 10192), d = 10, its = 37
i = 3399 (of 10192), d = 1.67652, its = 5
i = 3400 (of 10192), d = 5.28799, its = 7
i = 3401 (of 10192), d = 10, its = 55
i = 3402 (of 10192), d = 10, its = 55
i = 3403 (of 10192), d = 10, its = 55
i = 3404 (of 10192), d = 10, its = 40
i = 3405 (of 10192), d = 10, its = 55
i = 3406 (of 10192), d = 10, its = 54
i = 3407 (of 10192), d = 10, its = 52
i = 3408 (of 10192), d = 10, its = 37
i = 3409 (of 10192), d = 10, its = 55
i = 3410 (of 10192), d = 3.0824, its = 6
i = 3411 (of 10192), d = 1.5685, its = 6
i = 3412 (of 10192), d = 10, its = 39
i = 3413 (of 10192), d = 1.59171, its = 6
i = 3414 (of 10192), d = 10, its = 50
i = 3415 (of 10192), d = 1.98756, its = 3
i = 3416 (of 10192), d = 1.50196, its = 7
i = 3417 (of 10192), d = 10, its = 41
i = 3418 (of 10192), d = 10, its = 51
i = 3419 (of 10192), d = 10, its = 42
i = 3420 (of 10192), d = 10, its = 54
i = 3421 (of 10192), d = 10, its = 37
i = 3422 (of 10192), d = 10, its = 41
i = 3423 (of 10192), d = 2.96239, its = 6
i = 3424 (of 10192), d = 10, its = 41
i = 3425 (of 10192), d = 10, its = 55
i = 3426 (of 10192), d = 2.01509, its = 3
i = 3427 (of 10192), d = 10, its = 57
i = 3428 (of 10192), d = 10, its = 55
i = 3429 (of 10192), d = 10, its = 52
i = 3430 (of 10192), d = 10, its = 52
i = 3431 (of 10192), d = 3.77058, its = 8
i = 3432 (of 10192), d = 10, its = 40
i = 3433 (of 10192), d = 10, its = 41
i = 3434 (of 10192), d = 0.870018, its = 19
i = 3435 (of 10192), d = 10, its = 39
i = 3436 (of 10192), d = 10, its = 37
i = 3437 (of 10192), d = 10, its = 55
i = 3438 (of 10192), d = 3.15859, its = 6
i = 3439 (of 10192), d = 10, its = 53
i = 3440 (of 10192), d = 10, its = 53
i = 3441 (of 10192), d = 2.15677, its = 4
i = 3442 (of 10192), d = 10, its = 56
i = 3443 (of 10192), d = 2.21036, its = 5
i = 3444 (of 10192), d = 1.58064, its = 6
i = 3445 (of 10192), d = 10, its = 47
i = 3446 (of 10192), d = 10, its = 54
i = 3447 (of 10192), d = 10, its = 38
i = 3448 (of 10192), d = 2.32539, its = 5
i = 3449 (of 10192), d = 10, its = 41
i = 3450 (of 10192), d = 10, its = 50
i = 3451 (of 10192), d = 10, its = 55
i = 3452 (of 10192), d = 10, its = 52
i = 3453 (of 10192), d = 3.15686, its = 6
i = 3454 (of 10192), d = 1.5764, its = 6
i = 3455 (of 10192), d = 2.04934, its = 3
i = 3456 (of 10192), d = 10, its = 50
i = 3457 (of 10192), d = 10, its = 41
i = 3458 (of 10192), d = 10, its = 52
i = 3459 (of 10192), d = 4.86685, its = 8
i = 3460 (of 10192), d = 10, its = 21
i = 3461 (of 10192), d = 10, its = 50
i = 3462 (of 10192), d = 10, its = 37
i = 3463 (of 10192), d = 10, its = 56
i = 3464 (of 10192), d = 10, its = 37
i = 3465 (of 10192), d = 10, its = 53
i = 3466 (of 10192), d = 10, its = 54
i = 3467 (of 10192), d = 1.87271, its = 4
i = 3468 (of 10192), d = 10, its = 51
i = 3469 (of 10192), d = 10, its = 40
i = 3470 (of 10192), d = 10, its = 54
i = 3471 (of 10192), d = 1.92906, its = 6
i = 3472 (of 10192), d = 10, its = 57
i = 3473 (of 10192), d = 2.5387, its = 5
i = 3474 (of 10192), d = 10, its = 53
i = 3475 (of 10192), d = 10, its = 37
i = 3476 (of 10192), d = 10, its = 52
i = 3477 (of 10192), d = 10, its = 53
i = 3478 (of 10192), d = 2.48054, its = 5
i = 3479 (of 10192), d = 10, its = 55
i = 3480 (of 10192), d = 10, its = 53
i = 3481 (of 10192), d = 10, its = 49
i = 3482 (of 10192), d = 10, its = 52
i = 3483 (of 10192), d = 10, its = 39
i = 3484 (of 10192), d = 10, its = 53
i = 3485 (of 10192), d = 1.43801, its = 8
i = 3486 (of 10192), d = 10, its = 40
i = 3487 (of 10192), d = 10, its = 53
i = 3488 (of 10192), d = 10, its = 41
i = 3489 (of 10192), d = 10, its = 53
i = 3490 (of 10192), d = 10, its = 51
i = 3491 (of 10192), d = 10, its = 52
i = 3492 (of 10192), d = 10, its = 55
i = 3493 (of 10192), d = 10, its = 54
i = 3494 (of 10192), d = 10, its = 53
i = 3495 (of 10192), d = 10, its = 49
i = 3496 (of 10192), d = 2.60852, its = 5
i = 3497 (of 10192), d = 10, its = 53
i = 3498 (of 10192), d = 2.69756, its = 5
i = 3499 (of 10192), d = 10, its = 40
i = 3500 (of 10192), d = 10, its = 54
i = 3501 (of 10192), d = 2.03264, its = 3
i = 3502 (of 10192), d = 10, its = 37
i = 3503 (of 10192), d = 10, its = 40
i = 3504 (of 10192), d = 10, its = 55
i = 3505 (of 10192), d = 1.84399, its = 4
i = 3506 (of 10192), d = 10, its = 49
i = 3507 (of 10192), d = 1.71812, its = 5
i = 3508 (of 10192), d = 2.46994, its = 5
i = 3509 (of 10192), d = 10, its = 53
i = 3510 (of 10192), d = 10, its = 15
i = 3511 (of 10192), d = 10, its = 52
i = 3512 (of 10192), d = 10, its = 49
i = 3513 (of 10192), d = 10, its = 40
i = 3514 (of 10192), d = 1.89278, its = 4
i = 3515 (of 10192), d = 1.05484, its = 25
i = 3516 (of 10192), d = 10, its = 54
i = 3517 (of 10192), d = 10, its = 41
i = 3518 (of 10192), d = 10, its = 58
i = 3519 (of 10192), d = 10, its = 37
i = 3520 (of 10192), d = 10, its = 52
i = 3521 (of 10192), d = 2.55895, its = 5
i = 3522 (of 10192), d = 1.45188, its = 5
i = 3523 (of 10192), d = 10, its = 38
i = 3524 (of 10192), d = 10, its = 37
i = 3525 (of 10192), d = 2.17375, its = 5
i = 3526 (of 10192), d = 10, its = 50
i = 3527 (of 10192), d = 1.90776, its = 4
i = 3528 (of 10192), d = 10, its = 58
i = 3529 (of 10192), d = 0.536198, its = 17
i = 3530 (of 10192), d = 10, its = 37
i = 3531 (of 10192), d = 10, its = 55
i = 3532 (of 10192), d = 10, its = 52
i = 3533 (of 10192), d = 10, its = 52
i = 3534 (of 10192), d = 10, its = 37
i = 3535 (of 10192), d = 1.72183, its = 5
i = 3536 (of 10192), d = 10, its = 51
i = 3537 (of 10192), d = 2.82211, its = 5
i = 3538 (of 10192), d = 10, its = 57
i = 3539 (of 10192), d = 10, its = 40
i = 3540 (of 10192), d = 2.31604, its = 5
i = 3541 (of 10192), d = 10, its = 37
i = 3542 (of 10192), d = 10, its = 49
i = 3543 (of 10192), d = 10, its = 52
i = 3544 (of 10192), d = 10, its = 37
i = 3545 (of 10192), d = 10, its = 37
i = 3546 (of 10192), d = 10, its = 39
i = 3547 (of 10192), d = 10, its = 49
i = 3548 (of 10192), d = 10, its = 51
i = 3549 (of 10192), d = 2.51126, its = 5
i = 3550 (of 10192), d = 0.704923, its = 27
i = 3551 (of 10192), d = 10, its = 50
i = 3552 (of 10192), d = 0.970393, its = 18
i = 3553 (of 10192), d = 1.61318, its = 6
i = 3554 (of 10192), d = 2.69324, its = 5
i = 3555 (of 10192), d = 10, its = 37
i = 3556 (of 10192), d = 10, its = 54
i = 3557 (of 10192), d = 6.70602, its = 8
i = 3558 (of 10192), d = 2.49963, its = 5
i = 3559 (of 10192), d = 2.69249, its = 5
i = 3560 (of 10192), d = 10, its = 37
i = 3561 (of 10192), d = 10, its = 52
i = 3562 (of 10192), d = 10, its = 56
i = 3563 (of 10192), d = 3.09691, its = 6
i = 3564 (of 10192), d = 10, its = 51
i = 3565 (of 10192), d = 10, its = 51
i = 3566 (of 10192), d = 10, its = 52
i = 3567 (of 10192), d = 10, its = 55
i = 3568 (of 10192), d = 10, its = 54
i = 3569 (of 10192), d = 1.01251, its = 17
i = 3570 (of 10192), d = 10, its = 53
i = 3571 (of 10192), d = 10, its = 53
i = 3572 (of 10192), d = 2.02032, its = 3
i = 3573 (of 10192), d = 10, its = 53
i = 3574 (of 10192), d = 1.51453, its = 6
i = 3575 (of 10192), d = 2.59076, its = 6
i = 3576 (of 10192), d = 1.67881, its = 5
i = 3577 (of 10192), d = 10, its = 43
i = 3578 (of 10192), d = 3.52193, its = 7
i = 3579 (of 10192), d = 1.73372, its = 5
i = 3580 (of 10192), d = 10, its = 37
i = 3581 (of 10192), d = 10, its = 51
i = 3582 (of 10192), d = 10, its = 56
i = 3583 (of 10192), d = 1.52369, its = 6
i = 3584 (of 10192), d = 10, its = 51
i = 3585 (of 10192), d = 2.58075, its = 6
i = 3586 (of 10192), d = 10, its = 53
i = 3587 (of 10192), d = 10, its = 55
i = 3588 (of 10192), d = 10, its = 42
i = 3589 (of 10192), d = 10, its = 52
i = 3590 (of 10192), d = 10, its = 37
i = 3591 (of 10192), d = 2.24128, its = 5
i = 3592 (of 10192), d = 10, its = 57
i = 3593 (of 10192), d = 1.69607, its = 6
i = 3594 (of 10192), d = 1.82575, its = 4
i = 3595 (of 10192), d = 10, its = 40
i = 3596 (of 10192), d = 10, its = 51
i = 3597 (of 10192), d = 10, its = 52
i = 3598 (of 10192), d = 10, its = 53
i = 3599 (of 10192), d = 10, its = 53
i = 3600 (of 10192), d = 10, its = 53
i = 3601 (of 10192), d = 10, its = 54
i = 3602 (of 10192), d = 10, its = 53
i = 3603 (of 10192), d = 2.74568, its = 5
i = 3604 (of 10192), d = 1.54646, its = 6
i = 3605 (of 10192), d = 10, its = 59
i = 3606 (of 10192), d = 10, its = 54
i = 3607 (of 10192), d = 10, its = 54
i = 3608 (of 10192), d = 8.75095, its = 9
i = 3609 (of 10192), d = 10, its = 37
i = 3610 (of 10192), d = 10, its = 58
i = 3611 (of 10192), d = 10, its = 38
i = 3612 (of 10192), d = 10, its = 53
i = 3613 (of 10192), d = 10, its = 51
i = 3614 (of 10192), d = 10, its = 37
i = 3615 (of 10192), d = 4.64897, its = 7
i = 3616 (of 10192), d = 10, its = 49
i = 3617 (of 10192), d = 1.00918, its = 24
i = 3618 (of 10192), d = 10, its = 56
i = 3619 (of 10192), d = 10, its = 54
i = 3620 (of 10192), d = 10, its = 41
i = 3621 (of 10192), d = 1.69339, its = 6
i = 3622 (of 10192), d = 10, its = 43
i = 3623 (of 10192), d = 10, its = 38
i = 3624 (of 10192), d = 1.67334, its = 5
i = 3625 (of 10192), d = 10, its = 54
i = 3626 (of 10192), d = 10, its = 56
i = 3627 (of 10192), d = 10, its = 52
i = 3628 (of 10192), d = 10, its = 56
i = 3629 (of 10192), d = 10, its = 58
i = 3630 (of 10192), d = 0.969297, its = 18
i = 3631 (of 10192), d = 1.28482, its = 7
i = 3632 (of 10192), d = 10, its = 39
i = 3633 (of 10192), d = 10, its = 49
i = 3634 (of 10192), d = 1.45248, its = 8
i = 3635 (of 10192), d = 10, its = 55
i = 3636 (of 10192), d = 10, its = 58
i = 3637 (of 10192), d = 10, its = 52
i = 3638 (of 10192), d = 10, its = 39
i = 3639 (of 10192), d = 3.5839, its = 7
i = 3640 (of 10192), d = 10, its = 40
i = 3641 (of 10192), d = 2.17571, its = 5
i = 3642 (of 10192), d = 10, its = 53
i = 3643 (of 10192), d = 10, its = 39
i = 3644 (of 10192), d = 10, its = 50
i = 3645 (of 10192), d = 10, its = 40
i = 3646 (of 10192), d = 10, its = 51
i = 3647 (of 10192), d = 2.39931, its = 5
i = 3648 (of 10192), d = 1.60628, its = 6
i = 3649 (of 10192), d = 10, its = 53
i = 3650 (of 10192), d = 10, its = 54
i = 3651 (of 10192), d = 10, its = 49
i = 3652 (of 10192), d = 10, its = 54
i = 3653 (of 10192), d = 1.09312, its = 16
i = 3654 (of 10192), d = 6.50784, its = 8
i = 3655 (of 10192), d = 10, its = 53
i = 3656 (of 10192), d = 10, its = 39
i = 3657 (of 10192), d = 10, its = 52
i = 3658 (of 10192), d = 10, its = 37
i = 3659 (of 10192), d = 10, its = 53
i = 3660 (of 10192), d = 10, its = 54
i = 3661 (of 10192), d = 10, its = 50
i = 3662 (of 10192), d = 5.00561, its = 7
i = 3663 (of 10192), d = 10, its = 57
i = 3664 (of 10192), d = 10, its = 51
i = 3665 (of 10192), d = 10, its = 52
i = 3666 (of 10192), d = 10, its = 37
i = 3667 (of 10192), d = 10, its = 37
i = 3668 (of 10192), d = 2.94844, its = 6
i = 3669 (of 10192), d = 10, its = 40
i = 3670 (of 10192), d = 10, its = 41
i = 3671 (of 10192), d = 10, its = 55
i = 3672 (of 10192), d = 10, its = 41
i = 3673 (of 10192), d = 5.00521, its = 7
i = 3674 (of 10192), d = 10, its = 51
i = 3675 (of 10192), d = 10, its = 55
i = 3676 (of 10192), d = 10, its = 52
i = 3677 (of 10192), d = 2.97251, its = 6
i = 3678 (of 10192), d = 3.08586, its = 6
i = 3679 (of 10192), d = 10, its = 50
i = 3680 (of 10192), d = 10, its = 39
i = 3681 (of 10192), d = 4.09235, its = 6
i = 3682 (of 10192), d = 2.34358, its = 5
i = 3683 (of 10192), d = 10, its = 55
i = 3684 (of 10192), d = 2.85289, its = 5
i = 3685 (of 10192), d = 1.85183, its = 4
i = 3686 (of 10192), d = 2.34241, its = 5
i = 3687 (of 10192), d = 0.842365, its = 25
i = 3688 (of 10192), d = 10, its = 55
i = 3689 (of 10192), d = 10, its = 49
i = 3690 (of 10192), d = 10, its = 53
i = 3691 (of 10192), d = 1.91577, its = 4
i = 3692 (of 10192), d = 10, its = 52
i = 3693 (of 10192), d = 10, its = 41
i = 3694 (of 10192), d = 5.99433, its = 8
i = 3695 (of 10192), d = 10, its = 41
i = 3696 (of 10192), d = 10, its = 37
i = 3697 (of 10192), d = 10, its = 57
i = 3698 (of 10192), d = 10, its = 37
i = 3699 (of 10192), d = 10, its = 38
i = 3700 (of 10192), d = 10, its = 37
i = 3701 (of 10192), d = 10, its = 37
i = 3702 (of 10192), d = 10, its = 40
i = 3703 (of 10192), d = 10, its = 42
i = 3704 (of 10192), d = 10, its = 48
i = 3705 (of 10192), d = 2.18262, its = 4
i = 3706 (of 10192), d = 1.0052, its = 19
i = 3707 (of 10192), d = 3.21067, its = 6
i = 3708 (of 10192), d = 1.05821, its = 27
i = 3709 (of 10192), d = 10, its = 52
i = 3710 (of 10192), d = 0.812473, its = 21
i = 3711 (of 10192), d = 2.22834, its = 4
i = 3712 (of 10192), d = 2.86124, its = 7
i = 3713 (of 10192), d = 10, its = 54
i = 3714 (of 10192), d = 10, its = 51
i = 3715 (of 10192), d = 10, its = 37
i = 3716 (of 10192), d = 10, its = 55
i = 3717 (of 10192), d = 1.41759, its = 7
i = 3718 (of 10192), d = 1.81113, its = 4
i = 3719 (of 10192), d = 2.31253, its = 5
i = 3720 (of 10192), d = 10, its = 59
i = 3721 (of 10192), d = 10, its = 47
i = 3722 (of 10192), d = 10, its = 37
i = 3723 (of 10192), d = 2.96947, its = 6
i = 3724 (of 10192), d = 10, its = 53
i = 3725 (of 10192), d = 10, its = 58
i = 3726 (of 10192), d = 10, its = 37
i = 3727 (of 10192), d = 10, its = 55
i = 3728 (of 10192), d = 10, its = 53
i = 3729 (of 10192), d = 2.77833, its = 6
i = 3730 (of 10192), d = 10, its = 44
i = 3731 (of 10192), d = 1.56563, its = 7
i = 3732 (of 10192), d = 0.963228, its = 24
i = 3733 (of 10192), d = 10, its = 51
i = 3734 (of 10192), d = 10, its = 52
i = 3735 (of 10192), d = 0.672995, its = 16
i = 3736 (of 10192), d = 10, its = 54
i = 3737 (of 10192), d = 10, its = 58
i = 3738 (of 10192), d = 2.47259, its = 5
i = 3739 (of 10192), d = 10, its = 50
i = 3740 (of 10192), d = 0.829757, its = 28
i = 3741 (of 10192), d = 2.84133, its = 6
i = 3742 (of 10192), d = 2.51757, its = 5
i = 3743 (of 10192), d = 10, its = 53
i = 3744 (of 10192), d = 10, its = 51
i = 3745 (of 10192), d = 10, its = 37
i = 3746 (of 10192), d = 10, its = 52
i = 3747 (of 10192), d = 10, its = 49
i = 3748 (of 10192), d = 10, its = 37
i = 3749 (of 10192), d = 10, its = 53
i = 3750 (of 10192), d = 10, its = 53
i = 3751 (of 10192), d = 10, its = 50
i = 3752 (of 10192), d = 10, its = 52
i = 3753 (of 10192), d = 10, its = 37
i = 3754 (of 10192), d = 10, its = 39
i = 3755 (of 10192), d = 10, its = 53
i = 3756 (of 10192), d = 10, its = 40
i = 3757 (of 10192), d = 10, its = 39
i = 3758 (of 10192), d = 1.18339, its = 5
i = 3759 (of 10192), d = 10, its = 53
i = 3760 (of 10192), d = 10, its = 42
i = 3761 (of 10192), d = 10, its = 46
i = 3762 (of 10192), d = 1.84402, its = 4
i = 3763 (of 10192), d = 10, its = 53
i = 3764 (of 10192), d = 10, its = 47
i = 3765 (of 10192), d = 10, its = 51
i = 3766 (of 10192), d = 10, its = 40
i = 3767 (of 10192), d = 10, its = 54
i = 3768 (of 10192), d = 1.90017, its = 4
i = 3769 (of 10192), d = 10, its = 57
i = 3770 (of 10192), d = 10, its = 45
i = 3771 (of 10192), d = 10, its = 52
i = 3772 (of 10192), d = 10, its = 40
i = 3773 (of 10192), d = 10, its = 52
i = 3774 (of 10192), d = 10, its = 37
i = 3775 (of 10192), d = 10, its = 52
i = 3776 (of 10192), d = 10, its = 49
i = 3777 (of 10192), d = 10, its = 37
i = 3778 (of 10192), d = 2.32194, its = 5
i = 3779 (of 10192), d = 3.72102, its = 6
i = 3780 (of 10192), d = 10, its = 49
i = 3781 (of 10192), d = 2.27551, its = 5
i = 3782 (of 10192), d = 10, its = 42
i = 3783 (of 10192), d = 10, its = 37
i = 3784 (of 10192), d = 10, its = 46
i = 3785 (of 10192), d = 2.34734, its = 5
i = 3786 (of 10192), d = 10, its = 37
i = 3787 (of 10192), d = 10, its = 50
i = 3788 (of 10192), d = 10, its = 52
i = 3789 (of 10192), d = 1.80287, its = 4
i = 3790 (of 10192), d = 10, its = 55
i = 3791 (of 10192), d = 10, its = 37
i = 3792 (of 10192), d = 10, its = 39
i = 3793 (of 10192), d = 10, its = 54
i = 3794 (of 10192), d = 10, its = 54
i = 3795 (of 10192), d = 10, its = 55
i = 3796 (of 10192), d = 10, its = 51
i = 3797 (of 10192), d = 10, its = 50
i = 3798 (of 10192), d = 10, its = 56
i = 3799 (of 10192), d = 10, its = 48
i = 3800 (of 10192), d = 10, its = 39
i = 3801 (of 10192), d = 10, its = 37
i = 3802 (of 10192), d = 10, its = 54
i = 3803 (of 10192), d = 10, its = 57
i = 3804 (of 10192), d = 10, its = 39
i = 3805 (of 10192), d = 10, its = 48
i = 3806 (of 10192), d = 2.68926, its = 5
i = 3807 (of 10192), d = 10, its = 45
i = 3808 (of 10192), d = 10, its = 50
i = 3809 (of 10192), d = 10, its = 40
i = 3810 (of 10192), d = 10, its = 45
i = 3811 (of 10192), d = 10, its = 56
i = 3812 (of 10192), d = 10, its = 37
i = 3813 (of 10192), d = 1.62814, its = 6
i = 3814 (of 10192), d = 10, its = 54
i = 3815 (of 10192), d = 2.64499, its = 6
i = 3816 (of 10192), d = 10, its = 57
i = 3817 (of 10192), d = 10, its = 51
i = 3818 (of 10192), d = 10, its = 52
i = 3819 (of 10192), d = 10, its = 49
i = 3820 (of 10192), d = 1.59963, its = 6
i = 3821 (of 10192), d = 10, its = 52
i = 3822 (of 10192), d = 10, its = 50
i = 3823 (of 10192), d = 2.04694, its = 3
i = 3824 (of 10192), d = 0.698126, its = 18
i = 3825 (of 10192), d = 4.49472, its = 8
i = 3826 (of 10192), d = 10, its = 50
i = 3827 (of 10192), d = 10, its = 38
i = 3828 (of 10192), d = 1.71994, its = 5
i = 3829 (of 10192), d = 2.43322, its = 5
i = 3830 (of 10192), d = 10, its = 50
i = 3831 (of 10192), d = 1.38787, its = 7
i = 3832 (of 10192), d = 1.32231, its = 7
i = 3833 (of 10192), d = 10, its = 51
i = 3834 (of 10192), d = 10, its = 55
i = 3835 (of 10192), d = 1.88708, its = 4
i = 3836 (of 10192), d = 4.10779, its = 7
i = 3837 (of 10192), d = 10, its = 53
i = 3838 (of 10192), d = 10, its = 53
i = 3839 (of 10192), d = 10, its = 47
i = 3840 (of 10192), d = 10, its = 56
i = 3841 (of 10192), d = 10, its = 50
i = 3842 (of 10192), d = 10, its = 37
i = 3843 (of 10192), d = 10, its = 48
i = 3844 (of 10192), d = 1.44256, its = 7
i = 3845 (of 10192), d = 10, its = 37
i = 3846 (of 10192), d = 2.42553, its = 5
i = 3847 (of 10192), d = 2.34358, its = 5
i = 3848 (of 10192), d = 10, its = 40
i = 3849 (of 10192), d = 2.18704, its = 4
i = 3850 (of 10192), d = 10, its = 37
i = 3851 (of 10192), d = 10, its = 53
i = 3852 (of 10192), d = 10, its = 51
i = 3853 (of 10192), d = 10, its = 50
i = 3854 (of 10192), d = 0.808484, its = 15
i = 3855 (of 10192), d = 10, its = 37
i = 3856 (of 10192), d = 10, its = 51
i = 3857 (of 10192), d = 10, its = 37
i = 3858 (of 10192), d = 10, its = 40
i = 3859 (of 10192), d = 10, its = 37
i = 3860 (of 10192), d = 10, its = 57
i = 3861 (of 10192), d = 2.41357, its = 5
i = 3862 (of 10192), d = 2.67717, its = 6
i = 3863 (of 10192), d = 2.43825, its = 5
i = 3864 (of 10192), d = 2.02954, its = 3
i = 3865 (of 10192), d = 10, its = 39
i = 3866 (of 10192), d = 10, its = 57
i = 3867 (of 10192), d = 1.73969, its = 5
i = 3868 (of 10192), d = 10, its = 57
i = 3869 (of 10192), d = 10, its = 54
i = 3870 (of 10192), d = 10, its = 50
i = 3871 (of 10192), d = 10, its = 48
i = 3872 (of 10192), d = 10, its = 42
i = 3873 (of 10192), d = 10, its = 52
i = 3874 (of 10192), d = 10, its = 37
i = 3875 (of 10192), d = 1.26734, its = 25
i = 3876 (of 10192), d = 1.17406, its = 18
i = 3877 (of 10192), d = 4.40592, its = 7
i = 3878 (of 10192), d = 10, its = 37
i = 3879 (of 10192), d = 10, its = 55
i = 3880 (of 10192), d = 10, its = 37
i = 3881 (of 10192), d = 10, its = 53
i = 3882 (of 10192), d = 0.786717, its = 16
i = 3883 (of 10192), d = 10, its = 52
i = 3884 (of 10192), d = 10, its = 37
i = 3885 (of 10192), d = 10, its = 54
i = 3886 (of 10192), d = 10, its = 38
i = 3887 (of 10192), d = 10, its = 52
i = 3888 (of 10192), d = 10, its = 53
i = 3889 (of 10192), d = 10, its = 37
i = 3890 (of 10192), d = 10, its = 52
i = 3891 (of 10192), d = 10, its = 56
i = 3892 (of 10192), d = 10, its = 51
i = 3893 (of 10192), d = 10, its = 52
i = 3894 (of 10192), d = 2.62166, its = 5
i = 3895 (of 10192), d = 10, its = 54
i = 3896 (of 10192), d = 10, its = 52
i = 3897 (of 10192), d = 4.65108, its = 7
i = 3898 (of 10192), d = 10, its = 44
i = 3899 (of 10192), d = 10, its = 37
i = 3900 (of 10192), d = 10, its = 54
i = 3901 (of 10192), d = 10, its = 56
i = 3902 (of 10192), d = 10, its = 49
i = 3903 (of 10192), d = 10, its = 42
i = 3904 (of 10192), d = 10, its = 50
i = 3905 (of 10192), d = 10, its = 50
i = 3906 (of 10192), d = 10, its = 49
i = 3907 (of 10192), d = 10, its = 54
i = 3908 (of 10192), d = 0.929029, its = 25
i = 3909 (of 10192), d = 1.0984, its = 21
i = 3910 (of 10192), d = 10, its = 55
i = 3911 (of 10192), d = 10, its = 49
i = 3912 (of 10192), d = 1.09126, its = 20
i = 3913 (of 10192), d = 10, its = 37
i = 3914 (of 10192), d = 10, its = 52
i = 3915 (of 10192), d = 10, its = 40
i = 3916 (of 10192), d = 10, its = 53
i = 3917 (of 10192), d = 10, its = 51
i = 3918 (of 10192), d = 10, its = 37
i = 3919 (of 10192), d = 0.843609, its = 23
i = 3920 (of 10192), d = 10, its = 50
i = 3921 (of 10192), d = 10, its = 51
i = 3922 (of 10192), d = 10, its = 53
i = 3923 (of 10192), d = 0.852794, its = 17
i = 3924 (of 10192), d = 1.93627, its = 4
i = 3925 (of 10192), d = 10, its = 37
i = 3926 (of 10192), d = 2.62663, its = 6
i = 3927 (of 10192), d = 10, its = 53
i = 3928 (of 10192), d = 10, its = 51
i = 3929 (of 10192), d = 1.69703, its = 5
i = 3930 (of 10192), d = 10, its = 37
i = 3931 (of 10192), d = 10, its = 50
i = 3932 (of 10192), d = 10, its = 55
i = 3933 (of 10192), d = 2.56558, its = 6
i = 3934 (of 10192), d = 1.79042, its = 5
i = 3935 (of 10192), d = 10, its = 41
i = 3936 (of 10192), d = 10, its = 54
i = 3937 (of 10192), d = 10, its = 56
i = 3938 (of 10192), d = 2.73751, its = 7
i = 3939 (of 10192), d = 1.88432, its = 4
i = 3940 (of 10192), d = 10, its = 53
i = 3941 (of 10192), d = 10, its = 61
i = 3942 (of 10192), d = 10, its = 50
i = 3943 (of 10192), d = 10, its = 52
i = 3944 (of 10192), d = 10, its = 40
i = 3945 (of 10192), d = 2.63978, its = 5
i = 3946 (of 10192), d = 10, its = 50
i = 3947 (of 10192), d = 1.95544, its = 3
i = 3948 (of 10192), d = 10, its = 46
i = 3949 (of 10192), d = 10, its = 56
i = 3950 (of 10192), d = 1.58527, its = 7
i = 3951 (of 10192), d = 1.75913, its = 6
i = 3952 (of 10192), d = 2.63997, its = 6
i = 3953 (of 10192), d = 1.81168, its = 4
i = 3954 (of 10192), d = 10, its = 56
i = 3955 (of 10192), d = 10, its = 58
i = 3956 (of 10192), d = 10, its = 37
i = 3957 (of 10192), d = 2.15307, its = 4
i = 3958 (of 10192), d = 1.9131, its = 4
i = 3959 (of 10192), d = 10, its = 55
i = 3960 (of 10192), d = 3.27638, its = 6
i = 3961 (of 10192), d = 10, its = 51
i = 3962 (of 10192), d = 2.88739, its = 6
i = 3963 (of 10192), d = 1.17706, its = 6
i = 3964 (of 10192), d = 10, its = 53
i = 3965 (of 10192), d = 2.02983, its = 3
i = 3966 (of 10192), d = 1.94141, its = 3
i = 3967 (of 10192), d = 10, its = 52
i = 3968 (of 10192), d = 4.13998, its = 7
i = 3969 (of 10192), d = 3.87861, its = 7
i = 3970 (of 10192), d = 1.90437, its = 4
i = 3971 (of 10192), d = 10, its = 51
i = 3972 (of 10192), d = 0.797019, its = 16
i = 3973 (of 10192), d = 2.43712, its = 5
i = 3974 (of 10192), d = 10, its = 54
i = 3975 (of 10192), d = 10, its = 57
i = 3976 (of 10192), d = 10, its = 39
i = 3977 (of 10192), d = 10, its = 37
i = 3978 (of 10192), d = 10, its = 41
i = 3979 (of 10192), d = 10, its = 52
i = 3980 (of 10192), d = 10, its = 51
i = 3981 (of 10192), d = 1.66125, its = 8
i = 3982 (of 10192), d = 0.678416, its = 22
i = 3983 (of 10192), d = 10, its = 54
i = 3984 (of 10192), d = 10, its = 40
i = 3985 (of 10192), d = 10, its = 53
i = 3986 (of 10192), d = 10, its = 54
i = 3987 (of 10192), d = 1.81213, its = 5
i = 3988 (of 10192), d = 10, its = 37
i = 3989 (of 10192), d = 10, its = 54
i = 3990 (of 10192), d = 1.48331, its = 8
i = 3991 (of 10192), d = 10, its = 39
i = 3992 (of 10192), d = 10, its = 57
i = 3993 (of 10192), d = 10, its = 42
i = 3994 (of 10192), d = 10, its = 37
i = 3995 (of 10192), d = 1.12657, its = 19
i = 3996 (of 10192), d = 10, its = 37
i = 3997 (of 10192), d = 10, its = 43
i = 3998 (of 10192), d = 10, its = 53
i = 3999 (of 10192), d = 10, its = 54
i = 4000 (of 10192), d = 10, its = 37
i = 4001 (of 10192), d = 1.52516, its = 6
i = 4002 (of 10192), d = 10, its = 58
i = 4003 (of 10192), d = 1.96341, its = 3
i = 4004 (of 10192), d = 10, its = 40
i = 4005 (of 10192), d = 10, its = 53
i = 4006 (of 10192), d = 3.72437, its = 7
i = 4007 (of 10192), d = 1.91717, its = 4
i = 4008 (of 10192), d = 3.14771, its = 7
i = 4009 (of 10192), d = 5.20951, its = 7
i = 4010 (of 10192), d = 10, its = 53
i = 4011 (of 10192), d = 10, its = 50
i = 4012 (of 10192), d = 10, its = 56
i = 4013 (of 10192), d = 10, its = 53
i = 4014 (of 10192), d = 10, its = 40
i = 4015 (of 10192), d = 10, its = 57
i = 4016 (of 10192), d = 10, its = 51
i = 4017 (of 10192), d = 10, its = 55
i = 4018 (of 10192), d = 10, its = 52
i = 4019 (of 10192), d = 10, its = 56
i = 4020 (of 10192), d = 10, its = 48
i = 4021 (of 10192), d = 10, its = 53
i = 4022 (of 10192), d = 10, its = 39
i = 4023 (of 10192), d = 10, its = 37
i = 4024 (of 10192), d = 10, its = 52
i = 4025 (of 10192), d = 10, its = 54
i = 4026 (of 10192), d = 1.88546, its = 4
i = 4027 (of 10192), d = 10, its = 52
i = 4028 (of 10192), d = 10, its = 55
i = 4029 (of 10192), d = 0.787132, its = 19
i = 4030 (of 10192), d = 1.93377, its = 3
i = 4031 (of 10192), d = 2.22099, its = 4
i = 4032 (of 10192), d = 0.70744, its = 21
i = 4033 (of 10192), d = 1.96424, its = 3
i = 4034 (of 10192), d = 10, its = 44
i = 4035 (of 10192), d = 2.25219, its = 4
i = 4036 (of 10192), d = 10, its = 54
i = 4037 (of 10192), d = 1.195, its = 20
i = 4038 (of 10192), d = 10, its = 57
i = 4039 (of 10192), d = 3.1078, its = 5
i = 4040 (of 10192), d = 10, its = 41
i = 4041 (of 10192), d = 3.83713, its = 6
i = 4042 (of 10192), d = 1.46915, its = 26
i = 4043 (of 10192), d = 10, its = 52
i = 4044 (of 10192), d = 10, its = 50
i = 4045 (of 10192), d = 10, its = 37
i = 4046 (of 10192), d = 10, its = 54
i = 4047 (of 10192), d = 10, its = 53
i = 4048 (of 10192), d = 10, its = 55
i = 4049 (of 10192), d = 10, its = 53
i = 4050 (of 10192), d = 2.36249, its = 4
i = 4051 (of 10192), d = 10, its = 52
i = 4052 (of 10192), d = 10, its = 53
i = 4053 (of 10192), d = 10, its = 56
i = 4054 (of 10192), d = 10, its = 54
i = 4055 (of 10192), d = 10, its = 41
i = 4056 (of 10192), d = 2.68388, its = 6
i = 4057 (of 10192), d = 10, its = 40
i = 4058 (of 10192), d = 10, its = 51
i = 4059 (of 10192), d = 10, its = 56
i = 4060 (of 10192), d = 10, its = 40
i = 4061 (of 10192), d = 10, its = 56
i = 4062 (of 10192), d = 10, its = 54
i = 4063 (of 10192), d = 2.47275, its = 5
i = 4064 (of 10192), d = 2.78958, its = 5
i = 4065 (of 10192), d = 2.82515, its = 8
i = 4066 (of 10192), d = 10, its = 51
i = 4067 (of 10192), d = 10, its = 55
i = 4068 (of 10192), d = 0.718248, its = 17
i = 4069 (of 10192), d = 10, its = 49
i = 4070 (of 10192), d = 10, its = 55
i = 4071 (of 10192), d = 10, its = 50
i = 4072 (of 10192), d = 10, its = 49
i = 4073 (of 10192), d = 10, its = 51
i = 4074 (of 10192), d = 10, its = 54
i = 4075 (of 10192), d = 1.59505, its = 6
i = 4076 (of 10192), d = 2.68388, its = 6
i = 4077 (of 10192), d = 1.58815, its = 6
i = 4078 (of 10192), d = 0.885629, its = 18
i = 4079 (of 10192), d = 10, its = 52
i = 4080 (of 10192), d = 10, its = 55
i = 4081 (of 10192), d = 10, its = 57
i = 4082 (of 10192), d = 10, its = 54
i = 4083 (of 10192), d = 5.40608, its = 8
i = 4084 (of 10192), d = 4.07726, its = 7
i = 4085 (of 10192), d = 10, its = 54
i = 4086 (of 10192), d = 10, its = 37
i = 4087 (of 10192), d = 10, its = 37
i = 4088 (of 10192), d = 1.46164, its = 7
i = 4089 (of 10192), d = 10, its = 51
i = 4090 (of 10192), d = 2.45339, its = 5
i = 4091 (of 10192), d = 3.10969, its = 7
i = 4092 (of 10192), d = 3.75749, its = 7
i = 4093 (of 10192), d = 10, its = 53
i = 4094 (of 10192), d = 10, its = 54
i = 4095 (of 10192), d = 1.08283, its = 17
i = 4096 (of 10192), d = 10, its = 50
i = 4097 (of 10192), d = 10, its = 53
i = 4098 (of 10192), d = 10, its = 52
i = 4099 (of 10192), d = 10, its = 52
i = 4100 (of 10192), d = 2.11675, its = 4
i = 4101 (of 10192), d = 10, its = 49
i = 4102 (of 10192), d = 10, its = 56
i = 4103 (of 10192), d = 10, its = 50
i = 4104 (of 10192), d = 10, its = 54
i = 4105 (of 10192), d = 10, its = 54
i = 4106 (of 10192), d = 10, its = 37
i = 4107 (of 10192), d = 2.06277, its = 3
i = 4108 (of 10192), d = 10, its = 37
i = 4109 (of 10192), d = 10, its = 52
i = 4110 (of 10192), d = 10, its = 51
i = 4111 (of 10192), d = 10, its = 40
i = 4112 (of 10192), d = 10, its = 51
i = 4113 (of 10192), d = 1.85156, its = 4
i = 4114 (of 10192), d = 10, its = 41
i = 4115 (of 10192), d = 10, its = 53
i = 4116 (of 10192), d = 2.33529, its = 5
i = 4117 (of 10192), d = 10, its = 51
i = 4118 (of 10192), d = 10, its = 50
i = 4119 (of 10192), d = 10, its = 37
i = 4120 (of 10192), d = 10, its = 44
i = 4121 (of 10192), d = 1.59188, its = 7
i = 4122 (of 10192), d = 10, its = 52
i = 4123 (of 10192), d = 1.62008, its = 5
i = 4124 (of 10192), d = 10, its = 55
i = 4125 (of 10192), d = 10, its = 37
i = 4126 (of 10192), d = 3.22674, its = 6
i = 4127 (of 10192), d = 0.930626, its = 17
i = 4128 (of 10192), d = 10, its = 57
i = 4129 (of 10192), d = 10, its = 37
i = 4130 (of 10192), d = 2.30851, its = 4
i = 4131 (of 10192), d = 10, its = 52
i = 4132 (of 10192), d = 10, its = 53
i = 4133 (of 10192), d = 10, its = 55
i = 4134 (of 10192), d = 10, its = 52
i = 4135 (of 10192), d = 10, its = 54
i = 4136 (of 10192), d = 10, its = 39
i = 4137 (of 10192), d = 10, its = 40
i = 4138 (of 10192), d = 0.957856, its = 21
i = 4139 (of 10192), d = 2.59676, its = 6
i = 4140 (of 10192), d = 10, its = 37
i = 4141 (of 10192), d = 10, its = 44
i = 4142 (of 10192), d = 10, its = 39
i = 4143 (of 10192), d = 10, its = 53
i = 4144 (of 10192), d = 10, its = 41
i = 4145 (of 10192), d = 10, its = 53
i = 4146 (of 10192), d = 10, its = 37
i = 4147 (of 10192), d = 0.840678, its = 24
i = 4148 (of 10192), d = 10, its = 54
i = 4149 (of 10192), d = 10, its = 53
i = 4150 (of 10192), d = 10, its = 56
i = 4151 (of 10192), d = 10, its = 37
i = 4152 (of 10192), d = 10, its = 55
i = 4153 (of 10192), d = 10, its = 50
i = 4154 (of 10192), d = 10, its = 56
i = 4155 (of 10192), d = 10, its = 53
i = 4156 (of 10192), d = 10, its = 51
i = 4157 (of 10192), d = 10, its = 39
i = 4158 (of 10192), d = 2.17588, its = 4
i = 4159 (of 10192), d = 10, its = 54
i = 4160 (of 10192), d = 2.4734, its = 5
i = 4161 (of 10192), d = 0.90728, its = 18
i = 4162 (of 10192), d = 10, its = 53
i = 4163 (of 10192), d = 10, its = 41
i = 4164 (of 10192), d = 10, its = 56
i = 4165 (of 10192), d = 10, its = 41
i = 4166 (of 10192), d = 10, its = 52
i = 4167 (of 10192), d = 10, its = 55
i = 4168 (of 10192), d = 10, its = 54
i = 4169 (of 10192), d = 10, its = 51
i = 4170 (of 10192), d = 1.75506, its = 5
i = 4171 (of 10192), d = 10, its = 53
i = 4172 (of 10192), d = 10, its = 53
i = 4173 (of 10192), d = 1.16474, its = 24
i = 4174 (of 10192), d = 10, its = 53
i = 4175 (of 10192), d = 10, its = 52
i = 4176 (of 10192), d = 10, its = 56
i = 4177 (of 10192), d = 10, its = 49
i = 4178 (of 10192), d = 10, its = 51
i = 4179 (of 10192), d = 10, its = 41
i = 4180 (of 10192), d = 10, its = 41
i = 4181 (of 10192), d = 10, its = 54
i = 4182 (of 10192), d = 10, its = 41
i = 4183 (of 10192), d = 10, its = 50
i = 4184 (of 10192), d = 10, its = 52
i = 4185 (of 10192), d = 10, its = 49
i = 4186 (of 10192), d = 10, its = 37
i = 4187 (of 10192), d = 10, its = 55
i = 4188 (of 10192), d = 10, its = 51
i = 4189 (of 10192), d = 10, its = 52
i = 4190 (of 10192), d = 3.34745, its = 7
i = 4191 (of 10192), d = 10, its = 50
i = 4192 (of 10192), d = 1.77148, its = 4
i = 4193 (of 10192), d = 10, its = 56
i = 4194 (of 10192), d = 10, its = 54
i = 4195 (of 10192), d = 1.49037, its = 8
i = 4196 (of 10192), d = 1.76137, its = 6
i = 4197 (of 10192), d = 10, its = 56
i = 4198 (of 10192), d = 10, its = 49
i = 4199 (of 10192), d = 10, its = 54
i = 4200 (of 10192), d = 10, its = 54
i = 4201 (of 10192), d = 10, its = 56
i = 4202 (of 10192), d = 10, its = 37
i = 4203 (of 10192), d = 2.49071, its = 5
i = 4204 (of 10192), d = 10, its = 54
i = 4205 (of 10192), d = 1.60102, its = 6
i = 4206 (of 10192), d = 1.50113, its = 6
i = 4207 (of 10192), d = 2.0939, its = 4
i = 4208 (of 10192), d = 10, its = 54
i = 4209 (of 10192), d = 0.630807, its = 18
i = 4210 (of 10192), d = 10, its = 54
i = 4211 (of 10192), d = 10, its = 55
i = 4212 (of 10192), d = 1.01865, its = 18
i = 4213 (of 10192), d = 2.63983, its = 5
i = 4214 (of 10192), d = 10, its = 53
i = 4215 (of 10192), d = 10, its = 55
i = 4216 (of 10192), d = 10, its = 49
i = 4217 (of 10192), d = 10, its = 54
i = 4218 (of 10192), d = 10, its = 54
i = 4219 (of 10192), d = 4.7192, its = 10
i = 4220 (of 10192), d = 1.72756, its = 5
i = 4221 (of 10192), d = 10, its = 39
i = 4222 (of 10192), d = 10, its = 50
i = 4223 (of 10192), d = 10, its = 42
i = 4224 (of 10192), d = 3.47791, its = 6
i = 4225 (of 10192), d = 10, its = 54
i = 4226 (of 10192), d = 10, its = 57
i = 4227 (of 10192), d = 10, its = 50
i = 4228 (of 10192), d = 10, its = 49
i = 4229 (of 10192), d = 10, its = 54
i = 4230 (of 10192), d = 10, its = 53
i = 4231 (of 10192), d = 10, its = 53
i = 4232 (of 10192), d = 10, its = 49
i = 4233 (of 10192), d = 1.7233, its = 6
i = 4234 (of 10192), d = 10, its = 41
i = 4235 (of 10192), d = 10, its = 52
i = 4236 (of 10192), d = 5.08209, its = 7
i = 4237 (of 10192), d = 2.58687, its = 6
i = 4238 (of 10192), d = 10, its = 57
i = 4239 (of 10192), d = 10, its = 37
i = 4240 (of 10192), d = 10, its = 37
i = 4241 (of 10192), d = 10, its = 39
i = 4242 (of 10192), d = 3.13963, its = 7
i = 4243 (of 10192), d = 1.49312, its = 7
i = 4244 (of 10192), d = 0.857433, its = 18
i = 4245 (of 10192), d = 10, its = 53
i = 4246 (of 10192), d = 2.09053, its = 4
i = 4247 (of 10192), d = 10, its = 44
i = 4248 (of 10192), d = 10, its = 55
i = 4249 (of 10192), d = 10, its = 40
i = 4250 (of 10192), d = 10, its = 43
i = 4251 (of 10192), d = 1.37646, its = 6
i = 4252 (of 10192), d = 10, its = 54
i = 4253 (of 10192), d = 10, its = 53
i = 4254 (of 10192), d = 10, its = 52
i = 4255 (of 10192), d = 10, its = 54
i = 4256 (of 10192), d = 10, its = 51
i = 4257 (of 10192), d = 2.00181, its = 2
i = 4258 (of 10192), d = 1.33395, its = 6
i = 4259 (of 10192), d = 2.35137, its = 5
i = 4260 (of 10192), d = 10, its = 56
i = 4261 (of 10192), d = 1.46816, its = 6
i = 4262 (of 10192), d = 10, its = 50
i = 4263 (of 10192), d = 10, its = 52
i = 4264 (of 10192), d = 10, its = 55
i = 4265 (of 10192), d = 10, its = 46
i = 4266 (of 10192), d = 2.36456, its = 5
i = 4267 (of 10192), d = 1.93111, its = 4
i = 4268 (of 10192), d = 1.28515, its = 6
i = 4269 (of 10192), d = 10, its = 42
i = 4270 (of 10192), d = 10, its = 52
i = 4271 (of 10192), d = 10, its = 54
i = 4272 (of 10192), d = 4.56105, its = 7
i = 4273 (of 10192), d = 10, its = 53
i = 4274 (of 10192), d = 10, its = 51
i = 4275 (of 10192), d = 10, its = 56
i = 4276 (of 10192), d = 10, its = 56
i = 4277 (of 10192), d = 2.52828, its = 5
i = 4278 (of 10192), d = 2.17539, its = 4
i = 4279 (of 10192), d = 10, its = 53
i = 4280 (of 10192), d = 10, its = 53
i = 4281 (of 10192), d = 10, its = 55
i = 4282 (of 10192), d = 10, its = 55
i = 4283 (of 10192), d = 10, its = 48
i = 4284 (of 10192), d = 10, its = 50
i = 4285 (of 10192), d = 10, its = 52
i = 4286 (of 10192), d = 10, its = 51
i = 4287 (of 10192), d = 10, its = 37
i = 4288 (of 10192), d = 10, its = 55
i = 4289 (of 10192), d = 10, its = 52
i = 4290 (of 10192), d = 10, its = 53
i = 4291 (of 10192), d = 10, its = 38
i = 4292 (of 10192), d = 10, its = 54
i = 4293 (of 10192), d = 10, its = 50
i = 4294 (of 10192), d = 2.29514, its = 5
i = 4295 (of 10192), d = 10, its = 57
i = 4296 (of 10192), d = 10, its = 37
i = 4297 (of 10192), d = 2.30148, its = 5
i = 4298 (of 10192), d = 1.57277, its = 6
i = 4299 (of 10192), d = 10, its = 52
i = 4300 (of 10192), d = 10, its = 40
i = 4301 (of 10192), d = 10, its = 40
i = 4302 (of 10192), d = 10, its = 51
i = 4303 (of 10192), d = 10, its = 37
i = 4304 (of 10192), d = 10, its = 53
i = 4305 (of 10192), d = 1.43828, its = 7
i = 4306 (of 10192), d = 2.4734, its = 5
i = 4307 (of 10192), d = 10, its = 51
i = 4308 (of 10192), d = 10, its = 52
i = 4309 (of 10192), d = 10, its = 52
i = 4310 (of 10192), d = 10, its = 38
i = 4311 (of 10192), d = 10, its = 52
i = 4312 (of 10192), d = 3.37761, its = 6
i = 4313 (of 10192), d = 2.82952, its = 6
i = 4314 (of 10192), d = 10, its = 54
i = 4315 (of 10192), d = 10, its = 37
i = 4316 (of 10192), d = 1.49902, its = 26
i = 4317 (of 10192), d = 10, its = 54
i = 4318 (of 10192), d = 10, its = 50
i = 4319 (of 10192), d = 10, its = 57
i = 4320 (of 10192), d = 10, its = 40
i = 4321 (of 10192), d = 10, its = 52
i = 4322 (of 10192), d = 10, its = 50
i = 4323 (of 10192), d = 10, its = 53
i = 4324 (of 10192), d = 10, its = 55
i = 4325 (of 10192), d = 1.85177, its = 4
i = 4326 (of 10192), d = 10, its = 52
i = 4327 (of 10192), d = 10, its = 53
i = 4328 (of 10192), d = 10, its = 57
i = 4329 (of 10192), d = 10, its = 53
i = 4330 (of 10192), d = 10, its = 51
i = 4331 (of 10192), d = 1.17971, its = 20
i = 4332 (of 10192), d = 1.43417, its = 22
i = 4333 (of 10192), d = 2.62105, its = 5
i = 4334 (of 10192), d = 10, its = 37
i = 4335 (of 10192), d = 2.70936, its = 5
i = 4336 (of 10192), d = 1.99105, its = 3
i = 4337 (of 10192), d = 10, its = 41
i = 4338 (of 10192), d = 1.91099, its = 4
i = 4339 (of 10192), d = 10, its = 41
i = 4340 (of 10192), d = 10, its = 53
i = 4341 (of 10192), d = 10, its = 48
i = 4342 (of 10192), d = 10, its = 41
i = 4343 (of 10192), d = 1.68943, its = 5
i = 4344 (of 10192), d = 10, its = 41
i = 4345 (of 10192), d = 10, its = 42
i = 4346 (of 10192), d = 10, its = 55
i = 4347 (of 10192), d = 10, its = 43
i = 4348 (of 10192), d = 10, its = 56
i = 4349 (of 10192), d = 10, its = 37
i = 4350 (of 10192), d = 10, its = 37
i = 4351 (of 10192), d = 10, its = 54
i = 4352 (of 10192), d = 10, its = 39
i = 4353 (of 10192), d = 10, its = 50
i = 4354 (of 10192), d = 10, its = 53
i = 4355 (of 10192), d = 2.01957, its = 3
i = 4356 (of 10192), d = 10, its = 37
i = 4357 (of 10192), d = 10, its = 37
i = 4358 (of 10192), d = 2.31487, its = 4
i = 4359 (of 10192), d = 1.85199, its = 4
i = 4360 (of 10192), d = 4.24639, its = 7
i = 4361 (of 10192), d = 10, its = 52
i = 4362 (of 10192), d = 2.79103, its = 5
i = 4363 (of 10192), d = 10, its = 54
i = 4364 (of 10192), d = 9.07921, its = 9
i = 4365 (of 10192), d = 10, its = 57
i = 4366 (of 10192), d = 2.34587, its = 5
i = 4367 (of 10192), d = 3.55268, its = 7
i = 4368 (of 10192), d = 10, its = 51
i = 4369 (of 10192), d = 10, its = 45
i = 4370 (of 10192), d = 10, its = 42
i = 4371 (of 10192), d = 10, its = 53
i = 4372 (of 10192), d = 1.68186, its = 5
i = 4373 (of 10192), d = 10, its = 39
i = 4374 (of 10192), d = 10, its = 54
i = 4375 (of 10192), d = 10, its = 37
i = 4376 (of 10192), d = 10, its = 37
i = 4377 (of 10192), d = 10, its = 37
i = 4378 (of 10192), d = 10, its = 52
i = 4379 (of 10192), d = 10, its = 53
i = 4380 (of 10192), d = 10, its = 40
i = 4381 (of 10192), d = 2.50412, its = 5
i = 4382 (of 10192), d = 10, its = 53
i = 4383 (of 10192), d = 2.66327, its = 5
i = 4384 (of 10192), d = 10, its = 51
i = 4385 (of 10192), d = 10, its = 55
i = 4386 (of 10192), d = 10, its = 37
i = 4387 (of 10192), d = 10, its = 54
i = 4388 (of 10192), d = 10, its = 52
i = 4389 (of 10192), d = 10, its = 37
i = 4390 (of 10192), d = 10, its = 37
i = 4391 (of 10192), d = 3.57384, its = 7
i = 4392 (of 10192), d = 10, its = 52
i = 4393 (of 10192), d = 10, its = 51
i = 4394 (of 10192), d = 10, its = 37
i = 4395 (of 10192), d = 10, its = 37
i = 4396 (of 10192), d = 10, its = 52
i = 4397 (of 10192), d = 10, its = 53
i = 4398 (of 10192), d = 1.76711, its = 6
i = 4399 (of 10192), d = 10, its = 52
i = 4400 (of 10192), d = 10, its = 50
i = 4401 (of 10192), d = 10, its = 54
i = 4402 (of 10192), d = 10, its = 39
i = 4403 (of 10192), d = 10, its = 52
i = 4404 (of 10192), d = 2.11999, its = 4
i = 4405 (of 10192), d = 0.906228, its = 20
i = 4406 (of 10192), d = 10, its = 38
i = 4407 (of 10192), d = 10, its = 40
i = 4408 (of 10192), d = 10, its = 55
i = 4409 (of 10192), d = 10, its = 50
i = 4410 (of 10192), d = 10, its = 37
i = 4411 (of 10192), d = 10, its = 56
i = 4412 (of 10192), d = 0.799715, its = 16
i = 4413 (of 10192), d = 10, its = 49
i = 4414 (of 10192), d = 10, its = 54
i = 4415 (of 10192), d = 10, its = 53
i = 4416 (of 10192), d = 10, its = 55
i = 4417 (of 10192), d = 10, its = 50
i = 4418 (of 10192), d = 10, its = 50
i = 4419 (of 10192), d = 10, its = 48
i = 4420 (of 10192), d = 10, its = 55
i = 4421 (of 10192), d = 10, its = 42
i = 4422 (of 10192), d = 1.18917, its = 7
i = 4423 (of 10192), d = 10, its = 40
i = 4424 (of 10192), d = 10, its = 38
i = 4425 (of 10192), d = 1.5417, its = 8
i = 4426 (of 10192), d = 10, its = 53
i = 4427 (of 10192), d = 10, its = 53
i = 4428 (of 10192), d = 10, its = 50
i = 4429 (of 10192), d = 10, its = 55
i = 4430 (of 10192), d = 1.23286, its = 6
i = 4431 (of 10192), d = 10, its = 52
i = 4432 (of 10192), d = 2.07533, its = 3
i = 4433 (of 10192), d = 0.967625, its = 21
i = 4434 (of 10192), d = 2.83331, its = 5
i = 4435 (of 10192), d = 10, its = 52
i = 4436 (of 10192), d = 10, its = 37
i = 4437 (of 10192), d = 10, its = 55
i = 4438 (of 10192), d = 10, its = 42
i = 4439 (of 10192), d = 10, its = 37
i = 4440 (of 10192), d = 10, its = 37
i = 4441 (of 10192), d = 1.19154, its = 21
i = 4442 (of 10192), d = 10, its = 54
i = 4443 (of 10192), d = 2.38758, its = 5
i = 4444 (of 10192), d = 10, its = 50
i = 4445 (of 10192), d = 10, its = 37
i = 4446 (of 10192), d = 10, its = 50
i = 4447 (of 10192), d = 1.0128, its = 20
i = 4448 (of 10192), d = 10, its = 39
i = 4449 (of 10192), d = 10, its = 49
i = 4450 (of 10192), d = 0.576139, its = 17
i = 4451 (of 10192), d = 2.23872, its = 4
i = 4452 (of 10192), d = 10, its = 40
i = 4453 (of 10192), d = 10, its = 51
i = 4454 (of 10192), d = 1.82975, its = 5
i = 4455 (of 10192), d = 1.553, its = 6
i = 4456 (of 10192), d = 10, its = 43
i = 4457 (of 10192), d = 10, its = 37
i = 4458 (of 10192), d = 10, its = 50
i = 4459 (of 10192), d = 3.0696, its = 6
i = 4460 (of 10192), d = 5.11153, its = 7
i = 4461 (of 10192), d = 10, its = 50
i = 4462 (of 10192), d = 10, its = 53
i = 4463 (of 10192), d = 10, its = 42
i = 4464 (of 10192), d = 10, its = 37
i = 4465 (of 10192), d = 10, its = 55
i = 4466 (of 10192), d = 10, its = 37
i = 4467 (of 10192), d = 10, its = 56
i = 4468 (of 10192), d = 10, its = 52
i = 4469 (of 10192), d = 10, its = 40
i = 4470 (of 10192), d = 10, its = 51
i = 4471 (of 10192), d = 10, its = 50
i = 4472 (of 10192), d = 2.68604, its = 6
i = 4473 (of 10192), d = 10, its = 41
i = 4474 (of 10192), d = 10, its = 51
i = 4475 (of 10192), d = 10, its = 52
i = 4476 (of 10192), d = 0.880936, its = 18
i = 4477 (of 10192), d = 10, its = 39
i = 4478 (of 10192), d = 1.49252, its = 6
i = 4479 (of 10192), d = 5.00561, its = 7
i = 4480 (of 10192), d = 10, its = 52
i = 4481 (of 10192), d = 2.18938, its = 4
i = 4482 (of 10192), d = 10, its = 53
i = 4483 (of 10192), d = 2.77095, its = 6
i = 4484 (of 10192), d = 10, its = 52
i = 4485 (of 10192), d = 4.66864, its = 7
i = 4486 (of 10192), d = 7.69416, its = 10
i = 4487 (of 10192), d = 3.29946, its = 6
i = 4488 (of 10192), d = 8.41231, its = 22
i = 4489 (of 10192), d = 10, its = 53
i = 4490 (of 10192), d = 3.31014, its = 6
i = 4491 (of 10192), d = 10, its = 51
i = 4492 (of 10192), d = 2.46991, its = 6
i = 4493 (of 10192), d = 10, its = 37
i = 4494 (of 10192), d = 10, its = 37
i = 4495 (of 10192), d = 10, its = 37
i = 4496 (of 10192), d = 10, its = 56
i = 4497 (of 10192), d = 0.955958, its = 18
i = 4498 (of 10192), d = 10, its = 57
i = 4499 (of 10192), d = 10, its = 39
i = 4500 (of 10192), d = 1.93737, its = 4
i = 4501 (of 10192), d = 10, its = 54
i = 4502 (of 10192), d = 4.97521, its = 7
i = 4503 (of 10192), d = 10, its = 51
i = 4504 (of 10192), d = 10, its = 40
i = 4505 (of 10192), d = 1.28335, its = 6
i = 4506 (of 10192), d = 10, its = 41
i = 4507 (of 10192), d = 10, its = 50
i = 4508 (of 10192), d = 1.52656, its = 6
i = 4509 (of 10192), d = 3.849, its = 7
i = 4510 (of 10192), d = 10, its = 56
i = 4511 (of 10192), d = 10, its = 56
i = 4512 (of 10192), d = 10, its = 51
i = 4513 (of 10192), d = 1.20046, its = 6
i = 4514 (of 10192), d = 2.76802, its = 6
i = 4515 (of 10192), d = 10, its = 51
i = 4516 (of 10192), d = 10, its = 51
i = 4517 (of 10192), d = 10, its = 40
i = 4518 (of 10192), d = 10, its = 51
i = 4519 (of 10192), d = 2.66426, its = 5
i = 4520 (of 10192), d = 2.5792, its = 5
i = 4521 (of 10192), d = 10, its = 52
i = 4522 (of 10192), d = 10, its = 56
i = 4523 (of 10192), d = 10, its = 53
i = 4524 (of 10192), d = 1.42757, its = 7
i = 4525 (of 10192), d = 1.11067, its = 20
i = 4526 (of 10192), d = 10, its = 54
i = 4527 (of 10192), d = 10, its = 48
i = 4528 (of 10192), d = 10, its = 50
i = 4529 (of 10192), d = 3.47994, its = 6
i = 4530 (of 10192), d = 1.09598, its = 23
i = 4531 (of 10192), d = 10, its = 55
i = 4532 (of 10192), d = 10, its = 48
i = 4533 (of 10192), d = 10, its = 52
i = 4534 (of 10192), d = 10, its = 55
i = 4535 (of 10192), d = 10, its = 42
i = 4536 (of 10192), d = 10, its = 55
i = 4537 (of 10192), d = 3.03369, its = 6
i = 4538 (of 10192), d = 10, its = 40
i = 4539 (of 10192), d = 10, its = 37
i = 4540 (of 10192), d = 10, its = 46
i = 4541 (of 10192), d = 10, its = 57
i = 4542 (of 10192), d = 10, its = 37
i = 4543 (of 10192), d = 10, its = 52
i = 4544 (of 10192), d = 10, its = 53
i = 4545 (of 10192), d = 10, its = 49
i = 4546 (of 10192), d = 10, its = 51
i = 4547 (of 10192), d = 10, its = 53
i = 4548 (of 10192), d = 10, its = 52
i = 4549 (of 10192), d = 2.82907, its = 6
i = 4550 (of 10192), d = 10, its = 51
i = 4551 (of 10192), d = 1.54646, its = 6
i = 4552 (of 10192), d = 10, its = 41
i = 4553 (of 10192), d = 1.15374, its = 18
i = 4554 (of 10192), d = 10, its = 40
i = 4555 (of 10192), d = 10, its = 52
i = 4556 (of 10192), d = 2.44439, its = 5
i = 4557 (of 10192), d = 10, its = 37
i = 4558 (of 10192), d = 3.01538, its = 6
i = 4559 (of 10192), d = 10, its = 48
i = 4560 (of 10192), d = 1.76277, its = 5
i = 4561 (of 10192), d = 1.52744, its = 8
i = 4562 (of 10192), d = 10, its = 37
i = 4563 (of 10192), d = 0.815417, its = 18
i = 4564 (of 10192), d = 10, its = 40
i = 4565 (of 10192), d = 10, its = 39
i = 4566 (of 10192), d = 2.17817, its = 4
i = 4567 (of 10192), d = 10, its = 53
i = 4568 (of 10192), d = 10, its = 39
i = 4569 (of 10192), d = 1.91459, its = 4
i = 4570 (of 10192), d = 10, its = 37
i = 4571 (of 10192), d = 10, its = 54
i = 4572 (of 10192), d = 10, its = 51
i = 4573 (of 10192), d = 10, its = 37
i = 4574 (of 10192), d = 10, its = 42
i = 4575 (of 10192), d = 1.89978, its = 4
i = 4576 (of 10192), d = 10, its = 37
i = 4577 (of 10192), d = 10, its = 50
i = 4578 (of 10192), d = 10, its = 51
i = 4579 (of 10192), d = 10, its = 53
i = 4580 (of 10192), d = 10, its = 40
i = 4581 (of 10192), d = 10, its = 51
i = 4582 (of 10192), d = 10, its = 57
i = 4583 (of 10192), d = 2.06618, its = 4
i = 4584 (of 10192), d = 1.2769, its = 6
i = 4585 (of 10192), d = 2.89393, its = 6
i = 4586 (of 10192), d = 1.59471, its = 6
i = 4587 (of 10192), d = 2.25316, its = 4
i = 4588 (of 10192), d = 2.68993, its = 6
i = 4589 (of 10192), d = 1.91925, its = 4
i = 4590 (of 10192), d = 10, its = 53
i = 4591 (of 10192), d = 1.16318, its = 8
i = 4592 (of 10192), d = 10, its = 49
i = 4593 (of 10192), d = 2.62618, its = 6
i = 4594 (of 10192), d = 10, its = 48
i = 4595 (of 10192), d = 3.15815, its = 6
i = 4596 (of 10192), d = 10, its = 53
i = 4597 (of 10192), d = 10, its = 54
i = 4598 (of 10192), d = 10, its = 45
i = 4599 (of 10192), d = 1.0512, its = 29
i = 4600 (of 10192), d = 0.83599, its = 20
i = 4601 (of 10192), d = 10, its = 55
i = 4602 (of 10192), d = 1.43192, its = 6
i = 4603 (of 10192), d = 10, its = 53
i = 4604 (of 10192), d = 10, its = 54
i = 4605 (of 10192), d = 10, its = 53
i = 4606 (of 10192), d = 10, its = 53
i = 4607 (of 10192), d = 2.15517, its = 4
i = 4608 (of 10192), d = 2.08184, its = 4
i = 4609 (of 10192), d = 10, its = 39
i = 4610 (of 10192), d = 1.89015, its = 4
i = 4611 (of 10192), d = 10, its = 51
i = 4612 (of 10192), d = 10, its = 52
i = 4613 (of 10192), d = 10, its = 55
i = 4614 (of 10192), d = 10, its = 47
i = 4615 (of 10192), d = 10, its = 53
i = 4616 (of 10192), d = 7.2694, its = 8
i = 4617 (of 10192), d = 1.85077, its = 4
i = 4618 (of 10192), d = 10, its = 51
i = 4619 (of 10192), d = 10, its = 37
i = 4620 (of 10192), d = 10, its = 39
i = 4621 (of 10192), d = 10, its = 53
i = 4622 (of 10192), d = 10, its = 52
i = 4623 (of 10192), d = 10, its = 41
i = 4624 (of 10192), d = 2.23518, its = 4
i = 4625 (of 10192), d = 1.02387, its = 16
i = 4626 (of 10192), d = 10, its = 42
i = 4627 (of 10192), d = 10, its = 45
i = 4628 (of 10192), d = 1.18395, its = 21
i = 4629 (of 10192), d = 10, its = 55
i = 4630 (of 10192), d = 10, its = 37
i = 4631 (of 10192), d = 10, its = 48
i = 4632 (of 10192), d = 10, its = 37
i = 4633 (of 10192), d = 10, its = 56
i = 4634 (of 10192), d = 10, its = 53
i = 4635 (of 10192), d = 10, its = 37
i = 4636 (of 10192), d = 10, its = 58
i = 4637 (of 10192), d = 3.60136, its = 7
i = 4638 (of 10192), d = 10, its = 51
i = 4639 (of 10192), d = 10, its = 52
i = 4640 (of 10192), d = 10, its = 40
i = 4641 (of 10192), d = 10, its = 39
i = 4642 (of 10192), d = 3.3995, its = 7
i = 4643 (of 10192), d = 10, its = 51
i = 4644 (of 10192), d = 10, its = 37
i = 4645 (of 10192), d = 1.83486, its = 4
i = 4646 (of 10192), d = 3.05204, its = 6
i = 4647 (of 10192), d = 10, its = 37
i = 4648 (of 10192), d = 10, its = 52
i = 4649 (of 10192), d = 10, its = 41
i = 4650 (of 10192), d = 10, its = 52
i = 4651 (of 10192), d = 1.43867, its = 6
i = 4652 (of 10192), d = 10, its = 51
i = 4653 (of 10192), d = 10, its = 51
i = 4654 (of 10192), d = 1.75508, its = 5
i = 4655 (of 10192), d = 10, its = 56
i = 4656 (of 10192), d = 10, its = 39
i = 4657 (of 10192), d = 10, its = 50
i = 4658 (of 10192), d = 2.10432, its = 4
i = 4659 (of 10192), d = 10, its = 55
i = 4660 (of 10192), d = 10, its = 51
i = 4661 (of 10192), d = 10, its = 56
i = 4662 (of 10192), d = 10, its = 47
i = 4663 (of 10192), d = 10, its = 54
i = 4664 (of 10192), d = 10, its = 55
i = 4665 (of 10192), d = 1.75688, its = 4
i = 4666 (of 10192), d = 10, its = 51
i = 4667 (of 10192), d = 10, its = 37
i = 4668 (of 10192), d = 3.03029, its = 6
i = 4669 (of 10192), d = 10, its = 53
i = 4670 (of 10192), d = 10, its = 55
i = 4671 (of 10192), d = 2.5341, its = 5
i = 4672 (of 10192), d = 10, its = 44
i = 4673 (of 10192), d = 10, its = 53
i = 4674 (of 10192), d = 10, its = 52
i = 4675 (of 10192), d = 10, its = 52
i = 4676 (of 10192), d = 10, its = 54
i = 4677 (of 10192), d = 10, its = 37
i = 4678 (of 10192), d = 10, its = 51
i = 4679 (of 10192), d = 2.9277, its = 6
i = 4680 (of 10192), d = 10, its = 49
i = 4681 (of 10192), d = 10, its = 58
i = 4682 (of 10192), d = 2.61729, its = 6
i = 4683 (of 10192), d = 0.835429, its = 16
i = 4684 (of 10192), d = 10, its = 52
i = 4685 (of 10192), d = 0.880936, its = 18
i = 4686 (of 10192), d = 10, its = 55
i = 4687 (of 10192), d = 10, its = 37
i = 4688 (of 10192), d = 1.62361, its = 6
i = 4689 (of 10192), d = 10, its = 56
i = 4690 (of 10192), d = 2.99085, its = 6
i = 4691 (of 10192), d = 1.09964, its = 18
i = 4692 (of 10192), d = 10, its = 56
i = 4693 (of 10192), d = 10, its = 53
i = 4694 (of 10192), d = 10, its = 37
i = 4695 (of 10192), d = 1.82572, its = 5
i = 4696 (of 10192), d = 10, its = 37
i = 4697 (of 10192), d = 10, its = 53
i = 4698 (of 10192), d = 10, its = 53
i = 4699 (of 10192), d = 1.97612, its = 3
i = 4700 (of 10192), d = 10, its = 49
i = 4701 (of 10192), d = 1.39494, its = 6
i = 4702 (of 10192), d = 2.78568, its = 6
i = 4703 (of 10192), d = 0.98232, its = 21
i = 4704 (of 10192), d = 1.15833, its = 18
i = 4705 (of 10192), d = 2.03367, its = 3
i = 4706 (of 10192), d = 10, its = 52
i = 4707 (of 10192), d = 10, its = 41
i = 4708 (of 10192), d = 10, its = 37
i = 4709 (of 10192), d = 10, its = 37
i = 4710 (of 10192), d = 10, its = 51
i = 4711 (of 10192), d = 10, its = 50
i = 4712 (of 10192), d = 10, its = 55
i = 4713 (of 10192), d = 10, its = 55
i = 4714 (of 10192), d = 1.63126, its = 6
i = 4715 (of 10192), d = 1.13022, its = 25
i = 4716 (of 10192), d = 1.17541, its = 7
i = 4717 (of 10192), d = 10, its = 37
i = 4718 (of 10192), d = 10, its = 54
i = 4719 (of 10192), d = 10, its = 55
i = 4720 (of 10192), d = 1.60242, its = 6
i = 4721 (of 10192), d = 10, its = 52
i = 4722 (of 10192), d = 10, its = 50
i = 4723 (of 10192), d = 10, its = 38
i = 4724 (of 10192), d = 10, its = 37
i = 4725 (of 10192), d = 10, its = 37
i = 4726 (of 10192), d = 10, its = 52
i = 4727 (of 10192), d = 10, its = 40
i = 4728 (of 10192), d = 10, its = 52
i = 4729 (of 10192), d = 1.17494, its = 19
i = 4730 (of 10192), d = 10, its = 55
i = 4731 (of 10192), d = 10, its = 37
i = 4732 (of 10192), d = 10, its = 49
i = 4733 (of 10192), d = 2.00582, its = 3
i = 4734 (of 10192), d = 2.39746, its = 5
i = 4735 (of 10192), d = 5.08176, its = 8
i = 4736 (of 10192), d = 10, its = 58
i = 4737 (of 10192), d = 0.931415, its = 20
i = 4738 (of 10192), d = 10, its = 37
i = 4739 (of 10192), d = 10, its = 55
i = 4740 (of 10192), d = 10, its = 52
i = 4741 (of 10192), d = 10, its = 49
i = 4742 (of 10192), d = 3.01094, its = 6
i = 4743 (of 10192), d = 10, its = 52
i = 4744 (of 10192), d = 3.99248, its = 7
i = 4745 (of 10192), d = 10, its = 53
i = 4746 (of 10192), d = 4.55583, its = 8
i = 4747 (of 10192), d = 10, its = 51
i = 4748 (of 10192), d = 10, its = 52
i = 4749 (of 10192), d = 10, its = 37
i = 4750 (of 10192), d = 10, its = 56
i = 4751 (of 10192), d = 10, its = 37
i = 4752 (of 10192), d = 10, its = 52
i = 4753 (of 10192), d = 2.09431, its = 4
i = 4754 (of 10192), d = 1.03448, its = 17
i = 4755 (of 10192), d = 10, its = 51
i = 4756 (of 10192), d = 10, its = 52
i = 4757 (of 10192), d = 10, its = 53
i = 4758 (of 10192), d = 10, its = 42
i = 4759 (of 10192), d = 7.23208, its = 8
i = 4760 (of 10192), d = 3.03029, its = 6
i = 4761 (of 10192), d = 2.56013, its = 6
i = 4762 (of 10192), d = 10, its = 44
i = 4763 (of 10192), d = 3.5182, its = 6
i = 4764 (of 10192), d = 10, its = 40
i = 4765 (of 10192), d = 10, its = 54
i = 4766 (of 10192), d = 10, its = 55
i = 4767 (of 10192), d = 0.939714, its = 21
i = 4768 (of 10192), d = 2.60252, its = 5
i = 4769 (of 10192), d = 10, its = 51
i = 4770 (of 10192), d = 10, its = 48
i = 4771 (of 10192), d = 10, its = 48
i = 4772 (of 10192), d = 10, its = 40
i = 4773 (of 10192), d = 2.48912, its = 5
i = 4774 (of 10192), d = 10, its = 37
i = 4775 (of 10192), d = 10, its = 49
i = 4776 (of 10192), d = 1.78783, its = 5
i = 4777 (of 10192), d = 2.79534, its = 5
i = 4778 (of 10192), d = 10, its = 37
i = 4779 (of 10192), d = 10, its = 49
i = 4780 (of 10192), d = 1.88546, its = 4
i = 4781 (of 10192), d = 1.88998, its = 4
i = 4782 (of 10192), d = 10, its = 48
i = 4783 (of 10192), d = 2.21536, its = 4
i = 4784 (of 10192), d = 10, its = 52
i = 4785 (of 10192), d = 1.9531, its = 3
i = 4786 (of 10192), d = 10, its = 55
i = 4787 (of 10192), d = 10, its = 54
i = 4788 (of 10192), d = 7.74457, its = 7
i = 4789 (of 10192), d = 4.47202, its = 7
i = 4790 (of 10192), d = 10, its = 54
i = 4791 (of 10192), d = 10, its = 55
i = 4792 (of 10192), d = 10, its = 49
i = 4793 (of 10192), d = 2.0331, its = 3
i = 4794 (of 10192), d = 10, its = 53
i = 4795 (of 10192), d = 1.14537, its = 24
i = 4796 (of 10192), d = 10, its = 52
i = 4797 (of 10192), d = 1.87689, its = 4
i = 4798 (of 10192), d = 10, its = 40
i = 4799 (of 10192), d = 1.52744, its = 8
i = 4800 (of 10192), d = 10, its = 50
i = 4801 (of 10192), d = 2.79152, its = 5
i = 4802 (of 10192), d = 1.2716, its = 19
i = 4803 (of 10192), d = 2.96947, its = 6
i = 4804 (of 10192), d = 1.93541, its = 4
i = 4805 (of 10192), d = 7.67946, its = 10
i = 4806 (of 10192), d = 10, its = 52
i = 4807 (of 10192), d = 10, its = 49
i = 4808 (of 10192), d = 10, its = 37
i = 4809 (of 10192), d = 3.27536, its = 6
i = 4810 (of 10192), d = 10, its = 56
i = 4811 (of 10192), d = 10, its = 53
i = 4812 (of 10192), d = 0.935595, its = 25
i = 4813 (of 10192), d = 2.44943, its = 5
i = 4814 (of 10192), d = 10, its = 52
i = 4815 (of 10192), d = 10, its = 50
i = 4816 (of 10192), d = 1.34834, its = 21
i = 4817 (of 10192), d = 10, its = 52
i = 4818 (of 10192), d = 10, its = 53
i = 4819 (of 10192), d = 1.0422, its = 24
i = 4820 (of 10192), d = 10, its = 53
i = 4821 (of 10192), d = 1.28539, its = 6
i = 4822 (of 10192), d = 10, its = 49
i = 4823 (of 10192), d = 10, its = 37
i = 4824 (of 10192), d = 0.798261, its = 16
i = 4825 (of 10192), d = 10, its = 54
i = 4826 (of 10192), d = 10, its = 52
i = 4827 (of 10192), d = 10, its = 52
i = 4828 (of 10192), d = 10, its = 52
i = 4829 (of 10192), d = 10, its = 39
i = 4830 (of 10192), d = 10, its = 42
i = 4831 (of 10192), d = 10, its = 37
i = 4832 (of 10192), d = 1.65866, its = 6
i = 4833 (of 10192), d = 10, its = 51
i = 4834 (of 10192), d = 10, its = 52
i = 4835 (of 10192), d = 10, its = 51
i = 4836 (of 10192), d = 10, its = 52
i = 4837 (of 10192), d = 10, its = 38
i = 4838 (of 10192), d = 9.06353, its = 9
i = 4839 (of 10192), d = 2.5155, its = 5
i = 4840 (of 10192), d = 10, its = 49
i = 4841 (of 10192), d = 10, its = 53
i = 4842 (of 10192), d = 10, its = 53
i = 4843 (of 10192), d = 1.14633, its = 21
i = 4844 (of 10192), d = 10, its = 52
i = 4845 (of 10192), d = 10, its = 40
i = 4846 (of 10192), d = 10, its = 53
i = 4847 (of 10192), d = 10, its = 37
i = 4848 (of 10192), d = 2.36613, its = 5
i = 4849 (of 10192), d = 10, its = 51
i = 4850 (of 10192), d = 10, its = 53
i = 4851 (of 10192), d = 10, its = 59
i = 4852 (of 10192), d = 1.64821, its = 6
i = 4853 (of 10192), d = 10, its = 54
i = 4854 (of 10192), d = 10, its = 55
i = 4855 (of 10192), d = 10, its = 50
i = 4856 (of 10192), d = 10, its = 51
i = 4857 (of 10192), d = 10, its = 52
i = 4858 (of 10192), d = 10, its = 43
i = 4859 (of 10192), d = 10, its = 52
i = 4860 (of 10192), d = 10, its = 52
i = 4861 (of 10192), d = 2.79294, its = 6
i = 4862 (of 10192), d = 2.2285, its = 4
i = 4863 (of 10192), d = 10, its = 40
i = 4864 (of 10192), d = 10, its = 53
i = 4865 (of 10192), d = 10, its = 53
i = 4866 (of 10192), d = 2.12918, its = 4
i = 4867 (of 10192), d = 10, its = 55
i = 4868 (of 10192), d = 10, its = 51
i = 4869 (of 10192), d = 10, its = 55
i = 4870 (of 10192), d = 10, its = 54
i = 4871 (of 10192), d = 10, its = 52
i = 4872 (of 10192), d = 10, its = 51
i = 4873 (of 10192), d = 10, its = 37
i = 4874 (of 10192), d = 10, its = 37
i = 4875 (of 10192), d = 1.02642, its = 18
i = 4876 (of 10192), d = 1.20499, its = 8
i = 4877 (of 10192), d = 2.11718, its = 4
i = 4878 (of 10192), d = 10, its = 52
i = 4879 (of 10192), d = 1.8459, its = 4
i = 4880 (of 10192), d = 10, its = 55
i = 4881 (of 10192), d = 10, its = 52
i = 4882 (of 10192), d = 10, its = 41
i = 4883 (of 10192), d = 10, its = 56
i = 4884 (of 10192), d = 1.10702, its = 17
i = 4885 (of 10192), d = 10, its = 55
i = 4886 (of 10192), d = 10, its = 53
i = 4887 (of 10192), d = 10, its = 51
i = 4888 (of 10192), d = 0.910093, its = 18
i = 4889 (of 10192), d = 10, its = 52
i = 4890 (of 10192), d = 1.91996, its = 4
i = 4891 (of 10192), d = 10, its = 40
i = 4892 (of 10192), d = 10, its = 56
i = 4893 (of 10192), d = 2.95282, its = 5
i = 4894 (of 10192), d = 1.04422, its = 19
i = 4895 (of 10192), d = 10, its = 56
i = 4896 (of 10192), d = 10, its = 51
i = 4897 (of 10192), d = 10, its = 41
i = 4898 (of 10192), d = 10, its = 54
i = 4899 (of 10192), d = 2.90702, its = 6
i = 4900 (of 10192), d = 10, its = 51
i = 4901 (of 10192), d = 10, its = 37
i = 4902 (of 10192), d = 5.34273, its = 7
i = 4903 (of 10192), d = 10, its = 40
i = 4904 (of 10192), d = 10, its = 52
i = 4905 (of 10192), d = 10, its = 51
i = 4906 (of 10192), d = 10, its = 57
i = 4907 (of 10192), d = 10, its = 50
i = 4908 (of 10192), d = 10, its = 37
i = 4909 (of 10192), d = 10, its = 38
i = 4910 (of 10192), d = 10, its = 40
i = 4911 (of 10192), d = 10, its = 39
i = 4912 (of 10192), d = 10, its = 50
i = 4913 (of 10192), d = 0.822008, its = 18
i = 4914 (of 10192), d = 10, its = 51
i = 4915 (of 10192), d = 10, its = 53
i = 4916 (of 10192), d = 10, its = 57
i = 4917 (of 10192), d = 10, its = 50
i = 4918 (of 10192), d = 10, its = 37
i = 4919 (of 10192), d = 1.89814, its = 4
i = 4920 (of 10192), d = 10, its = 54
i = 4921 (of 10192), d = 2.505, its = 5
i = 4922 (of 10192), d = 10, its = 54
i = 4923 (of 10192), d = 10, its = 49
i = 4924 (of 10192), d = 1.57233, its = 6
i = 4925 (of 10192), d = 10, its = 55
i = 4926 (of 10192), d = 1.09115, its = 17
i = 4927 (of 10192), d = 10, its = 57
i = 4928 (of 10192), d = 1.88432, its = 4
i = 4929 (of 10192), d = 2.48602, its = 5
i = 4930 (of 10192), d = 10, its = 52
i = 4931 (of 10192), d = 10, its = 56
i = 4932 (of 10192), d = 10, its = 37
i = 4933 (of 10192), d = 10, its = 37
i = 4934 (of 10192), d = 10, its = 48
i = 4935 (of 10192), d = 2.06242, its = 4
i = 4936 (of 10192), d = 10, its = 41
i = 4937 (of 10192), d = 10, its = 52
i = 4938 (of 10192), d = 2.02758, its = 3
i = 4939 (of 10192), d = 10, its = 51
i = 4940 (of 10192), d = 10, its = 57
i = 4941 (of 10192), d = 10, its = 51
i = 4942 (of 10192), d = 10, its = 47
i = 4943 (of 10192), d = 10, its = 51
i = 4944 (of 10192), d = 10, its = 49
i = 4945 (of 10192), d = 3.56812, its = 7
i = 4946 (of 10192), d = 10, its = 52
i = 4947 (of 10192), d = 10, its = 54
i = 4948 (of 10192), d = 0.654703, its = 17
i = 4949 (of 10192), d = 6.77795, its = 9
i = 4950 (of 10192), d = 2.88894, its = 6
i = 4951 (of 10192), d = 10, its = 39
i = 4952 (of 10192), d = 3.92548, its = 6
i = 4953 (of 10192), d = 1.7821, its = 5
i = 4954 (of 10192), d = 10, its = 41
i = 4955 (of 10192), d = 10, its = 39
i = 4956 (of 10192), d = 10, its = 42
i = 4957 (of 10192), d = 10, its = 52
i = 4958 (of 10192), d = 2.45165, its = 5
i = 4959 (of 10192), d = 10, its = 51
i = 4960 (of 10192), d = 10, its = 50
i = 4961 (of 10192), d = 10, its = 42
i = 4962 (of 10192), d = 10, its = 52
i = 4963 (of 10192), d = 10, its = 42
i = 4964 (of 10192), d = 10, its = 50
i = 4965 (of 10192), d = 10, its = 40
i = 4966 (of 10192), d = 10, its = 55
i = 4967 (of 10192), d = 10, its = 54
i = 4968 (of 10192), d = 10, its = 53
i = 4969 (of 10192), d = 10, its = 42
i = 4970 (of 10192), d = 10, its = 49
i = 4971 (of 10192), d = 10, its = 53
i = 4972 (of 10192), d = 10, its = 54
i = 4973 (of 10192), d = 2.59751, its = 5
i = 4974 (of 10192), d = 1.34646, its = 8
i = 4975 (of 10192), d = 10, its = 50
i = 4976 (of 10192), d = 10, its = 56
i = 4977 (of 10192), d = 2.61624, its = 5
i = 4978 (of 10192), d = 5.87139, its = 8
i = 4979 (of 10192), d = 2.04941, its = 4
i = 4980 (of 10192), d = 10, its = 40
i = 4981 (of 10192), d = 10, its = 58
i = 4982 (of 10192), d = 2.54023, its = 5
i = 4983 (of 10192), d = 10, its = 40
i = 4984 (of 10192), d = 10, its = 52
i = 4985 (of 10192), d = 10, its = 53
i = 4986 (of 10192), d = 3.19617, its = 6
i = 4987 (of 10192), d = 3.30782, its = 7
i = 4988 (of 10192), d = 10, its = 41
i = 4989 (of 10192), d = 10, its = 50
i = 4990 (of 10192), d = 2.11676, its = 4
i = 4991 (of 10192), d = 10, its = 54
i = 4992 (of 10192), d = 10, its = 48
i = 4993 (of 10192), d = 10, its = 37
i = 4994 (of 10192), d = 10, its = 52
i = 4995 (of 10192), d = 10, its = 53
i = 4996 (of 10192), d = 10, its = 51
i = 4997 (of 10192), d = 10, its = 40
i = 4998 (of 10192), d = 10, its = 42
i = 4999 (of 10192), d = 8.72314, its = 8
i = 5000 (of 10192), d = 10, its = 54
i = 5001 (of 10192), d = 10, its = 51
i = 5002 (of 10192), d = 10, its = 51
i = 5003 (of 10192), d = 10, its = 54
i = 5004 (of 10192), d = 10, its = 49
i = 5005 (of 10192), d = 1.3178, its = 6
i = 5006 (of 10192), d = 2.09372, its = 4
i = 5007 (of 10192), d = 2.19592, its = 4
i = 5008 (of 10192), d = 10, its = 50
i = 5009 (of 10192), d = 10, its = 55
i = 5010 (of 10192), d = 1.84346, its = 4
i = 5011 (of 10192), d = 10, its = 42
i = 5012 (of 10192), d = 10, its = 37
i = 5013 (of 10192), d = 10, its = 51
i = 5014 (of 10192), d = 1.60635, its = 6
i = 5015 (of 10192), d = 10, its = 50
i = 5016 (of 10192), d = 10, its = 51
i = 5017 (of 10192), d = 10, its = 55
i = 5018 (of 10192), d = 10, its = 54
i = 5019 (of 10192), d = 10, its = 56
i = 5020 (of 10192), d = 10, its = 55
i = 5021 (of 10192), d = 1.38901, its = 7
i = 5022 (of 10192), d = 10, its = 52
i = 5023 (of 10192), d = 2.36223, its = 5
i = 5024 (of 10192), d = 10, its = 37
i = 5025 (of 10192), d = 10, its = 40
i = 5026 (of 10192), d = 2.98555, its = 7
i = 5027 (of 10192), d = 10, its = 53
i = 5028 (of 10192), d = 2.69324, its = 5
i = 5029 (of 10192), d = 1.68101, its = 5
i = 5030 (of 10192), d = 10, its = 40
i = 5031 (of 10192), d = 1.47618, its = 6
i = 5032 (of 10192), d = 10, its = 39
i = 5033 (of 10192), d = 10, its = 58
i = 5034 (of 10192), d = 10, its = 46
i = 5035 (of 10192), d = 10, its = 41
i = 5036 (of 10192), d = 10, its = 51
i = 5037 (of 10192), d = 10, its = 55
i = 5038 (of 10192), d = 1.96901, its = 3
i = 5039 (of 10192), d = 10, its = 49
i = 5040 (of 10192), d = 1.38074, its = 8
i = 5041 (of 10192), d = 10, its = 52
i = 5042 (of 10192), d = 10, its = 54
i = 5043 (of 10192), d = 1.36409, its = 7
i = 5044 (of 10192), d = 10, its = 41
i = 5045 (of 10192), d = 10, its = 37
i = 5046 (of 10192), d = 10, its = 54
i = 5047 (of 10192), d = 10, its = 37
i = 5048 (of 10192), d = 1.7811, its = 5
i = 5049 (of 10192), d = 10, its = 52
i = 5050 (of 10192), d = 10, its = 41
i = 5051 (of 10192), d = 0.609438, its = 18
i = 5052 (of 10192), d = 10, its = 37
i = 5053 (of 10192), d = 10, its = 55
i = 5054 (of 10192), d = 10, its = 37
i = 5055 (of 10192), d = 10, its = 53
i = 5056 (of 10192), d = 10, its = 55
i = 5057 (of 10192), d = 3.54281, its = 6
i = 5058 (of 10192), d = 2.34615, its = 5
i = 5059 (of 10192), d = 10, its = 57
i = 5060 (of 10192), d = 10, its = 52
i = 5061 (of 10192), d = 10, its = 49
i = 5062 (of 10192), d = 10, its = 48
i = 5063 (of 10192), d = 10, its = 50
i = 5064 (of 10192), d = 10, its = 54
i = 5065 (of 10192), d = 2.26266, its = 4
i = 5066 (of 10192), d = 10, its = 43
i = 5067 (of 10192), d = 10, its = 52
i = 5068 (of 10192), d = 10, its = 50
i = 5069 (of 10192), d = 10, its = 51
i = 5070 (of 10192), d = 2.26676, its = 4
i = 5071 (of 10192), d = 1.73524, its = 5
i = 5072 (of 10192), d = 10, its = 53
i = 5073 (of 10192), d = 10, its = 53
i = 5074 (of 10192), d = 10, its = 43
i = 5075 (of 10192), d = 10, its = 37
i = 5076 (of 10192), d = 1.71073, its = 5
i = 5077 (of 10192), d = 10, its = 40
i = 5078 (of 10192), d = 2.29956, its = 5
i = 5079 (of 10192), d = 4.06367, its = 7
i = 5080 (of 10192), d = 1.61071, its = 6
i = 5081 (of 10192), d = 10, its = 40
i = 5082 (of 10192), d = 0.844524, its = 19
i = 5083 (of 10192), d = 10, its = 55
i = 5084 (of 10192), d = 10, its = 55
i = 5085 (of 10192), d = 10, its = 53
i = 5086 (of 10192), d = 1.92718, its = 4
i = 5087 (of 10192), d = 10, its = 55
i = 5088 (of 10192), d = 1.84363, its = 5
i = 5089 (of 10192), d = 1.79611, its = 5
i = 5090 (of 10192), d = 10, its = 39
i = 5091 (of 10192), d = 10, its = 37
i = 5092 (of 10192), d = 10, its = 52
i = 5093 (of 10192), d = 10, its = 40
i = 5094 (of 10192), d = 10, its = 37
i = 5095 (of 10192), d = 3.00402, its = 6
i = 5096 (of 10192), d = 10, its = 54
i = 5097 (of 10192), d = 10, its = 56
i = 5098 (of 10192), d = 10, its = 54
i = 5099 (of 10192), d = 10, its = 53
i = 5100 (of 10192), d = 2.52808, its = 5
i = 5101 (of 10192), d = 10, its = 53
i = 5102 (of 10192), d = 1.0984, its = 21
i = 5103 (of 10192), d = 10, its = 52
i = 5104 (of 10192), d = 10, its = 41
i = 5105 (of 10192), d = 10, its = 41
i = 5106 (of 10192), d = 10, its = 55
i = 5107 (of 10192), d = 2.0872, its = 4
i = 5108 (of 10192), d = 1.53686, its = 7
i = 5109 (of 10192), d = 1.74354, its = 6
i = 5110 (of 10192), d = 6.41489, its = 7
i = 5111 (of 10192), d = 10, its = 53
i = 5112 (of 10192), d = 4.093, its = 7
i = 5113 (of 10192), d = 10, its = 37
i = 5114 (of 10192), d = 10, its = 50
i = 5115 (of 10192), d = 10, its = 57
i = 5116 (of 10192), d = 10, its = 53
i = 5117 (of 10192), d = 3.36368, its = 6
i = 5118 (of 10192), d = 10, its = 50
i = 5119 (of 10192), d = 10, its = 53
i = 5120 (of 10192), d = 10, its = 54
i = 5121 (of 10192), d = 10, its = 42
i = 5122 (of 10192), d = 10, its = 37
i = 5123 (of 10192), d = 10, its = 50
i = 5124 (of 10192), d = 1.33214, its = 8
i = 5125 (of 10192), d = 10, its = 55
i = 5126 (of 10192), d = 10, its = 53
i = 5127 (of 10192), d = 10, its = 54
i = 5128 (of 10192), d = 2.18651, its = 4
i = 5129 (of 10192), d = 10, its = 52
i = 5130 (of 10192), d = 1.81417, its = 5
i = 5131 (of 10192), d = 10, its = 52
i = 5132 (of 10192), d = 10, its = 53
i = 5133 (of 10192), d = 10, its = 40
i = 5134 (of 10192), d = 10, its = 55
i = 5135 (of 10192), d = 10, its = 42
i = 5136 (of 10192), d = 10, its = 53
i = 5137 (of 10192), d = 1.45422, its = 7
i = 5138 (of 10192), d = 10, its = 40
i = 5139 (of 10192), d = 10, its = 41
i = 5140 (of 10192), d = 10, its = 48
i = 5141 (of 10192), d = 10, its = 57
i = 5142 (of 10192), d = 10, its = 52
i = 5143 (of 10192), d = 10, its = 53
i = 5144 (of 10192), d = 10, its = 40
i = 5145 (of 10192), d = 10, its = 51
i = 5146 (of 10192), d = 10, its = 51
i = 5147 (of 10192), d = 10, its = 53
i = 5148 (of 10192), d = 10, its = 50
i = 5149 (of 10192), d = 10, its = 52
i = 5150 (of 10192), d = 0.929694, its = 20
i = 5151 (of 10192), d = 10, its = 52
i = 5152 (of 10192), d = 10, its = 37
i = 5153 (of 10192), d = 0.950593, its = 17
i = 5154 (of 10192), d = 10, its = 53
i = 5155 (of 10192), d = 10, its = 55
i = 5156 (of 10192), d = 10, its = 53
i = 5157 (of 10192), d = 10, its = 55
i = 5158 (of 10192), d = 2.71382, its = 6
i = 5159 (of 10192), d = 1.7095, its = 5
i = 5160 (of 10192), d = 10, its = 53
i = 5161 (of 10192), d = 10, its = 50
i = 5162 (of 10192), d = 10, its = 46
i = 5163 (of 10192), d = 10, its = 49
i = 5164 (of 10192), d = 10, its = 37
i = 5165 (of 10192), d = 10, its = 40
i = 5166 (of 10192), d = 10, its = 55
i = 5167 (of 10192), d = 10, its = 54
i = 5168 (of 10192), d = 2.68867, its = 6
i = 5169 (of 10192), d = 10, its = 37
i = 5170 (of 10192), d = 10, its = 53
i = 5171 (of 10192), d = 10, its = 54
i = 5172 (of 10192), d = 1.68019, its = 5
i = 5173 (of 10192), d = 10, its = 51
i = 5174 (of 10192), d = 2.42888, its = 5
i = 5175 (of 10192), d = 10, its = 52
i = 5176 (of 10192), d = 0.96501, its = 19
i = 5177 (of 10192), d = 1.14007, its = 19
i = 5178 (of 10192), d = 10, its = 40
i = 5179 (of 10192), d = 10, its = 37
i = 5180 (of 10192), d = 10, its = 49
i = 5181 (of 10192), d = 10, its = 55
i = 5182 (of 10192), d = 10, its = 55
i = 5183 (of 10192), d = 10, its = 56
i = 5184 (of 10192), d = 10, its = 41
i = 5185 (of 10192), d = 10, its = 49
i = 5186 (of 10192), d = 10, its = 53
i = 5187 (of 10192), d = 10, its = 52
i = 5188 (of 10192), d = 10, its = 40
i = 5189 (of 10192), d = 10, its = 37
i = 5190 (of 10192), d = 10, its = 52
i = 5191 (of 10192), d = 10, its = 51
i = 5192 (of 10192), d = 10, its = 51
i = 5193 (of 10192), d = 1.96476, its = 3
i = 5194 (of 10192), d = 10, its = 51
i = 5195 (of 10192), d = 10, its = 53
i = 5196 (of 10192), d = 1.0818, its = 18
i = 5197 (of 10192), d = 10, its = 56
i = 5198 (of 10192), d = 10, its = 37
i = 5199 (of 10192), d = 1.48885, its = 7
i = 5200 (of 10192), d = 10, its = 52
i = 5201 (of 10192), d = 10, its = 58
i = 5202 (of 10192), d = 10, its = 49
i = 5203 (of 10192), d = 10, its = 58
i = 5204 (of 10192), d = 10, its = 37
i = 5205 (of 10192), d = 10, its = 58
i = 5206 (of 10192), d = 0.882522, its = 18
i = 5207 (of 10192), d = 10, its = 37
i = 5208 (of 10192), d = 10, its = 56
i = 5209 (of 10192), d = 10, its = 37
i = 5210 (of 10192), d = 10, its = 41
i = 5211 (of 10192), d = 2.70984, its = 6
i = 5212 (of 10192), d = 10, its = 40
i = 5213 (of 10192), d = 10, its = 40
i = 5214 (of 10192), d = 10, its = 50
i = 5215 (of 10192), d = 10, its = 55
i = 5216 (of 10192), d = 2.25043, its = 4
i = 5217 (of 10192), d = 10, its = 37
i = 5218 (of 10192), d = 10, its = 51
i = 5219 (of 10192), d = 10, its = 39
i = 5220 (of 10192), d = 10, its = 40
i = 5221 (of 10192), d = 10, its = 53
i = 5222 (of 10192), d = 10, its = 54
i = 5223 (of 10192), d = 4.23858, its = 8
i = 5224 (of 10192), d = 10, its = 39
i = 5225 (of 10192), d = 1.81159, its = 4
i = 5226 (of 10192), d = 10, its = 51
i = 5227 (of 10192), d = 1.50469, its = 7
i = 5228 (of 10192), d = 0.678275, its = 17
i = 5229 (of 10192), d = 10, its = 53
i = 5230 (of 10192), d = 2.18585, its = 4
i = 5231 (of 10192), d = 10, its = 56
i = 5232 (of 10192), d = 10, its = 53
i = 5233 (of 10192), d = 10, its = 54
i = 5234 (of 10192), d = 10, its = 55
i = 5235 (of 10192), d = 10, its = 41
i = 5236 (of 10192), d = 1.98444, its = 3
i = 5237 (of 10192), d = 10, its = 56
i = 5238 (of 10192), d = 10, its = 53
i = 5239 (of 10192), d = 10, its = 53
i = 5240 (of 10192), d = 1.16863, its = 5
i = 5241 (of 10192), d = 10, its = 56
i = 5242 (of 10192), d = 10, its = 54
i = 5243 (of 10192), d = 1.14569, its = 20
i = 5244 (of 10192), d = 0.825364, its = 18
i = 5245 (of 10192), d = 10, its = 37
i = 5246 (of 10192), d = 10, its = 53
i = 5247 (of 10192), d = 10, its = 51
i = 5248 (of 10192), d = 10, its = 49
i = 5249 (of 10192), d = 10, its = 51
i = 5250 (of 10192), d = 10, its = 52
i = 5251 (of 10192), d = 10, its = 52
i = 5252 (of 10192), d = 10, its = 48
i = 5253 (of 10192), d = 10, its = 51
i = 5254 (of 10192), d = 10, its = 53
i = 5255 (of 10192), d = 1.82555, its = 4
i = 5256 (of 10192), d = 1.87211, its = 4
i = 5257 (of 10192), d = 1.40564, its = 7
i = 5258 (of 10192), d = 10, its = 51
i = 5259 (of 10192), d = 10, its = 52
i = 5260 (of 10192), d = 10, its = 37
i = 5261 (of 10192), d = 2.41355, its = 5
i = 5262 (of 10192), d = 1.0049, its = 18
i = 5263 (of 10192), d = 2.4329, its = 5
i = 5264 (of 10192), d = 10, its = 54
i = 5265 (of 10192), d = 10, its = 60
i = 5266 (of 10192), d = 10, its = 54
i = 5267 (of 10192), d = 10, its = 56
i = 5268 (of 10192), d = 10, its = 44
i = 5269 (of 10192), d = 10, its = 53
i = 5270 (of 10192), d = 10, its = 43
i = 5271 (of 10192), d = 10, its = 37
i = 5272 (of 10192), d = 10, its = 40
i = 5273 (of 10192), d = 10, its = 57
i = 5274 (of 10192), d = 1.64691, its = 6
i = 5275 (of 10192), d = 2.11397, its = 4
i = 5276 (of 10192), d = 10, its = 56
i = 5277 (of 10192), d = 10, its = 49
i = 5278 (of 10192), d = 10, its = 54
i = 5279 (of 10192), d = 10, its = 52
i = 5280 (of 10192), d = 0.922154, its = 22
i = 5281 (of 10192), d = 10, its = 53
i = 5282 (of 10192), d = 10, its = 56
i = 5283 (of 10192), d = 1.04927, its = 17
i = 5284 (of 10192), d = 10, its = 57
i = 5285 (of 10192), d = 10, its = 54
i = 5286 (of 10192), d = 10, its = 51
i = 5287 (of 10192), d = 4.52353, its = 7
i = 5288 (of 10192), d = 10, its = 41
i = 5289 (of 10192), d = 10, its = 37
i = 5290 (of 10192), d = 10, its = 52
i = 5291 (of 10192), d = 10, its = 51
i = 5292 (of 10192), d = 10, its = 39
i = 5293 (of 10192), d = 10, its = 53
i = 5294 (of 10192), d = 10, its = 50
i = 5295 (of 10192), d = 10, its = 52
i = 5296 (of 10192), d = 10, its = 49
i = 5297 (of 10192), d = 10, its = 55
i = 5298 (of 10192), d = 2.95282, its = 5
i = 5299 (of 10192), d = 2.55109, its = 5
i = 5300 (of 10192), d = 10, its = 51
i = 5301 (of 10192), d = 10, its = 39
i = 5302 (of 10192), d = 10, its = 40
i = 5303 (of 10192), d = 10, its = 49
i = 5304 (of 10192), d = 10, its = 55
i = 5305 (of 10192), d = 1.95998, its = 3
i = 5306 (of 10192), d = 7.10303, its = 8
i = 5307 (of 10192), d = 1.9493, its = 4
i = 5308 (of 10192), d = 10, its = 54
i = 5309 (of 10192), d = 10, its = 50
i = 5310 (of 10192), d = 10, its = 60
i = 5311 (of 10192), d = 10, its = 52
i = 5312 (of 10192), d = 0.759192, its = 18
i = 5313 (of 10192), d = 1.26198, its = 8
i = 5314 (of 10192), d = 10, its = 37
i = 5315 (of 10192), d = 10, its = 50
i = 5316 (of 10192), d = 10, its = 55
i = 5317 (of 10192), d = 10, its = 54
i = 5318 (of 10192), d = 10, its = 37
i = 5319 (of 10192), d = 10, its = 53
i = 5320 (of 10192), d = 10, its = 55
i = 5321 (of 10192), d = 10, its = 51
i = 5322 (of 10192), d = 1.37523, its = 6
i = 5323 (of 10192), d = 10, its = 49
i = 5324 (of 10192), d = 0.90026, its = 19
i = 5325 (of 10192), d = 2.47138, its = 5
i = 5326 (of 10192), d = 10, its = 40
i = 5327 (of 10192), d = 10, its = 40
i = 5328 (of 10192), d = 10, its = 37
i = 5329 (of 10192), d = 0.820672, its = 19
i = 5330 (of 10192), d = 10, its = 52
i = 5331 (of 10192), d = 10, its = 54
i = 5332 (of 10192), d = 10, its = 50
i = 5333 (of 10192), d = 10, its = 37
i = 5334 (of 10192), d = 1.38656, its = 6
i = 5335 (of 10192), d = 10, its = 50
i = 5336 (of 10192), d = 10, its = 43
i = 5337 (of 10192), d = 1.77889, its = 5
i = 5338 (of 10192), d = 1.89889, its = 4
i = 5339 (of 10192), d = 10, its = 42
i = 5340 (of 10192), d = 10, its = 50
i = 5341 (of 10192), d = 1.75425, its = 5
i = 5342 (of 10192), d = 10, its = 51
i = 5343 (of 10192), d = 0.91993, its = 23
i = 5344 (of 10192), d = 10, its = 37
i = 5345 (of 10192), d = 10, its = 43
i = 5346 (of 10192), d = 1.3121, its = 19
i = 5347 (of 10192), d = 10, its = 56
i = 5348 (of 10192), d = 10, its = 47
i = 5349 (of 10192), d = 2.21951, its = 4
i = 5350 (of 10192), d = 1.00242, its = 19
i = 5351 (of 10192), d = 10, its = 51
i = 5352 (of 10192), d = 1.59614, its = 7
i = 5353 (of 10192), d = 10, its = 50
i = 5354 (of 10192), d = 10, its = 54
i = 5355 (of 10192), d = 10, its = 37
i = 5356 (of 10192), d = 10, its = 52
i = 5357 (of 10192), d = 4.7716, its = 24
i = 5358 (of 10192), d = 10, its = 40
i = 5359 (of 10192), d = 10, its = 52
i = 5360 (of 10192), d = 10, its = 55
i = 5361 (of 10192), d = 10, its = 52
i = 5362 (of 10192), d = 10, its = 55
i = 5363 (of 10192), d = 10, its = 48
i = 5364 (of 10192), d = 10, its = 49
i = 5365 (of 10192), d = 1.78553, its = 5
i = 5366 (of 10192), d = 10, its = 51
i = 5367 (of 10192), d = 10, its = 53
i = 5368 (of 10192), d = 10, its = 54
i = 5369 (of 10192), d = 10, its = 40
i = 5370 (of 10192), d = 10, its = 39
i = 5371 (of 10192), d = 10, its = 51
i = 5372 (of 10192), d = 10, its = 56
i = 5373 (of 10192), d = 10, its = 51
i = 5374 (of 10192), d = 3.00326, its = 6
i = 5375 (of 10192), d = 7.33781, its = 22
i = 5376 (of 10192), d = 10, its = 52
i = 5377 (of 10192), d = 1.55302, its = 7
i = 5378 (of 10192), d = 1.99856, its = 2
i = 5379 (of 10192), d = 2.40744, its = 5
i = 5380 (of 10192), d = 10, its = 37
i = 5381 (of 10192), d = 10, its = 54
i = 5382 (of 10192), d = 10, its = 50
i = 5383 (of 10192), d = 10, its = 50
i = 5384 (of 10192), d = 10, its = 37
i = 5385 (of 10192), d = 2.10847, its = 4
i = 5386 (of 10192), d = 10, its = 40
i = 5387 (of 10192), d = 10, its = 42
i = 5388 (of 10192), d = 2.25679, its = 5
i = 5389 (of 10192), d = 10, its = 42
i = 5390 (of 10192), d = 10, its = 37
i = 5391 (of 10192), d = 10, its = 50
i = 5392 (of 10192), d = 10, its = 52
i = 5393 (of 10192), d = 10, its = 55
i = 5394 (of 10192), d = 2.12475, its = 4
i = 5395 (of 10192), d = 10, its = 52
i = 5396 (of 10192), d = 10, its = 51
i = 5397 (of 10192), d = 2.11013, its = 4
i = 5398 (of 10192), d = 10, its = 54
i = 5399 (of 10192), d = 0.734767, its = 18
i = 5400 (of 10192), d = 10, its = 52
i = 5401 (of 10192), d = 10, its = 37
i = 5402 (of 10192), d = 2.02447, its = 3
i = 5403 (of 10192), d = 10, its = 50
i = 5404 (of 10192), d = 1.60845, its = 6
i = 5405 (of 10192), d = 10, its = 56
i = 5406 (of 10192), d = 10, its = 37
i = 5407 (of 10192), d = 10, its = 54
i = 5408 (of 10192), d = 10, its = 37
i = 5409 (of 10192), d = 2.9094, its = 5
i = 5410 (of 10192), d = 10, its = 37
i = 5411 (of 10192), d = 1.69701, its = 5
i = 5412 (of 10192), d = 3.55967, its = 6
i = 5413 (of 10192), d = 10, its = 37
i = 5414 (of 10192), d = 10, its = 49
i = 5415 (of 10192), d = 1.16915, its = 18
i = 5416 (of 10192), d = 10, its = 53
i = 5417 (of 10192), d = 10, its = 50
i = 5418 (of 10192), d = 10, its = 37
i = 5419 (of 10192), d = 1.94636, its = 3
i = 5420 (of 10192), d = 10, its = 41
i = 5421 (of 10192), d = 2.26444, its = 4
i = 5422 (of 10192), d = 10, its = 41
i = 5423 (of 10192), d = 10, its = 37
i = 5424 (of 10192), d = 10, its = 52
i = 5425 (of 10192), d = 10, its = 47
i = 5426 (of 10192), d = 10, its = 54
i = 5427 (of 10192), d = 10, its = 50
i = 5428 (of 10192), d = 4.08146, its = 6
i = 5429 (of 10192), d = 10, its = 37
i = 5430 (of 10192), d = 10, its = 55
i = 5431 (of 10192), d = 10, its = 37
i = 5432 (of 10192), d = 10, its = 48
i = 5433 (of 10192), d = 10, its = 51
i = 5434 (of 10192), d = 10, its = 53
i = 5435 (of 10192), d = 4.56106, its = 7
i = 5436 (of 10192), d = 1.10829, its = 21
i = 5437 (of 10192), d = 10, its = 50
i = 5438 (of 10192), d = 1.39607, its = 7
i = 5439 (of 10192), d = 10, its = 39
i = 5440 (of 10192), d = 10, its = 42
i = 5441 (of 10192), d = 10, its = 40
i = 5442 (of 10192), d = 10, its = 51
i = 5443 (of 10192), d = 0.886999, its = 18
i = 5444 (of 10192), d = 10, its = 54
i = 5445 (of 10192), d = 10, its = 53
i = 5446 (of 10192), d = 10, its = 51
i = 5447 (of 10192), d = 10, its = 59
i = 5448 (of 10192), d = 10, its = 50
i = 5449 (of 10192), d = 10, its = 52
i = 5450 (of 10192), d = 10, its = 53
i = 5451 (of 10192), d = 10, its = 56
i = 5452 (of 10192), d = 10, its = 53
i = 5453 (of 10192), d = 10, its = 56
i = 5454 (of 10192), d = 10, its = 52
i = 5455 (of 10192), d = 10, its = 52
i = 5456 (of 10192), d = 2.96009, its = 6
i = 5457 (of 10192), d = 10, its = 53
i = 5458 (of 10192), d = 1.63514, its = 6
i = 5459 (of 10192), d = 10, its = 49
i = 5460 (of 10192), d = 10, its = 51
i = 5461 (of 10192), d = 10, its = 54
i = 5462 (of 10192), d = 5.11153, its = 7
i = 5463 (of 10192), d = 3.10895, its = 6
i = 5464 (of 10192), d = 10, its = 49
i = 5465 (of 10192), d = 10, its = 53
i = 5466 (of 10192), d = 10, its = 50
i = 5467 (of 10192), d = 2.18657, its = 4
i = 5468 (of 10192), d = 10, its = 37
i = 5469 (of 10192), d = 10, its = 50
i = 5470 (of 10192), d = 2.79544, its = 5
i = 5471 (of 10192), d = 10, its = 39
i = 5472 (of 10192), d = 2.15787, its = 4
i = 5473 (of 10192), d = 10, its = 54
i = 5474 (of 10192), d = 10, its = 52
i = 5475 (of 10192), d = 10, its = 55
i = 5476 (of 10192), d = 2.08117, its = 4
i = 5477 (of 10192), d = 10, its = 41
i = 5478 (of 10192), d = 10, its = 43
i = 5479 (of 10192), d = 10, its = 44
i = 5480 (of 10192), d = 1.28873, its = 7
i = 5481 (of 10192), d = 3.01757, its = 6
i = 5482 (of 10192), d = 2.85709, its = 8
i = 5483 (of 10192), d = 10, its = 53
i = 5484 (of 10192), d = 3.88912, its = 7
i = 5485 (of 10192), d = 10, its = 52
i = 5486 (of 10192), d = 10, its = 54
i = 5487 (of 10192), d = 10, its = 54
i = 5488 (of 10192), d = 10, its = 41
i = 5489 (of 10192), d = 10, its = 48
i = 5490 (of 10192), d = 2.68752, its = 5
i = 5491 (of 10192), d = 10, its = 47
i = 5492 (of 10192), d = 10, its = 37
i = 5493 (of 10192), d = 2.03668, its = 3
i = 5494 (of 10192), d = 10, its = 54
i = 5495 (of 10192), d = 10, its = 55
i = 5496 (of 10192), d = 2.21447, its = 4
i = 5497 (of 10192), d = 2.10293, its = 4
i = 5498 (of 10192), d = 10, its = 55
i = 5499 (of 10192), d = 10, its = 50
i = 5500 (of 10192), d = 1.95813, its = 3
i = 5501 (of 10192), d = 10, its = 50
i = 5502 (of 10192), d = 10, its = 41
i = 5503 (of 10192), d = 1.35115, its = 6
i = 5504 (of 10192), d = 10, its = 39
i = 5505 (of 10192), d = 10, its = 54
i = 5506 (of 10192), d = 10, its = 52
i = 5507 (of 10192), d = 10, its = 38
i = 5508 (of 10192), d = 2.09481, its = 4
i = 5509 (of 10192), d = 10, its = 51
i = 5510 (of 10192), d = 10, its = 53
i = 5511 (of 10192), d = 10, its = 52
i = 5512 (of 10192), d = 10, its = 55
i = 5513 (of 10192), d = 10, its = 37
i = 5514 (of 10192), d = 10, its = 51
i = 5515 (of 10192), d = 10, its = 57
i = 5516 (of 10192), d = 10, its = 57
i = 5517 (of 10192), d = 10, its = 53
i = 5518 (of 10192), d = 10, its = 37
i = 5519 (of 10192), d = 10, its = 51
i = 5520 (of 10192), d = 2.7851, its = 6
i = 5521 (of 10192), d = 8.44649, its = 8
i = 5522 (of 10192), d = 3.52889, its = 6
i = 5523 (of 10192), d = 10, its = 37
i = 5524 (of 10192), d = 10, its = 55
i = 5525 (of 10192), d = 10, its = 59
i = 5526 (of 10192), d = 10, its = 52
i = 5527 (of 10192), d = 10, its = 51
i = 5528 (of 10192), d = 10, its = 37
i = 5529 (of 10192), d = 0.915179, its = 24
i = 5530 (of 10192), d = 10, its = 43
i = 5531 (of 10192), d = 2.52158, its = 5
i = 5532 (of 10192), d = 1.19887, its = 19
i = 5533 (of 10192), d = 10, its = 51
i = 5534 (of 10192), d = 10, its = 54
i = 5535 (of 10192), d = 0.930917, its = 20
i = 5536 (of 10192), d = 10, its = 51
i = 5537 (of 10192), d = 10, its = 56
i = 5538 (of 10192), d = 1.14612, its = 20
i = 5539 (of 10192), d = 10, its = 37
i = 5540 (of 10192), d = 4.79007, its = 7
i = 5541 (of 10192), d = 2.98555, its = 6
i = 5542 (of 10192), d = 2.42789, its = 5
i = 5543 (of 10192), d = 10, its = 53
i = 5544 (of 10192), d = 2.64022, its = 5
i = 5545 (of 10192), d = 10, its = 37
i = 5546 (of 10192), d = 10, its = 51
i = 5547 (of 10192), d = 10, its = 51
i = 5548 (of 10192), d = 10, its = 52
i = 5549 (of 10192), d = 10, its = 49
i = 5550 (of 10192), d = 10, its = 42
i = 5551 (of 10192), d = 10, its = 54
i = 5552 (of 10192), d = 10, its = 54
i = 5553 (of 10192), d = 3.63754, its = 7
i = 5554 (of 10192), d = 4.47202, its = 7
i = 5555 (of 10192), d = 10, its = 39
i = 5556 (of 10192), d = 2.74242, its = 6
i = 5557 (of 10192), d = 10, its = 40
i = 5558 (of 10192), d = 0.901989, its = 20
i = 5559 (of 10192), d = 10, its = 40
i = 5560 (of 10192), d = 10, its = 50
i = 5561 (of 10192), d = 10, its = 42
i = 5562 (of 10192), d = 10, its = 51
i = 5563 (of 10192), d = 1.13015, its = 20
i = 5564 (of 10192), d = 10, its = 19
i = 5565 (of 10192), d = 2.17587, its = 4
i = 5566 (of 10192), d = 2.72023, its = 6
i = 5567 (of 10192), d = 10, its = 56
i = 5568 (of 10192), d = 2.72898, its = 5
i = 5569 (of 10192), d = 10, its = 54
i = 5570 (of 10192), d = 10, its = 55
i = 5571 (of 10192), d = 10, its = 57
i = 5572 (of 10192), d = 10, its = 54
i = 5573 (of 10192), d = 2.938, its = 6
i = 5574 (of 10192), d = 2.14322, its = 4
i = 5575 (of 10192), d = 10, its = 56
i = 5576 (of 10192), d = 4.79461, its = 10
i = 5577 (of 10192), d = 10, its = 55
i = 5578 (of 10192), d = 10, its = 54
i = 5579 (of 10192), d = 1.40288, its = 7
i = 5580 (of 10192), d = 1.60628, its = 6
i = 5581 (of 10192), d = 10, its = 50
i = 5582 (of 10192), d = 2.36463, its = 5
i = 5583 (of 10192), d = 10, its = 54
i = 5584 (of 10192), d = 10, its = 59
i = 5585 (of 10192), d = 1.83486, its = 4
i = 5586 (of 10192), d = 10, its = 52
i = 5587 (of 10192), d = 10, its = 56
i = 5588 (of 10192), d = 0.483495, its = 25
i = 5589 (of 10192), d = 2.33069, its = 5
i = 5590 (of 10192), d = 2.20976, its = 4
i = 5591 (of 10192), d = 0.483495, its = 25
i = 5592 (of 10192), d = 3.00401, its = 6
i = 5593 (of 10192), d = 10, its = 49
i = 5594 (of 10192), d = 10, its = 49
i = 5595 (of 10192), d = 10, its = 50
i = 5596 (of 10192), d = 10, its = 50
i = 5597 (of 10192), d = 10, its = 40
i = 5598 (of 10192), d = 10, its = 39
i = 5599 (of 10192), d = 10, its = 37
i = 5600 (of 10192), d = 10, its = 55
i = 5601 (of 10192), d = 2.05046, its = 4
i = 5602 (of 10192), d = 10, its = 50
i = 5603 (of 10192), d = 1.46253, its = 6
i = 5604 (of 10192), d = 10, its = 53
i = 5605 (of 10192), d = 5.1594, its = 7
i = 5606 (of 10192), d = 10, its = 37
i = 5607 (of 10192), d = 10, its = 49
i = 5608 (of 10192), d = 2.06985, its = 4
i = 5609 (of 10192), d = 10, its = 52
i = 5610 (of 10192), d = 10, its = 52
i = 5611 (of 10192), d = 0.873966, its = 19
i = 5612 (of 10192), d = 10, its = 53
i = 5613 (of 10192), d = 10, its = 50
i = 5614 (of 10192), d = 10, its = 56
i = 5615 (of 10192), d = 10, its = 50
i = 5616 (of 10192), d = 10, its = 55
i = 5617 (of 10192), d = 0.841604, its = 17
i = 5618 (of 10192), d = 10, its = 53
i = 5619 (of 10192), d = 10, its = 37
i = 5620 (of 10192), d = 10, its = 52
i = 5621 (of 10192), d = 2.90366, its = 6
i = 5622 (of 10192), d = 10, its = 40
i = 5623 (of 10192), d = 10, its = 39
i = 5624 (of 10192), d = 10, its = 52
i = 5625 (of 10192), d = 10, its = 37
i = 5626 (of 10192), d = 10, its = 55
i = 5627 (of 10192), d = 1.63279, its = 5
i = 5628 (of 10192), d = 10, its = 40
i = 5629 (of 10192), d = 10, its = 40
i = 5630 (of 10192), d = 10, its = 52
i = 5631 (of 10192), d = 10, its = 56
i = 5632 (of 10192), d = 10, its = 40
i = 5633 (of 10192), d = 10, its = 53
i = 5634 (of 10192), d = 10, its = 57
i = 5635 (of 10192), d = 1.87711, its = 4
i = 5636 (of 10192), d = 10, its = 52
i = 5637 (of 10192), d = 10, its = 53
i = 5638 (of 10192), d = 10, its = 55
i = 5639 (of 10192), d = 10, its = 55
i = 5640 (of 10192), d = 10, its = 42
i = 5641 (of 10192), d = 10, its = 55
i = 5642 (of 10192), d = 10, its = 49
i = 5643 (of 10192), d = 2.14517, its = 4
i = 5644 (of 10192), d = 1.02363, its = 19
i = 5645 (of 10192), d = 10, its = 53
i = 5646 (of 10192), d = 10, its = 42
i = 5647 (of 10192), d = 10, its = 56
i = 5648 (of 10192), d = 10, its = 37
i = 5649 (of 10192), d = 10, its = 50
i = 5650 (of 10192), d = 10, its = 55
i = 5651 (of 10192), d = 2.18761, its = 4
i = 5652 (of 10192), d = 10, its = 55
i = 5653 (of 10192), d = 0.801032, its = 16
i = 5654 (of 10192), d = 10, its = 56
i = 5655 (of 10192), d = 10, its = 39
i = 5656 (of 10192), d = 10, its = 37
i = 5657 (of 10192), d = 2.58315, its = 5
i = 5658 (of 10192), d = 10, its = 39
i = 5659 (of 10192), d = 2.15712, its = 4
i = 5660 (of 10192), d = 10, its = 45
i = 5661 (of 10192), d = 10, its = 54
i = 5662 (of 10192), d = 10, its = 54
i = 5663 (of 10192), d = 10, its = 53
i = 5664 (of 10192), d = 2.32043, its = 4
i = 5665 (of 10192), d = 10, its = 52
i = 5666 (of 10192), d = 10, its = 37
i = 5667 (of 10192), d = 10, its = 54
i = 5668 (of 10192), d = 10, its = 53
i = 5669 (of 10192), d = 1.46455, its = 7
i = 5670 (of 10192), d = 10, its = 54
i = 5671 (of 10192), d = 1.74121, its = 5
i = 5672 (of 10192), d = 10, its = 51
i = 5673 (of 10192), d = 10, its = 40
i = 5674 (of 10192), d = 2.50974, its = 5
i = 5675 (of 10192), d = 10, its = 39
i = 5676 (of 10192), d = 10, its = 56
i = 5677 (of 10192), d = 10, its = 51
i = 5678 (of 10192), d = 10, its = 58
i = 5679 (of 10192), d = 2.10837, its = 4
i = 5680 (of 10192), d = 1.02363, its = 19
i = 5681 (of 10192), d = 2.52518, its = 5
i = 5682 (of 10192), d = 3.52277, its = 9
i = 5683 (of 10192), d = 10, its = 51
i = 5684 (of 10192), d = 1.7675, its = 5
i = 5685 (of 10192), d = 2.53746, its = 5
i = 5686 (of 10192), d = 10, its = 40
i = 5687 (of 10192), d = 10, its = 56
i = 5688 (of 10192), d = 2.23863, its = 4
i = 5689 (of 10192), d = 10, its = 40
i = 5690 (of 10192), d = 1.83845, its = 5
i = 5691 (of 10192), d = 2.21604, its = 4
i = 5692 (of 10192), d = 10, its = 53
i = 5693 (of 10192), d = 10, its = 51
i = 5694 (of 10192), d = 1.64323, its = 5
i = 5695 (of 10192), d = 2.36401, its = 5
i = 5696 (of 10192), d = 4.52832, its = 7
i = 5697 (of 10192), d = 10, its = 51
i = 5698 (of 10192), d = 10, its = 54
i = 5699 (of 10192), d = 10, its = 53
i = 5700 (of 10192), d = 10, its = 55
i = 5701 (of 10192), d = 10, its = 51
i = 5702 (of 10192), d = 10, its = 41
i = 5703 (of 10192), d = 10, its = 52
i = 5704 (of 10192), d = 1.59215, its = 6
i = 5705 (of 10192), d = 10, its = 55
i = 5706 (of 10192), d = 1.96424, its = 3
i = 5707 (of 10192), d = 10, its = 53
i = 5708 (of 10192), d = 10, its = 53
i = 5709 (of 10192), d = 2.38886, its = 5
i = 5710 (of 10192), d = 10, its = 52
i = 5711 (of 10192), d = 10, its = 51
i = 5712 (of 10192), d = 1.60886, its = 7
i = 5713 (of 10192), d = 10, its = 55
i = 5714 (of 10192), d = 10, its = 52
i = 5715 (of 10192), d = 10, its = 57
i = 5716 (of 10192), d = 10, its = 41
i = 5717 (of 10192), d = 10, its = 37
i = 5718 (of 10192), d = 10, its = 55
i = 5719 (of 10192), d = 2.32231, its = 5
i = 5720 (of 10192), d = 10, its = 52
i = 5721 (of 10192), d = 2.84053, its = 5
i = 5722 (of 10192), d = 2.43693, its = 5
i = 5723 (of 10192), d = 10, its = 55
i = 5724 (of 10192), d = 1.67408, its = 5
i = 5725 (of 10192), d = 0.621722, its = 22
i = 5726 (of 10192), d = 10, its = 54
i = 5727 (of 10192), d = 10, its = 54
i = 5728 (of 10192), d = 10, its = 51
i = 5729 (of 10192), d = 10, its = 54
i = 5730 (of 10192), d = 10, its = 41
i = 5731 (of 10192), d = 10, its = 52
i = 5732 (of 10192), d = 2.22572, its = 4
i = 5733 (of 10192), d = 10, its = 43
i = 5734 (of 10192), d = 10, its = 53
i = 5735 (of 10192), d = 10, its = 53
i = 5736 (of 10192), d = 10, its = 52
i = 5737 (of 10192), d = 2.5728, its = 5
i = 5738 (of 10192), d = 1.91838, its = 4
i = 5739 (of 10192), d = 10, its = 49
i = 5740 (of 10192), d = 10, its = 54
i = 5741 (of 10192), d = 10, its = 37
i = 5742 (of 10192), d = 10, its = 53
i = 5743 (of 10192), d = 10, its = 37
i = 5744 (of 10192), d = 10, its = 41
i = 5745 (of 10192), d = 10, its = 56
i = 5746 (of 10192), d = 10, its = 50
i = 5747 (of 10192), d = 1.90243, its = 4
i = 5748 (of 10192), d = 1.92272, its = 4
i = 5749 (of 10192), d = 1.06207, its = 25
i = 5750 (of 10192), d = 2.88132, its = 6
i = 5751 (of 10192), d = 10, its = 55
i = 5752 (of 10192), d = 10, its = 59
i = 5753 (of 10192), d = 3.2008, its = 7
i = 5754 (of 10192), d = 2.56284, its = 5
i = 5755 (of 10192), d = 2.26673, its = 4
i = 5756 (of 10192), d = 10, its = 54
i = 5757 (of 10192), d = 0.8022, its = 22
i = 5758 (of 10192), d = 2.93488, its = 6
i = 5759 (of 10192), d = 10, its = 52
i = 5760 (of 10192), d = 10, its = 37
i = 5761 (of 10192), d = 10, its = 53
i = 5762 (of 10192), d = 10, its = 52
i = 5763 (of 10192), d = 10, its = 55
i = 5764 (of 10192), d = 10, its = 54
i = 5765 (of 10192), d = 10, its = 54
i = 5766 (of 10192), d = 10, its = 54
i = 5767 (of 10192), d = 2.95564, its = 6
i = 5768 (of 10192), d = 10, its = 56
i = 5769 (of 10192), d = 10, its = 37
i = 5770 (of 10192), d = 10, its = 40
i = 5771 (of 10192), d = 10, its = 53
i = 5772 (of 10192), d = 10, its = 49
i = 5773 (of 10192), d = 1.56385, its = 7
i = 5774 (of 10192), d = 10, its = 51
i = 5775 (of 10192), d = 10, its = 40
i = 5776 (of 10192), d = 2.37229, its = 5
i = 5777 (of 10192), d = 10, its = 40
i = 5778 (of 10192), d = 10, its = 54
i = 5779 (of 10192), d = 2.14619, its = 4
i = 5780 (of 10192), d = 10, its = 58
i = 5781 (of 10192), d = 0.754797, its = 23
i = 5782 (of 10192), d = 10, its = 51
i = 5783 (of 10192), d = 10, its = 51
i = 5784 (of 10192), d = 10, its = 50
i = 5785 (of 10192), d = 10, its = 52
i = 5786 (of 10192), d = 10, its = 40
i = 5787 (of 10192), d = 10, its = 48
i = 5788 (of 10192), d = 10, its = 50
i = 5789 (of 10192), d = 1.42358, its = 7
i = 5790 (of 10192), d = 10, its = 49
i = 5791 (of 10192), d = 10, its = 51
i = 5792 (of 10192), d = 10, its = 50
i = 5793 (of 10192), d = 10, its = 52
i = 5794 (of 10192), d = 10, its = 50
i = 5795 (of 10192), d = 10, its = 37
i = 5796 (of 10192), d = 10, its = 37
i = 5797 (of 10192), d = 10, its = 54
i = 5798 (of 10192), d = 10, its = 51
i = 5799 (of 10192), d = 10, its = 53
i = 5800 (of 10192), d = 10, its = 55
i = 5801 (of 10192), d = 1.65633, its = 6
i = 5802 (of 10192), d = 10, its = 49
i = 5803 (of 10192), d = 10, its = 56
i = 5804 (of 10192), d = 10, its = 48
i = 5805 (of 10192), d = 1.63748, its = 6
i = 5806 (of 10192), d = 10, its = 41
i = 5807 (of 10192), d = 10, its = 48
i = 5808 (of 10192), d = 2.02923, its = 3
i = 5809 (of 10192), d = 10, its = 55
i = 5810 (of 10192), d = 2.33664, its = 4
i = 5811 (of 10192), d = 2.82243, its = 6
i = 5812 (of 10192), d = 1.86656, its = 5
i = 5813 (of 10192), d = 10, its = 49
i = 5814 (of 10192), d = 10, its = 55
i = 5815 (of 10192), d = 10, its = 49
i = 5816 (of 10192), d = 10, its = 37
i = 5817 (of 10192), d = 1.90455, its = 4
i = 5818 (of 10192), d = 10, its = 40
i = 5819 (of 10192), d = 10, its = 55
i = 5820 (of 10192), d = 10, its = 37
i = 5821 (of 10192), d = 1.74242, its = 5
i = 5822 (of 10192), d = 10, its = 52
i = 5823 (of 10192), d = 10, its = 52
i = 5824 (of 10192), d = 10, its = 37
i = 5825 (of 10192), d = 10, its = 52
i = 5826 (of 10192), d = 10, its = 54
i = 5827 (of 10192), d = 10, its = 41
i = 5828 (of 10192), d = 4.0734, its = 7
i = 5829 (of 10192), d = 10, its = 53
i = 5830 (of 10192), d = 10, its = 52
i = 5831 (of 10192), d = 2.52499, its = 5
i = 5832 (of 10192), d = 0.931565, its = 25
i = 5833 (of 10192), d = 10, its = 39
i = 5834 (of 10192), d = 10, its = 53
i = 5835 (of 10192), d = 10, its = 37
i = 5836 (of 10192), d = 10, its = 53
i = 5837 (of 10192), d = 10, its = 56
i = 5838 (of 10192), d = 10, its = 51
i = 5839 (of 10192), d = 10, its = 38
i = 5840 (of 10192), d = 10, its = 52
i = 5841 (of 10192), d = 0.769266, its = 19
i = 5842 (of 10192), d = 10, its = 37
i = 5843 (of 10192), d = 10, its = 52
i = 5844 (of 10192), d = 0.640073, its = 19
i = 5845 (of 10192), d = 10, its = 50
i = 5846 (of 10192), d = 10, its = 56
i = 5847 (of 10192), d = 2.75468, its = 6
i = 5848 (of 10192), d = 10, its = 53
i = 5849 (of 10192), d = 1.30407, its = 23
i = 5850 (of 10192), d = 1.80328, its = 5
i = 5851 (of 10192), d = 10, its = 40
i = 5852 (of 10192), d = 10, its = 57
i = 5853 (of 10192), d = 2.17545, its = 4
i = 5854 (of 10192), d = 10, its = 37
i = 5855 (of 10192), d = 10, its = 55
i = 5856 (of 10192), d = 10, its = 58
i = 5857 (of 10192), d = 2.31786, its = 5
i = 5858 (of 10192), d = 10, its = 37
i = 5859 (of 10192), d = 10, its = 55
i = 5860 (of 10192), d = 10, its = 37
i = 5861 (of 10192), d = 10, its = 37
i = 5862 (of 10192), d = 0.917817, its = 18
i = 5863 (of 10192), d = 10, its = 43
i = 5864 (of 10192), d = 8.10343, its = 10
i = 5865 (of 10192), d = 10, its = 53
i = 5866 (of 10192), d = 3.7376, its = 6
i = 5867 (of 10192), d = 10, its = 51
i = 5868 (of 10192), d = 3.71141, its = 6
i = 5869 (of 10192), d = 8.02293, its = 9
i = 5870 (of 10192), d = 10, its = 52
i = 5871 (of 10192), d = 1.00223, its = 18
i = 5872 (of 10192), d = 10, its = 55
i = 5873 (of 10192), d = 3.54076, its = 6
i = 5874 (of 10192), d = 10, its = 53
i = 5875 (of 10192), d = 10, its = 55
i = 5876 (of 10192), d = 10, its = 55
i = 5877 (of 10192), d = 10, its = 37
i = 5878 (of 10192), d = 10, its = 37
i = 5879 (of 10192), d = 10, its = 54
i = 5880 (of 10192), d = 2.91281, its = 6
i = 5881 (of 10192), d = 2.56867, its = 5
i = 5882 (of 10192), d = 10, its = 51
i = 5883 (of 10192), d = 2.71591, its = 6
i = 5884 (of 10192), d = 10, its = 37
i = 5885 (of 10192), d = 10, its = 37
i = 5886 (of 10192), d = 1.85698, its = 4
i = 5887 (of 10192), d = 10, its = 40
i = 5888 (of 10192), d = 10, its = 50
i = 5889 (of 10192), d = 2.13297, its = 4
i = 5890 (of 10192), d = 10, its = 53
i = 5891 (of 10192), d = 10, its = 54
i = 5892 (of 10192), d = 10, its = 39
i = 5893 (of 10192), d = 10, its = 52
i = 5894 (of 10192), d = 10, its = 37
i = 5895 (of 10192), d = 10, its = 48
i = 5896 (of 10192), d = 1.90693, its = 4
i = 5897 (of 10192), d = 10, its = 50
i = 5898 (of 10192), d = 10, its = 40
i = 5899 (of 10192), d = 1.52926, its = 7
i = 5900 (of 10192), d = 10, its = 37
i = 5901 (of 10192), d = 10, its = 52
i = 5902 (of 10192), d = 10, its = 52
i = 5903 (of 10192), d = 10, its = 50
i = 5904 (of 10192), d = 10, its = 53
i = 5905 (of 10192), d = 2.00729, its = 3
i = 5906 (of 10192), d = 10, its = 39
i = 5907 (of 10192), d = 10, its = 51
i = 5908 (of 10192), d = 2.01508, its = 3
i = 5909 (of 10192), d = 10, its = 51
i = 5910 (of 10192), d = 10, its = 46
i = 5911 (of 10192), d = 10, its = 38
i = 5912 (of 10192), d = 10, its = 41
i = 5913 (of 10192), d = 10, its = 42
i = 5914 (of 10192), d = 10, its = 53
i = 5915 (of 10192), d = 2.42791, its = 5
i = 5916 (of 10192), d = 10, its = 50
i = 5917 (of 10192), d = 10, its = 55
i = 5918 (of 10192), d = 10, its = 54
i = 5919 (of 10192), d = 2.23082, its = 4
i = 5920 (of 10192), d = 10, its = 53
i = 5921 (of 10192), d = 0.554581, its = 19
i = 5922 (of 10192), d = 10, its = 37
i = 5923 (of 10192), d = 10, its = 55
i = 5924 (of 10192), d = 10, its = 51
i = 5925 (of 10192), d = 10, its = 41
i = 5926 (of 10192), d = 10, its = 55
i = 5927 (of 10192), d = 10, its = 55
i = 5928 (of 10192), d = 10, its = 54
i = 5929 (of 10192), d = 2.63978, its = 5
i = 5930 (of 10192), d = 1.99068, its = 3
i = 5931 (of 10192), d = 1.76661, its = 5
i = 5932 (of 10192), d = 10, its = 54
i = 5933 (of 10192), d = 10, its = 50
i = 5934 (of 10192), d = 10, its = 54
i = 5935 (of 10192), d = 10, its = 50
i = 5936 (of 10192), d = 2.82383, its = 6
i = 5937 (of 10192), d = 2.90095, its = 7
i = 5938 (of 10192), d = 1.9587, its = 3
i = 5939 (of 10192), d = 10, its = 50
i = 5940 (of 10192), d = 10, its = 37
i = 5941 (of 10192), d = 10, its = 57
i = 5942 (of 10192), d = 10, its = 53
i = 5943 (of 10192), d = 2.11304, its = 4
i = 5944 (of 10192), d = 0.654703, its = 17
i = 5945 (of 10192), d = 4.51916, its = 7
i = 5946 (of 10192), d = 10, its = 49
i = 5947 (of 10192), d = 1.97904, its = 3
i = 5948 (of 10192), d = 10, its = 37
i = 5949 (of 10192), d = 10, its = 40
i = 5950 (of 10192), d = 0.678416, its = 22
i = 5951 (of 10192), d = 10, its = 37
i = 5952 (of 10192), d = 10, its = 37
i = 5953 (of 10192), d = 10, its = 54
i = 5954 (of 10192), d = 10, its = 51
i = 5955 (of 10192), d = 10, its = 50
i = 5956 (of 10192), d = 10, its = 51
i = 5957 (of 10192), d = 10, its = 37
i = 5958 (of 10192), d = 1.6848, its = 6
i = 5959 (of 10192), d = 10, its = 37
i = 5960 (of 10192), d = 3.74852, its = 7
i = 5961 (of 10192), d = 10, its = 37
i = 5962 (of 10192), d = 1.42018, its = 6
i = 5963 (of 10192), d = 1.52414, its = 6
i = 5964 (of 10192), d = 10, its = 37
i = 5965 (of 10192), d = 1.52737, its = 7
i = 5966 (of 10192), d = 10, its = 54
i = 5967 (of 10192), d = 10, its = 51
i = 5968 (of 10192), d = 10, its = 57
i = 5969 (of 10192), d = 10, its = 51
i = 5970 (of 10192), d = 2.50633, its = 6
i = 5971 (of 10192), d = 10, its = 54
i = 5972 (of 10192), d = 4.53756, its = 7
i = 5973 (of 10192), d = 10, its = 14
i = 5974 (of 10192), d = 10, its = 53
i = 5975 (of 10192), d = 10, its = 50
i = 5976 (of 10192), d = 10, its = 44
i = 5977 (of 10192), d = 10, its = 39
i = 5978 (of 10192), d = 10, its = 49
i = 5979 (of 10192), d = 10, its = 52
i = 5980 (of 10192), d = 10, its = 43
i = 5981 (of 10192), d = 10, its = 37
i = 5982 (of 10192), d = 10, its = 52
i = 5983 (of 10192), d = 4.45151, its = 7
i = 5984 (of 10192), d = 10, its = 56
i = 5985 (of 10192), d = 2.50229, its = 5
i = 5986 (of 10192), d = 2.32141, its = 5
i = 5987 (of 10192), d = 3.29234, its = 6
i = 5988 (of 10192), d = 10, its = 37
i = 5989 (of 10192), d = 10, its = 37
i = 5990 (of 10192), d = 0.65611, its = 18
i = 5991 (of 10192), d = 2.63961, its = 6
i = 5992 (of 10192), d = 10, its = 49
i = 5993 (of 10192), d = 10, its = 48
i = 5994 (of 10192), d = 10, its = 41
i = 5995 (of 10192), d = 10, its = 40
i = 5996 (of 10192), d = 10, its = 40
i = 5997 (of 10192), d = 10, its = 56
i = 5998 (of 10192), d = 10, its = 43
i = 5999 (of 10192), d = 10, its = 56
i = 6000 (of 10192), d = 10, its = 53
i = 6001 (of 10192), d = 10, its = 39
i = 6002 (of 10192), d = 2.4207, its = 5
i = 6003 (of 10192), d = 10, its = 54
i = 6004 (of 10192), d = 10, its = 37
i = 6005 (of 10192), d = 10, its = 40
i = 6006 (of 10192), d = 1.06149, its = 17
i = 6007 (of 10192), d = 0.883632, its = 23
i = 6008 (of 10192), d = 3.30151, its = 8
i = 6009 (of 10192), d = 1.41013, its = 6
i = 6010 (of 10192), d = 10, its = 50
i = 6011 (of 10192), d = 10, its = 53
i = 6012 (of 10192), d = 10, its = 44
i = 6013 (of 10192), d = 2.58018, its = 5
i = 6014 (of 10192), d = 10, its = 37
i = 6015 (of 10192), d = 10, its = 51
i = 6016 (of 10192), d = 2.46671, its = 6
i = 6017 (of 10192), d = 2.91465, its = 6
i = 6018 (of 10192), d = 1.69678, its = 6
i = 6019 (of 10192), d = 10, its = 37
i = 6020 (of 10192), d = 1.15416, its = 20
i = 6021 (of 10192), d = 10, its = 54
i = 6022 (of 10192), d = 10, its = 52
i = 6023 (of 10192), d = 1.84516, its = 5
i = 6024 (of 10192), d = 10, its = 38
i = 6025 (of 10192), d = 10, its = 55
i = 6026 (of 10192), d = 10, its = 37
i = 6027 (of 10192), d = 3.0153, its = 6
i = 6028 (of 10192), d = 10, its = 46
i = 6029 (of 10192), d = 6.838, its = 8
i = 6030 (of 10192), d = 1.77889, its = 5
i = 6031 (of 10192), d = 10, its = 55
i = 6032 (of 10192), d = 10, its = 55
i = 6033 (of 10192), d = 10, its = 48
i = 6034 (of 10192), d = 2.96873, its = 6
i = 6035 (of 10192), d = 10, its = 41
i = 6036 (of 10192), d = 10, its = 41
i = 6037 (of 10192), d = 1.36032, its = 6
i = 6038 (of 10192), d = 2.19067, its = 5
i = 6039 (of 10192), d = 10, its = 40
i = 6040 (of 10192), d = 10, its = 51
i = 6041 (of 10192), d = 10, its = 37
i = 6042 (of 10192), d = 10, its = 37
i = 6043 (of 10192), d = 10, its = 53
i = 6044 (of 10192), d = 10, its = 43
i = 6045 (of 10192), d = 10, its = 38
i = 6046 (of 10192), d = 10, its = 49
i = 6047 (of 10192), d = 10, its = 52
i = 6048 (of 10192), d = 10, its = 55
i = 6049 (of 10192), d = 10, its = 53
i = 6050 (of 10192), d = 1.67185, its = 5
i = 6051 (of 10192), d = 10, its = 56
i = 6052 (of 10192), d = 10, its = 54
i = 6053 (of 10192), d = 2.41126, its = 5
i = 6054 (of 10192), d = 10, its = 48
i = 6055 (of 10192), d = 10, its = 51
i = 6056 (of 10192), d = 10, its = 37
i = 6057 (of 10192), d = 4.87758, its = 7
i = 6058 (of 10192), d = 0.975567, its = 19
i = 6059 (of 10192), d = 10, its = 50
i = 6060 (of 10192), d = 10, its = 54
i = 6061 (of 10192), d = 3.72517, its = 7
i = 6062 (of 10192), d = 10, its = 37
i = 6063 (of 10192), d = 2.86515, its = 7
i = 6064 (of 10192), d = 10, its = 52
i = 6065 (of 10192), d = 10, its = 53
i = 6066 (of 10192), d = 10, its = 51
i = 6067 (of 10192), d = 10, its = 50
i = 6068 (of 10192), d = 10, its = 50
i = 6069 (of 10192), d = 2.72963, its = 6
i = 6070 (of 10192), d = 10, its = 37
i = 6071 (of 10192), d = 10, its = 50
i = 6072 (of 10192), d = 2.12682, its = 4
i = 6073 (of 10192), d = 1.43953, its = 8
i = 6074 (of 10192), d = 10, its = 37
i = 6075 (of 10192), d = 1.61266, its = 6
i = 6076 (of 10192), d = 10, its = 54
i = 6077 (of 10192), d = 1.98414, its = 3
i = 6078 (of 10192), d = 2.1569, its = 4
i = 6079 (of 10192), d = 2.13212, its = 4
i = 6080 (of 10192), d = 10, its = 37
i = 6081 (of 10192), d = 10, its = 51
i = 6082 (of 10192), d = 10, its = 37
i = 6083 (of 10192), d = 10, its = 51
i = 6084 (of 10192), d = 4.03998, its = 7
i = 6085 (of 10192), d = 1.70051, its = 6
i = 6086 (of 10192), d = 1.57129, its = 6
i = 6087 (of 10192), d = 0.325814, its = 19
i = 6088 (of 10192), d = 10, its = 54
i = 6089 (of 10192), d = 1.76987, its = 5
i = 6090 (of 10192), d = 1.26213, its = 6
i = 6091 (of 10192), d = 1.6427, its = 5
i = 6092 (of 10192), d = 10, its = 37
i = 6093 (of 10192), d = 10, its = 50
i = 6094 (of 10192), d = 10, its = 38
i = 6095 (of 10192), d = 1.67502, its = 5
i = 6096 (of 10192), d = 10, its = 41
i = 6097 (of 10192), d = 1.70894, its = 5
i = 6098 (of 10192), d = 10, its = 40
i = 6099 (of 10192), d = 0.886467, its = 18
i = 6100 (of 10192), d = 10, its = 37
i = 6101 (of 10192), d = 10, its = 54
i = 6102 (of 10192), d = 2.86148, its = 6
i = 6103 (of 10192), d = 10, its = 58
i = 6104 (of 10192), d = 10, its = 49
i = 6105 (of 10192), d = 3.30838, its = 6
i = 6106 (of 10192), d = 10, its = 53
i = 6107 (of 10192), d = 10, its = 44
i = 6108 (of 10192), d = 10, its = 41
i = 6109 (of 10192), d = 1.78503, its = 5
i = 6110 (of 10192), d = 10, its = 51
i = 6111 (of 10192), d = 10, its = 51
i = 6112 (of 10192), d = 10, its = 48
i = 6113 (of 10192), d = 10, its = 20
i = 6114 (of 10192), d = 0.680479, its = 16
i = 6115 (of 10192), d = 1.32387, its = 6
i = 6116 (of 10192), d = 10, its = 43
i = 6117 (of 10192), d = 1.93126, its = 4
i = 6118 (of 10192), d = 4.07325, its = 6
i = 6119 (of 10192), d = 10, its = 37
i = 6120 (of 10192), d = 10, its = 53
i = 6121 (of 10192), d = 10, its = 49
i = 6122 (of 10192), d = 10, its = 40
i = 6123 (of 10192), d = 10, its = 37
i = 6124 (of 10192), d = 10, its = 54
i = 6125 (of 10192), d = 10, its = 51
i = 6126 (of 10192), d = 10, its = 37
i = 6127 (of 10192), d = 2.31483, its = 5
i = 6128 (of 10192), d = 10, its = 51
i = 6129 (of 10192), d = 10, its = 52
i = 6130 (of 10192), d = 10, its = 55
i = 6131 (of 10192), d = 10, its = 50
i = 6132 (of 10192), d = 1.91395, its = 4
i = 6133 (of 10192), d = 10, its = 43
i = 6134 (of 10192), d = 10, its = 55
i = 6135 (of 10192), d = 10, its = 54
i = 6136 (of 10192), d = 10, its = 55
i = 6137 (of 10192), d = 10, its = 52
i = 6138 (of 10192), d = 1.94908, its = 3
i = 6139 (of 10192), d = 10, its = 51
i = 6140 (of 10192), d = 2.45175, its = 5
i = 6141 (of 10192), d = 10, its = 55
i = 6142 (of 10192), d = 10, its = 39
i = 6143 (of 10192), d = 10, its = 48
i = 6144 (of 10192), d = 10, its = 37
i = 6145 (of 10192), d = 10, its = 47
i = 6146 (of 10192), d = 10, its = 51
i = 6147 (of 10192), d = 1.4059, its = 7
i = 6148 (of 10192), d = 10, its = 56
i = 6149 (of 10192), d = 10, its = 49
i = 6150 (of 10192), d = 10, its = 53
i = 6151 (of 10192), d = 0.87789, its = 20
i = 6152 (of 10192), d = 1.43893, its = 7
i = 6153 (of 10192), d = 10, its = 53
i = 6154 (of 10192), d = 10, its = 56
i = 6155 (of 10192), d = 10, its = 55
i = 6156 (of 10192), d = 10, its = 48
i = 6157 (of 10192), d = 1.88272, its = 5
i = 6158 (of 10192), d = 10, its = 52
i = 6159 (of 10192), d = 10, its = 38
i = 6160 (of 10192), d = 10, its = 51
i = 6161 (of 10192), d = 10, its = 51
i = 6162 (of 10192), d = 1.99231, its = 3
i = 6163 (of 10192), d = 10, its = 39
i = 6164 (of 10192), d = 2.5967, its = 5
i = 6165 (of 10192), d = 5.07299, its = 7
i = 6166 (of 10192), d = 10, its = 52
i = 6167 (of 10192), d = 3.23198, its = 6
i = 6168 (of 10192), d = 1.2909, its = 21
i = 6169 (of 10192), d = 3.1715, its = 6
i = 6170 (of 10192), d = 10, its = 39
i = 6171 (of 10192), d = 10, its = 53
i = 6172 (of 10192), d = 10, its = 57
i = 6173 (of 10192), d = 10, its = 50
i = 6174 (of 10192), d = 1.53899, its = 7
i = 6175 (of 10192), d = 10, its = 37
i = 6176 (of 10192), d = 2.38458, its = 5
i = 6177 (of 10192), d = 2.06557, its = 3
i = 6178 (of 10192), d = 1.09744, its = 17
i = 6179 (of 10192), d = 10, its = 57
i = 6180 (of 10192), d = 0.904971, its = 20
i = 6181 (of 10192), d = 10, its = 37
i = 6182 (of 10192), d = 1.93703, its = 4
i = 6183 (of 10192), d = 10, its = 60
i = 6184 (of 10192), d = 10, its = 53
i = 6185 (of 10192), d = 3.92425, its = 7
i = 6186 (of 10192), d = 10, its = 52
i = 6187 (of 10192), d = 10, its = 40
i = 6188 (of 10192), d = 10, its = 43
i = 6189 (of 10192), d = 1.60456, its = 6
i = 6190 (of 10192), d = 10, its = 54
i = 6191 (of 10192), d = 10, its = 37
i = 6192 (of 10192), d = 2.17952, its = 4
i = 6193 (of 10192), d = 10, its = 52
i = 6194 (of 10192), d = 10, its = 55
i = 6195 (of 10192), d = 4.57347, its = 8
i = 6196 (of 10192), d = 10, its = 57
i = 6197 (of 10192), d = 10, its = 37
i = 6198 (of 10192), d = 10, its = 52
i = 6199 (of 10192), d = 10, its = 51
i = 6200 (of 10192), d = 10, its = 52
i = 6201 (of 10192), d = 10, its = 52
i = 6202 (of 10192), d = 1.51453, its = 6
i = 6203 (of 10192), d = 10, its = 37
i = 6204 (of 10192), d = 1.67787, its = 5
i = 6205 (of 10192), d = 10, its = 48
i = 6206 (of 10192), d = 10, its = 41
i = 6207 (of 10192), d = 0.98531, its = 18
i = 6208 (of 10192), d = 10, its = 37
i = 6209 (of 10192), d = 10, its = 52
i = 6210 (of 10192), d = 1.52943, its = 7
i = 6211 (of 10192), d = 10, its = 52
i = 6212 (of 10192), d = 3.49248, its = 6
i = 6213 (of 10192), d = 1.76563, its = 5
i = 6214 (of 10192), d = 10, its = 52
i = 6215 (of 10192), d = 2.0331, its = 3
i = 6216 (of 10192), d = 10, its = 41
i = 6217 (of 10192), d = 2.34029, its = 5
i = 6218 (of 10192), d = 2.4365, its = 5
i = 6219 (of 10192), d = 10, its = 54
i = 6220 (of 10192), d = 0.886292, its = 21
i = 6221 (of 10192), d = 10, its = 54
i = 6222 (of 10192), d = 10, its = 53
i = 6223 (of 10192), d = 10, its = 37
i = 6224 (of 10192), d = 10, its = 54
i = 6225 (of 10192), d = 10, its = 52
i = 6226 (of 10192), d = 10, its = 51
i = 6227 (of 10192), d = 10, its = 58
i = 6228 (of 10192), d = 10, its = 49
i = 6229 (of 10192), d = 10, its = 43
i = 6230 (of 10192), d = 10, its = 38
i = 6231 (of 10192), d = 1.60456, its = 6
i = 6232 (of 10192), d = 10, its = 37
i = 6233 (of 10192), d = 10, its = 50
i = 6234 (of 10192), d = 10, its = 40
i = 6235 (of 10192), d = 3.07387, its = 6
i = 6236 (of 10192), d = 10, its = 54
i = 6237 (of 10192), d = 10, its = 49
i = 6238 (of 10192), d = 0.735027, its = 18
i = 6239 (of 10192), d = 10, its = 49
i = 6240 (of 10192), d = 10, its = 53
i = 6241 (of 10192), d = 10, its = 38
i = 6242 (of 10192), d = 3.37322, its = 7
i = 6243 (of 10192), d = 2.93078, its = 6
i = 6244 (of 10192), d = 1.08835, its = 19
i = 6245 (of 10192), d = 10, its = 39
i = 6246 (of 10192), d = 10, its = 52
i = 6247 (of 10192), d = 10, its = 53
i = 6248 (of 10192), d = 1.59847, its = 7
i = 6249 (of 10192), d = 10, its = 58
i = 6250 (of 10192), d = 0.729794, its = 17
i = 6251 (of 10192), d = 10, its = 54
i = 6252 (of 10192), d = 10, its = 53
i = 6253 (of 10192), d = 2.17545, its = 4
i = 6254 (of 10192), d = 10, its = 49
i = 6255 (of 10192), d = 10, its = 53
i = 6256 (of 10192), d = 10, its = 40
i = 6257 (of 10192), d = 10, its = 58
i = 6258 (of 10192), d = 5.86258, its = 9
i = 6259 (of 10192), d = 10, its = 19
i = 6260 (of 10192), d = 10, its = 55
i = 6261 (of 10192), d = 10, its = 41
i = 6262 (of 10192), d = 10, its = 54
i = 6263 (of 10192), d = 10, its = 41
i = 6264 (of 10192), d = 2.28222, its = 5
i = 6265 (of 10192), d = 10, its = 39
i = 6266 (of 10192), d = 10, its = 37
i = 6267 (of 10192), d = 1.40904, its = 23
i = 6268 (of 10192), d = 10, its = 51
i = 6269 (of 10192), d = 1.28539, its = 6
i = 6270 (of 10192), d = 10, its = 53
i = 6271 (of 10192), d = 4.08994, its = 8
i = 6272 (of 10192), d = 10, its = 54
i = 6273 (of 10192), d = 10, its = 53
i = 6274 (of 10192), d = 10, its = 54
i = 6275 (of 10192), d = 10, its = 37
i = 6276 (of 10192), d = 2.49293, its = 5
i = 6277 (of 10192), d = 10, its = 51
i = 6278 (of 10192), d = 10, its = 43
i = 6279 (of 10192), d = 10, its = 37
i = 6280 (of 10192), d = 10, its = 58
i = 6281 (of 10192), d = 0.732264, its = 19
i = 6282 (of 10192), d = 2.08715, its = 4
i = 6283 (of 10192), d = 10, its = 37
i = 6284 (of 10192), d = 2.58325, its = 5
i = 6285 (of 10192), d = 10, its = 52
i = 6286 (of 10192), d = 10, its = 54
i = 6287 (of 10192), d = 10, its = 48
i = 6288 (of 10192), d = 10, its = 53
i = 6289 (of 10192), d = 10, its = 37
i = 6290 (of 10192), d = 1.27555, its = 6
i = 6291 (of 10192), d = 10, its = 50
i = 6292 (of 10192), d = 10, its = 40
i = 6293 (of 10192), d = 2.78405, its = 6
i = 6294 (of 10192), d = 10, its = 54
i = 6295 (of 10192), d = 2.17723, its = 4
i = 6296 (of 10192), d = 1.99877, its = 2
i = 6297 (of 10192), d = 1.49679, its = 7
i = 6298 (of 10192), d = 1.10681, its = 19
i = 6299 (of 10192), d = 10, its = 54
i = 6300 (of 10192), d = 0.665504, its = 18
i = 6301 (of 10192), d = 1.08616, its = 21
i = 6302 (of 10192), d = 2.15088, its = 4
i = 6303 (of 10192), d = 10, its = 51
i = 6304 (of 10192), d = 1.90963, its = 4
i = 6305 (of 10192), d = 10, its = 40
i = 6306 (of 10192), d = 10, its = 53
i = 6307 (of 10192), d = 2.48637, its = 5
i = 6308 (of 10192), d = 10, its = 51
i = 6309 (of 10192), d = 4.71428, its = 7
i = 6310 (of 10192), d = 10, its = 41
i = 6311 (of 10192), d = 10, its = 51
i = 6312 (of 10192), d = 10, its = 55
i = 6313 (of 10192), d = 1.27977, its = 18
i = 6314 (of 10192), d = 10, its = 50
i = 6315 (of 10192), d = 10, its = 48
i = 6316 (of 10192), d = 1.52214, its = 8
i = 6317 (of 10192), d = 10, its = 59
i = 6318 (of 10192), d = 0.669572, its = 16
i = 6319 (of 10192), d = 10, its = 51
i = 6320 (of 10192), d = 10, its = 54
i = 6321 (of 10192), d = 10, its = 55
i = 6322 (of 10192), d = 10, its = 37
i = 6323 (of 10192), d = 10, its = 58
i = 6324 (of 10192), d = 10, its = 52
i = 6325 (of 10192), d = 10, its = 55
i = 6326 (of 10192), d = 10, its = 54
i = 6327 (of 10192), d = 10, its = 51
i = 6328 (of 10192), d = 10, its = 58
i = 6329 (of 10192), d = 10, its = 53
i = 6330 (of 10192), d = 10, its = 51
i = 6331 (of 10192), d = 0.967191, its = 16
i = 6332 (of 10192), d = 10, its = 40
i = 6333 (of 10192), d = 1.04787, its = 19
i = 6334 (of 10192), d = 10, its = 55
i = 6335 (of 10192), d = 10, its = 53
i = 6336 (of 10192), d = 2.33346, its = 5
i = 6337 (of 10192), d = 10, its = 43
i = 6338 (of 10192), d = 1.8188, its = 5
i = 6339 (of 10192), d = 10, its = 55
i = 6340 (of 10192), d = 10, its = 40
i = 6341 (of 10192), d = 10, its = 40
i = 6342 (of 10192), d = 10, its = 41
i = 6343 (of 10192), d = 10, its = 54
i = 6344 (of 10192), d = 1.91927, its = 4
i = 6345 (of 10192), d = 10, its = 54
i = 6346 (of 10192), d = 0.871381, its = 23
i = 6347 (of 10192), d = 3.39117, its = 7
i = 6348 (of 10192), d = 10, its = 45
i = 6349 (of 10192), d = 10, its = 50
i = 6350 (of 10192), d = 1.44908, its = 6
i = 6351 (of 10192), d = 10, its = 37
i = 6352 (of 10192), d = 10, its = 37
i = 6353 (of 10192), d = 10, its = 50
i = 6354 (of 10192), d = 1.12149, its = 18
i = 6355 (of 10192), d = 10, its = 37
i = 6356 (of 10192), d = 10, its = 55
i = 6357 (of 10192), d = 10, its = 52
i = 6358 (of 10192), d = 1.28349, its = 7
i = 6359 (of 10192), d = 10, its = 53
i = 6360 (of 10192), d = 2.63497, its = 5
i = 6361 (of 10192), d = 10, its = 53
i = 6362 (of 10192), d = 10, its = 52
i = 6363 (of 10192), d = 10, its = 43
i = 6364 (of 10192), d = 10, its = 53
i = 6365 (of 10192), d = 10, its = 48
i = 6366 (of 10192), d = 2.67812, its = 5
i = 6367 (of 10192), d = 10, its = 37
i = 6368 (of 10192), d = 10, its = 52
i = 6369 (of 10192), d = 2.66733, its = 5
i = 6370 (of 10192), d = 10, its = 39
i = 6371 (of 10192), d = 10, its = 59
i = 6372 (of 10192), d = 2.61843, its = 5
i = 6373 (of 10192), d = 10, its = 52
i = 6374 (of 10192), d = 2.13563, its = 4
i = 6375 (of 10192), d = 10, its = 52
i = 6376 (of 10192), d = 10, its = 53
i = 6377 (of 10192), d = 10, its = 41
i = 6378 (of 10192), d = 1.58966, its = 6
i = 6379 (of 10192), d = 1.89503, its = 4
i = 6380 (of 10192), d = 10, its = 52
i = 6381 (of 10192), d = 10, its = 41
i = 6382 (of 10192), d = 10, its = 37
i = 6383 (of 10192), d = 10, its = 56
i = 6384 (of 10192), d = 10, its = 41
i = 6385 (of 10192), d = 10, its = 55
i = 6386 (of 10192), d = 10, its = 51
i = 6387 (of 10192), d = 10, its = 58
i = 6388 (of 10192), d = 8.92941, its = 8
i = 6389 (of 10192), d = 10, its = 54
i = 6390 (of 10192), d = 0.867787, its = 17
i = 6391 (of 10192), d = 10, its = 57
i = 6392 (of 10192), d = 10, its = 40
i = 6393 (of 10192), d = 10, its = 55
i = 6394 (of 10192), d = 1.66617, its = 5
i = 6395 (of 10192), d = 3.13898, its = 6
i = 6396 (of 10192), d = 10, its = 43
i = 6397 (of 10192), d = 5.57649, its = 7
i = 6398 (of 10192), d = 10, its = 48
i = 6399 (of 10192), d = 2.2461, its = 4
i = 6400 (of 10192), d = 1.81866, its = 5
i = 6401 (of 10192), d = 2.72117, its = 6
i = 6402 (of 10192), d = 1.9114, its = 4
i = 6403 (of 10192), d = 10, its = 39
i = 6404 (of 10192), d = 10, its = 53
i = 6405 (of 10192), d = 4.11978, its = 8
i = 6406 (of 10192), d = 10, its = 54
i = 6407 (of 10192), d = 10, its = 37
i = 6408 (of 10192), d = 4.53756, its = 7
i = 6409 (of 10192), d = 10, its = 41
i = 6410 (of 10192), d = 5.77736, its = 8
i = 6411 (of 10192), d = 10, its = 57
i = 6412 (of 10192), d = 10, its = 50
i = 6413 (of 10192), d = 10, its = 52
i = 6414 (of 10192), d = 10, its = 50
i = 6415 (of 10192), d = 2.53667, its = 6
i = 6416 (of 10192), d = 0.447037, its = 18
i = 6417 (of 10192), d = 10, its = 39
i = 6418 (of 10192), d = 2.94744, its = 6
i = 6419 (of 10192), d = 10, its = 54
i = 6420 (of 10192), d = 3.30912, its = 7
i = 6421 (of 10192), d = 2.552, its = 6
i = 6422 (of 10192), d = 10, its = 53
i = 6423 (of 10192), d = 10, its = 49
i = 6424 (of 10192), d = 10, its = 42
i = 6425 (of 10192), d = 10, its = 50
i = 6426 (of 10192), d = 2.74924, its = 6
i = 6427 (of 10192), d = 10, its = 51
i = 6428 (of 10192), d = 3.80903, its = 7
i = 6429 (of 10192), d = 10, its = 55
i = 6430 (of 10192), d = 2.69324, its = 5
i = 6431 (of 10192), d = 10, its = 52
i = 6432 (of 10192), d = 10, its = 55
i = 6433 (of 10192), d = 10, its = 53
i = 6434 (of 10192), d = 10, its = 52
i = 6435 (of 10192), d = 1.10877, its = 22
i = 6436 (of 10192), d = 10, its = 37
i = 6437 (of 10192), d = 1.65074, its = 6
i = 6438 (of 10192), d = 10, its = 53
i = 6439 (of 10192), d = 10, its = 52
i = 6440 (of 10192), d = 4.84755, its = 7
i = 6441 (of 10192), d = 2.58621, its = 5
i = 6442 (of 10192), d = 1.56205, its = 6
i = 6443 (of 10192), d = 10, its = 52
i = 6444 (of 10192), d = 10, its = 52
i = 6445 (of 10192), d = 10, its = 37
i = 6446 (of 10192), d = 10, its = 54
i = 6447 (of 10192), d = 8.43979, its = 11
i = 6448 (of 10192), d = 10, its = 54
i = 6449 (of 10192), d = 1.16759, its = 22
i = 6450 (of 10192), d = 10, its = 50
i = 6451 (of 10192), d = 10, its = 41
i = 6452 (of 10192), d = 2.13472, its = 4
i = 6453 (of 10192), d = 10, its = 53
i = 6454 (of 10192), d = 10, its = 54
i = 6455 (of 10192), d = 10, its = 54
i = 6456 (of 10192), d = 10, its = 58
i = 6457 (of 10192), d = 1.7811, its = 5
i = 6458 (of 10192), d = 10, its = 39
i = 6459 (of 10192), d = 10, its = 51
i = 6460 (of 10192), d = 10, its = 54
i = 6461 (of 10192), d = 10, its = 53
i = 6462 (of 10192), d = 0.985537, its = 19
i = 6463 (of 10192), d = 10, its = 53
i = 6464 (of 10192), d = 0.834182, its = 25
i = 6465 (of 10192), d = 10, its = 59
i = 6466 (of 10192), d = 10, its = 52
i = 6467 (of 10192), d = 10, its = 53
i = 6468 (of 10192), d = 10, its = 53
i = 6469 (of 10192), d = 2.94123, its = 6
i = 6470 (of 10192), d = 7.88017, its = 27
i = 6471 (of 10192), d = 10, its = 54
i = 6472 (of 10192), d = 10, its = 37
i = 6473 (of 10192), d = 1.92196, its = 4
i = 6474 (of 10192), d = 10, its = 50
i = 6475 (of 10192), d = 10, its = 37
i = 6476 (of 10192), d = 10, its = 46
i = 6477 (of 10192), d = 10, its = 52
i = 6478 (of 10192), d = 10, its = 52
i = 6479 (of 10192), d = 10, its = 52
i = 6480 (of 10192), d = 10, its = 52
i = 6481 (of 10192), d = 1.39144, its = 7
i = 6482 (of 10192), d = 10, its = 53
i = 6483 (of 10192), d = 10, its = 52
i = 6484 (of 10192), d = 10, its = 50
i = 6485 (of 10192), d = 10, its = 53
i = 6486 (of 10192), d = 10, its = 53
i = 6487 (of 10192), d = 0.845355, its = 25
i = 6488 (of 10192), d = 10, its = 54
i = 6489 (of 10192), d = 10, its = 51
i = 6490 (of 10192), d = 10, its = 51
i = 6491 (of 10192), d = 1.79758, its = 5
i = 6492 (of 10192), d = 10, its = 53
i = 6493 (of 10192), d = 2.28966, its = 5
i = 6494 (of 10192), d = 10, its = 41
i = 6495 (of 10192), d = 10, its = 53
i = 6496 (of 10192), d = 0.893152, its = 21
i = 6497 (of 10192), d = 10, its = 54
i = 6498 (of 10192), d = 10, its = 50
i = 6499 (of 10192), d = 10, its = 40
i = 6500 (of 10192), d = 1.4807, its = 8
i = 6501 (of 10192), d = 2.91087, its = 5
i = 6502 (of 10192), d = 10, its = 37
i = 6503 (of 10192), d = 10, its = 50
i = 6504 (of 10192), d = 2.59176, its = 5
i = 6505 (of 10192), d = 10, its = 44
i = 6506 (of 10192), d = 10, its = 52
i = 6507 (of 10192), d = 10, its = 54
i = 6508 (of 10192), d = 10, its = 37
i = 6509 (of 10192), d = 10, its = 52
i = 6510 (of 10192), d = 10, its = 39
i = 6511 (of 10192), d = 10, its = 51
i = 6512 (of 10192), d = 0.772465, its = 18
i = 6513 (of 10192), d = 10, its = 52
i = 6514 (of 10192), d = 10, its = 57
i = 6515 (of 10192), d = 2.17652, its = 4
i = 6516 (of 10192), d = 10, its = 53
i = 6517 (of 10192), d = 1.89832, its = 4
i = 6518 (of 10192), d = 10, its = 51
i = 6519 (of 10192), d = 10, its = 54
i = 6520 (of 10192), d = 1.74903, its = 5
i = 6521 (of 10192), d = 3.44811, its = 6
i = 6522 (of 10192), d = 10, its = 53
i = 6523 (of 10192), d = 10, its = 51
i = 6524 (of 10192), d = 10, its = 56
i = 6525 (of 10192), d = 10, its = 37
i = 6526 (of 10192), d = 10, its = 52
i = 6527 (of 10192), d = 10, its = 53
i = 6528 (of 10192), d = 10, its = 52
i = 6529 (of 10192), d = 10, its = 52
i = 6530 (of 10192), d = 2.88384, its = 6
i = 6531 (of 10192), d = 10, its = 55
i = 6532 (of 10192), d = 10, its = 57
i = 6533 (of 10192), d = 1.02387, its = 16
i = 6534 (of 10192), d = 10, its = 55
i = 6535 (of 10192), d = 10, its = 55
i = 6536 (of 10192), d = 2.53956, its = 5
i = 6537 (of 10192), d = 10, its = 47
i = 6538 (of 10192), d = 5.25308, its = 8
i = 6539 (of 10192), d = 10, its = 52
i = 6540 (of 10192), d = 10, its = 58
i = 6541 (of 10192), d = 1.37588, its = 6
i = 6542 (of 10192), d = 0.831998, its = 17
i = 6543 (of 10192), d = 10, its = 52
i = 6544 (of 10192), d = 2.1045, its = 4
i = 6545 (of 10192), d = 10, its = 37
i = 6546 (of 10192), d = 10, its = 51
i = 6547 (of 10192), d = 10, its = 55
i = 6548 (of 10192), d = 10, its = 54
i = 6549 (of 10192), d = 1.63912, its = 7
i = 6550 (of 10192), d = 10, its = 55
i = 6551 (of 10192), d = 10, its = 54
i = 6552 (of 10192), d = 10, its = 54
i = 6553 (of 10192), d = 1.30798, its = 7
i = 6554 (of 10192), d = 1.37506, its = 6
i = 6555 (of 10192), d = 10, its = 37
i = 6556 (of 10192), d = 0.800039, its = 16
i = 6557 (of 10192), d = 4.85078, its = 8
i = 6558 (of 10192), d = 10, its = 53
i = 6559 (of 10192), d = 10, its = 51
i = 6560 (of 10192), d = 1.50575, its = 6
i = 6561 (of 10192), d = 10, its = 54
i = 6562 (of 10192), d = 10, its = 53
i = 6563 (of 10192), d = 10, its = 41
i = 6564 (of 10192), d = 10, its = 51
i = 6565 (of 10192), d = 2.37788, its = 5
i = 6566 (of 10192), d = 10, its = 37
i = 6567 (of 10192), d = 10, its = 51
i = 6568 (of 10192), d = 10, its = 52
i = 6569 (of 10192), d = 10, its = 54
i = 6570 (of 10192), d = 10, its = 54
i = 6571 (of 10192), d = 10, its = 54
i = 6572 (of 10192), d = 1.15367, its = 22
i = 6573 (of 10192), d = 1.69594, its = 5
i = 6574 (of 10192), d = 10, its = 37
i = 6575 (of 10192), d = 10, its = 39
i = 6576 (of 10192), d = 1.93991, its = 4
i = 6577 (of 10192), d = 10, its = 53
i = 6578 (of 10192), d = 10, its = 52
i = 6579 (of 10192), d = 10, its = 54
i = 6580 (of 10192), d = 10, its = 39
i = 6581 (of 10192), d = 10, its = 40
i = 6582 (of 10192), d = 1.06023, its = 20
i = 6583 (of 10192), d = 10, its = 41
i = 6584 (of 10192), d = 10, its = 54
i = 6585 (of 10192), d = 10, its = 53
i = 6586 (of 10192), d = 10, its = 37
i = 6587 (of 10192), d = 2.09762, its = 4
i = 6588 (of 10192), d = 10, its = 51
i = 6589 (of 10192), d = 10, its = 56
i = 6590 (of 10192), d = 1.91033, its = 4
i = 6591 (of 10192), d = 10, its = 54
i = 6592 (of 10192), d = 0.849314, its = 25
i = 6593 (of 10192), d = 0.889794, its = 20
i = 6594 (of 10192), d = 10, its = 50
i = 6595 (of 10192), d = 10, its = 53
i = 6596 (of 10192), d = 10, its = 55
i = 6597 (of 10192), d = 10, its = 37
i = 6598 (of 10192), d = 10, its = 52
i = 6599 (of 10192), d = 3.16371, its = 7
i = 6600 (of 10192), d = 2.66287, its = 6
i = 6601 (of 10192), d = 0.950085, its = 17
i = 6602 (of 10192), d = 2.43295, its = 5
i = 6603 (of 10192), d = 10, its = 39
i = 6604 (of 10192), d = 2.541, its = 6
i = 6605 (of 10192), d = 1.15288, its = 20
i = 6606 (of 10192), d = 2.73054, its = 6
i = 6607 (of 10192), d = 3.00039, its = 6
i = 6608 (of 10192), d = 10, its = 54
i = 6609 (of 10192), d = 10, its = 55
i = 6610 (of 10192), d = 2.68642, its = 6
i = 6611 (of 10192), d = 10, its = 40
i = 6612 (of 10192), d = 10, its = 50
i = 6613 (of 10192), d = 10, its = 43
i = 6614 (of 10192), d = 10, its = 55
i = 6615 (of 10192), d = 10, its = 56
i = 6616 (of 10192), d = 10, its = 37
i = 6617 (of 10192), d = 10, its = 40
i = 6618 (of 10192), d = 2.78848, its = 5
i = 6619 (of 10192), d = 2.95134, its = 6
i = 6620 (of 10192), d = 1.59971, its = 7
i = 6621 (of 10192), d = 10, its = 50
i = 6622 (of 10192), d = 10, its = 58
i = 6623 (of 10192), d = 10, its = 39
i = 6624 (of 10192), d = 4.02969, its = 7
i = 6625 (of 10192), d = 10, its = 51
i = 6626 (of 10192), d = 10, its = 54
i = 6627 (of 10192), d = 10, its = 37
i = 6628 (of 10192), d = 10, its = 52
i = 6629 (of 10192), d = 10, its = 52
i = 6630 (of 10192), d = 10, its = 51
i = 6631 (of 10192), d = 10, its = 53
i = 6632 (of 10192), d = 10, its = 53
i = 6633 (of 10192), d = 4.884, its = 8
i = 6634 (of 10192), d = 2.82347, its = 5
i = 6635 (of 10192), d = 2.21785, its = 4
i = 6636 (of 10192), d = 10, its = 57
i = 6637 (of 10192), d = 10, its = 53
i = 6638 (of 10192), d = 10, its = 40
i = 6639 (of 10192), d = 3.29946, its = 6
i = 6640 (of 10192), d = 10, its = 53
i = 6641 (of 10192), d = 1.60832, its = 6
i = 6642 (of 10192), d = 2.24257, its = 4
i = 6643 (of 10192), d = 10, its = 53
i = 6644 (of 10192), d = 10, its = 39
i = 6645 (of 10192), d = 10, its = 52
i = 6646 (of 10192), d = 10, its = 54
i = 6647 (of 10192), d = 10, its = 40
i = 6648 (of 10192), d = 2.48137, its = 5
i = 6649 (of 10192), d = 10, its = 55
i = 6650 (of 10192), d = 10, its = 54
i = 6651 (of 10192), d = 10, its = 39
i = 6652 (of 10192), d = 2.60418, its = 7
i = 6653 (of 10192), d = 10, its = 38
i = 6654 (of 10192), d = 10, its = 54
i = 6655 (of 10192), d = 10, its = 54
i = 6656 (of 10192), d = 1.12379, its = 18
i = 6657 (of 10192), d = 10, its = 46
i = 6658 (of 10192), d = 10, its = 37
i = 6659 (of 10192), d = 10, its = 57
i = 6660 (of 10192), d = 10, its = 49
i = 6661 (of 10192), d = 10, its = 57
i = 6662 (of 10192), d = 10, its = 49
i = 6663 (of 10192), d = 10, its = 37
i = 6664 (of 10192), d = 10, its = 51
i = 6665 (of 10192), d = 10, its = 53
i = 6666 (of 10192), d = 3.72102, its = 6
i = 6667 (of 10192), d = 10, its = 53
i = 6668 (of 10192), d = 10, its = 37
i = 6669 (of 10192), d = 1.54051, its = 6
i = 6670 (of 10192), d = 10, its = 37
i = 6671 (of 10192), d = 10, its = 51
i = 6672 (of 10192), d = 4.18808, its = 7
i = 6673 (of 10192), d = 10, its = 37
i = 6674 (of 10192), d = 1.59406, its = 7
i = 6675 (of 10192), d = 10, its = 37
i = 6676 (of 10192), d = 2.55828, its = 5
i = 6677 (of 10192), d = 7.11412, its = 8
i = 6678 (of 10192), d = 1.60253, its = 7
i = 6679 (of 10192), d = 10, its = 55
i = 6680 (of 10192), d = 10, its = 41
i = 6681 (of 10192), d = 10, its = 53
i = 6682 (of 10192), d = 10, its = 37
i = 6683 (of 10192), d = 10, its = 53
i = 6684 (of 10192), d = 0.706585, its = 18
i = 6685 (of 10192), d = 10, its = 53
i = 6686 (of 10192), d = 10, its = 51
i = 6687 (of 10192), d = 0.800185, its = 22
i = 6688 (of 10192), d = 10, its = 37
i = 6689 (of 10192), d = 1.23996, its = 6
i = 6690 (of 10192), d = 1.76695, its = 5
i = 6691 (of 10192), d = 10, its = 52
i = 6692 (of 10192), d = 2.25902, its = 4
i = 6693 (of 10192), d = 10, its = 48
i = 6694 (of 10192), d = 10, its = 47
i = 6695 (of 10192), d = 3.91947, its = 8
i = 6696 (of 10192), d = 10, its = 56
i = 6697 (of 10192), d = 3.09291, its = 6
i = 6698 (of 10192), d = 10, its = 51
i = 6699 (of 10192), d = 1.90324, its = 5
i = 6700 (of 10192), d = 8.72109, its = 8
i = 6701 (of 10192), d = 10, its = 54
i = 6702 (of 10192), d = 10, its = 59
i = 6703 (of 10192), d = 10, its = 54
i = 6704 (of 10192), d = 10, its = 51
i = 6705 (of 10192), d = 10, its = 38
i = 6706 (of 10192), d = 10, its = 43
i = 6707 (of 10192), d = 10, its = 52
i = 6708 (of 10192), d = 10, its = 48
i = 6709 (of 10192), d = 10, its = 52
i = 6710 (of 10192), d = 10, its = 51
i = 6711 (of 10192), d = 10, its = 52
i = 6712 (of 10192), d = 10, its = 48
i = 6713 (of 10192), d = 10, its = 56
i = 6714 (of 10192), d = 10, its = 43
i = 6715 (of 10192), d = 1.99376, its = 3
i = 6716 (of 10192), d = 10, its = 57
i = 6717 (of 10192), d = 2.22092, its = 4
i = 6718 (of 10192), d = 10, its = 48
i = 6719 (of 10192), d = 10, its = 50
i = 6720 (of 10192), d = 2.1366, its = 4
i = 6721 (of 10192), d = 10, its = 54
i = 6722 (of 10192), d = 1.95635, its = 4
i = 6723 (of 10192), d = 10, its = 40
i = 6724 (of 10192), d = 10, its = 53
i = 6725 (of 10192), d = 3.87628, its = 7
i = 6726 (of 10192), d = 10, its = 52
i = 6727 (of 10192), d = 10, its = 49
i = 6728 (of 10192), d = 10, its = 39
i = 6729 (of 10192), d = 10, its = 55
i = 6730 (of 10192), d = 10, its = 55
i = 6731 (of 10192), d = 10, its = 56
i = 6732 (of 10192), d = 1.50255, its = 7
i = 6733 (of 10192), d = 1.63179, its = 5
i = 6734 (of 10192), d = 2.69731, its = 5
i = 6735 (of 10192), d = 8.39598, its = 8
i = 6736 (of 10192), d = 1.86595, its = 4
i = 6737 (of 10192), d = 2.73196, its = 5
i = 6738 (of 10192), d = 10, its = 37
i = 6739 (of 10192), d = 10, its = 37
i = 6740 (of 10192), d = 10, its = 42
i = 6741 (of 10192), d = 2.60418, its = 7
i = 6742 (of 10192), d = 10, its = 49
i = 6743 (of 10192), d = 10, its = 48
i = 6744 (of 10192), d = 10, its = 54
i = 6745 (of 10192), d = 10, its = 54
i = 6746 (of 10192), d = 10, its = 51
i = 6747 (of 10192), d = 1.79964, its = 5
i = 6748 (of 10192), d = 10, its = 50
i = 6749 (of 10192), d = 10, its = 50
i = 6750 (of 10192), d = 2.15524, its = 4
i = 6751 (of 10192), d = 10, its = 40
i = 6752 (of 10192), d = 10, its = 49
i = 6753 (of 10192), d = 10, its = 37
i = 6754 (of 10192), d = 1.08111, its = 17
i = 6755 (of 10192), d = 10, its = 48
i = 6756 (of 10192), d = 10, its = 41
i = 6757 (of 10192), d = 10, its = 55
i = 6758 (of 10192), d = 10, its = 37
i = 6759 (of 10192), d = 10, its = 55
i = 6760 (of 10192), d = 10, its = 48
i = 6761 (of 10192), d = 1.38086, its = 6
i = 6762 (of 10192), d = 1.37588, its = 6
i = 6763 (of 10192), d = 10, its = 44
i = 6764 (of 10192), d = 10, its = 40
i = 6765 (of 10192), d = 0.85635, its = 19
i = 6766 (of 10192), d = 1.26734, its = 25
i = 6767 (of 10192), d = 10, its = 37
i = 6768 (of 10192), d = 1.73295, its = 7
i = 6769 (of 10192), d = 10, its = 39
i = 6770 (of 10192), d = 10, its = 51
i = 6771 (of 10192), d = 10, its = 54
i = 6772 (of 10192), d = 10, its = 50
i = 6773 (of 10192), d = 10, its = 49
i = 6774 (of 10192), d = 10, its = 58
i = 6775 (of 10192), d = 10, its = 41
i = 6776 (of 10192), d = 10, its = 52
i = 6777 (of 10192), d = 10, its = 52
i = 6778 (of 10192), d = 10, its = 38
i = 6779 (of 10192), d = 0.726555, its = 16
i = 6780 (of 10192), d = 1.77712, its = 5
i = 6781 (of 10192), d = 10, its = 41
i = 6782 (of 10192), d = 2.07411, its = 4
i = 6783 (of 10192), d = 10, its = 52
i = 6784 (of 10192), d = 10, its = 51
i = 6785 (of 10192), d = 3.4654, its = 7
i = 6786 (of 10192), d = 10, its = 55
i = 6787 (of 10192), d = 10, its = 56
i = 6788 (of 10192), d = 10, its = 50
i = 6789 (of 10192), d = 10, its = 51
i = 6790 (of 10192), d = 2.3499, its = 5
i = 6791 (of 10192), d = 10, its = 43
i = 6792 (of 10192), d = 10, its = 37
i = 6793 (of 10192), d = 10, its = 51
i = 6794 (of 10192), d = 10, its = 53
i = 6795 (of 10192), d = 10, its = 52
i = 6796 (of 10192), d = 10, its = 52
i = 6797 (of 10192), d = 10, its = 49
i = 6798 (of 10192), d = 1.48626, its = 7
i = 6799 (of 10192), d = 10, its = 50
i = 6800 (of 10192), d = 10, its = 55
i = 6801 (of 10192), d = 0.938153, its = 23
i = 6802 (of 10192), d = 10, its = 37
i = 6803 (of 10192), d = 1.13104, its = 19
i = 6804 (of 10192), d = 10, its = 37
i = 6805 (of 10192), d = 10, its = 52
i = 6806 (of 10192), d = 1.6103, its = 7
i = 6807 (of 10192), d = 1.86856, its = 4
i = 6808 (of 10192), d = 3.43127, its = 6
i = 6809 (of 10192), d = 10, its = 54
i = 6810 (of 10192), d = 10, its = 48
i = 6811 (of 10192), d = 2.69106, its = 6
i = 6812 (of 10192), d = 10, its = 54
i = 6813 (of 10192), d = 5.74328, its = 10
i = 6814 (of 10192), d = 10, its = 53
i = 6815 (of 10192), d = 10, its = 55
i = 6816 (of 10192), d = 4.74605, its = 9
i = 6817 (of 10192), d = 10, its = 56
i = 6818 (of 10192), d = 10, its = 52
i = 6819 (of 10192), d = 1.71288, its = 5
i = 6820 (of 10192), d = 10, its = 40
i = 6821 (of 10192), d = 10, its = 51
i = 6822 (of 10192), d = 2.88068, its = 6
i = 6823 (of 10192), d = 10, its = 55
i = 6824 (of 10192), d = 10, its = 54
i = 6825 (of 10192), d = 10, its = 53
i = 6826 (of 10192), d = 10, its = 41
i = 6827 (of 10192), d = 1.89154, its = 4
i = 6828 (of 10192), d = 3.42056, its = 6
i = 6829 (of 10192), d = 10, its = 41
i = 6830 (of 10192), d = 4.11821, its = 9
i = 6831 (of 10192), d = 10, its = 52
i = 6832 (of 10192), d = 1.28755, its = 8
i = 6833 (of 10192), d = 2.66853, its = 6
i = 6834 (of 10192), d = 3.5839, its = 7
i = 6835 (of 10192), d = 10, its = 37
i = 6836 (of 10192), d = 2.37348, its = 5
i = 6837 (of 10192), d = 10, its = 40
i = 6838 (of 10192), d = 10, its = 56
i = 6839 (of 10192), d = 1.5417, its = 6
i = 6840 (of 10192), d = 10, its = 54
i = 6841 (of 10192), d = 10, its = 40
i = 6842 (of 10192), d = 2.73881, its = 5
i = 6843 (of 10192), d = 10, its = 39
i = 6844 (of 10192), d = 10, its = 54
i = 6845 (of 10192), d = 10, its = 56
i = 6846 (of 10192), d = 10, its = 55
i = 6847 (of 10192), d = 10, its = 55
i = 6848 (of 10192), d = 10, its = 51
i = 6849 (of 10192), d = 10, its = 56
i = 6850 (of 10192), d = 10, its = 52
i = 6851 (of 10192), d = 1.78503, its = 5
i = 6852 (of 10192), d = 10, its = 39
i = 6853 (of 10192), d = 10, its = 52
i = 6854 (of 10192), d = 10, its = 51
i = 6855 (of 10192), d = 10, its = 49
i = 6856 (of 10192), d = 10, its = 51
i = 6857 (of 10192), d = 10, its = 53
i = 6858 (of 10192), d = 1.36999, its = 6
i = 6859 (of 10192), d = 0.640073, its = 19
i = 6860 (of 10192), d = 2.29556, its = 4
i = 6861 (of 10192), d = 1.80343, its = 5
i = 6862 (of 10192), d = 10, its = 52
i = 6863 (of 10192), d = 10, its = 52
i = 6864 (of 10192), d = 0.929737, its = 17
i = 6865 (of 10192), d = 1.70163, its = 5
i = 6866 (of 10192), d = 10, its = 53
i = 6867 (of 10192), d = 4.19268, its = 6
i = 6868 (of 10192), d = 1.43972, its = 6
i = 6869 (of 10192), d = 1.93807, its = 3
i = 6870 (of 10192), d = 2.61254, its = 5
i = 6871 (of 10192), d = 10, its = 54
i = 6872 (of 10192), d = 1.46287, its = 8
i = 6873 (of 10192), d = 10, its = 52
i = 6874 (of 10192), d = 10, its = 37
i = 6875 (of 10192), d = 10, its = 52
i = 6876 (of 10192), d = 10, its = 40
i = 6877 (of 10192), d = 10, its = 56
i = 6878 (of 10192), d = 10, its = 56
i = 6879 (of 10192), d = 10, its = 41
i = 6880 (of 10192), d = 2.48814, its = 5
i = 6881 (of 10192), d = 10, its = 53
i = 6882 (of 10192), d = 10, its = 51
i = 6883 (of 10192), d = 1.42465, its = 6
i = 6884 (of 10192), d = 1.2048, its = 26
i = 6885 (of 10192), d = 10, its = 52
i = 6886 (of 10192), d = 10, its = 52
i = 6887 (of 10192), d = 1.47516, its = 7
i = 6888 (of 10192), d = 1.98927, its = 3
i = 6889 (of 10192), d = 10, its = 40
i = 6890 (of 10192), d = 10, its = 53
i = 6891 (of 10192), d = 1.49103, its = 7
i = 6892 (of 10192), d = 3.59567, its = 7
i = 6893 (of 10192), d = 10, its = 50
i = 6894 (of 10192), d = 10, its = 52
i = 6895 (of 10192), d = 10, its = 53
i = 6896 (of 10192), d = 10, its = 55
i = 6897 (of 10192), d = 10, its = 55
i = 6898 (of 10192), d = 10, its = 54
i = 6899 (of 10192), d = 2.1569, its = 4
i = 6900 (of 10192), d = 2.64087, its = 5
i = 6901 (of 10192), d = 10, its = 37
i = 6902 (of 10192), d = 1.71517, its = 6
i = 6903 (of 10192), d = 1.88448, its = 4
i = 6904 (of 10192), d = 10, its = 52
i = 6905 (of 10192), d = 1.58308, its = 6
i = 6906 (of 10192), d = 1.60247, its = 7
i = 6907 (of 10192), d = 10, its = 53
i = 6908 (of 10192), d = 10, its = 37
i = 6909 (of 10192), d = 10, its = 51
i = 6910 (of 10192), d = 10, its = 37
i = 6911 (of 10192), d = 10, its = 52
i = 6912 (of 10192), d = 10, its = 40
i = 6913 (of 10192), d = 10, its = 43
i = 6914 (of 10192), d = 1.95898, its = 4
i = 6915 (of 10192), d = 10, its = 54
i = 6916 (of 10192), d = 10, its = 52
i = 6917 (of 10192), d = 1.05236, its = 16
i = 6918 (of 10192), d = 1.6024, its = 6
i = 6919 (of 10192), d = 2.36936, its = 5
i = 6920 (of 10192), d = 10, its = 50
i = 6921 (of 10192), d = 10, its = 48
i = 6922 (of 10192), d = 10, its = 49
i = 6923 (of 10192), d = 1.68053, its = 6
i = 6924 (of 10192), d = 10, its = 54
i = 6925 (of 10192), d = 10, its = 37
i = 6926 (of 10192), d = 1.57066, its = 8
i = 6927 (of 10192), d = 4.57961, its = 8
i = 6928 (of 10192), d = 10, its = 56
i = 6929 (of 10192), d = 2.23673, its = 4
i = 6930 (of 10192), d = 10, its = 37
i = 6931 (of 10192), d = 10, its = 40
i = 6932 (of 10192), d = 10, its = 43
i = 6933 (of 10192), d = 2.24722, its = 4
i = 6934 (of 10192), d = 10, its = 43
i = 6935 (of 10192), d = 2.23477, its = 4
i = 6936 (of 10192), d = 10, its = 39
i = 6937 (of 10192), d = 2.3626, its = 5
i = 6938 (of 10192), d = 10, its = 49
i = 6939 (of 10192), d = 6.62615, its = 8
i = 6940 (of 10192), d = 10, its = 56
i = 6941 (of 10192), d = 10, its = 51
i = 6942 (of 10192), d = 10, its = 58
i = 6943 (of 10192), d = 1.96631, its = 3
i = 6944 (of 10192), d = 10, its = 40
i = 6945 (of 10192), d = 2.93502, its = 6
i = 6946 (of 10192), d = 10, its = 37
i = 6947 (of 10192), d = 0.795543, its = 21
i = 6948 (of 10192), d = 10, its = 52
i = 6949 (of 10192), d = 1.9853, its = 3
i = 6950 (of 10192), d = 10, its = 42
i = 6951 (of 10192), d = 6.47082, its = 8
i = 6952 (of 10192), d = 10, its = 52
i = 6953 (of 10192), d = 2.38108, its = 5
i = 6954 (of 10192), d = 10, its = 41
i = 6955 (of 10192), d = 10, its = 52
i = 6956 (of 10192), d = 2.62105, its = 5
i = 6957 (of 10192), d = 10, its = 54
i = 6958 (of 10192), d = 10, its = 54
i = 6959 (of 10192), d = 10, its = 37
i = 6960 (of 10192), d = 10, its = 54
i = 6961 (of 10192), d = 10, its = 59
i = 6962 (of 10192), d = 10, its = 55
i = 6963 (of 10192), d = 2.40424, its = 5
i = 6964 (of 10192), d = 10, its = 37
i = 6965 (of 10192), d = 1.73532, its = 5
i = 6966 (of 10192), d = 10, its = 40
i = 6967 (of 10192), d = 10, its = 54
i = 6968 (of 10192), d = 1.44876, its = 7
i = 6969 (of 10192), d = 10, its = 52
i = 6970 (of 10192), d = 10, its = 51
i = 6971 (of 10192), d = 10, its = 51
i = 6972 (of 10192), d = 10, its = 49
i = 6973 (of 10192), d = 1.17699, its = 19
i = 6974 (of 10192), d = 1.13914, its = 20
i = 6975 (of 10192), d = 10, its = 50
i = 6976 (of 10192), d = 10, its = 38
i = 6977 (of 10192), d = 10, its = 52
i = 6978 (of 10192), d = 2.76045, its = 5
i = 6979 (of 10192), d = 3.75435, its = 7
i = 6980 (of 10192), d = 10, its = 37
i = 6981 (of 10192), d = 10, its = 37
i = 6982 (of 10192), d = 1.46817, its = 7
i = 6983 (of 10192), d = 0.904407, its = 22
i = 6984 (of 10192), d = 10, its = 49
i = 6985 (of 10192), d = 10, its = 53
i = 6986 (of 10192), d = 2.24005, its = 4
i = 6987 (of 10192), d = 10, its = 50
i = 6988 (of 10192), d = 10, its = 55
i = 6989 (of 10192), d = 10, its = 41
i = 6990 (of 10192), d = 10, its = 43
i = 6991 (of 10192), d = 2.11776, its = 4
i = 6992 (of 10192), d = 10, its = 55
i = 6993 (of 10192), d = 0.917415, its = 22
i = 6994 (of 10192), d = 10, its = 53
i = 6995 (of 10192), d = 10, its = 37
i = 6996 (of 10192), d = 2.34734, its = 5
i = 6997 (of 10192), d = 10, its = 39
i = 6998 (of 10192), d = 1.3772, its = 6
i = 6999 (of 10192), d = 10, its = 37
i = 7000 (of 10192), d = 10, its = 37
i = 7001 (of 10192), d = 10, its = 50
i = 7002 (of 10192), d = 10, its = 52
i = 7003 (of 10192), d = 10, its = 52
i = 7004 (of 10192), d = 2.70736, its = 5
i = 7005 (of 10192), d = 10, its = 56
i = 7006 (of 10192), d = 10, its = 40
i = 7007 (of 10192), d = 1.26114, its = 8
i = 7008 (of 10192), d = 1.39662, its = 8
i = 7009 (of 10192), d = 10, its = 49
i = 7010 (of 10192), d = 10, its = 56
i = 7011 (of 10192), d = 10, its = 50
i = 7012 (of 10192), d = 10, its = 49
i = 7013 (of 10192), d = 10, its = 55
i = 7014 (of 10192), d = 10, its = 40
i = 7015 (of 10192), d = 10, its = 51
i = 7016 (of 10192), d = 10, its = 41
i = 7017 (of 10192), d = 10, its = 39
i = 7018 (of 10192), d = 10, its = 56
i = 7019 (of 10192), d = 10, its = 52
i = 7020 (of 10192), d = 10, its = 37
i = 7021 (of 10192), d = 10, its = 42
i = 7022 (of 10192), d = 3.98452, its = 7
i = 7023 (of 10192), d = 10, its = 47
i = 7024 (of 10192), d = 10, its = 52
i = 7025 (of 10192), d = 10, its = 58
i = 7026 (of 10192), d = 10, its = 52
i = 7027 (of 10192), d = 10, its = 53
i = 7028 (of 10192), d = 10, its = 53
i = 7029 (of 10192), d = 4.58239, its = 7
i = 7030 (of 10192), d = 1.93012, its = 4
i = 7031 (of 10192), d = 1.0583, its = 18
i = 7032 (of 10192), d = 10, its = 55
i = 7033 (of 10192), d = 10, its = 37
i = 7034 (of 10192), d = 10, its = 52
i = 7035 (of 10192), d = 2.25477, its = 4
i = 7036 (of 10192), d = 10, its = 53
i = 7037 (of 10192), d = 10, its = 50
i = 7038 (of 10192), d = 10, its = 37
i = 7039 (of 10192), d = 10, its = 37
i = 7040 (of 10192), d = 3.03546, its = 6
i = 7041 (of 10192), d = 10, its = 51
i = 7042 (of 10192), d = 10, its = 40
i = 7043 (of 10192), d = 10, its = 53
i = 7044 (of 10192), d = 10, its = 40
i = 7045 (of 10192), d = 10, its = 57
i = 7046 (of 10192), d = 0.537038, its = 18
i = 7047 (of 10192), d = 10, its = 51
i = 7048 (of 10192), d = 10, its = 52
i = 7049 (of 10192), d = 1.6977, its = 5
i = 7050 (of 10192), d = 1.12308, its = 20
i = 7051 (of 10192), d = 10, its = 37
i = 7052 (of 10192), d = 1.11265, its = 19
i = 7053 (of 10192), d = 10, its = 37
i = 7054 (of 10192), d = 10, its = 57
i = 7055 (of 10192), d = 1.11678, its = 19
i = 7056 (of 10192), d = 6.27343, its = 9
i = 7057 (of 10192), d = 7.97091, its = 9
i = 7058 (of 10192), d = 10, its = 50
i = 7059 (of 10192), d = 10, its = 49
i = 7060 (of 10192), d = 1.81595, its = 5
i = 7061 (of 10192), d = 1.73025, its = 5
i = 7062 (of 10192), d = 2.71038, its = 6
i = 7063 (of 10192), d = 10, its = 40
i = 7064 (of 10192), d = 10, its = 55
i = 7065 (of 10192), d = 10, its = 51
i = 7066 (of 10192), d = 10, its = 55
i = 7067 (of 10192), d = 10, its = 55
i = 7068 (of 10192), d = 2.22868, its = 4
i = 7069 (of 10192), d = 10, its = 52
i = 7070 (of 10192), d = 2.87836, its = 6
i = 7071 (of 10192), d = 10, its = 50
i = 7072 (of 10192), d = 1.59406, its = 7
i = 7073 (of 10192), d = 10, its = 46
i = 7074 (of 10192), d = 10, its = 48
i = 7075 (of 10192), d = 10, its = 55
i = 7076 (of 10192), d = 10, its = 40
i = 7077 (of 10192), d = 10, its = 55
i = 7078 (of 10192), d = 10, its = 41
i = 7079 (of 10192), d = 10, its = 54
i = 7080 (of 10192), d = 10, its = 52
i = 7081 (of 10192), d = 10, its = 41
i = 7082 (of 10192), d = 10, its = 53
i = 7083 (of 10192), d = 10, its = 51
i = 7084 (of 10192), d = 2.16129, its = 4
i = 7085 (of 10192), d = 1.04839, its = 26
i = 7086 (of 10192), d = 2.58639, its = 5
i = 7087 (of 10192), d = 10, its = 52
i = 7088 (of 10192), d = 10, its = 37
i = 7089 (of 10192), d = 10, its = 51
i = 7090 (of 10192), d = 10, its = 50
i = 7091 (of 10192), d = 10, its = 56
i = 7092 (of 10192), d = 2.07265, its = 3
i = 7093 (of 10192), d = 3.42807, its = 6
i = 7094 (of 10192), d = 10, its = 50
i = 7095 (of 10192), d = 10, its = 37
i = 7096 (of 10192), d = 10, its = 54
i = 7097 (of 10192), d = 10, its = 55
i = 7098 (of 10192), d = 10, its = 51
i = 7099 (of 10192), d = 0.647033, its = 19
i = 7100 (of 10192), d = 10, its = 52
i = 7101 (of 10192), d = 1.76189, its = 5
i = 7102 (of 10192), d = 1.61855, its = 6
i = 7103 (of 10192), d = 10, its = 48
i = 7104 (of 10192), d = 10, its = 50
i = 7105 (of 10192), d = 10, its = 37
i = 7106 (of 10192), d = 3.29512, its = 6
i = 7107 (of 10192), d = 10, its = 49
i = 7108 (of 10192), d = 10, its = 37
i = 7109 (of 10192), d = 10, its = 50
i = 7110 (of 10192), d = 10, its = 40
i = 7111 (of 10192), d = 10, its = 53
i = 7112 (of 10192), d = 10, its = 52
i = 7113 (of 10192), d = 0.913356, its = 22
i = 7114 (of 10192), d = 10, its = 42
i = 7115 (of 10192), d = 10, its = 42
i = 7116 (of 10192), d = 10, its = 55
i = 7117 (of 10192), d = 10, its = 47
i = 7118 (of 10192), d = 6.76133, its = 8
i = 7119 (of 10192), d = 2.08126, its = 4
i = 7120 (of 10192), d = 10, its = 50
i = 7121 (of 10192), d = 10, its = 53
i = 7122 (of 10192), d = 1.60553, its = 6
i = 7123 (of 10192), d = 2.62549, its = 6
i = 7124 (of 10192), d = 1.86366, its = 4
i = 7125 (of 10192), d = 1.72567, its = 5
i = 7126 (of 10192), d = 4.88977, its = 8
i = 7127 (of 10192), d = 1.13511, its = 17
i = 7128 (of 10192), d = 10, its = 53
i = 7129 (of 10192), d = 10, its = 55
i = 7130 (of 10192), d = 3.75769, its = 7
i = 7131 (of 10192), d = 2.55828, its = 5
i = 7132 (of 10192), d = 1.83001, its = 5
i = 7133 (of 10192), d = 10, its = 51
i = 7134 (of 10192), d = 1.82548, its = 4
i = 7135 (of 10192), d = 1.85698, its = 4
i = 7136 (of 10192), d = 10, its = 54
i = 7137 (of 10192), d = 2.11427, its = 4
i = 7138 (of 10192), d = 10, its = 50
i = 7139 (of 10192), d = 10, its = 37
i = 7140 (of 10192), d = 10, its = 37
i = 7141 (of 10192), d = 10, its = 51
i = 7142 (of 10192), d = 10, its = 53
i = 7143 (of 10192), d = 10, its = 37
i = 7144 (of 10192), d = 10, its = 48
i = 7145 (of 10192), d = 10, its = 50
i = 7146 (of 10192), d = 10, its = 43
i = 7147 (of 10192), d = 10, its = 37
i = 7148 (of 10192), d = 1.39782, its = 7
i = 7149 (of 10192), d = 5.16863, its = 7
i = 7150 (of 10192), d = 10, its = 56
i = 7151 (of 10192), d = 10, its = 42
i = 7152 (of 10192), d = 10, its = 41
i = 7153 (of 10192), d = 10, its = 55
i = 7154 (of 10192), d = 1.65284, its = 6
i = 7155 (of 10192), d = 0.732264, its = 19
i = 7156 (of 10192), d = 10, its = 41
i = 7157 (of 10192), d = 2.78548, its = 5
i = 7158 (of 10192), d = 10, its = 52
i = 7159 (of 10192), d = 10, its = 53
i = 7160 (of 10192), d = 10, its = 52
i = 7161 (of 10192), d = 2.07166, its = 4
i = 7162 (of 10192), d = 10, its = 37
i = 7163 (of 10192), d = 2.03772, its = 3
i = 7164 (of 10192), d = 10, its = 54
i = 7165 (of 10192), d = 1.5443, its = 7
i = 7166 (of 10192), d = 10, its = 54
i = 7167 (of 10192), d = 0.572039, its = 19
i = 7168 (of 10192), d = 10, its = 52
i = 7169 (of 10192), d = 10, its = 54
i = 7170 (of 10192), d = 10, its = 54
i = 7171 (of 10192), d = 2.63978, its = 5
i = 7172 (of 10192), d = 10, its = 51
i = 7173 (of 10192), d = 10, its = 40
i = 7174 (of 10192), d = 10, its = 49
i = 7175 (of 10192), d = 10, its = 49
i = 7176 (of 10192), d = 0.56112, its = 18
i = 7177 (of 10192), d = 0.841604, its = 17
i = 7178 (of 10192), d = 10, its = 43
i = 7179 (of 10192), d = 1.15196, its = 19
i = 7180 (of 10192), d = 10, its = 43
i = 7181 (of 10192), d = 4.32397, its = 7
i = 7182 (of 10192), d = 10, its = 37
i = 7183 (of 10192), d = 0.88291, its = 18
i = 7184 (of 10192), d = 10, its = 39
i = 7185 (of 10192), d = 10, its = 43
i = 7186 (of 10192), d = 3.37327, its = 6
i = 7187 (of 10192), d = 10, its = 39
i = 7188 (of 10192), d = 2.02032, its = 3
i = 7189 (of 10192), d = 10, its = 51
i = 7190 (of 10192), d = 3.12096, its = 6
i = 7191 (of 10192), d = 10, its = 37
i = 7192 (of 10192), d = 10, its = 40
i = 7193 (of 10192), d = 10, its = 41
i = 7194 (of 10192), d = 10, its = 53
i = 7195 (of 10192), d = 10, its = 52
i = 7196 (of 10192), d = 4.0077, its = 6
i = 7197 (of 10192), d = 10, its = 55
i = 7198 (of 10192), d = 1.96931, its = 3
i = 7199 (of 10192), d = 10, its = 41
i = 7200 (of 10192), d = 1.02387, its = 16
i = 7201 (of 10192), d = 2.66199, its = 5
i = 7202 (of 10192), d = 10, its = 37
i = 7203 (of 10192), d = 3.07448, its = 6
i = 7204 (of 10192), d = 10, its = 54
i = 7205 (of 10192), d = 10, its = 52
i = 7206 (of 10192), d = 10, its = 55
i = 7207 (of 10192), d = 10, its = 55
i = 7208 (of 10192), d = 10, its = 37
i = 7209 (of 10192), d = 1.74781, its = 5
i = 7210 (of 10192), d = 10, its = 38
i = 7211 (of 10192), d = 1.90805, its = 4
i = 7212 (of 10192), d = 4.84755, its = 7
i = 7213 (of 10192), d = 10, its = 37
i = 7214 (of 10192), d = 10, its = 40
i = 7215 (of 10192), d = 10, its = 49
i = 7216 (of 10192), d = 10, its = 55
i = 7217 (of 10192), d = 10, its = 54
i = 7218 (of 10192), d = 3.60097, its = 6
i = 7219 (of 10192), d = 10, its = 40
i = 7220 (of 10192), d = 2.31544, its = 5
i = 7221 (of 10192), d = 10, its = 51
i = 7222 (of 10192), d = 1.74883, its = 5
i = 7223 (of 10192), d = 10, its = 40
i = 7224 (of 10192), d = 10, its = 37
i = 7225 (of 10192), d = 5.97116, its = 8
i = 7226 (of 10192), d = 0.988166, its = 24
i = 7227 (of 10192), d = 3.06239, its = 6
i = 7228 (of 10192), d = 10, its = 52
i = 7229 (of 10192), d = 10, its = 47
i = 7230 (of 10192), d = 10, its = 40
i = 7231 (of 10192), d = 2.93082, its = 7
i = 7232 (of 10192), d = 10, its = 50
i = 7233 (of 10192), d = 10, its = 51
i = 7234 (of 10192), d = 10, its = 56
i = 7235 (of 10192), d = 10, its = 53
i = 7236 (of 10192), d = 10, its = 40
i = 7237 (of 10192), d = 10, its = 39
i = 7238 (of 10192), d = 10, its = 41
i = 7239 (of 10192), d = 10, its = 53
i = 7240 (of 10192), d = 10, its = 55
i = 7241 (of 10192), d = 1.77808, its = 5
i = 7242 (of 10192), d = 10, its = 48
i = 7243 (of 10192), d = 1.96066, its = 3
i = 7244 (of 10192), d = 10, its = 54
i = 7245 (of 10192), d = 2.29937, its = 5
i = 7246 (of 10192), d = 10, its = 55
i = 7247 (of 10192), d = 10, its = 56
i = 7248 (of 10192), d = 0.684623, its = 17
i = 7249 (of 10192), d = 10, its = 37
i = 7250 (of 10192), d = 3.61923, its = 7
i = 7251 (of 10192), d = 3.31165, its = 9
i = 7252 (of 10192), d = 10, its = 50
i = 7253 (of 10192), d = 2.6558, its = 6
i = 7254 (of 10192), d = 1.65188, its = 5
i = 7255 (of 10192), d = 1.87008, its = 4
i = 7256 (of 10192), d = 10, its = 51
i = 7257 (of 10192), d = 10, its = 52
i = 7258 (of 10192), d = 10, its = 55
i = 7259 (of 10192), d = 2.24009, its = 4
i = 7260 (of 10192), d = 10, its = 49
i = 7261 (of 10192), d = 10, its = 41
i = 7262 (of 10192), d = 10, its = 55
i = 7263 (of 10192), d = 1.51896, its = 7
i = 7264 (of 10192), d = 10, its = 37
i = 7265 (of 10192), d = 10, its = 53
i = 7266 (of 10192), d = 10, its = 56
i = 7267 (of 10192), d = 10, its = 53
i = 7268 (of 10192), d = 4.3135, its = 7
i = 7269 (of 10192), d = 10, its = 51
i = 7270 (of 10192), d = 10, its = 55
i = 7271 (of 10192), d = 3.73607, its = 6
i = 7272 (of 10192), d = 10, its = 41
i = 7273 (of 10192), d = 10, its = 49
i = 7274 (of 10192), d = 2.01281, its = 3
i = 7275 (of 10192), d = 3.95648, its = 7
i = 7276 (of 10192), d = 10, its = 53
i = 7277 (of 10192), d = 10, its = 42
i = 7278 (of 10192), d = 1.2699, its = 6
i = 7279 (of 10192), d = 10, its = 52
i = 7280 (of 10192), d = 10, its = 53
i = 7281 (of 10192), d = 10, its = 48
i = 7282 (of 10192), d = 10, its = 37
i = 7283 (of 10192), d = 2.45631, its = 5
i = 7284 (of 10192), d = 10, its = 49
i = 7285 (of 10192), d = 1.28965, its = 17
i = 7286 (of 10192), d = 0.873768, its = 17
i = 7287 (of 10192), d = 1.36231, its = 7
i = 7288 (of 10192), d = 1.37063, its = 6
i = 7289 (of 10192), d = 10, its = 40
i = 7290 (of 10192), d = 9.79289, its = 11
i = 7291 (of 10192), d = 2.35583, its = 5
i = 7292 (of 10192), d = 1.7095, its = 5
i = 7293 (of 10192), d = 10, its = 37
i = 7294 (of 10192), d = 1.90519, its = 4
i = 7295 (of 10192), d = 10, its = 52
i = 7296 (of 10192), d = 10, its = 56
i = 7297 (of 10192), d = 2.08117, its = 4
i = 7298 (of 10192), d = 10, its = 55
i = 7299 (of 10192), d = 10, its = 48
i = 7300 (of 10192), d = 10, its = 51
i = 7301 (of 10192), d = 2.8679, its = 7
i = 7302 (of 10192), d = 10, its = 37
i = 7303 (of 10192), d = 10, its = 54
i = 7304 (of 10192), d = 10, its = 42
i = 7305 (of 10192), d = 10, its = 37
i = 7306 (of 10192), d = 10, its = 42
i = 7307 (of 10192), d = 10, its = 54
i = 7308 (of 10192), d = 10, its = 53
i = 7309 (of 10192), d = 10, its = 52
i = 7310 (of 10192), d = 10, its = 37
i = 7311 (of 10192), d = 2.15932, its = 4
i = 7312 (of 10192), d = 10, its = 50
i = 7313 (of 10192), d = 10, its = 57
i = 7314 (of 10192), d = 10, its = 52
i = 7315 (of 10192), d = 1.41496, its = 6
i = 7316 (of 10192), d = 10, its = 50
i = 7317 (of 10192), d = 10, its = 52
i = 7318 (of 10192), d = 10, its = 49
i = 7319 (of 10192), d = 10, its = 41
i = 7320 (of 10192), d = 2.84711, its = 5
i = 7321 (of 10192), d = 1.79616, its = 4
i = 7322 (of 10192), d = 10, its = 39
i = 7323 (of 10192), d = 10, its = 48
i = 7324 (of 10192), d = 2.33554, its = 5
i = 7325 (of 10192), d = 2.06542, its = 3
i = 7326 (of 10192), d = 10, its = 50
i = 7327 (of 10192), d = 10, its = 54
i = 7328 (of 10192), d = 10, its = 55
i = 7329 (of 10192), d = 10, its = 50
i = 7330 (of 10192), d = 10, its = 50
i = 7331 (of 10192), d = 10, its = 51
i = 7332 (of 10192), d = 10, its = 52
i = 7333 (of 10192), d = 10, its = 54
i = 7334 (of 10192), d = 1.60925, its = 6
i = 7335 (of 10192), d = 10, its = 52
i = 7336 (of 10192), d = 10, its = 37
i = 7337 (of 10192), d = 10, its = 54
i = 7338 (of 10192), d = 10, its = 48
i = 7339 (of 10192), d = 10, its = 57
i = 7340 (of 10192), d = 2.65261, its = 5
i = 7341 (of 10192), d = 10, its = 41
i = 7342 (of 10192), d = 10, its = 52
i = 7343 (of 10192), d = 10, its = 41
i = 7344 (of 10192), d = 3.11775, its = 6
i = 7345 (of 10192), d = 10, its = 56
i = 7346 (of 10192), d = 10, its = 57
i = 7347 (of 10192), d = 3.90951, its = 6
i = 7348 (of 10192), d = 1.07855, its = 17
i = 7349 (of 10192), d = 8.36936, its = 8
i = 7350 (of 10192), d = 10, its = 51
i = 7351 (of 10192), d = 10, its = 57
i = 7352 (of 10192), d = 10, its = 52
i = 7353 (of 10192), d = 0.940821, its = 19
i = 7354 (of 10192), d = 10, its = 53
i = 7355 (of 10192), d = 10, its = 41
i = 7356 (of 10192), d = 8.36936, its = 8
i = 7357 (of 10192), d = 10, its = 53
i = 7358 (of 10192), d = 10, its = 37
i = 7359 (of 10192), d = 10, its = 45
i = 7360 (of 10192), d = 10, its = 50
i = 7361 (of 10192), d = 0.948835, its = 25
i = 7362 (of 10192), d = 1.33104, its = 7
i = 7363 (of 10192), d = 1.2986, its = 6
i = 7364 (of 10192), d = 1.65504, its = 6
i = 7365 (of 10192), d = 1.51857, its = 7
i = 7366 (of 10192), d = 10, its = 39
i = 7367 (of 10192), d = 10, its = 53
i = 7368 (of 10192), d = 10, its = 55
i = 7369 (of 10192), d = 10, its = 37
i = 7370 (of 10192), d = 1.41321, its = 21
i = 7371 (of 10192), d = 10, its = 55
i = 7372 (of 10192), d = 10, its = 52
i = 7373 (of 10192), d = 10, its = 54
i = 7374 (of 10192), d = 10, its = 50
i = 7375 (of 10192), d = 10, its = 53
i = 7376 (of 10192), d = 4.79605, its = 7
i = 7377 (of 10192), d = 10, its = 54
i = 7378 (of 10192), d = 10, its = 48
i = 7379 (of 10192), d = 10, its = 43
i = 7380 (of 10192), d = 10, its = 37
i = 7381 (of 10192), d = 10, its = 53
i = 7382 (of 10192), d = 10, its = 52
i = 7383 (of 10192), d = 10, its = 52
i = 7384 (of 10192), d = 10, its = 56
i = 7385 (of 10192), d = 2.63355, its = 6
i = 7386 (of 10192), d = 10, its = 54
i = 7387 (of 10192), d = 10, its = 52
i = 7388 (of 10192), d = 9.06353, its = 9
i = 7389 (of 10192), d = 10, its = 52
i = 7390 (of 10192), d = 10, its = 52
i = 7391 (of 10192), d = 10, its = 50
i = 7392 (of 10192), d = 10, its = 56
i = 7393 (of 10192), d = 10, its = 41
i = 7394 (of 10192), d = 2.41868, its = 5
i = 7395 (of 10192), d = 10, its = 37
i = 7396 (of 10192), d = 10, its = 51
i = 7397 (of 10192), d = 10, its = 54
i = 7398 (of 10192), d = 2.37285, its = 5
i = 7399 (of 10192), d = 10, its = 53
i = 7400 (of 10192), d = 10, its = 53
i = 7401 (of 10192), d = 10, its = 53
i = 7402 (of 10192), d = 10, its = 19
i = 7403 (of 10192), d = 1.41198, its = 6
i = 7404 (of 10192), d = 10, its = 39
i = 7405 (of 10192), d = 10, its = 55
i = 7406 (of 10192), d = 4.56344, its = 7
i = 7407 (of 10192), d = 10, its = 39
i = 7408 (of 10192), d = 10, its = 50
i = 7409 (of 10192), d = 10, its = 43
i = 7410 (of 10192), d = 10, its = 39
i = 7411 (of 10192), d = 10, its = 54
i = 7412 (of 10192), d = 1.98927, its = 3
i = 7413 (of 10192), d = 10, its = 37
i = 7414 (of 10192), d = 10, its = 37
i = 7415 (of 10192), d = 10, its = 49
i = 7416 (of 10192), d = 2.22001, its = 4
i = 7417 (of 10192), d = 10, its = 57
i = 7418 (of 10192), d = 0.769125, its = 22
i = 7419 (of 10192), d = 10, its = 37
i = 7420 (of 10192), d = 10, its = 52
i = 7421 (of 10192), d = 0.956483, its = 21
i = 7422 (of 10192), d = 2.50612, its = 5
i = 7423 (of 10192), d = 10, its = 47
i = 7424 (of 10192), d = 2.53776, its = 5
i = 7425 (of 10192), d = 10, its = 51
i = 7426 (of 10192), d = 1.46686, its = 7
i = 7427 (of 10192), d = 10, its = 53
i = 7428 (of 10192), d = 1.53437, its = 7
i = 7429 (of 10192), d = 10, its = 53
i = 7430 (of 10192), d = 10, its = 56
i = 7431 (of 10192), d = 0.947401, its = 22
i = 7432 (of 10192), d = 10, its = 47
i = 7433 (of 10192), d = 2.03226, its = 3
i = 7434 (of 10192), d = 10, its = 37
i = 7435 (of 10192), d = 10, its = 39
i = 7436 (of 10192), d = 10, its = 37
i = 7437 (of 10192), d = 10, its = 41
i = 7438 (of 10192), d = 1.65815, its = 6
i = 7439 (of 10192), d = 10, its = 40
i = 7440 (of 10192), d = 4.84755, its = 7
i = 7441 (of 10192), d = 10, its = 50
i = 7442 (of 10192), d = 1.34413, its = 6
i = 7443 (of 10192), d = 10, its = 46
i = 7444 (of 10192), d = 10, its = 58
i = 7445 (of 10192), d = 0.863201, its = 17
i = 7446 (of 10192), d = 6.81922, its = 8
i = 7447 (of 10192), d = 10, its = 53
i = 7448 (of 10192), d = 10, its = 56
i = 7449 (of 10192), d = 1.8218, its = 5
i = 7450 (of 10192), d = 10, its = 55
i = 7451 (of 10192), d = 10, its = 55
i = 7452 (of 10192), d = 0.759192, its = 18
i = 7453 (of 10192), d = 10, its = 50
i = 7454 (of 10192), d = 10, its = 54
i = 7455 (of 10192), d = 2.04375, its = 3
i = 7456 (of 10192), d = 0.848026, its = 17
i = 7457 (of 10192), d = 2.88439, its = 5
i = 7458 (of 10192), d = 2.69465, its = 5
i = 7459 (of 10192), d = 10, its = 37
i = 7460 (of 10192), d = 10, its = 37
i = 7461 (of 10192), d = 1.13708, its = 17
i = 7462 (of 10192), d = 10, its = 37
i = 7463 (of 10192), d = 1.40904, its = 23
i = 7464 (of 10192), d = 2.50744, its = 5
i = 7465 (of 10192), d = 1.49752, its = 6
i = 7466 (of 10192), d = 10, its = 42
i = 7467 (of 10192), d = 10, its = 52
i = 7468 (of 10192), d = 10, its = 37
i = 7469 (of 10192), d = 10, its = 48
i = 7470 (of 10192), d = 10, its = 52
i = 7471 (of 10192), d = 10, its = 40
i = 7472 (of 10192), d = 10, its = 52
i = 7473 (of 10192), d = 10, its = 40
i = 7474 (of 10192), d = 1.87202, its = 4
i = 7475 (of 10192), d = 3.82924, its = 7
i = 7476 (of 10192), d = 1.83892, its = 4
i = 7477 (of 10192), d = 10, its = 51
i = 7478 (of 10192), d = 1.77667, its = 5
i = 7479 (of 10192), d = 10, its = 51
i = 7480 (of 10192), d = 10, its = 37
i = 7481 (of 10192), d = 1.64983, its = 5
i = 7482 (of 10192), d = 1.68992, its = 6
i = 7483 (of 10192), d = 10, its = 51
i = 7484 (of 10192), d = 1.46332, its = 8
i = 7485 (of 10192), d = 10, its = 53
i = 7486 (of 10192), d = 1.53897, its = 7
i = 7487 (of 10192), d = 3.82052, its = 7
i = 7488 (of 10192), d = 4.46253, its = 7
i = 7489 (of 10192), d = 10, its = 41
i = 7490 (of 10192), d = 10, its = 52
i = 7491 (of 10192), d = 1.43562, its = 6
i = 7492 (of 10192), d = 1.88321, its = 4
i = 7493 (of 10192), d = 10, its = 37
i = 7494 (of 10192), d = 10, its = 53
i = 7495 (of 10192), d = 10, its = 53
i = 7496 (of 10192), d = 1.91228, its = 4
i = 7497 (of 10192), d = 10, its = 57
i = 7498 (of 10192), d = 10, its = 37
i = 7499 (of 10192), d = 2.49343, its = 5
i = 7500 (of 10192), d = 2.01431, its = 3
i = 7501 (of 10192), d = 7.532, its = 9
i = 7502 (of 10192), d = 10, its = 56
i = 7503 (of 10192), d = 10, its = 40
i = 7504 (of 10192), d = 0.821634, its = 18
i = 7505 (of 10192), d = 10, its = 54
i = 7506 (of 10192), d = 10, its = 53
i = 7507 (of 10192), d = 10, its = 37
i = 7508 (of 10192), d = 10, its = 37
i = 7509 (of 10192), d = 2.36212, its = 5
i = 7510 (of 10192), d = 1.68452, its = 6
i = 7511 (of 10192), d = 10, its = 57
i = 7512 (of 10192), d = 10, its = 55
i = 7513 (of 10192), d = 1.40271, its = 6
i = 7514 (of 10192), d = 10, its = 51
i = 7515 (of 10192), d = 2.96544, its = 6
i = 7516 (of 10192), d = 10, its = 52
i = 7517 (of 10192), d = 10, its = 40
i = 7518 (of 10192), d = 2.99589, its = 6
i = 7519 (of 10192), d = 2.73751, its = 7
i = 7520 (of 10192), d = 10, its = 54
i = 7521 (of 10192), d = 10, its = 37
i = 7522 (of 10192), d = 1.77422, its = 5
i = 7523 (of 10192), d = 10, its = 41
i = 7524 (of 10192), d = 10, its = 50
i = 7525 (of 10192), d = 3.06456, its = 6
i = 7526 (of 10192), d = 10, its = 53
i = 7527 (of 10192), d = 10, its = 53
i = 7528 (of 10192), d = 10, its = 52
i = 7529 (of 10192), d = 2.52159, its = 5
i = 7530 (of 10192), d = 2.70495, its = 6
i = 7531 (of 10192), d = 10, its = 53
i = 7532 (of 10192), d = 10, its = 50
i = 7533 (of 10192), d = 1.87664, its = 4
i = 7534 (of 10192), d = 10, its = 37
i = 7535 (of 10192), d = 10, its = 53
i = 7536 (of 10192), d = 1.50063, its = 7
i = 7537 (of 10192), d = 10, its = 39
i = 7538 (of 10192), d = 10, its = 52
i = 7539 (of 10192), d = 10, its = 58
i = 7540 (of 10192), d = 10, its = 53
i = 7541 (of 10192), d = 10, its = 56
i = 7542 (of 10192), d = 10, its = 40
i = 7543 (of 10192), d = 10, its = 51
i = 7544 (of 10192), d = 10, its = 40
i = 7545 (of 10192), d = 1.9963, its = 2
i = 7546 (of 10192), d = 10, its = 52
i = 7547 (of 10192), d = 10, its = 52
i = 7548 (of 10192), d = 2.17545, its = 4
i = 7549 (of 10192), d = 10, its = 39
i = 7550 (of 10192), d = 10, its = 54
i = 7551 (of 10192), d = 10, its = 56
i = 7552 (of 10192), d = 10, its = 52
i = 7553 (of 10192), d = 10, its = 53
i = 7554 (of 10192), d = 3.50666, its = 7
i = 7555 (of 10192), d = 2.43049, its = 5
i = 7556 (of 10192), d = 10, its = 37
i = 7557 (of 10192), d = 10, its = 37
i = 7558 (of 10192), d = 10, its = 37
i = 7559 (of 10192), d = 1.54528, its = 6
i = 7560 (of 10192), d = 2.16792, its = 4
i = 7561 (of 10192), d = 10, its = 56
i = 7562 (of 10192), d = 10, its = 55
i = 7563 (of 10192), d = 10, its = 56
i = 7564 (of 10192), d = 6.10101, its = 7
i = 7565 (of 10192), d = 10, its = 54
i = 7566 (of 10192), d = 1.47129, its = 7
i = 7567 (of 10192), d = 10, its = 53
i = 7568 (of 10192), d = 2.18655, its = 5
i = 7569 (of 10192), d = 2.89217, its = 5
i = 7570 (of 10192), d = 1.39538, its = 7
i = 7571 (of 10192), d = 10, its = 49
i = 7572 (of 10192), d = 10, its = 51
i = 7573 (of 10192), d = 10, its = 49
i = 7574 (of 10192), d = 1.93569, its = 3
i = 7575 (of 10192), d = 10, its = 51
i = 7576 (of 10192), d = 10, its = 37
i = 7577 (of 10192), d = 10, its = 59
i = 7578 (of 10192), d = 10, its = 53
i = 7579 (of 10192), d = 10, its = 52
i = 7580 (of 10192), d = 10, its = 53
i = 7581 (of 10192), d = 10, its = 55
i = 7582 (of 10192), d = 2.26723, its = 4
i = 7583 (of 10192), d = 10, its = 50
i = 7584 (of 10192), d = 3.5078, its = 7
i = 7585 (of 10192), d = 10, its = 51
i = 7586 (of 10192), d = 10, its = 51
i = 7587 (of 10192), d = 10, its = 54
i = 7588 (of 10192), d = 10, its = 49
i = 7589 (of 10192), d = 10, its = 39
i = 7590 (of 10192), d = 1.53035, its = 7
i = 7591 (of 10192), d = 2.53971, its = 5
i = 7592 (of 10192), d = 10, its = 37
i = 7593 (of 10192), d = 10, its = 53
i = 7594 (of 10192), d = 7.85423, its = 9
i = 7595 (of 10192), d = 10, its = 41
i = 7596 (of 10192), d = 10, its = 53
i = 7597 (of 10192), d = 10, its = 57
i = 7598 (of 10192), d = 10, its = 52
i = 7599 (of 10192), d = 10, its = 39
i = 7600 (of 10192), d = 1.72078, its = 5
i = 7601 (of 10192), d = 10, its = 54
i = 7602 (of 10192), d = 10, its = 38
i = 7603 (of 10192), d = 2.50974, its = 5
i = 7604 (of 10192), d = 10, its = 45
i = 7605 (of 10192), d = 2.41621, its = 5
i = 7606 (of 10192), d = 10, its = 40
i = 7607 (of 10192), d = 2.04055, its = 4
i = 7608 (of 10192), d = 10, its = 53
i = 7609 (of 10192), d = 0.701786, its = 17
i = 7610 (of 10192), d = 10, its = 58
i = 7611 (of 10192), d = 10, its = 37
i = 7612 (of 10192), d = 10, its = 41
i = 7613 (of 10192), d = 10, its = 53
i = 7614 (of 10192), d = 10, its = 52
i = 7615 (of 10192), d = 0.928766, its = 20
i = 7616 (of 10192), d = 2.19826, its = 4
i = 7617 (of 10192), d = 2.73475, its = 5
i = 7618 (of 10192), d = 10, its = 54
i = 7619 (of 10192), d = 10, its = 38
i = 7620 (of 10192), d = 10, its = 50
i = 7621 (of 10192), d = 10, its = 37
i = 7622 (of 10192), d = 10, its = 53
i = 7623 (of 10192), d = 0.927582, its = 20
i = 7624 (of 10192), d = 1.21968, its = 18
i = 7625 (of 10192), d = 10, its = 40
i = 7626 (of 10192), d = 10, its = 41
i = 7627 (of 10192), d = 10, its = 51
i = 7628 (of 10192), d = 10, its = 51
i = 7629 (of 10192), d = 1.5228, its = 6
i = 7630 (of 10192), d = 1.9308, its = 3
i = 7631 (of 10192), d = 10, its = 37
i = 7632 (of 10192), d = 4.53164, its = 7
i = 7633 (of 10192), d = 10, its = 52
i = 7634 (of 10192), d = 10, its = 53
i = 7635 (of 10192), d = 10, its = 54
i = 7636 (of 10192), d = 3.03029, its = 6
i = 7637 (of 10192), d = 10, its = 53
i = 7638 (of 10192), d = 2.14224, its = 4
i = 7639 (of 10192), d = 10, its = 37
i = 7640 (of 10192), d = 10, its = 48
i = 7641 (of 10192), d = 10, its = 53
i = 7642 (of 10192), d = 10, its = 41
i = 7643 (of 10192), d = 10, its = 42
i = 7644 (of 10192), d = 10, its = 50
i = 7645 (of 10192), d = 10, its = 41
i = 7646 (of 10192), d = 10, its = 42
i = 7647 (of 10192), d = 0.595328, its = 17
i = 7648 (of 10192), d = 10, its = 52
i = 7649 (of 10192), d = 1.68589, its = 5
i = 7650 (of 10192), d = 10, its = 37
i = 7651 (of 10192), d = 10, its = 54
i = 7652 (of 10192), d = 0.962406, its = 19
i = 7653 (of 10192), d = 1.69412, its = 5
i = 7654 (of 10192), d = 1.29115, its = 7
i = 7655 (of 10192), d = 1.18312, its = 17
i = 7656 (of 10192), d = 1.87471, its = 4
i = 7657 (of 10192), d = 10, its = 51
i = 7658 (of 10192), d = 10, its = 42
i = 7659 (of 10192), d = 10, its = 56
i = 7660 (of 10192), d = 10, its = 53
i = 7661 (of 10192), d = 10, its = 53
i = 7662 (of 10192), d = 10, its = 51
i = 7663 (of 10192), d = 2.50188, its = 5
i = 7664 (of 10192), d = 10, its = 49
i = 7665 (of 10192), d = 5.60902, its = 8
i = 7666 (of 10192), d = 10, its = 49
i = 7667 (of 10192), d = 10, its = 54
i = 7668 (of 10192), d = 10, its = 48
i = 7669 (of 10192), d = 0.855948, its = 20
i = 7670 (of 10192), d = 1.37646, its = 6
i = 7671 (of 10192), d = 3.2562, its = 7
i = 7672 (of 10192), d = 2.46671, its = 6
i = 7673 (of 10192), d = 10, its = 39
i = 7674 (of 10192), d = 1.99168, its = 3
i = 7675 (of 10192), d = 1.81284, its = 4
i = 7676 (of 10192), d = 1.49697, its = 8
i = 7677 (of 10192), d = 1.36569, its = 7
i = 7678 (of 10192), d = 10, its = 45
i = 7679 (of 10192), d = 10, its = 40
i = 7680 (of 10192), d = 10, its = 56
i = 7681 (of 10192), d = 10, its = 55
i = 7682 (of 10192), d = 10, its = 57
i = 7683 (of 10192), d = 10, its = 37
i = 7684 (of 10192), d = 1.76189, its = 5
i = 7685 (of 10192), d = 10, its = 41
i = 7686 (of 10192), d = 2.41355, its = 5
i = 7687 (of 10192), d = 10, its = 47
i = 7688 (of 10192), d = 10, its = 40
i = 7689 (of 10192), d = 10, its = 54
i = 7690 (of 10192), d = 2.83688, its = 6
i = 7691 (of 10192), d = 2.84188, its = 5
i = 7692 (of 10192), d = 1.41016, its = 7
i = 7693 (of 10192), d = 1.45405, its = 7
i = 7694 (of 10192), d = 10, its = 52
i = 7695 (of 10192), d = 10, its = 53
i = 7696 (of 10192), d = 1.90749, its = 4
i = 7697 (of 10192), d = 10, its = 56
i = 7698 (of 10192), d = 10, its = 52
i = 7699 (of 10192), d = 2.90324, its = 6
i = 7700 (of 10192), d = 10, its = 38
i = 7701 (of 10192), d = 10, its = 40
i = 7702 (of 10192), d = 10, its = 57
i = 7703 (of 10192), d = 10, its = 39
i = 7704 (of 10192), d = 10, its = 39
i = 7705 (of 10192), d = 2.34127, its = 5
i = 7706 (of 10192), d = 10, its = 54
i = 7707 (of 10192), d = 10, its = 52
i = 7708 (of 10192), d = 10, its = 47
i = 7709 (of 10192), d = 10, its = 55
i = 7710 (of 10192), d = 10, its = 41
i = 7711 (of 10192), d = 2.13212, its = 4
i = 7712 (of 10192), d = 1.42734, its = 7
i = 7713 (of 10192), d = 10, its = 44
i = 7714 (of 10192), d = 10, its = 55
i = 7715 (of 10192), d = 10, its = 56
i = 7716 (of 10192), d = 10, its = 52
i = 7717 (of 10192), d = 10, its = 49
i = 7718 (of 10192), d = 10, its = 37
i = 7719 (of 10192), d = 10, its = 40
i = 7720 (of 10192), d = 10, its = 37
i = 7721 (of 10192), d = 10, its = 37
i = 7722 (of 10192), d = 1.86785, its = 4
i = 7723 (of 10192), d = 1.53198, its = 6
i = 7724 (of 10192), d = 10, its = 52
i = 7725 (of 10192), d = 10, its = 37
i = 7726 (of 10192), d = 2.24758, its = 4
i = 7727 (of 10192), d = 3.00804, its = 6
i = 7728 (of 10192), d = 10, its = 55
i = 7729 (of 10192), d = 1.80605, its = 4
i = 7730 (of 10192), d = 10, its = 53
i = 7731 (of 10192), d = 10, its = 55
i = 7732 (of 10192), d = 10, its = 42
i = 7733 (of 10192), d = 10, its = 52
i = 7734 (of 10192), d = 10, its = 52
i = 7735 (of 10192), d = 10, its = 58
i = 7736 (of 10192), d = 10, its = 51
i = 7737 (of 10192), d = 1.32439, its = 8
i = 7738 (of 10192), d = 10, its = 37
i = 7739 (of 10192), d = 1.74354, its = 6
i = 7740 (of 10192), d = 10, its = 54
i = 7741 (of 10192), d = 1.56721, its = 7
i = 7742 (of 10192), d = 2.18763, its = 5
i = 7743 (of 10192), d = 0.889542, its = 22
i = 7744 (of 10192), d = 10, its = 43
i = 7745 (of 10192), d = 10, its = 53
i = 7746 (of 10192), d = 2.19143, its = 4
i = 7747 (of 10192), d = 2.09973, its = 4
i = 7748 (of 10192), d = 10, its = 53
i = 7749 (of 10192), d = 1.79566, its = 5
i = 7750 (of 10192), d = 10, its = 48
i = 7751 (of 10192), d = 10, its = 52
i = 7752 (of 10192), d = 10, its = 53
i = 7753 (of 10192), d = 10, its = 55
i = 7754 (of 10192), d = 10, its = 52
i = 7755 (of 10192), d = 10, its = 53
i = 7756 (of 10192), d = 10, its = 39
i = 7757 (of 10192), d = 10, its = 49
i = 7758 (of 10192), d = 10, its = 52
i = 7759 (of 10192), d = 10, its = 50
i = 7760 (of 10192), d = 10, its = 51
i = 7761 (of 10192), d = 0.950085, its = 17
i = 7762 (of 10192), d = 2.56013, its = 6
i = 7763 (of 10192), d = 10, its = 54
i = 7764 (of 10192), d = 10, its = 52
i = 7765 (of 10192), d = 10, its = 40
i = 7766 (of 10192), d = 10, its = 50
i = 7767 (of 10192), d = 10, its = 48
i = 7768 (of 10192), d = 4.57857, its = 8
i = 7769 (of 10192), d = 10, its = 54
i = 7770 (of 10192), d = 9.90329, its = 9
i = 7771 (of 10192), d = 10, its = 53
i = 7772 (of 10192), d = 10, its = 53
i = 7773 (of 10192), d = 10, its = 51
i = 7774 (of 10192), d = 10, its = 54
i = 7775 (of 10192), d = 1.33214, its = 8
i = 7776 (of 10192), d = 0.915642, its = 23
i = 7777 (of 10192), d = 10, its = 54
i = 7778 (of 10192), d = 10, its = 51
i = 7779 (of 10192), d = 10, its = 51
i = 7780 (of 10192), d = 10, its = 50
i = 7781 (of 10192), d = 3.53901, its = 6
i = 7782 (of 10192), d = 10, its = 47
i = 7783 (of 10192), d = 10, its = 53
i = 7784 (of 10192), d = 10, its = 47
i = 7785 (of 10192), d = 2.5986, its = 5
i = 7786 (of 10192), d = 10, its = 53
i = 7787 (of 10192), d = 2.57955, its = 5
i = 7788 (of 10192), d = 10, its = 51
i = 7789 (of 10192), d = 10, its = 49
i = 7790 (of 10192), d = 10, its = 53
i = 7791 (of 10192), d = 0.82602, its = 17
i = 7792 (of 10192), d = 10, its = 40
i = 7793 (of 10192), d = 10, its = 52
i = 7794 (of 10192), d = 10, its = 52
i = 7795 (of 10192), d = 10, its = 37
i = 7796 (of 10192), d = 6.62678, its = 8
i = 7797 (of 10192), d = 10, its = 39
i = 7798 (of 10192), d = 1.33065, its = 7
i = 7799 (of 10192), d = 10, its = 52
i = 7800 (of 10192), d = 10, its = 54
i = 7801 (of 10192), d = 10, its = 59
i = 7802 (of 10192), d = 10, its = 38
i = 7803 (of 10192), d = 1.36541, its = 22
i = 7804 (of 10192), d = 10, its = 39
i = 7805 (of 10192), d = 10, its = 55
i = 7806 (of 10192), d = 10, its = 48
i = 7807 (of 10192), d = 10, its = 53
i = 7808 (of 10192), d = 2.33284, its = 5
i = 7809 (of 10192), d = 10, its = 52
i = 7810 (of 10192), d = 10, its = 55
i = 7811 (of 10192), d = 10, its = 41
i = 7812 (of 10192), d = 10, its = 37
i = 7813 (of 10192), d = 2.81352, its = 6
i = 7814 (of 10192), d = 10, its = 52
i = 7815 (of 10192), d = 10, its = 40
i = 7816 (of 10192), d = 10, its = 55
i = 7817 (of 10192), d = 0.932485, its = 20
i = 7818 (of 10192), d = 1.71355, its = 6
i = 7819 (of 10192), d = 10, its = 51
i = 7820 (of 10192), d = 10, its = 52
i = 7821 (of 10192), d = 10, its = 43
i = 7822 (of 10192), d = 10, its = 55
i = 7823 (of 10192), d = 10, its = 48
i = 7824 (of 10192), d = 2.07068, its = 4
i = 7825 (of 10192), d = 10, its = 39
i = 7826 (of 10192), d = 10, its = 51
i = 7827 (of 10192), d = 10, its = 40
i = 7828 (of 10192), d = 10, its = 54
i = 7829 (of 10192), d = 1.87158, its = 4
i = 7830 (of 10192), d = 10, its = 57
i = 7831 (of 10192), d = 2.07877, its = 4
i = 7832 (of 10192), d = 10, its = 39
i = 7833 (of 10192), d = 10, its = 57
i = 7834 (of 10192), d = 10, its = 37
i = 7835 (of 10192), d = 2.29487, its = 5
i = 7836 (of 10192), d = 10, its = 54
i = 7837 (of 10192), d = 2.57524, its = 5
i = 7838 (of 10192), d = 2.50387, its = 6
i = 7839 (of 10192), d = 1.66413, its = 6
i = 7840 (of 10192), d = 10, its = 55
i = 7841 (of 10192), d = 10, its = 51
i = 7842 (of 10192), d = 1.76739, its = 6
i = 7843 (of 10192), d = 10, its = 50
i = 7844 (of 10192), d = 0.922044, its = 20
i = 7845 (of 10192), d = 10, its = 37
i = 7846 (of 10192), d = 3.08635, its = 6
i = 7847 (of 10192), d = 10, its = 51
i = 7848 (of 10192), d = 10, its = 37
i = 7849 (of 10192), d = 10, its = 43
i = 7850 (of 10192), d = 10, its = 40
i = 7851 (of 10192), d = 1.16474, its = 24
i = 7852 (of 10192), d = 10, its = 51
i = 7853 (of 10192), d = 2.16608, its = 4
i = 7854 (of 10192), d = 10, its = 37
i = 7855 (of 10192), d = 10, its = 40
i = 7856 (of 10192), d = 10, its = 39
i = 7857 (of 10192), d = 10, its = 51
i = 7858 (of 10192), d = 10, its = 52
i = 7859 (of 10192), d = 2.80796, its = 6
i = 7860 (of 10192), d = 10, its = 51
i = 7861 (of 10192), d = 10, its = 37
i = 7862 (of 10192), d = 10, its = 38
i = 7863 (of 10192), d = 10, its = 54
i = 7864 (of 10192), d = 2.3778, its = 5
i = 7865 (of 10192), d = 2.58784, its = 5
i = 7866 (of 10192), d = 1.64304, its = 6
i = 7867 (of 10192), d = 10, its = 49
i = 7868 (of 10192), d = 10, its = 51
i = 7869 (of 10192), d = 10, its = 41
i = 7870 (of 10192), d = 10, its = 54
i = 7871 (of 10192), d = 10, its = 53
i = 7872 (of 10192), d = 10, its = 39
i = 7873 (of 10192), d = 10, its = 41
i = 7874 (of 10192), d = 10, its = 54
i = 7875 (of 10192), d = 10, its = 58
i = 7876 (of 10192), d = 10, its = 38
i = 7877 (of 10192), d = 10, its = 52
i = 7878 (of 10192), d = 10, its = 49
i = 7879 (of 10192), d = 10, its = 39
i = 7880 (of 10192), d = 10, its = 52
i = 7881 (of 10192), d = 1.85477, its = 4
i = 7882 (of 10192), d = 10, its = 52
i = 7883 (of 10192), d = 10, its = 55
i = 7884 (of 10192), d = 1.45248, its = 8
i = 7885 (of 10192), d = 10, its = 57
i = 7886 (of 10192), d = 10, its = 53
i = 7887 (of 10192), d = 3.47163, its = 6
i = 7888 (of 10192), d = 10, its = 46
i = 7889 (of 10192), d = 10, its = 37
i = 7890 (of 10192), d = 1.80184, its = 5
i = 7891 (of 10192), d = 1.61778, its = 7
i = 7892 (of 10192), d = 10, its = 37
i = 7893 (of 10192), d = 8.27327, its = 8
i = 7894 (of 10192), d = 10, its = 54
i = 7895 (of 10192), d = 4.26321, its = 7
i = 7896 (of 10192), d = 2.4578, its = 5
i = 7897 (of 10192), d = 10, its = 55
i = 7898 (of 10192), d = 3.37215, its = 9
i = 7899 (of 10192), d = 10, its = 53
i = 7900 (of 10192), d = 10, its = 55
i = 7901 (of 10192), d = 10, its = 39
i = 7902 (of 10192), d = 1.19826, its = 17
i = 7903 (of 10192), d = 10, its = 56
i = 7904 (of 10192), d = 10, its = 49
i = 7905 (of 10192), d = 10, its = 54
i = 7906 (of 10192), d = 4.60497, its = 9
i = 7907 (of 10192), d = 10, its = 53
i = 7908 (of 10192), d = 10, its = 43
i = 7909 (of 10192), d = 2.11851, its = 4
i = 7910 (of 10192), d = 10, its = 55
i = 7911 (of 10192), d = 1.55261, its = 7
i = 7912 (of 10192), d = 10, its = 54
i = 7913 (of 10192), d = 0.537985, its = 21
i = 7914 (of 10192), d = 2.88964, its = 6
i = 7915 (of 10192), d = 10, its = 52
i = 7916 (of 10192), d = 1.59469, its = 7
i = 7917 (of 10192), d = 10, its = 41
i = 7918 (of 10192), d = 10, its = 52
i = 7919 (of 10192), d = 10, its = 52
i = 7920 (of 10192), d = 2.86843, its = 6
i = 7921 (of 10192), d = 10, its = 52
i = 7922 (of 10192), d = 10, its = 55
i = 7923 (of 10192), d = 10, its = 53
i = 7924 (of 10192), d = 10, its = 40
i = 7925 (of 10192), d = 10, its = 52
i = 7926 (of 10192), d = 2.36409, its = 5
i = 7927 (of 10192), d = 10, its = 53
i = 7928 (of 10192), d = 2.82296, its = 9
i = 7929 (of 10192), d = 10, its = 37
i = 7930 (of 10192), d = 2.39634, its = 5
i = 7931 (of 10192), d = 0.941199, its = 19
i = 7932 (of 10192), d = 10, its = 37
i = 7933 (of 10192), d = 10, its = 50
i = 7934 (of 10192), d = 2.30851, its = 4
i = 7935 (of 10192), d = 1.86259, its = 4
i = 7936 (of 10192), d = 10, its = 53
i = 7937 (of 10192), d = 10, its = 55
i = 7938 (of 10192), d = 10, its = 41
i = 7939 (of 10192), d = 10, its = 49
i = 7940 (of 10192), d = 10, its = 53
i = 7941 (of 10192), d = 10, its = 53
i = 7942 (of 10192), d = 10, its = 52
i = 7943 (of 10192), d = 10, its = 52
i = 7944 (of 10192), d = 3.83713, its = 6
i = 7945 (of 10192), d = 10, its = 37
i = 7946 (of 10192), d = 10, its = 53
i = 7947 (of 10192), d = 2.5398, its = 5
i = 7948 (of 10192), d = 2.86564, its = 6
i = 7949 (of 10192), d = 2.0298, its = 3
i = 7950 (of 10192), d = 10, its = 40
i = 7951 (of 10192), d = 10, its = 52
i = 7952 (of 10192), d = 2.81367, its = 6
i = 7953 (of 10192), d = 2.88324, its = 6
i = 7954 (of 10192), d = 2.39347, its = 5
i = 7955 (of 10192), d = 6.76505, its = 8
i = 7956 (of 10192), d = 10, its = 37
i = 7957 (of 10192), d = 1.5443, its = 7
i = 7958 (of 10192), d = 10, its = 37
i = 7959 (of 10192), d = 10, its = 50
i = 7960 (of 10192), d = 10, its = 54
i = 7961 (of 10192), d = 10, its = 54
i = 7962 (of 10192), d = 3.32417, its = 6
i = 7963 (of 10192), d = 10, its = 53
i = 7964 (of 10192), d = 9.3893, its = 9
i = 7965 (of 10192), d = 10, its = 42
i = 7966 (of 10192), d = 1.80184, its = 5
i = 7967 (of 10192), d = 10, its = 48
i = 7968 (of 10192), d = 10, its = 51
i = 7969 (of 10192), d = 10, its = 40
i = 7970 (of 10192), d = 1.31898, its = 6
i = 7971 (of 10192), d = 1.63859, its = 7
i = 7972 (of 10192), d = 0.93107, its = 19
i = 7973 (of 10192), d = 1.16149, its = 24
i = 7974 (of 10192), d = 10, its = 55
i = 7975 (of 10192), d = 10, its = 50
i = 7976 (of 10192), d = 0.875899, its = 18
i = 7977 (of 10192), d = 10, its = 41
i = 7978 (of 10192), d = 1.30649, its = 6
i = 7979 (of 10192), d = 10, its = 52
i = 7980 (of 10192), d = 2.06129, its = 4
i = 7981 (of 10192), d = 10, its = 37
i = 7982 (of 10192), d = 10, its = 56
i = 7983 (of 10192), d = 10, its = 53
i = 7984 (of 10192), d = 10, its = 40
i = 7985 (of 10192), d = 10, its = 51
i = 7986 (of 10192), d = 10, its = 52
i = 7987 (of 10192), d = 10, its = 39
i = 7988 (of 10192), d = 10, its = 48
i = 7989 (of 10192), d = 10, its = 54
i = 7990 (of 10192), d = 10, its = 50
i = 7991 (of 10192), d = 10, its = 54
i = 7992 (of 10192), d = 10, its = 53
i = 7993 (of 10192), d = 10, its = 37
i = 7994 (of 10192), d = 10, its = 55
i = 7995 (of 10192), d = 10, its = 53
i = 7996 (of 10192), d = 10, its = 39
i = 7997 (of 10192), d = 10, its = 55
i = 7998 (of 10192), d = 10, its = 52
i = 7999 (of 10192), d = 8.33864, its = 10
i = 8000 (of 10192), d = 10, its = 48
i = 8001 (of 10192), d = 10, its = 40
i = 8002 (of 10192), d = 2.50671, its = 5
i = 8003 (of 10192), d = 1.2521, its = 6
i = 8004 (of 10192), d = 3.49822, its = 8
i = 8005 (of 10192), d = 5.30356, its = 7
i = 8006 (of 10192), d = 10, its = 17
i = 8007 (of 10192), d = 10, its = 54
i = 8008 (of 10192), d = 3.59253, its = 8
i = 8009 (of 10192), d = 10, its = 54
i = 8010 (of 10192), d = 10, its = 51
i = 8011 (of 10192), d = 2.25398, its = 4
i = 8012 (of 10192), d = 10, its = 40
i = 8013 (of 10192), d = 10, its = 54
i = 8014 (of 10192), d = 10, its = 42
i = 8015 (of 10192), d = 10, its = 53
i = 8016 (of 10192), d = 10, its = 53
i = 8017 (of 10192), d = 10, its = 39
i = 8018 (of 10192), d = 10, its = 38
i = 8019 (of 10192), d = 10, its = 50
i = 8020 (of 10192), d = 10, its = 42
i = 8021 (of 10192), d = 10, its = 53
i = 8022 (of 10192), d = 10, its = 50
i = 8023 (of 10192), d = 1.70932, its = 5
i = 8024 (of 10192), d = 10, its = 37
i = 8025 (of 10192), d = 10, its = 53
i = 8026 (of 10192), d = 10, its = 54
i = 8027 (of 10192), d = 10, its = 40
i = 8028 (of 10192), d = 10, its = 54
i = 8029 (of 10192), d = 5.41023, its = 8
i = 8030 (of 10192), d = 10, its = 52
i = 8031 (of 10192), d = 2.78536, its = 6
i = 8032 (of 10192), d = 1.60985, its = 6
i = 8033 (of 10192), d = 10, its = 38
i = 8034 (of 10192), d = 10, its = 49
i = 8035 (of 10192), d = 10, its = 54
i = 8036 (of 10192), d = 1.36582, its = 6
i = 8037 (of 10192), d = 1.50017, its = 8
i = 8038 (of 10192), d = 2.77236, its = 6
i = 8039 (of 10192), d = 0.809516, its = 17
i = 8040 (of 10192), d = 10, its = 43
i = 8041 (of 10192), d = 10, its = 57
i = 8042 (of 10192), d = 10, its = 53
i = 8043 (of 10192), d = 10, its = 38
i = 8044 (of 10192), d = 10, its = 55
i = 8045 (of 10192), d = 10, its = 53
i = 8046 (of 10192), d = 10, its = 58
i = 8047 (of 10192), d = 10, its = 53
i = 8048 (of 10192), d = 10, its = 52
i = 8049 (of 10192), d = 1.60116, its = 6
i = 8050 (of 10192), d = 10, its = 41
i = 8051 (of 10192), d = 10, its = 52
i = 8052 (of 10192), d = 10, its = 53
i = 8053 (of 10192), d = 10, its = 51
i = 8054 (of 10192), d = 10, its = 52
i = 8055 (of 10192), d = 10, its = 40
i = 8056 (of 10192), d = 0.883784, its = 18
i = 8057 (of 10192), d = 10, its = 37
i = 8058 (of 10192), d = 10, its = 37
i = 8059 (of 10192), d = 1.46194, its = 6
i = 8060 (of 10192), d = 3.30738, its = 6
i = 8061 (of 10192), d = 10, its = 37
i = 8062 (of 10192), d = 10, its = 49
i = 8063 (of 10192), d = 10, its = 37
i = 8064 (of 10192), d = 10, its = 45
i = 8065 (of 10192), d = 10, its = 51
i = 8066 (of 10192), d = 10, its = 51
i = 8067 (of 10192), d = 10, its = 50
i = 8068 (of 10192), d = 10, its = 40
i = 8069 (of 10192), d = 10, its = 47
i = 8070 (of 10192), d = 10, its = 50
i = 8071 (of 10192), d = 10, its = 53
i = 8072 (of 10192), d = 10, its = 40
i = 8073 (of 10192), d = 4.08084, its = 7
i = 8074 (of 10192), d = 10, its = 54
i = 8075 (of 10192), d = 1.12149, its = 18
i = 8076 (of 10192), d = 10, its = 37
i = 8077 (of 10192), d = 10, its = 51
i = 8078 (of 10192), d = 10, its = 37
i = 8079 (of 10192), d = 10, its = 37
i = 8080 (of 10192), d = 10, its = 51
i = 8081 (of 10192), d = 10, its = 37
i = 8082 (of 10192), d = 10, its = 52
i = 8083 (of 10192), d = 10, its = 53
i = 8084 (of 10192), d = 10, its = 37
i = 8085 (of 10192), d = 2.3047, its = 4
i = 8086 (of 10192), d = 10, its = 37
i = 8087 (of 10192), d = 0.623288, its = 18
i = 8088 (of 10192), d = 10, its = 41
i = 8089 (of 10192), d = 10, its = 37
i = 8090 (of 10192), d = 2.22572, its = 4
i = 8091 (of 10192), d = 10, its = 57
i = 8092 (of 10192), d = 10, its = 41
i = 8093 (of 10192), d = 2.6432, its = 5
i = 8094 (of 10192), d = 10, its = 53
i = 8095 (of 10192), d = 10, its = 53
i = 8096 (of 10192), d = 9.85615, its = 21
i = 8097 (of 10192), d = 10, its = 54
i = 8098 (of 10192), d = 0.752794, its = 25
i = 8099 (of 10192), d = 10, its = 56
i = 8100 (of 10192), d = 10, its = 38
i = 8101 (of 10192), d = 2.35631, its = 5
i = 8102 (of 10192), d = 10, its = 49
i = 8103 (of 10192), d = 10, its = 46
i = 8104 (of 10192), d = 10, its = 53
i = 8105 (of 10192), d = 1.70598, its = 5
i = 8106 (of 10192), d = 10, its = 37
i = 8107 (of 10192), d = 10, its = 55
i = 8108 (of 10192), d = 10, its = 37
i = 8109 (of 10192), d = 10, its = 51
i = 8110 (of 10192), d = 3.27843, its = 6
i = 8111 (of 10192), d = 10, its = 49
i = 8112 (of 10192), d = 10, its = 37
i = 8113 (of 10192), d = 10, its = 51
i = 8114 (of 10192), d = 10, its = 54
i = 8115 (of 10192), d = 2.21957, its = 4
i = 8116 (of 10192), d = 10, its = 53
i = 8117 (of 10192), d = 10, its = 42
i = 8118 (of 10192), d = 0.803152, its = 21
i = 8119 (of 10192), d = 10, its = 39
i = 8120 (of 10192), d = 2.24743, its = 4
i = 8121 (of 10192), d = 10, its = 38
i = 8122 (of 10192), d = 10, its = 37
i = 8123 (of 10192), d = 0.75224, its = 28
i = 8124 (of 10192), d = 1.22548, its = 5
i = 8125 (of 10192), d = 2.66183, its = 5
i = 8126 (of 10192), d = 10, its = 41
i = 8127 (of 10192), d = 1.50936, its = 6
i = 8128 (of 10192), d = 2.677, its = 6
i = 8129 (of 10192), d = 10, its = 48
i = 8130 (of 10192), d = 1.72147, its = 5
i = 8131 (of 10192), d = 10, its = 37
i = 8132 (of 10192), d = 10, its = 53
i = 8133 (of 10192), d = 10, its = 57
i = 8134 (of 10192), d = 10, its = 42
i = 8135 (of 10192), d = 10, its = 39
i = 8136 (of 10192), d = 10, its = 41
i = 8137 (of 10192), d = 10, its = 51
i = 8138 (of 10192), d = 10, its = 54
i = 8139 (of 10192), d = 0.882455, its = 18
i = 8140 (of 10192), d = 10, its = 41
i = 8141 (of 10192), d = 10, its = 52
i = 8142 (of 10192), d = 10, its = 51
i = 8143 (of 10192), d = 10, its = 52
i = 8144 (of 10192), d = 10, its = 37
i = 8145 (of 10192), d = 4.02183, its = 7
i = 8146 (of 10192), d = 4.44136, its = 8
i = 8147 (of 10192), d = 10, its = 55
i = 8148 (of 10192), d = 2.44726, its = 5
i = 8149 (of 10192), d = 10, its = 51
i = 8150 (of 10192), d = 10, its = 40
i = 8151 (of 10192), d = 10, its = 50
i = 8152 (of 10192), d = 10, its = 58
i = 8153 (of 10192), d = 10, its = 47
i = 8154 (of 10192), d = 10, its = 54
i = 8155 (of 10192), d = 10, its = 51
i = 8156 (of 10192), d = 1.52744, its = 8
i = 8157 (of 10192), d = 2.21803, its = 4
i = 8158 (of 10192), d = 10, its = 54
i = 8159 (of 10192), d = 1.14534, its = 19
i = 8160 (of 10192), d = 10, its = 54
i = 8161 (of 10192), d = 0.803299, its = 16
i = 8162 (of 10192), d = 2.58181, its = 5
i = 8163 (of 10192), d = 10, its = 44
i = 8164 (of 10192), d = 2.38426, its = 5
i = 8165 (of 10192), d = 1.05636, its = 25
i = 8166 (of 10192), d = 10, its = 55
i = 8167 (of 10192), d = 10, its = 50
i = 8168 (of 10192), d = 10, its = 57
i = 8169 (of 10192), d = 10, its = 39
i = 8170 (of 10192), d = 10, its = 52
i = 8171 (of 10192), d = 10, its = 51
i = 8172 (of 10192), d = 10, its = 58
i = 8173 (of 10192), d = 10, its = 53
i = 8174 (of 10192), d = 10, its = 37
i = 8175 (of 10192), d = 10, its = 54
i = 8176 (of 10192), d = 0.987029, its = 19
i = 8177 (of 10192), d = 3.04625, its = 6
i = 8178 (of 10192), d = 10, its = 49
i = 8179 (of 10192), d = 10, its = 53
i = 8180 (of 10192), d = 10, its = 55
i = 8181 (of 10192), d = 0.684836, its = 19
i = 8182 (of 10192), d = 10, its = 54
i = 8183 (of 10192), d = 10, its = 54
i = 8184 (of 10192), d = 10, its = 54
i = 8185 (of 10192), d = 10, its = 54
i = 8186 (of 10192), d = 10, its = 37
i = 8187 (of 10192), d = 1.84656, its = 4
i = 8188 (of 10192), d = 3.47163, its = 6
i = 8189 (of 10192), d = 10, its = 49
i = 8190 (of 10192), d = 2.49292, its = 6
i = 8191 (of 10192), d = 10, its = 52
i = 8192 (of 10192), d = 1.71792, its = 5
i = 8193 (of 10192), d = 10, its = 58
i = 8194 (of 10192), d = 10, its = 47
i = 8195 (of 10192), d = 10, its = 55
i = 8196 (of 10192), d = 10, its = 42
i = 8197 (of 10192), d = 10, its = 57
i = 8198 (of 10192), d = 1.61417, its = 6
i = 8199 (of 10192), d = 10, its = 50
i = 8200 (of 10192), d = 10, its = 54
i = 8201 (of 10192), d = 10, its = 53
i = 8202 (of 10192), d = 10, its = 47
i = 8203 (of 10192), d = 1.87211, its = 4
i = 8204 (of 10192), d = 6.60324, its = 7
i = 8205 (of 10192), d = 10, its = 45
i = 8206 (of 10192), d = 3.49913, its = 6
i = 8207 (of 10192), d = 3.86805, its = 8
i = 8208 (of 10192), d = 10, its = 57
i = 8209 (of 10192), d = 10, its = 52
i = 8210 (of 10192), d = 10, its = 53
i = 8211 (of 10192), d = 10, its = 39
i = 8212 (of 10192), d = 10, its = 37
i = 8213 (of 10192), d = 1.7119, its = 5
i = 8214 (of 10192), d = 10, its = 53
i = 8215 (of 10192), d = 2.52721, its = 5
i = 8216 (of 10192), d = 10, its = 56
i = 8217 (of 10192), d = 10, its = 53
i = 8218 (of 10192), d = 10, its = 55
i = 8219 (of 10192), d = 10, its = 55
i = 8220 (of 10192), d = 10, its = 40
i = 8221 (of 10192), d = 1.59379, its = 6
i = 8222 (of 10192), d = 10, its = 39
i = 8223 (of 10192), d = 2.48171, its = 5
i = 8224 (of 10192), d = 1.43099, its = 6
i = 8225 (of 10192), d = 1.93107, its = 3
i = 8226 (of 10192), d = 10, its = 37
i = 8227 (of 10192), d = 1.00703, its = 17
i = 8228 (of 10192), d = 0.968259, its = 20
i = 8229 (of 10192), d = 10, its = 37
i = 8230 (of 10192), d = 10, its = 40
i = 8231 (of 10192), d = 0.937444, its = 24
i = 8232 (of 10192), d = 10, its = 51
i = 8233 (of 10192), d = 10, its = 40
i = 8234 (of 10192), d = 10, its = 52
i = 8235 (of 10192), d = 1.08415, its = 21
i = 8236 (of 10192), d = 2.16918, its = 4
i = 8237 (of 10192), d = 1.85847, its = 4
i = 8238 (of 10192), d = 1.57289, its = 6
i = 8239 (of 10192), d = 10, its = 40
i = 8240 (of 10192), d = 10, its = 52
i = 8241 (of 10192), d = 10, its = 52
i = 8242 (of 10192), d = 10, its = 55
i = 8243 (of 10192), d = 1.80774, its = 5
i = 8244 (of 10192), d = 10, its = 53
i = 8245 (of 10192), d = 10, its = 55
i = 8246 (of 10192), d = 10, its = 55
i = 8247 (of 10192), d = 10, its = 53
i = 8248 (of 10192), d = 10, its = 51
i = 8249 (of 10192), d = 10, its = 53
i = 8250 (of 10192), d = 10, its = 41
i = 8251 (of 10192), d = 10, its = 37
i = 8252 (of 10192), d = 10, its = 51
i = 8253 (of 10192), d = 10, its = 58
i = 8254 (of 10192), d = 10, its = 55
i = 8255 (of 10192), d = 10, its = 54
i = 8256 (of 10192), d = 2.14859, its = 4
i = 8257 (of 10192), d = 10, its = 51
i = 8258 (of 10192), d = 10, its = 37
i = 8259 (of 10192), d = 10, its = 51
i = 8260 (of 10192), d = 10, its = 52
i = 8261 (of 10192), d = 10, its = 52
i = 8262 (of 10192), d = 4.77021, its = 7
i = 8263 (of 10192), d = 10, its = 54
i = 8264 (of 10192), d = 10, its = 39
i = 8265 (of 10192), d = 1.74474, its = 6
i = 8266 (of 10192), d = 10, its = 55
i = 8267 (of 10192), d = 10, its = 53
i = 8268 (of 10192), d = 10, its = 49
i = 8269 (of 10192), d = 10, its = 55
i = 8270 (of 10192), d = 10, its = 53
i = 8271 (of 10192), d = 10, its = 53
i = 8272 (of 10192), d = 10, its = 53
i = 8273 (of 10192), d = 10, its = 54
i = 8274 (of 10192), d = 1.97448, its = 3
i = 8275 (of 10192), d = 1.51341, its = 6
i = 8276 (of 10192), d = 10, its = 47
i = 8277 (of 10192), d = 10, its = 51
i = 8278 (of 10192), d = 0.681995, its = 25
i = 8279 (of 10192), d = 10, its = 54
i = 8280 (of 10192), d = 10, its = 51
i = 8281 (of 10192), d = 10, its = 52
i = 8282 (of 10192), d = 10, its = 51
i = 8283 (of 10192), d = 10, its = 54
i = 8284 (of 10192), d = 10, its = 54
i = 8285 (of 10192), d = 10, its = 53
i = 8286 (of 10192), d = 0.729594, its = 17
i = 8287 (of 10192), d = 10, its = 37
i = 8288 (of 10192), d = 10, its = 52
i = 8289 (of 10192), d = 2.12239, its = 4
i = 8290 (of 10192), d = 10, its = 40
i = 8291 (of 10192), d = 2.42185, its = 5
i = 8292 (of 10192), d = 10, its = 37
i = 8293 (of 10192), d = 10, its = 37
i = 8294 (of 10192), d = 10, its = 53
i = 8295 (of 10192), d = 10, its = 50
i = 8296 (of 10192), d = 10, its = 53
i = 8297 (of 10192), d = 10, its = 50
i = 8298 (of 10192), d = 2.87345, its = 6
i = 8299 (of 10192), d = 10, its = 53
i = 8300 (of 10192), d = 10, its = 50
i = 8301 (of 10192), d = 1.43121, its = 6
i = 8302 (of 10192), d = 10, its = 51
i = 8303 (of 10192), d = 10, its = 56
i = 8304 (of 10192), d = 10, its = 37
i = 8305 (of 10192), d = 5.54096, its = 8
i = 8306 (of 10192), d = 1.03182, its = 23
i = 8307 (of 10192), d = 5.82411, its = 9
i = 8308 (of 10192), d = 10, its = 52
i = 8309 (of 10192), d = 10, its = 50
i = 8310 (of 10192), d = 3.46934, its = 8
i = 8311 (of 10192), d = 10, its = 37
i = 8312 (of 10192), d = 10, its = 49
i = 8313 (of 10192), d = 10, its = 49
i = 8314 (of 10192), d = 2.53321, its = 5
i = 8315 (of 10192), d = 10, its = 52
i = 8316 (of 10192), d = 2.10868, its = 4
i = 8317 (of 10192), d = 10, its = 37
i = 8318 (of 10192), d = 10, its = 55
i = 8319 (of 10192), d = 10, its = 37
i = 8320 (of 10192), d = 0.614313, its = 18
i = 8321 (of 10192), d = 10, its = 48
i = 8322 (of 10192), d = 1.40765, its = 6
i = 8323 (of 10192), d = 10, its = 54
i = 8324 (of 10192), d = 10, its = 54
i = 8325 (of 10192), d = 1.27317, its = 22
i = 8326 (of 10192), d = 10, its = 37
i = 8327 (of 10192), d = 10, its = 37
i = 8328 (of 10192), d = 0.930063, its = 23
i = 8329 (of 10192), d = 0.930917, its = 20
i = 8330 (of 10192), d = 10, its = 37
i = 8331 (of 10192), d = 10, its = 37
i = 8332 (of 10192), d = 10, its = 54
i = 8333 (of 10192), d = 10, its = 53
i = 8334 (of 10192), d = 2.32042, its = 4
i = 8335 (of 10192), d = 0.576568, its = 22
i = 8336 (of 10192), d = 10, its = 54
i = 8337 (of 10192), d = 1.96434, its = 3
i = 8338 (of 10192), d = 0.967191, its = 16
i = 8339 (of 10192), d = 10, its = 49
i = 8340 (of 10192), d = 10, its = 50
i = 8341 (of 10192), d = 10, its = 52
i = 8342 (of 10192), d = 10, its = 54
i = 8343 (of 10192), d = 0.937655, its = 25
i = 8344 (of 10192), d = 2.96406, its = 7
i = 8345 (of 10192), d = 10, its = 39
i = 8346 (of 10192), d = 4.53693, its = 7
i = 8347 (of 10192), d = 10, its = 39
i = 8348 (of 10192), d = 10, its = 48
i = 8349 (of 10192), d = 10, its = 51
i = 8350 (of 10192), d = 10, its = 39
i = 8351 (of 10192), d = 10, its = 51
i = 8352 (of 10192), d = 10, its = 37
i = 8353 (of 10192), d = 10, its = 52
i = 8354 (of 10192), d = 10, its = 51
i = 8355 (of 10192), d = 3.44607, its = 7
i = 8356 (of 10192), d = 2.97003, its = 6
i = 8357 (of 10192), d = 1.86412, its = 4
i = 8358 (of 10192), d = 10, its = 40
i = 8359 (of 10192), d = 1.36187, its = 7
i = 8360 (of 10192), d = 2.53971, its = 5
i = 8361 (of 10192), d = 10, its = 57
i = 8362 (of 10192), d = 10, its = 49
i = 8363 (of 10192), d = 10, its = 50
i = 8364 (of 10192), d = 10, its = 49
i = 8365 (of 10192), d = 10, its = 55
i = 8366 (of 10192), d = 10, its = 49
i = 8367 (of 10192), d = 10, its = 41
i = 8368 (of 10192), d = 1.77446, its = 5
i = 8369 (of 10192), d = 10, its = 51
i = 8370 (of 10192), d = 10, its = 39
i = 8371 (of 10192), d = 5.25956, its = 9
i = 8372 (of 10192), d = 2.68188, its = 5
i = 8373 (of 10192), d = 10, its = 40
i = 8374 (of 10192), d = 10, its = 41
i = 8375 (of 10192), d = 1.38291, its = 7
i = 8376 (of 10192), d = 10, its = 39
i = 8377 (of 10192), d = 10, its = 54
i = 8378 (of 10192), d = 2.28972, its = 4
i = 8379 (of 10192), d = 10, its = 55
i = 8380 (of 10192), d = 10, its = 37
i = 8381 (of 10192), d = 2.145, its = 4
i = 8382 (of 10192), d = 1.96778, its = 3
i = 8383 (of 10192), d = 1.83852, its = 5
i = 8384 (of 10192), d = 10, its = 52
i = 8385 (of 10192), d = 1.4059, its = 7
i = 8386 (of 10192), d = 10, its = 42
i = 8387 (of 10192), d = 10, its = 55
i = 8388 (of 10192), d = 10, its = 52
i = 8389 (of 10192), d = 10, its = 40
i = 8390 (of 10192), d = 2.29773, its = 4
i = 8391 (of 10192), d = 1.91285, its = 5
i = 8392 (of 10192), d = 10, its = 49
i = 8393 (of 10192), d = 10, its = 40
i = 8394 (of 10192), d = 2.06572, its = 4
i = 8395 (of 10192), d = 0.746794, its = 20
i = 8396 (of 10192), d = 10, its = 46
i = 8397 (of 10192), d = 10, its = 49
i = 8398 (of 10192), d = 10, its = 37
i = 8399 (of 10192), d = 10, its = 53
i = 8400 (of 10192), d = 10, its = 37
i = 8401 (of 10192), d = 2.39472, its = 5
i = 8402 (of 10192), d = 2.50164, its = 5
i = 8403 (of 10192), d = 10, its = 54
i = 8404 (of 10192), d = 1.54728, its = 6
i = 8405 (of 10192), d = 1.36716, its = 7
i = 8406 (of 10192), d = 10, its = 57
i = 8407 (of 10192), d = 10, its = 42
i = 8408 (of 10192), d = 10, its = 50
i = 8409 (of 10192), d = 10, its = 54
i = 8410 (of 10192), d = 0.663544, its = 18
i = 8411 (of 10192), d = 10, its = 52
i = 8412 (of 10192), d = 10, its = 51
i = 8413 (of 10192), d = 2.51646, its = 5
i = 8414 (of 10192), d = 1.81389, its = 5
i = 8415 (of 10192), d = 10, its = 50
i = 8416 (of 10192), d = 3.03264, its = 6
i = 8417 (of 10192), d = 10, its = 49
i = 8418 (of 10192), d = 10, its = 43
i = 8419 (of 10192), d = 0.602159, its = 17
i = 8420 (of 10192), d = 2.95134, its = 6
i = 8421 (of 10192), d = 10, its = 39
i = 8422 (of 10192), d = 2.39174, its = 5
i = 8423 (of 10192), d = 2.83761, its = 7
i = 8424 (of 10192), d = 10, its = 37
i = 8425 (of 10192), d = 1.69872, its = 5
i = 8426 (of 10192), d = 10, its = 52
i = 8427 (of 10192), d = 10, its = 56
i = 8428 (of 10192), d = 10, its = 37
i = 8429 (of 10192), d = 10, its = 54
i = 8430 (of 10192), d = 10, its = 37
i = 8431 (of 10192), d = 10, its = 48
i = 8432 (of 10192), d = 10, its = 53
i = 8433 (of 10192), d = 10, its = 52
i = 8434 (of 10192), d = 2.78257, its = 7
i = 8435 (of 10192), d = 10, its = 54
i = 8436 (of 10192), d = 10, its = 51
i = 8437 (of 10192), d = 10, its = 37
i = 8438 (of 10192), d = 10, its = 50
i = 8439 (of 10192), d = 2.61452, its = 6
i = 8440 (of 10192), d = 10, its = 52
i = 8441 (of 10192), d = 10, its = 51
i = 8442 (of 10192), d = 0.993048, its = 17
i = 8443 (of 10192), d = 10, its = 54
i = 8444 (of 10192), d = 10, its = 57
i = 8445 (of 10192), d = 10, its = 54
i = 8446 (of 10192), d = 10, its = 54
i = 8447 (of 10192), d = 10, its = 55
i = 8448 (of 10192), d = 10, its = 55
i = 8449 (of 10192), d = 10, its = 57
i = 8450 (of 10192), d = 10, its = 42
i = 8451 (of 10192), d = 10, its = 54
i = 8452 (of 10192), d = 10, its = 51
i = 8453 (of 10192), d = 1.71824, its = 5
i = 8454 (of 10192), d = 10, its = 37
i = 8455 (of 10192), d = 10, its = 14
i = 8456 (of 10192), d = 2.98054, its = 6
i = 8457 (of 10192), d = 10, its = 40
i = 8458 (of 10192), d = 0.835429, its = 16
i = 8459 (of 10192), d = 10, its = 37
i = 8460 (of 10192), d = 10, its = 57
i = 8461 (of 10192), d = 10, its = 53
i = 8462 (of 10192), d = 1.44543, its = 6
i = 8463 (of 10192), d = 1.18569, its = 17
i = 8464 (of 10192), d = 1.34661, its = 20
i = 8465 (of 10192), d = 10, its = 51
i = 8466 (of 10192), d = 10, its = 49
i = 8467 (of 10192), d = 10, its = 41
i = 8468 (of 10192), d = 10, its = 54
i = 8469 (of 10192), d = 10, its = 39
i = 8470 (of 10192), d = 10, its = 53
i = 8471 (of 10192), d = 10, its = 55
i = 8472 (of 10192), d = 10, its = 52
i = 8473 (of 10192), d = 2.37338, its = 5
i = 8474 (of 10192), d = 10, its = 39
i = 8475 (of 10192), d = 10, its = 52
i = 8476 (of 10192), d = 10, its = 55
i = 8477 (of 10192), d = 10, its = 38
i = 8478 (of 10192), d = 4.39313, its = 7
i = 8479 (of 10192), d = 10, its = 54
i = 8480 (of 10192), d = 0.961755, its = 18
i = 8481 (of 10192), d = 1.82384, its = 4
i = 8482 (of 10192), d = 10, its = 46
i = 8483 (of 10192), d = 1.08941, its = 19
i = 8484 (of 10192), d = 10, its = 53
i = 8485 (of 10192), d = 10, its = 40
i = 8486 (of 10192), d = 0.798261, its = 16
i = 8487 (of 10192), d = 2.32684, its = 5
i = 8488 (of 10192), d = 1.82746, its = 5
i = 8489 (of 10192), d = 10, its = 40
i = 8490 (of 10192), d = 5.28799, its = 7
i = 8491 (of 10192), d = 10, its = 37
i = 8492 (of 10192), d = 10, its = 55
i = 8493 (of 10192), d = 10, its = 40
i = 8494 (of 10192), d = 10, its = 41
i = 8495 (of 10192), d = 2.33596, its = 5
i = 8496 (of 10192), d = 10, its = 56
i = 8497 (of 10192), d = 1.83106, its = 4
i = 8498 (of 10192), d = 10, its = 55
i = 8499 (of 10192), d = 10, its = 37
i = 8500 (of 10192), d = 10, its = 51
i = 8501 (of 10192), d = 2.06242, its = 4
i = 8502 (of 10192), d = 1.44693, its = 8
i = 8503 (of 10192), d = 10, its = 51
i = 8504 (of 10192), d = 10, its = 53
i = 8505 (of 10192), d = 10, its = 56
i = 8506 (of 10192), d = 10, its = 54
i = 8507 (of 10192), d = 10, its = 53
i = 8508 (of 10192), d = 10, its = 55
i = 8509 (of 10192), d = 2.16038, its = 4
i = 8510 (of 10192), d = 4.02849, its = 9
i = 8511 (of 10192), d = 10, its = 42
i = 8512 (of 10192), d = 10, its = 37
i = 8513 (of 10192), d = 10, its = 41
i = 8514 (of 10192), d = 10, its = 51
i = 8515 (of 10192), d = 10, its = 56
i = 8516 (of 10192), d = 10, its = 51
i = 8517 (of 10192), d = 10, its = 40
i = 8518 (of 10192), d = 3.01555, its = 6
i = 8519 (of 10192), d = 10, its = 54
i = 8520 (of 10192), d = 10, its = 37
i = 8521 (of 10192), d = 10, its = 52
i = 8522 (of 10192), d = 10, its = 56
i = 8523 (of 10192), d = 4.14803, its = 26
i = 8524 (of 10192), d = 10, its = 52
i = 8525 (of 10192), d = 2.11044, its = 4
i = 8526 (of 10192), d = 10, its = 40
i = 8527 (of 10192), d = 10, its = 44
i = 8528 (of 10192), d = 10, its = 52
i = 8529 (of 10192), d = 3.09291, its = 6
i = 8530 (of 10192), d = 10, its = 50
i = 8531 (of 10192), d = 10, its = 51
i = 8532 (of 10192), d = 10, its = 55
i = 8533 (of 10192), d = 10, its = 55
i = 8534 (of 10192), d = 10, its = 46
i = 8535 (of 10192), d = 10, its = 56
i = 8536 (of 10192), d = 10, its = 48
i = 8537 (of 10192), d = 10, its = 37
i = 8538 (of 10192), d = 10, its = 49
i = 8539 (of 10192), d = 10, its = 52
i = 8540 (of 10192), d = 10, its = 57
i = 8541 (of 10192), d = 1.51066, its = 7
i = 8542 (of 10192), d = 10, its = 51
i = 8543 (of 10192), d = 10, its = 56
i = 8544 (of 10192), d = 3.62278, its = 7
i = 8545 (of 10192), d = 1.29664, its = 6
i = 8546 (of 10192), d = 5.8183, its = 16
i = 8547 (of 10192), d = 5.84028, its = 8
i = 8548 (of 10192), d = 10, its = 53
i = 8549 (of 10192), d = 10, its = 52
i = 8550 (of 10192), d = 10, its = 37
i = 8551 (of 10192), d = 10, its = 37
i = 8552 (of 10192), d = 10, its = 52
i = 8553 (of 10192), d = 10, its = 55
i = 8554 (of 10192), d = 10, its = 40
i = 8555 (of 10192), d = 10, its = 43
i = 8556 (of 10192), d = 1.24267, its = 17
i = 8557 (of 10192), d = 3.05052, its = 6
i = 8558 (of 10192), d = 10, its = 38
i = 8559 (of 10192), d = 10, its = 54
i = 8560 (of 10192), d = 10, its = 53
i = 8561 (of 10192), d = 10, its = 53
i = 8562 (of 10192), d = 10, its = 39
i = 8563 (of 10192), d = 10, its = 40
i = 8564 (of 10192), d = 10, its = 55
i = 8565 (of 10192), d = 7.93612, its = 9
i = 8566 (of 10192), d = 10, its = 50
i = 8567 (of 10192), d = 0.831509, its = 25
i = 8568 (of 10192), d = 10, its = 49
i = 8569 (of 10192), d = 10, its = 49
i = 8570 (of 10192), d = 2.59335, its = 5
i = 8571 (of 10192), d = 10, its = 53
i = 8572 (of 10192), d = 4.92058, its = 7
i = 8573 (of 10192), d = 10, its = 52
i = 8574 (of 10192), d = 2.61247, its = 5
i = 8575 (of 10192), d = 1.88124, its = 4
i = 8576 (of 10192), d = 10, its = 56
i = 8577 (of 10192), d = 10, its = 53
i = 8578 (of 10192), d = 10, its = 50
i = 8579 (of 10192), d = 10, its = 40
i = 8580 (of 10192), d = 10, its = 49
i = 8581 (of 10192), d = 10, its = 51
i = 8582 (of 10192), d = 2.22608, its = 4
i = 8583 (of 10192), d = 10, its = 52
i = 8584 (of 10192), d = 2.08069, its = 3
i = 8585 (of 10192), d = 10, its = 53
i = 8586 (of 10192), d = 10, its = 55
i = 8587 (of 10192), d = 1.82548, its = 4
i = 8588 (of 10192), d = 10, its = 40
i = 8589 (of 10192), d = 10, its = 55
i = 8590 (of 10192), d = 10, its = 53
i = 8591 (of 10192), d = 10, its = 51
i = 8592 (of 10192), d = 10, its = 54
i = 8593 (of 10192), d = 10, its = 55
i = 8594 (of 10192), d = 10, its = 52
i = 8595 (of 10192), d = 10, its = 54
i = 8596 (of 10192), d = 1.89305, its = 4
i = 8597 (of 10192), d = 10, its = 51
i = 8598 (of 10192), d = 10, its = 49
i = 8599 (of 10192), d = 6.89655, its = 8
i = 8600 (of 10192), d = 10, its = 41
i = 8601 (of 10192), d = 10, its = 52
i = 8602 (of 10192), d = 10, its = 53
i = 8603 (of 10192), d = 10, its = 46
i = 8604 (of 10192), d = 4.32549, its = 6
i = 8605 (of 10192), d = 3.82855, its = 8
i = 8606 (of 10192), d = 10, its = 40
i = 8607 (of 10192), d = 10, its = 51
i = 8608 (of 10192), d = 10, its = 37
i = 8609 (of 10192), d = 10, its = 37
i = 8610 (of 10192), d = 1.49054, its = 6
i = 8611 (of 10192), d = 3.30912, its = 7
i = 8612 (of 10192), d = 10, its = 40
i = 8613 (of 10192), d = 10, its = 49
i = 8614 (of 10192), d = 10, its = 37
i = 8615 (of 10192), d = 2.9339, its = 6
i = 8616 (of 10192), d = 4.99379, its = 8
i = 8617 (of 10192), d = 10, its = 49
i = 8618 (of 10192), d = 10, its = 49
i = 8619 (of 10192), d = 10, its = 37
i = 8620 (of 10192), d = 10, its = 56
i = 8621 (of 10192), d = 10, its = 37
i = 8622 (of 10192), d = 10, its = 52
i = 8623 (of 10192), d = 10, its = 55
i = 8624 (of 10192), d = 10, its = 53
i = 8625 (of 10192), d = 10, its = 50
i = 8626 (of 10192), d = 10, its = 42
i = 8627 (of 10192), d = 10, its = 54
i = 8628 (of 10192), d = 1.99231, its = 3
i = 8629 (of 10192), d = 2.82244, its = 6
i = 8630 (of 10192), d = 10, its = 49
i = 8631 (of 10192), d = 10, its = 37
i = 8632 (of 10192), d = 10, its = 53
i = 8633 (of 10192), d = 10, its = 52
i = 8634 (of 10192), d = 10, its = 52
i = 8635 (of 10192), d = 10, its = 51
i = 8636 (of 10192), d = 10, its = 57
i = 8637 (of 10192), d = 10, its = 37
i = 8638 (of 10192), d = 7.33781, its = 22
i = 8639 (of 10192), d = 1.39306, its = 6
i = 8640 (of 10192), d = 10, its = 51
i = 8641 (of 10192), d = 10, its = 53
i = 8642 (of 10192), d = 10, its = 37
i = 8643 (of 10192), d = 10, its = 50
i = 8644 (of 10192), d = 2.64426, its = 5
i = 8645 (of 10192), d = 10, its = 46
i = 8646 (of 10192), d = 10, its = 39
i = 8647 (of 10192), d = 10, its = 55
i = 8648 (of 10192), d = 10, its = 53
i = 8649 (of 10192), d = 10, its = 52
i = 8650 (of 10192), d = 10, its = 41
i = 8651 (of 10192), d = 1.20448, its = 19
i = 8652 (of 10192), d = 1.56777, its = 6
i = 8653 (of 10192), d = 10, its = 52
i = 8654 (of 10192), d = 2.0987, its = 4
i = 8655 (of 10192), d = 1.13418, its = 17
i = 8656 (of 10192), d = 10, its = 51
i = 8657 (of 10192), d = 10, its = 40
i = 8658 (of 10192), d = 10, its = 50
i = 8659 (of 10192), d = 10, its = 56
i = 8660 (of 10192), d = 10, its = 53
i = 8661 (of 10192), d = 10, its = 40
i = 8662 (of 10192), d = 10, its = 50
i = 8663 (of 10192), d = 10, its = 59
i = 8664 (of 10192), d = 1.79215, its = 7
i = 8665 (of 10192), d = 10, its = 37
i = 8666 (of 10192), d = 10, its = 55
i = 8667 (of 10192), d = 10, its = 51
i = 8668 (of 10192), d = 1.89161, its = 4
i = 8669 (of 10192), d = 10, its = 55
i = 8670 (of 10192), d = 2.80483, its = 6
i = 8671 (of 10192), d = 3.8249, its = 6
i = 8672 (of 10192), d = 10, its = 55
i = 8673 (of 10192), d = 10, its = 52
i = 8674 (of 10192), d = 1.30132, its = 7
i = 8675 (of 10192), d = 10, its = 39
i = 8676 (of 10192), d = 0.889794, its = 20
i = 8677 (of 10192), d = 10, its = 52
i = 8678 (of 10192), d = 1.36117, its = 6
i = 8679 (of 10192), d = 1.71744, its = 5
i = 8680 (of 10192), d = 2.8616, its = 6
i = 8681 (of 10192), d = 2.04572, its = 3
i = 8682 (of 10192), d = 10, its = 49
i = 8683 (of 10192), d = 10, its = 40
i = 8684 (of 10192), d = 10, its = 52
i = 8685 (of 10192), d = 1.71879, its = 5
i = 8686 (of 10192), d = 10, its = 37
i = 8687 (of 10192), d = 10, its = 54
i = 8688 (of 10192), d = 10, its = 51
i = 8689 (of 10192), d = 10, its = 41
i = 8690 (of 10192), d = 10, its = 49
i = 8691 (of 10192), d = 10, its = 55
i = 8692 (of 10192), d = 1.95744, its = 3
i = 8693 (of 10192), d = 1.77482, its = 5
i = 8694 (of 10192), d = 10, its = 37
i = 8695 (of 10192), d = 10, its = 37
i = 8696 (of 10192), d = 10, its = 49
i = 8697 (of 10192), d = 1.78017, its = 5
i = 8698 (of 10192), d = 1.44178, its = 21
i = 8699 (of 10192), d = 1.85934, its = 4
i = 8700 (of 10192), d = 10, its = 51
i = 8701 (of 10192), d = 10, its = 53
i = 8702 (of 10192), d = 2.51354, its = 6
i = 8703 (of 10192), d = 10, its = 52
i = 8704 (of 10192), d = 1.35038, its = 7
i = 8705 (of 10192), d = 10, its = 54
i = 8706 (of 10192), d = 1.87148, its = 4
i = 8707 (of 10192), d = 10, its = 37
i = 8708 (of 10192), d = 10, its = 40
i = 8709 (of 10192), d = 10, its = 54
i = 8710 (of 10192), d = 1.99414, its = 2
i = 8711 (of 10192), d = 3.66687, its = 8
i = 8712 (of 10192), d = 2.51279, its = 5
i = 8713 (of 10192), d = 10, its = 38
i = 8714 (of 10192), d = 0.967152, its = 19
i = 8715 (of 10192), d = 10, its = 50
i = 8716 (of 10192), d = 10, its = 51
i = 8717 (of 10192), d = 10, its = 56
i = 8718 (of 10192), d = 10, its = 49
i = 8719 (of 10192), d = 10, its = 54
i = 8720 (of 10192), d = 10, its = 52
i = 8721 (of 10192), d = 10, its = 53
i = 8722 (of 10192), d = 1.06844, its = 17
i = 8723 (of 10192), d = 10, its = 56
i = 8724 (of 10192), d = 10, its = 53
i = 8725 (of 10192), d = 5.34273, its = 7
i = 8726 (of 10192), d = 10, its = 51
i = 8727 (of 10192), d = 2.00457, its = 3
i = 8728 (of 10192), d = 2.80642, its = 6
i = 8729 (of 10192), d = 10, its = 37
i = 8730 (of 10192), d = 10, its = 52
i = 8731 (of 10192), d = 10, its = 37
i = 8732 (of 10192), d = 1.4095, its = 6
i = 8733 (of 10192), d = 1.54573, its = 6
i = 8734 (of 10192), d = 2.01446, its = 3
i = 8735 (of 10192), d = 10, its = 55
i = 8736 (of 10192), d = 10, its = 51
i = 8737 (of 10192), d = 10, its = 55
i = 8738 (of 10192), d = 2.23428, its = 4
i = 8739 (of 10192), d = 1.04313, its = 17
i = 8740 (of 10192), d = 2.86597, its = 6
i = 8741 (of 10192), d = 10, its = 49
i = 8742 (of 10192), d = 1.66216, its = 5
i = 8743 (of 10192), d = 10, its = 41
i = 8744 (of 10192), d = 1.57665, its = 6
i = 8745 (of 10192), d = 10, its = 52
i = 8746 (of 10192), d = 10, its = 54
i = 8747 (of 10192), d = 10, its = 41
i = 8748 (of 10192), d = 10, its = 52
i = 8749 (of 10192), d = 10, its = 55
i = 8750 (of 10192), d = 10, its = 37
i = 8751 (of 10192), d = 0.870073, its = 18
i = 8752 (of 10192), d = 0.933347, its = 18
i = 8753 (of 10192), d = 3.11173, its = 6
i = 8754 (of 10192), d = 10, its = 48
i = 8755 (of 10192), d = 0.638913, its = 18
i = 8756 (of 10192), d = 10, its = 52
i = 8757 (of 10192), d = 10, its = 56
i = 8758 (of 10192), d = 10, its = 51
i = 8759 (of 10192), d = 10, its = 53
i = 8760 (of 10192), d = 10, its = 38
i = 8761 (of 10192), d = 10, its = 51
i = 8762 (of 10192), d = 2.46258, its = 5
i = 8763 (of 10192), d = 3.34986, its = 7
i = 8764 (of 10192), d = 10, its = 53
i = 8765 (of 10192), d = 10, its = 56
i = 8766 (of 10192), d = 10, its = 51
i = 8767 (of 10192), d = 10, its = 43
i = 8768 (of 10192), d = 10, its = 54
i = 8769 (of 10192), d = 10, its = 44
i = 8770 (of 10192), d = 10, its = 52
i = 8771 (of 10192), d = 10, its = 39
i = 8772 (of 10192), d = 10, its = 52
i = 8773 (of 10192), d = 10, its = 39
i = 8774 (of 10192), d = 10, its = 53
i = 8775 (of 10192), d = 10, its = 54
i = 8776 (of 10192), d = 10, its = 40
i = 8777 (of 10192), d = 10, its = 54
i = 8778 (of 10192), d = 10, its = 50
i = 8779 (of 10192), d = 2.81668, its = 5
i = 8780 (of 10192), d = 10, its = 53
i = 8781 (of 10192), d = 10, its = 52
i = 8782 (of 10192), d = 10, its = 52
i = 8783 (of 10192), d = 10, its = 54
i = 8784 (of 10192), d = 1.94935, its = 4
i = 8785 (of 10192), d = 10, its = 52
i = 8786 (of 10192), d = 10, its = 53
i = 8787 (of 10192), d = 10, its = 56
i = 8788 (of 10192), d = 10, its = 42
i = 8789 (of 10192), d = 10, its = 40
i = 8790 (of 10192), d = 1.56411, its = 6
i = 8791 (of 10192), d = 10, its = 45
i = 8792 (of 10192), d = 4.70741, its = 8
i = 8793 (of 10192), d = 2.30648, its = 4
i = 8794 (of 10192), d = 2.70907, its = 5
i = 8795 (of 10192), d = 10, its = 41
i = 8796 (of 10192), d = 0.699799, its = 17
i = 8797 (of 10192), d = 10, its = 51
i = 8798 (of 10192), d = 2.19931, its = 4
i = 8799 (of 10192), d = 10, its = 42
i = 8800 (of 10192), d = 10, its = 54
i = 8801 (of 10192), d = 10, its = 46
i = 8802 (of 10192), d = 10, its = 49
i = 8803 (of 10192), d = 1.92037, its = 4
i = 8804 (of 10192), d = 2.45228, its = 5
i = 8805 (of 10192), d = 10, its = 50
i = 8806 (of 10192), d = 10, its = 52
i = 8807 (of 10192), d = 10, its = 54
i = 8808 (of 10192), d = 10, its = 56
i = 8809 (of 10192), d = 10, its = 39
i = 8810 (of 10192), d = 1.4752, its = 7
i = 8811 (of 10192), d = 1.68177, its = 5
i = 8812 (of 10192), d = 10, its = 40
i = 8813 (of 10192), d = 10, its = 52
i = 8814 (of 10192), d = 10, its = 57
i = 8815 (of 10192), d = 3.6093, its = 6
i = 8816 (of 10192), d = 1.38199, its = 7
i = 8817 (of 10192), d = 10, its = 49
i = 8818 (of 10192), d = 10, its = 37
i = 8819 (of 10192), d = 2.46024, its = 5
i = 8820 (of 10192), d = 10, its = 37
i = 8821 (of 10192), d = 10, its = 53
i = 8822 (of 10192), d = 10, its = 53
i = 8823 (of 10192), d = 0.965952, its = 18
i = 8824 (of 10192), d = 3.67509, its = 9
i = 8825 (of 10192), d = 0.969886, its = 19
i = 8826 (of 10192), d = 4.0382, its = 6
i = 8827 (of 10192), d = 10, its = 54
i = 8828 (of 10192), d = 10, its = 37
i = 8829 (of 10192), d = 10, its = 58
i = 8830 (of 10192), d = 10, its = 40
i = 8831 (of 10192), d = 10, its = 53
i = 8832 (of 10192), d = 10, its = 39
i = 8833 (of 10192), d = 10, its = 54
i = 8834 (of 10192), d = 1.25343, its = 6
i = 8835 (of 10192), d = 2.28353, its = 5
i = 8836 (of 10192), d = 10, its = 54
i = 8837 (of 10192), d = 2.35989, its = 4
i = 8838 (of 10192), d = 10, its = 37
i = 8839 (of 10192), d = 10, its = 56
i = 8840 (of 10192), d = 10, its = 52
i = 8841 (of 10192), d = 0.922834, its = 21
i = 8842 (of 10192), d = 2.10868, its = 4
i = 8843 (of 10192), d = 10, its = 55
i = 8844 (of 10192), d = 10, its = 37
i = 8845 (of 10192), d = 10, its = 40
i = 8846 (of 10192), d = 10, its = 37
i = 8847 (of 10192), d = 10, its = 53
i = 8848 (of 10192), d = 1.5604, its = 6
i = 8849 (of 10192), d = 10, its = 51
i = 8850 (of 10192), d = 10, its = 51
i = 8851 (of 10192), d = 10, its = 49
i = 8852 (of 10192), d = 10, its = 54
i = 8853 (of 10192), d = 10, its = 37
i = 8854 (of 10192), d = 10, its = 44
i = 8855 (of 10192), d = 2.49086, its = 5
i = 8856 (of 10192), d = 1.89905, its = 4
i = 8857 (of 10192), d = 10, its = 51
i = 8858 (of 10192), d = 10, its = 52
i = 8859 (of 10192), d = 10, its = 53
i = 8860 (of 10192), d = 10, its = 49
i = 8861 (of 10192), d = 10, its = 37
i = 8862 (of 10192), d = 6.86352, its = 9
i = 8863 (of 10192), d = 0.90211, its = 21
i = 8864 (of 10192), d = 1.75456, its = 5
i = 8865 (of 10192), d = 1.10551, its = 19
i = 8866 (of 10192), d = 0.825278, its = 17
i = 8867 (of 10192), d = 0.83599, its = 20
i = 8868 (of 10192), d = 10, its = 49
i = 8869 (of 10192), d = 1.78728, its = 4
i = 8870 (of 10192), d = 10, its = 41
i = 8871 (of 10192), d = 1.56441, its = 6
i = 8872 (of 10192), d = 1.77088, its = 8
i = 8873 (of 10192), d = 0.652352, its = 18
i = 8874 (of 10192), d = 10, its = 49
i = 8875 (of 10192), d = 10, its = 52
i = 8876 (of 10192), d = 10, its = 38
i = 8877 (of 10192), d = 10, its = 39
i = 8878 (of 10192), d = 10, its = 21
i = 8879 (of 10192), d = 10, its = 48
i = 8880 (of 10192), d = 10, its = 41
i = 8881 (of 10192), d = 0.863035, its = 16
i = 8882 (of 10192), d = 10, its = 52
i = 8883 (of 10192), d = 3.0343, its = 6
i = 8884 (of 10192), d = 10, its = 55
i = 8885 (of 10192), d = 10, its = 59
i = 8886 (of 10192), d = 1.76247, its = 5
i = 8887 (of 10192), d = 10, its = 43
i = 8888 (of 10192), d = 10, its = 48
i = 8889 (of 10192), d = 10, its = 45
i = 8890 (of 10192), d = 2.26845, its = 5
i = 8891 (of 10192), d = 1.6071, its = 6
i = 8892 (of 10192), d = 1.28997, its = 7
i = 8893 (of 10192), d = 3.17339, its = 6
i = 8894 (of 10192), d = 2.95282, its = 5
i = 8895 (of 10192), d = 10, its = 44
i = 8896 (of 10192), d = 2.42725, its = 5
i = 8897 (of 10192), d = 10, its = 40
i = 8898 (of 10192), d = 1.78382, its = 5
i = 8899 (of 10192), d = 1.34321, its = 6
i = 8900 (of 10192), d = 10, its = 37
i = 8901 (of 10192), d = 10, its = 53
i = 8902 (of 10192), d = 10, its = 52
i = 8903 (of 10192), d = 1.09063, its = 18
i = 8904 (of 10192), d = 10, its = 53
i = 8905 (of 10192), d = 10, its = 54
i = 8906 (of 10192), d = 2.62531, its = 6
i = 8907 (of 10192), d = 10, its = 41
i = 8908 (of 10192), d = 10, its = 41
i = 8909 (of 10192), d = 10, its = 49
i = 8910 (of 10192), d = 1.8665, its = 4
i = 8911 (of 10192), d = 10, its = 55
i = 8912 (of 10192), d = 2.05709, its = 3
i = 8913 (of 10192), d = 3.02596, its = 6
i = 8914 (of 10192), d = 1.40751, its = 6
i = 8915 (of 10192), d = 10, its = 40
i = 8916 (of 10192), d = 1.81213, its = 5
i = 8917 (of 10192), d = 10, its = 54
i = 8918 (of 10192), d = 3.27843, its = 6
i = 8919 (of 10192), d = 1.36504, its = 7
i = 8920 (of 10192), d = 10, its = 40
i = 8921 (of 10192), d = 1.63843, its = 6
i = 8922 (of 10192), d = 10, its = 54
i = 8923 (of 10192), d = 10, its = 37
i = 8924 (of 10192), d = 10, its = 55
i = 8925 (of 10192), d = 10, its = 40
i = 8926 (of 10192), d = 1.58599, its = 6
i = 8927 (of 10192), d = 10, its = 54
i = 8928 (of 10192), d = 3.7126, its = 7
i = 8929 (of 10192), d = 2.04835, its = 3
i = 8930 (of 10192), d = 10, its = 50
i = 8931 (of 10192), d = 10, its = 37
i = 8932 (of 10192), d = 0.890692, its = 19
i = 8933 (of 10192), d = 1.60845, its = 6
i = 8934 (of 10192), d = 10, its = 52
i = 8935 (of 10192), d = 10, its = 54
i = 8936 (of 10192), d = 0.919609, its = 19
i = 8937 (of 10192), d = 10, its = 46
i = 8938 (of 10192), d = 1.54635, its = 7
i = 8939 (of 10192), d = 10, its = 54
i = 8940 (of 10192), d = 10, its = 53
i = 8941 (of 10192), d = 10, its = 54
i = 8942 (of 10192), d = 10, its = 37
i = 8943 (of 10192), d = 10, its = 37
i = 8944 (of 10192), d = 1.92824, its = 4
i = 8945 (of 10192), d = 10, its = 37
i = 8946 (of 10192), d = 10, its = 54
i = 8947 (of 10192), d = 10, its = 41
i = 8948 (of 10192), d = 10, its = 50
i = 8949 (of 10192), d = 0.859105, its = 20
i = 8950 (of 10192), d = 10, its = 54
i = 8951 (of 10192), d = 10, its = 51
i = 8952 (of 10192), d = 1.9235, its = 4
i = 8953 (of 10192), d = 10, its = 37
i = 8954 (of 10192), d = 1.66231, its = 6
i = 8955 (of 10192), d = 10, its = 53
i = 8956 (of 10192), d = 3.01876, its = 6
i = 8957 (of 10192), d = 10, its = 52
i = 8958 (of 10192), d = 10, its = 41
i = 8959 (of 10192), d = 10, its = 48
i = 8960 (of 10192), d = 10, its = 50
i = 8961 (of 10192), d = 10, its = 52
i = 8962 (of 10192), d = 10, its = 55
i = 8963 (of 10192), d = 2.80224, its = 7
i = 8964 (of 10192), d = 1.54897, its = 6
i = 8965 (of 10192), d = 10, its = 51
i = 8966 (of 10192), d = 10, its = 52
i = 8967 (of 10192), d = 10, its = 40
i = 8968 (of 10192), d = 10, its = 43
i = 8969 (of 10192), d = 10, its = 37
i = 8970 (of 10192), d = 10, its = 55
i = 8971 (of 10192), d = 10, its = 53
i = 8972 (of 10192), d = 1.75196, its = 5
i = 8973 (of 10192), d = 1.91995, its = 4
i = 8974 (of 10192), d = 10, its = 52
i = 8975 (of 10192), d = 10, its = 55
i = 8976 (of 10192), d = 10, its = 37
i = 8977 (of 10192), d = 10, its = 54
i = 8978 (of 10192), d = 10, its = 43
i = 8979 (of 10192), d = 0.907386, its = 18
i = 8980 (of 10192), d = 10, its = 40
i = 8981 (of 10192), d = 2.30058, its = 4
i = 8982 (of 10192), d = 10, its = 37
i = 8983 (of 10192), d = 4.45685, its = 7
i = 8984 (of 10192), d = 10, its = 56
i = 8985 (of 10192), d = 2.233, its = 4
i = 8986 (of 10192), d = 1.76993, its = 5
i = 8987 (of 10192), d = 2.07714, its = 4
i = 8988 (of 10192), d = 10, its = 53
i = 8989 (of 10192), d = 10, its = 39
i = 8990 (of 10192), d = 10, its = 53
i = 8991 (of 10192), d = 10, its = 37
i = 8992 (of 10192), d = 10, its = 37
i = 8993 (of 10192), d = 10, its = 56
i = 8994 (of 10192), d = 10, its = 50
i = 8995 (of 10192), d = 10, its = 37
i = 8996 (of 10192), d = 10, its = 55
i = 8997 (of 10192), d = 10, its = 55
i = 8998 (of 10192), d = 1.596, its = 8
i = 8999 (of 10192), d = 10, its = 54
i = 9000 (of 10192), d = 10, its = 53
i = 9001 (of 10192), d = 10, its = 37
i = 9002 (of 10192), d = 10, its = 40
i = 9003 (of 10192), d = 10, its = 56
i = 9004 (of 10192), d = 1.14139, its = 21
i = 9005 (of 10192), d = 2.09262, its = 4
i = 9006 (of 10192), d = 10, its = 51
i = 9007 (of 10192), d = 10, its = 37
i = 9008 (of 10192), d = 2.39475, its = 5
i = 9009 (of 10192), d = 10, its = 40
i = 9010 (of 10192), d = 1.0726, its = 18
i = 9011 (of 10192), d = 1.98772, its = 3
i = 9012 (of 10192), d = 0.79999, its = 21
i = 9013 (of 10192), d = 10, its = 57
i = 9014 (of 10192), d = 2.33492, its = 4
i = 9015 (of 10192), d = 1.89867, its = 4
i = 9016 (of 10192), d = 10, its = 55
i = 9017 (of 10192), d = 10, its = 52
i = 9018 (of 10192), d = 10, its = 48
i = 9019 (of 10192), d = 2.61121, its = 6
i = 9020 (of 10192), d = 10, its = 54
i = 9021 (of 10192), d = 10, its = 49
i = 9022 (of 10192), d = 10, its = 40
i = 9023 (of 10192), d = 10, its = 52
i = 9024 (of 10192), d = 10, its = 56
i = 9025 (of 10192), d = 10, its = 40
i = 9026 (of 10192), d = 10, its = 39
i = 9027 (of 10192), d = 10, its = 55
i = 9028 (of 10192), d = 10, its = 37
i = 9029 (of 10192), d = 1.95021, its = 3
i = 9030 (of 10192), d = 10, its = 53
i = 9031 (of 10192), d = 10, its = 52
i = 9032 (of 10192), d = 10, its = 51
i = 9033 (of 10192), d = 10, its = 43
i = 9034 (of 10192), d = 10, its = 50
i = 9035 (of 10192), d = 2.72905, its = 5
i = 9036 (of 10192), d = 10, its = 40
i = 9037 (of 10192), d = 1.84117, its = 4
i = 9038 (of 10192), d = 10, its = 51
i = 9039 (of 10192), d = 10, its = 37
i = 9040 (of 10192), d = 10, its = 49
i = 9041 (of 10192), d = 10, its = 37
i = 9042 (of 10192), d = 10, its = 50
i = 9043 (of 10192), d = 10, its = 57
i = 9044 (of 10192), d = 10, its = 49
i = 9045 (of 10192), d = 10, its = 37
i = 9046 (of 10192), d = 10, its = 60
i = 9047 (of 10192), d = 10, its = 37
i = 9048 (of 10192), d = 10, its = 55
i = 9049 (of 10192), d = 2.34715, its = 5
i = 9050 (of 10192), d = 10, its = 53
i = 9051 (of 10192), d = 10, its = 54
i = 9052 (of 10192), d = 10, its = 39
i = 9053 (of 10192), d = 10, its = 52
i = 9054 (of 10192), d = 1.15862, its = 24
i = 9055 (of 10192), d = 10, its = 37
i = 9056 (of 10192), d = 10, its = 54
i = 9057 (of 10192), d = 10, its = 54
i = 9058 (of 10192), d = 3.24426, its = 6
i = 9059 (of 10192), d = 10, its = 37
i = 9060 (of 10192), d = 10, its = 39
i = 9061 (of 10192), d = 10, its = 57
i = 9062 (of 10192), d = 10, its = 55
i = 9063 (of 10192), d = 2.20709, its = 4
i = 9064 (of 10192), d = 2.08126, its = 4
i = 9065 (of 10192), d = 1.75848, its = 5
i = 9066 (of 10192), d = 10, its = 52
i = 9067 (of 10192), d = 1.46895, its = 7
i = 9068 (of 10192), d = 10, its = 50
i = 9069 (of 10192), d = 10, its = 37
i = 9070 (of 10192), d = 10, its = 54
i = 9071 (of 10192), d = 7.24239, its = 9
i = 9072 (of 10192), d = 10, its = 41
i = 9073 (of 10192), d = 10, its = 49
i = 9074 (of 10192), d = 10, its = 41
i = 9075 (of 10192), d = 10, its = 37
i = 9076 (of 10192), d = 10, its = 51
i = 9077 (of 10192), d = 2.51646, its = 5
i = 9078 (of 10192), d = 1.64822, its = 6
i = 9079 (of 10192), d = 10, its = 49
i = 9080 (of 10192), d = 10, its = 52
i = 9081 (of 10192), d = 10, its = 54
i = 9082 (of 10192), d = 0.49783, its = 17
i = 9083 (of 10192), d = 1.42263, its = 8
i = 9084 (of 10192), d = 10, its = 51
i = 9085 (of 10192), d = 10, its = 57
i = 9086 (of 10192), d = 10, its = 38
i = 9087 (of 10192), d = 10, its = 53
i = 9088 (of 10192), d = 1.37093, its = 8
i = 9089 (of 10192), d = 10, its = 40
i = 9090 (of 10192), d = 10, its = 37
i = 9091 (of 10192), d = 10, its = 54
i = 9092 (of 10192), d = 10, its = 56
i = 9093 (of 10192), d = 3.63754, its = 7
i = 9094 (of 10192), d = 10, its = 37
i = 9095 (of 10192), d = 1.71501, its = 6
i = 9096 (of 10192), d = 10, its = 40
i = 9097 (of 10192), d = 10, its = 37
i = 9098 (of 10192), d = 10, its = 49
i = 9099 (of 10192), d = 10, its = 54
i = 9100 (of 10192), d = 10, its = 56
i = 9101 (of 10192), d = 10, its = 59
i = 9102 (of 10192), d = 10, its = 50
i = 9103 (of 10192), d = 10, its = 50
i = 9104 (of 10192), d = 10, its = 55
i = 9105 (of 10192), d = 4.64772, its = 8
i = 9106 (of 10192), d = 1.64369, its = 7
i = 9107 (of 10192), d = 10, its = 48
i = 9108 (of 10192), d = 10, its = 37
i = 9109 (of 10192), d = 10, its = 51
i = 9110 (of 10192), d = 10, its = 51
i = 9111 (of 10192), d = 2.33806, its = 5
i = 9112 (of 10192), d = 10, its = 53
i = 9113 (of 10192), d = 10, its = 37
i = 9114 (of 10192), d = 10, its = 56
i = 9115 (of 10192), d = 10, its = 37
i = 9116 (of 10192), d = 3.3761, its = 6
i = 9117 (of 10192), d = 10, its = 43
i = 9118 (of 10192), d = 10, its = 50
i = 9119 (of 10192), d = 2.61405, its = 5
i = 9120 (of 10192), d = 10, its = 53
i = 9121 (of 10192), d = 10, its = 55
i = 9122 (of 10192), d = 10, its = 52
i = 9123 (of 10192), d = 10, its = 37
i = 9124 (of 10192), d = 10, its = 55
i = 9125 (of 10192), d = 7.47662, its = 9
i = 9126 (of 10192), d = 2.11395, its = 4
i = 9127 (of 10192), d = 10, its = 37
i = 9128 (of 10192), d = 10, its = 37
i = 9129 (of 10192), d = 10, its = 55
i = 9130 (of 10192), d = 10, its = 52
i = 9131 (of 10192), d = 10, its = 54
i = 9132 (of 10192), d = 2.7966, its = 6
i = 9133 (of 10192), d = 10, its = 19
i = 9134 (of 10192), d = 10, its = 56
i = 9135 (of 10192), d = 10, its = 55
i = 9136 (of 10192), d = 2.45339, its = 5
i = 9137 (of 10192), d = 0.881228, its = 19
i = 9138 (of 10192), d = 1.51929, its = 6
i = 9139 (of 10192), d = 10, its = 51
i = 9140 (of 10192), d = 10, its = 54
i = 9141 (of 10192), d = 2.76556, its = 6
i = 9142 (of 10192), d = 10, its = 55
i = 9143 (of 10192), d = 10, its = 50
i = 9144 (of 10192), d = 10, its = 41
i = 9145 (of 10192), d = 10, its = 51
i = 9146 (of 10192), d = 1.73365, its = 6
i = 9147 (of 10192), d = 10, its = 52
i = 9148 (of 10192), d = 10, its = 37
i = 9149 (of 10192), d = 10, its = 43
i = 9150 (of 10192), d = 10, its = 52
i = 9151 (of 10192), d = 10, its = 53
i = 9152 (of 10192), d = 10, its = 57
i = 9153 (of 10192), d = 10, its = 49
i = 9154 (of 10192), d = 1.11306, its = 6
i = 9155 (of 10192), d = 5.4723, its = 7
i = 9156 (of 10192), d = 10, its = 49
i = 9157 (of 10192), d = 10, its = 39
i = 9158 (of 10192), d = 10, its = 41
i = 9159 (of 10192), d = 10, its = 43
i = 9160 (of 10192), d = 10, its = 40
i = 9161 (of 10192), d = 10, its = 50
i = 9162 (of 10192), d = 10, its = 37
i = 9163 (of 10192), d = 10, its = 52
i = 9164 (of 10192), d = 10, its = 51
i = 9165 (of 10192), d = 1.09553, its = 21
i = 9166 (of 10192), d = 10, its = 37
i = 9167 (of 10192), d = 10, its = 40
i = 9168 (of 10192), d = 10, its = 56
i = 9169 (of 10192), d = 2.26835, its = 5
i = 9170 (of 10192), d = 3.2163, its = 6
i = 9171 (of 10192), d = 10, its = 48
i = 9172 (of 10192), d = 10, its = 55
i = 9173 (of 10192), d = 2.9356, its = 6
i = 9174 (of 10192), d = 1.82806, its = 4
i = 9175 (of 10192), d = 10, its = 49
i = 9176 (of 10192), d = 10, its = 54
i = 9177 (of 10192), d = 0.963228, its = 24
i = 9178 (of 10192), d = 4.36982, its = 7
i = 9179 (of 10192), d = 2.90366, its = 6
i = 9180 (of 10192), d = 10, its = 37
i = 9181 (of 10192), d = 6.51626, its = 7
i = 9182 (of 10192), d = 10, its = 55
i = 9183 (of 10192), d = 1.48802, its = 7
i = 9184 (of 10192), d = 10, its = 56
i = 9185 (of 10192), d = 10, its = 54
i = 9186 (of 10192), d = 3.40419, its = 6
i = 9187 (of 10192), d = 10, its = 53
i = 9188 (of 10192), d = 2.19937, its = 4
i = 9189 (of 10192), d = 10, its = 41
i = 9190 (of 10192), d = 1.79242, its = 5
i = 9191 (of 10192), d = 10, its = 37
i = 9192 (of 10192), d = 10, its = 37
i = 9193 (of 10192), d = 10, its = 57
i = 9194 (of 10192), d = 10, its = 59
i = 9195 (of 10192), d = 10, its = 51
i = 9196 (of 10192), d = 2.48192, its = 5
i = 9197 (of 10192), d = 10, its = 50
i = 9198 (of 10192), d = 0.95768, its = 24
i = 9199 (of 10192), d = 3.48306, its = 7
i = 9200 (of 10192), d = 1.07939, its = 17
i = 9201 (of 10192), d = 10, its = 52
i = 9202 (of 10192), d = 10, its = 49
i = 9203 (of 10192), d = 2.68676, its = 7
i = 9204 (of 10192), d = 10, its = 50
i = 9205 (of 10192), d = 10, its = 39
i = 9206 (of 10192), d = 10, its = 55
i = 9207 (of 10192), d = 0.741857, its = 16
i = 9208 (of 10192), d = 10, its = 56
i = 9209 (of 10192), d = 10, its = 49
i = 9210 (of 10192), d = 10, its = 37
i = 9211 (of 10192), d = 10, its = 39
i = 9212 (of 10192), d = 10, its = 37
i = 9213 (of 10192), d = 10, its = 51
i = 9214 (of 10192), d = 10, its = 53
i = 9215 (of 10192), d = 10, its = 54
i = 9216 (of 10192), d = 10, its = 54
i = 9217 (of 10192), d = 1.96796, its = 3
i = 9218 (of 10192), d = 10, its = 54
i = 9219 (of 10192), d = 1.36409, its = 7
i = 9220 (of 10192), d = 10, its = 52
i = 9221 (of 10192), d = 10, its = 53
i = 9222 (of 10192), d = 2.14922, its = 4
i = 9223 (of 10192), d = 5.08244, its = 7
i = 9224 (of 10192), d = 2.00975, its = 3
i = 9225 (of 10192), d = 10, its = 55
i = 9226 (of 10192), d = 10, its = 55
i = 9227 (of 10192), d = 10, its = 37
i = 9228 (of 10192), d = 10, its = 53
i = 9229 (of 10192), d = 10, its = 51
i = 9230 (of 10192), d = 10, its = 51
i = 9231 (of 10192), d = 10, its = 55
i = 9232 (of 10192), d = 10, its = 37
i = 9233 (of 10192), d = 10, its = 53
i = 9234 (of 10192), d = 1.24798, its = 24
i = 9235 (of 10192), d = 10, its = 51
i = 9236 (of 10192), d = 10, its = 39
i = 9237 (of 10192), d = 1.15367, its = 22
i = 9238 (of 10192), d = 10, its = 56
i = 9239 (of 10192), d = 10, its = 37
i = 9240 (of 10192), d = 10, its = 39
i = 9241 (of 10192), d = 10, its = 52
i = 9242 (of 10192), d = 10, its = 37
i = 9243 (of 10192), d = 10, its = 37
i = 9244 (of 10192), d = 10, its = 49
i = 9245 (of 10192), d = 10, its = 48
i = 9246 (of 10192), d = 10, its = 55
i = 9247 (of 10192), d = 10, its = 52
i = 9248 (of 10192), d = 10, its = 51
i = 9249 (of 10192), d = 10, its = 54
i = 9250 (of 10192), d = 10, its = 56
i = 9251 (of 10192), d = 2.90667, its = 6
i = 9252 (of 10192), d = 10, its = 37
i = 9253 (of 10192), d = 10, its = 54
i = 9254 (of 10192), d = 1.43953, its = 8
i = 9255 (of 10192), d = 3.28385, its = 7
i = 9256 (of 10192), d = 10, its = 41
i = 9257 (of 10192), d = 10, its = 56
i = 9258 (of 10192), d = 10, its = 54
i = 9259 (of 10192), d = 10, its = 51
i = 9260 (of 10192), d = 2.25424, its = 4
i = 9261 (of 10192), d = 1.58471, its = 7
i = 9262 (of 10192), d = 10, its = 55
i = 9263 (of 10192), d = 10, its = 37
i = 9264 (of 10192), d = 10, its = 41
i = 9265 (of 10192), d = 10, its = 41
i = 9266 (of 10192), d = 10, its = 40
i = 9267 (of 10192), d = 1.73812, its = 5
i = 9268 (of 10192), d = 2.61729, its = 6
i = 9269 (of 10192), d = 10, its = 51
i = 9270 (of 10192), d = 10, its = 51
i = 9271 (of 10192), d = 10, its = 39
i = 9272 (of 10192), d = 4.69036, its = 7
i = 9273 (of 10192), d = 1.01086, its = 15
i = 9274 (of 10192), d = 10, its = 57
i = 9275 (of 10192), d = 10, its = 37
i = 9276 (of 10192), d = 10, its = 52
i = 9277 (of 10192), d = 10, its = 55
i = 9278 (of 10192), d = 10, its = 56
i = 9279 (of 10192), d = 10, its = 51
i = 9280 (of 10192), d = 2.90952, its = 5
i = 9281 (of 10192), d = 0.746668, its = 27
i = 9282 (of 10192), d = 10, its = 52
i = 9283 (of 10192), d = 10, its = 51
i = 9284 (of 10192), d = 10, its = 53
i = 9285 (of 10192), d = 2.61663, its = 5
i = 9286 (of 10192), d = 10, its = 56
i = 9287 (of 10192), d = 10, its = 41
i = 9288 (of 10192), d = 10, its = 54
i = 9289 (of 10192), d = 10, its = 51
i = 9290 (of 10192), d = 10, its = 37
i = 9291 (of 10192), d = 10, its = 40
i = 9292 (of 10192), d = 10, its = 53
i = 9293 (of 10192), d = 10, its = 52
i = 9294 (of 10192), d = 10, its = 52
i = 9295 (of 10192), d = 10, its = 53
i = 9296 (of 10192), d = 10, its = 50
i = 9297 (of 10192), d = 10, its = 49
i = 9298 (of 10192), d = 1.19721, its = 20
i = 9299 (of 10192), d = 1.82572, its = 5
i = 9300 (of 10192), d = 10, its = 56
i = 9301 (of 10192), d = 10, its = 54
i = 9302 (of 10192), d = 10, its = 37
i = 9303 (of 10192), d = 2.01319, its = 3
i = 9304 (of 10192), d = 10, its = 47
i = 9305 (of 10192), d = 1.89832, its = 4
i = 9306 (of 10192), d = 10, its = 50
i = 9307 (of 10192), d = 10, its = 39
i = 9308 (of 10192), d = 10, its = 46
i = 9309 (of 10192), d = 10, its = 50
i = 9310 (of 10192), d = 2.5392, its = 5
i = 9311 (of 10192), d = 10, its = 58
i = 9312 (of 10192), d = 10, its = 37
i = 9313 (of 10192), d = 10, its = 51
i = 9314 (of 10192), d = 1.1668, its = 20
i = 9315 (of 10192), d = 10, its = 55
i = 9316 (of 10192), d = 10, its = 53
i = 9317 (of 10192), d = 10, its = 37
i = 9318 (of 10192), d = 10, its = 40
i = 9319 (of 10192), d = 10, its = 48
i = 9320 (of 10192), d = 1.43972, its = 6
i = 9321 (of 10192), d = 10, its = 42
i = 9322 (of 10192), d = 1.40533, its = 6
i = 9323 (of 10192), d = 10, its = 56
i = 9324 (of 10192), d = 10, its = 54
i = 9325 (of 10192), d = 2.00528, its = 3
i = 9326 (of 10192), d = 10, its = 51
i = 9327 (of 10192), d = 10, its = 49
i = 9328 (of 10192), d = 10, its = 56
i = 9329 (of 10192), d = 10, its = 38
i = 9330 (of 10192), d = 2.01926, its = 3
i = 9331 (of 10192), d = 10, its = 40
i = 9332 (of 10192), d = 10, its = 54
i = 9333 (of 10192), d = 10, its = 43
i = 9334 (of 10192), d = 10, its = 40
i = 9335 (of 10192), d = 10, its = 37
i = 9336 (of 10192), d = 2.79187, its = 5
i = 9337 (of 10192), d = 10, its = 52
i = 9338 (of 10192), d = 2.04694, its = 3
i = 9339 (of 10192), d = 10, its = 53
i = 9340 (of 10192), d = 10, its = 51
i = 9341 (of 10192), d = 10, its = 53
i = 9342 (of 10192), d = 10, its = 37
i = 9343 (of 10192), d = 10, its = 48
i = 9344 (of 10192), d = 10, its = 55
i = 9345 (of 10192), d = 10, its = 42
i = 9346 (of 10192), d = 10, its = 37
i = 9347 (of 10192), d = 10, its = 37
i = 9348 (of 10192), d = 10, its = 41
i = 9349 (of 10192), d = 10, its = 38
i = 9350 (of 10192), d = 10, its = 37
i = 9351 (of 10192), d = 10, its = 51
i = 9352 (of 10192), d = 10, its = 53
i = 9353 (of 10192), d = 10, its = 52
i = 9354 (of 10192), d = 10, its = 49
i = 9355 (of 10192), d = 10, its = 52
i = 9356 (of 10192), d = 1.55698, its = 6
i = 9357 (of 10192), d = 10, its = 37
i = 9358 (of 10192), d = 10, its = 40
i = 9359 (of 10192), d = 10, its = 52
i = 9360 (of 10192), d = 10, its = 37
i = 9361 (of 10192), d = 10, its = 52
i = 9362 (of 10192), d = 10, its = 37
i = 9363 (of 10192), d = 10, its = 40
i = 9364 (of 10192), d = 1.84516, its = 5
i = 9365 (of 10192), d = 10, its = 54
i = 9366 (of 10192), d = 10, its = 57
i = 9367 (of 10192), d = 10, its = 52
i = 9368 (of 10192), d = 1.68606, its = 5
i = 9369 (of 10192), d = 10, its = 37
i = 9370 (of 10192), d = 10, its = 50
i = 9371 (of 10192), d = 1.40769, its = 7
i = 9372 (of 10192), d = 10, its = 50
i = 9373 (of 10192), d = 10, its = 52
i = 9374 (of 10192), d = 2.33542, its = 5
i = 9375 (of 10192), d = 0.906856, its = 23
i = 9376 (of 10192), d = 10, its = 48
i = 9377 (of 10192), d = 1.416, its = 6
i = 9378 (of 10192), d = 10, its = 42
i = 9379 (of 10192), d = 10, its = 49
i = 9380 (of 10192), d = 1.20155, its = 6
i = 9381 (of 10192), d = 10, its = 54
i = 9382 (of 10192), d = 10, its = 37
i = 9383 (of 10192), d = 10, its = 52
i = 9384 (of 10192), d = 10, its = 37
i = 9385 (of 10192), d = 10, its = 54
i = 9386 (of 10192), d = 1.96538, its = 4
i = 9387 (of 10192), d = 10, its = 53
i = 9388 (of 10192), d = 10, its = 54
i = 9389 (of 10192), d = 10, its = 58
i = 9390 (of 10192), d = 10, its = 37
i = 9391 (of 10192), d = 10, its = 54
i = 9392 (of 10192), d = 10, its = 53
i = 9393 (of 10192), d = 1.28965, its = 17
i = 9394 (of 10192), d = 10, its = 37
i = 9395 (of 10192), d = 0.876292, its = 16
i = 9396 (of 10192), d = 10, its = 41
i = 9397 (of 10192), d = 10, its = 52
i = 9398 (of 10192), d = 10, its = 52
i = 9399 (of 10192), d = 2.17209, its = 4
i = 9400 (of 10192), d = 10, its = 53
i = 9401 (of 10192), d = 10, its = 52
i = 9402 (of 10192), d = 10, its = 37
i = 9403 (of 10192), d = 10, its = 55
i = 9404 (of 10192), d = 10, its = 41
i = 9405 (of 10192), d = 1.86802, its = 4
i = 9406 (of 10192), d = 10, its = 57
i = 9407 (of 10192), d = 10, its = 51
i = 9408 (of 10192), d = 10, its = 49
i = 9409 (of 10192), d = 0.826506, its = 25
i = 9410 (of 10192), d = 10, its = 52
i = 9411 (of 10192), d = 2.16709, its = 4
i = 9412 (of 10192), d = 10, its = 55
i = 9413 (of 10192), d = 10, its = 52
i = 9414 (of 10192), d = 10, its = 50
i = 9415 (of 10192), d = 10, its = 37
i = 9416 (of 10192), d = 10, its = 59
i = 9417 (of 10192), d = 10, its = 56
i = 9418 (of 10192), d = 2.60471, its = 6
i = 9419 (of 10192), d = 10, its = 55
i = 9420 (of 10192), d = 2.07714, its = 4
i = 9421 (of 10192), d = 10, its = 47
i = 9422 (of 10192), d = 10, its = 42
i = 9423 (of 10192), d = 1.95172, its = 3
i = 9424 (of 10192), d = 10, its = 51
i = 9425 (of 10192), d = 1.26833, its = 6
i = 9426 (of 10192), d = 1.61245, its = 6
i = 9427 (of 10192), d = 1.33589, its = 7
i = 9428 (of 10192), d = 1.38764, its = 20
i = 9429 (of 10192), d = 10, its = 54
i = 9430 (of 10192), d = 10, its = 39
i = 9431 (of 10192), d = 10, its = 53
i = 9432 (of 10192), d = 10, its = 53
i = 9433 (of 10192), d = 10, its = 37
i = 9434 (of 10192), d = 8.05451, its = 10
i = 9435 (of 10192), d = 10, its = 53
i = 9436 (of 10192), d = 10, its = 52
i = 9437 (of 10192), d = 10, its = 47
i = 9438 (of 10192), d = 10, its = 53
i = 9439 (of 10192), d = 10, its = 56
i = 9440 (of 10192), d = 10, its = 37
i = 9441 (of 10192), d = 10, its = 52
i = 9442 (of 10192), d = 10, its = 53
i = 9443 (of 10192), d = 10, its = 54
i = 9444 (of 10192), d = 2.31067, its = 4
i = 9445 (of 10192), d = 10, its = 37
i = 9446 (of 10192), d = 2.81438, its = 5
i = 9447 (of 10192), d = 2.14186, its = 4
i = 9448 (of 10192), d = 10, its = 52
i = 9449 (of 10192), d = 10, its = 37
i = 9450 (of 10192), d = 9.95784, its = 10
i = 9451 (of 10192), d = 10, its = 55
i = 9452 (of 10192), d = 2.10882, its = 4
i = 9453 (of 10192), d = 1.56505, its = 7
i = 9454 (of 10192), d = 10, its = 55
i = 9455 (of 10192), d = 10, its = 52
i = 9456 (of 10192), d = 10, its = 37
i = 9457 (of 10192), d = 1.3694, its = 8
i = 9458 (of 10192), d = 9.18814, its = 9
i = 9459 (of 10192), d = 0.79999, its = 21
i = 9460 (of 10192), d = 10, its = 46
i = 9461 (of 10192), d = 10, its = 52
i = 9462 (of 10192), d = 1.6995, its = 5
i = 9463 (of 10192), d = 10, its = 50
i = 9464 (of 10192), d = 10, its = 46
i = 9465 (of 10192), d = 1.10742, its = 20
i = 9466 (of 10192), d = 10, its = 51
i = 9467 (of 10192), d = 10, its = 40
i = 9468 (of 10192), d = 10, its = 37
i = 9469 (of 10192), d = 10, its = 41
i = 9470 (of 10192), d = 10, its = 53
i = 9471 (of 10192), d = 10, its = 56
i = 9472 (of 10192), d = 10, its = 55
i = 9473 (of 10192), d = 10, its = 40
i = 9474 (of 10192), d = 0.584345, its = 17
i = 9475 (of 10192), d = 10, its = 49
i = 9476 (of 10192), d = 10, its = 52
i = 9477 (of 10192), d = 10, its = 52
i = 9478 (of 10192), d = 10, its = 54
i = 9479 (of 10192), d = 10, its = 56
i = 9480 (of 10192), d = 10, its = 37
i = 9481 (of 10192), d = 10, its = 54
i = 9482 (of 10192), d = 10, its = 50
i = 9483 (of 10192), d = 3.01266, its = 6
i = 9484 (of 10192), d = 10, its = 39
i = 9485 (of 10192), d = 1.57233, its = 6
i = 9486 (of 10192), d = 10, its = 53
i = 9487 (of 10192), d = 2.98054, its = 6
i = 9488 (of 10192), d = 10, its = 38
i = 9489 (of 10192), d = 10, its = 40
i = 9490 (of 10192), d = 10, its = 50
i = 9491 (of 10192), d = 4.65009, its = 7
i = 9492 (of 10192), d = 10, its = 38
i = 9493 (of 10192), d = 10, its = 54
i = 9494 (of 10192), d = 10, its = 47
i = 9495 (of 10192), d = 3.84146, its = 7
i = 9496 (of 10192), d = 1.08989, its = 18
i = 9497 (of 10192), d = 10, its = 55
i = 9498 (of 10192), d = 1.34691, its = 7
i = 9499 (of 10192), d = 10, its = 53
i = 9500 (of 10192), d = 10, its = 56
i = 9501 (of 10192), d = 10, its = 56
i = 9502 (of 10192), d = 1.16973, its = 21
i = 9503 (of 10192), d = 10, its = 41
i = 9504 (of 10192), d = 10, its = 37
i = 9505 (of 10192), d = 1.79126, its = 5
i = 9506 (of 10192), d = 2.70667, its = 5
i = 9507 (of 10192), d = 1.21453, its = 8
i = 9508 (of 10192), d = 10, its = 52
i = 9509 (of 10192), d = 10, its = 37
i = 9510 (of 10192), d = 10, its = 39
i = 9511 (of 10192), d = 10, its = 56
i = 9512 (of 10192), d = 10, its = 54
i = 9513 (of 10192), d = 10, its = 39
i = 9514 (of 10192), d = 10, its = 53
i = 9515 (of 10192), d = 10, its = 54
i = 9516 (of 10192), d = 2.40835, its = 5
i = 9517 (of 10192), d = 3.21015, its = 6
i = 9518 (of 10192), d = 10, its = 54
i = 9519 (of 10192), d = 10, its = 54
i = 9520 (of 10192), d = 10, its = 47
i = 9521 (of 10192), d = 10, its = 50
i = 9522 (of 10192), d = 4.49076, its = 9
i = 9523 (of 10192), d = 10, its = 54
i = 9524 (of 10192), d = 10, its = 51
i = 9525 (of 10192), d = 10, its = 42
i = 9526 (of 10192), d = 3.15465, its = 6
i = 9527 (of 10192), d = 10, its = 49
i = 9528 (of 10192), d = 10, its = 41
i = 9529 (of 10192), d = 1.65875, its = 6
i = 9530 (of 10192), d = 1.83385, its = 5
i = 9531 (of 10192), d = 10, its = 54
i = 9532 (of 10192), d = 10, its = 49
i = 9533 (of 10192), d = 10, its = 53
i = 9534 (of 10192), d = 10, its = 37
i = 9535 (of 10192), d = 10, its = 59
i = 9536 (of 10192), d = 10, its = 51
i = 9537 (of 10192), d = 10, its = 51
i = 9538 (of 10192), d = 2.97998, its = 6
i = 9539 (of 10192), d = 10, its = 41
i = 9540 (of 10192), d = 10, its = 46
i = 9541 (of 10192), d = 10, its = 37
i = 9542 (of 10192), d = 10, its = 50
i = 9543 (of 10192), d = 2.0811, its = 4
i = 9544 (of 10192), d = 1.85524, its = 4
i = 9545 (of 10192), d = 10, its = 53
i = 9546 (of 10192), d = 10, its = 50
i = 9547 (of 10192), d = 10, its = 37
i = 9548 (of 10192), d = 3.73241, its = 7
i = 9549 (of 10192), d = 1.88764, its = 4
i = 9550 (of 10192), d = 10, its = 50
i = 9551 (of 10192), d = 10, its = 54
i = 9552 (of 10192), d = 10, its = 50
i = 9553 (of 10192), d = 1.62856, its = 6
i = 9554 (of 10192), d = 10, its = 51
i = 9555 (of 10192), d = 10, its = 40
i = 9556 (of 10192), d = 1.22501, its = 6
i = 9557 (of 10192), d = 10, its = 43
i = 9558 (of 10192), d = 10, its = 56
i = 9559 (of 10192), d = 10, its = 41
i = 9560 (of 10192), d = 10, its = 54
i = 9561 (of 10192), d = 2.99085, its = 6
i = 9562 (of 10192), d = 1.67394, its = 5
i = 9563 (of 10192), d = 0.904714, its = 21
i = 9564 (of 10192), d = 10, its = 37
i = 9565 (of 10192), d = 10, its = 46
i = 9566 (of 10192), d = 10, its = 54
i = 9567 (of 10192), d = 10, its = 37
i = 9568 (of 10192), d = 10, its = 55
i = 9569 (of 10192), d = 10, its = 55
i = 9570 (of 10192), d = 10, its = 51
i = 9571 (of 10192), d = 10, its = 37
i = 9572 (of 10192), d = 0.890161, its = 18
i = 9573 (of 10192), d = 10, its = 41
i = 9574 (of 10192), d = 10, its = 37
i = 9575 (of 10192), d = 10, its = 52
i = 9576 (of 10192), d = 10, its = 42
i = 9577 (of 10192), d = 10, its = 52
i = 9578 (of 10192), d = 10, its = 37
i = 9579 (of 10192), d = 10, its = 37
i = 9580 (of 10192), d = 10, its = 55
i = 9581 (of 10192), d = 1.83404, its = 4
i = 9582 (of 10192), d = 10, its = 37
i = 9583 (of 10192), d = 1.85944, its = 4
i = 9584 (of 10192), d = 10, its = 56
i = 9585 (of 10192), d = 6.81655, its = 9
i = 9586 (of 10192), d = 1.06895, its = 21
i = 9587 (of 10192), d = 10, its = 53
i = 9588 (of 10192), d = 10, its = 40
i = 9589 (of 10192), d = 1.97669, its = 3
i = 9590 (of 10192), d = 1.02194, its = 16
i = 9591 (of 10192), d = 10, its = 55
i = 9592 (of 10192), d = 10, its = 52
i = 9593 (of 10192), d = 10, its = 49
i = 9594 (of 10192), d = 10, its = 54
i = 9595 (of 10192), d = 10, its = 37
i = 9596 (of 10192), d = 10, its = 53
i = 9597 (of 10192), d = 10, its = 55
i = 9598 (of 10192), d = 2.09781, its = 4
i = 9599 (of 10192), d = 10, its = 50
i = 9600 (of 10192), d = 10, its = 53
i = 9601 (of 10192), d = 10, its = 50
i = 9602 (of 10192), d = 10, its = 40
i = 9603 (of 10192), d = 0.783279, its = 16
i = 9604 (of 10192), d = 10, its = 52
i = 9605 (of 10192), d = 10, its = 51
i = 9606 (of 10192), d = 10, its = 53
i = 9607 (of 10192), d = 1.56321, its = 6
i = 9608 (of 10192), d = 10, its = 37
i = 9609 (of 10192), d = 10, its = 51
i = 9610 (of 10192), d = 1.53762, its = 6
i = 9611 (of 10192), d = 10, its = 49
i = 9612 (of 10192), d = 10, its = 54
i = 9613 (of 10192), d = 4.18856, its = 7
i = 9614 (of 10192), d = 10, its = 53
i = 9615 (of 10192), d = 2.25474, its = 4
i = 9616 (of 10192), d = 1.29427, its = 6
i = 9617 (of 10192), d = 10, its = 53
i = 9618 (of 10192), d = 10, its = 52
i = 9619 (of 10192), d = 10, its = 53
i = 9620 (of 10192), d = 10, its = 37
i = 9621 (of 10192), d = 1.67652, its = 5
i = 9622 (of 10192), d = 10, its = 37
i = 9623 (of 10192), d = 10, its = 49
i = 9624 (of 10192), d = 2.16272, its = 4
i = 9625 (of 10192), d = 10, its = 55
i = 9626 (of 10192), d = 10, its = 52
i = 9627 (of 10192), d = 10, its = 53
i = 9628 (of 10192), d = 10, its = 57
i = 9629 (of 10192), d = 2.12313, its = 4
i = 9630 (of 10192), d = 1.341, its = 20
i = 9631 (of 10192), d = 10, its = 52
i = 9632 (of 10192), d = 10, its = 51
i = 9633 (of 10192), d = 3.63754, its = 7
i = 9634 (of 10192), d = 10, its = 50
i = 9635 (of 10192), d = 10, its = 52
i = 9636 (of 10192), d = 10, its = 37
i = 9637 (of 10192), d = 2.23074, its = 4
i = 9638 (of 10192), d = 10, its = 51
i = 9639 (of 10192), d = 10, its = 48
i = 9640 (of 10192), d = 4.92192, its = 8
i = 9641 (of 10192), d = 0.866382, its = 16
i = 9642 (of 10192), d = 1.26538, its = 6
i = 9643 (of 10192), d = 10, its = 50
i = 9644 (of 10192), d = 9.18814, its = 9
i = 9645 (of 10192), d = 10, its = 43
i = 9646 (of 10192), d = 1.79606, its = 4
i = 9647 (of 10192), d = 1.57471, its = 6
i = 9648 (of 10192), d = 10, its = 53
i = 9649 (of 10192), d = 10, its = 51
i = 9650 (of 10192), d = 10, its = 37
i = 9651 (of 10192), d = 10, its = 39
i = 9652 (of 10192), d = 10, its = 51
i = 9653 (of 10192), d = 10, its = 39
i = 9654 (of 10192), d = 1.62846, its = 7
i = 9655 (of 10192), d = 10, its = 52
i = 9656 (of 10192), d = 1.58874, its = 6
i = 9657 (of 10192), d = 10, its = 42
i = 9658 (of 10192), d = 10, its = 37
i = 9659 (of 10192), d = 10, its = 56
i = 9660 (of 10192), d = 10, its = 49
i = 9661 (of 10192), d = 1.96469, its = 3
i = 9662 (of 10192), d = 1.66818, its = 5
i = 9663 (of 10192), d = 10, its = 55
i = 9664 (of 10192), d = 3.1029, its = 6
i = 9665 (of 10192), d = 2.44692, its = 5
i = 9666 (of 10192), d = 10, its = 37
i = 9667 (of 10192), d = 10, its = 51
i = 9668 (of 10192), d = 10, its = 52
i = 9669 (of 10192), d = 1.76554, its = 5
i = 9670 (of 10192), d = 10, its = 44
i = 9671 (of 10192), d = 1.52943, its = 7
i = 9672 (of 10192), d = 10, its = 41
i = 9673 (of 10192), d = 10, its = 48
i = 9674 (of 10192), d = 10, its = 48
i = 9675 (of 10192), d = 10, its = 49
i = 9676 (of 10192), d = 10, its = 37
i = 9677 (of 10192), d = 10, its = 52
i = 9678 (of 10192), d = 1.95, its = 3
i = 9679 (of 10192), d = 10, its = 44
i = 9680 (of 10192), d = 10, its = 40
i = 9681 (of 10192), d = 10, its = 49
i = 9682 (of 10192), d = 2.70495, its = 6
i = 9683 (of 10192), d = 2.01981, its = 3
i = 9684 (of 10192), d = 4.31916, its = 7
i = 9685 (of 10192), d = 10, its = 45
i = 9686 (of 10192), d = 2.05104, its = 4
i = 9687 (of 10192), d = 10, its = 37
i = 9688 (of 10192), d = 10, its = 39
i = 9689 (of 10192), d = 10, its = 54
i = 9690 (of 10192), d = 10, its = 39
i = 9691 (of 10192), d = 1.75744, its = 5
i = 9692 (of 10192), d = 10, its = 37
i = 9693 (of 10192), d = 10, its = 37
i = 9694 (of 10192), d = 10, its = 55
i = 9695 (of 10192), d = 0.936912, its = 25
i = 9696 (of 10192), d = 10, its = 56
i = 9697 (of 10192), d = 10, its = 51
i = 9698 (of 10192), d = 10, its = 55
i = 9699 (of 10192), d = 10, its = 55
i = 9700 (of 10192), d = 10, its = 53
i = 9701 (of 10192), d = 2.07861, its = 4
i = 9702 (of 10192), d = 10, its = 40
i = 9703 (of 10192), d = 2.86854, its = 6
i = 9704 (of 10192), d = 10, its = 37
i = 9705 (of 10192), d = 10, its = 55
i = 9706 (of 10192), d = 10, its = 41
i = 9707 (of 10192), d = 10, its = 50
i = 9708 (of 10192), d = 10, its = 54
i = 9709 (of 10192), d = 10, its = 58
i = 9710 (of 10192), d = 4.03745, its = 7
i = 9711 (of 10192), d = 1.64236, its = 6
i = 9712 (of 10192), d = 10, its = 53
i = 9713 (of 10192), d = 3.9963, its = 6
i = 9714 (of 10192), d = 3.6151, its = 7
i = 9715 (of 10192), d = 10, its = 59
i = 9716 (of 10192), d = 10, its = 53
i = 9717 (of 10192), d = 0.963915, its = 26
i = 9718 (of 10192), d = 10, its = 57
i = 9719 (of 10192), d = 10, its = 51
i = 9720 (of 10192), d = 1.95264, its = 3
i = 9721 (of 10192), d = 0.81733, its = 19
i = 9722 (of 10192), d = 1.08603, its = 27
i = 9723 (of 10192), d = 10, its = 53
i = 9724 (of 10192), d = 10, its = 38
i = 9725 (of 10192), d = 0.969297, its = 18
i = 9726 (of 10192), d = 10, its = 51
i = 9727 (of 10192), d = 10, its = 37
i = 9728 (of 10192), d = 10, its = 49
i = 9729 (of 10192), d = 10, its = 54
i = 9730 (of 10192), d = 1.73524, its = 5
i = 9731 (of 10192), d = 10, its = 48
i = 9732 (of 10192), d = 10, its = 54
i = 9733 (of 10192), d = 2.03008, its = 3
i = 9734 (of 10192), d = 10, its = 40
i = 9735 (of 10192), d = 10, its = 39
i = 9736 (of 10192), d = 10, its = 49
i = 9737 (of 10192), d = 1.16778, its = 23
i = 9738 (of 10192), d = 4.97582, its = 9
i = 9739 (of 10192), d = 1.77284, its = 5
i = 9740 (of 10192), d = 10, its = 54
i = 9741 (of 10192), d = 2.20869, its = 5
i = 9742 (of 10192), d = 10, its = 53
i = 9743 (of 10192), d = 10, its = 52
i = 9744 (of 10192), d = 2.77539, its = 5
i = 9745 (of 10192), d = 1.67589, its = 7
i = 9746 (of 10192), d = 10, its = 54
i = 9747 (of 10192), d = 2.46022, its = 6
i = 9748 (of 10192), d = 10, its = 53
i = 9749 (of 10192), d = 10, its = 55
i = 9750 (of 10192), d = 10, its = 51
i = 9751 (of 10192), d = 10, its = 52
i = 9752 (of 10192), d = 10, its = 52
i = 9753 (of 10192), d = 10, its = 43
i = 9754 (of 10192), d = 10, its = 49
i = 9755 (of 10192), d = 2.84353, its = 6
i = 9756 (of 10192), d = 10, its = 52
i = 9757 (of 10192), d = 10, its = 53
i = 9758 (of 10192), d = 10, its = 55
i = 9759 (of 10192), d = 10, its = 37
i = 9760 (of 10192), d = 10, its = 57
i = 9761 (of 10192), d = 10, its = 41
i = 9762 (of 10192), d = 1.76516, its = 5
i = 9763 (of 10192), d = 1.90593, its = 4
i = 9764 (of 10192), d = 0.902464, its = 20
i = 9765 (of 10192), d = 4.22785, its = 7
i = 9766 (of 10192), d = 2.66061, its = 6
i = 9767 (of 10192), d = 3.33144, its = 6
i = 9768 (of 10192), d = 10, its = 50
i = 9769 (of 10192), d = 10, its = 49
i = 9770 (of 10192), d = 10, its = 41
i = 9771 (of 10192), d = 10, its = 37
i = 9772 (of 10192), d = 10, its = 41
i = 9773 (of 10192), d = 10, its = 56
i = 9774 (of 10192), d = 2.67992, its = 6
i = 9775 (of 10192), d = 1.50353, its = 7
i = 9776 (of 10192), d = 1.83881, its = 4
i = 9777 (of 10192), d = 10, its = 53
i = 9778 (of 10192), d = 10, its = 55
i = 9779 (of 10192), d = 10, its = 51
i = 9780 (of 10192), d = 5.82411, its = 9
i = 9781 (of 10192), d = 1.60183, its = 6
i = 9782 (of 10192), d = 10, its = 37
i = 9783 (of 10192), d = 10, its = 50
i = 9784 (of 10192), d = 8.6145, its = 8
i = 9785 (of 10192), d = 10, its = 52
i = 9786 (of 10192), d = 10, its = 49
i = 9787 (of 10192), d = 7.71866, its = 8
i = 9788 (of 10192), d = 10, its = 37
i = 9789 (of 10192), d = 2.96873, its = 6
i = 9790 (of 10192), d = 2.24257, its = 4
i = 9791 (of 10192), d = 10, its = 50
i = 9792 (of 10192), d = 2.04202, its = 3
i = 9793 (of 10192), d = 10, its = 51
i = 9794 (of 10192), d = 10, its = 52
i = 9795 (of 10192), d = 1.99256, its = 3
i = 9796 (of 10192), d = 10, its = 60
i = 9797 (of 10192), d = 10, its = 37
i = 9798 (of 10192), d = 10, its = 53
i = 9799 (of 10192), d = 1.46684, its = 7
i = 9800 (of 10192), d = 1.14633, its = 21
i = 9801 (of 10192), d = 10, its = 48
i = 9802 (of 10192), d = 10, its = 56
i = 9803 (of 10192), d = 10, its = 53
i = 9804 (of 10192), d = 10, its = 53
i = 9805 (of 10192), d = 10, its = 51
i = 9806 (of 10192), d = 10, its = 37
i = 9807 (of 10192), d = 10, its = 51
i = 9808 (of 10192), d = 10, its = 41
i = 9809 (of 10192), d = 1.73864, its = 6
i = 9810 (of 10192), d = 0.85423, its = 26
i = 9811 (of 10192), d = 10, its = 53
i = 9812 (of 10192), d = 10, its = 51
i = 9813 (of 10192), d = 10, its = 53
i = 9814 (of 10192), d = 10, its = 41
i = 9815 (of 10192), d = 10, its = 39
i = 9816 (of 10192), d = 10, its = 51
i = 9817 (of 10192), d = 10, its = 51
i = 9818 (of 10192), d = 1.41297, its = 7
i = 9819 (of 10192), d = 10, its = 51
i = 9820 (of 10192), d = 1.07705, its = 19
i = 9821 (of 10192), d = 1.68734, its = 5
i = 9822 (of 10192), d = 1.4438, its = 7
i = 9823 (of 10192), d = 10, its = 37
i = 9824 (of 10192), d = 10, its = 55
i = 9825 (of 10192), d = 10, its = 37
i = 9826 (of 10192), d = 1.44876, its = 7
i = 9827 (of 10192), d = 3.94326, its = 7
i = 9828 (of 10192), d = 10, its = 54
i = 9829 (of 10192), d = 10, its = 41
i = 9830 (of 10192), d = 3.04896, its = 7
i = 9831 (of 10192), d = 10, its = 51
i = 9832 (of 10192), d = 1.30144, its = 6
i = 9833 (of 10192), d = 10, its = 52
i = 9834 (of 10192), d = 10, its = 54
i = 9835 (of 10192), d = 1.94419, its = 3
i = 9836 (of 10192), d = 10, its = 37
i = 9837 (of 10192), d = 10, its = 37
i = 9838 (of 10192), d = 10, its = 50
i = 9839 (of 10192), d = 10, its = 54
i = 9840 (of 10192), d = 10, its = 37
i = 9841 (of 10192), d = 2.37863, its = 6
i = 9842 (of 10192), d = 10, its = 39
i = 9843 (of 10192), d = 10, its = 40
i = 9844 (of 10192), d = 10, its = 52
i = 9845 (of 10192), d = 10, its = 56
i = 9846 (of 10192), d = 10, its = 39
i = 9847 (of 10192), d = 10, its = 52
i = 9848 (of 10192), d = 10, its = 54
i = 9849 (of 10192), d = 1.4476, its = 7
i = 9850 (of 10192), d = 10, its = 37
i = 9851 (of 10192), d = 4.16324, its = 7
i = 9852 (of 10192), d = 10, its = 51
i = 9853 (of 10192), d = 2.83567, its = 5
i = 9854 (of 10192), d = 10, its = 41
i = 9855 (of 10192), d = 10, its = 54
i = 9856 (of 10192), d = 10, its = 55
i = 9857 (of 10192), d = 2.48717, its = 5
i = 9858 (of 10192), d = 10, its = 52
i = 9859 (of 10192), d = 1.44631, its = 6
i = 9860 (of 10192), d = 10, its = 54
i = 9861 (of 10192), d = 2.78405, its = 6
i = 9862 (of 10192), d = 10, its = 49
i = 9863 (of 10192), d = 10, its = 50
i = 9864 (of 10192), d = 10, its = 55
i = 9865 (of 10192), d = 10, its = 41
i = 9866 (of 10192), d = 2.09148, its = 4
i = 9867 (of 10192), d = 10, its = 39
i = 9868 (of 10192), d = 10, its = 55
i = 9869 (of 10192), d = 10, its = 39
i = 9870 (of 10192), d = 1.96916, its = 3
i = 9871 (of 10192), d = 3.06514, its = 6
i = 9872 (of 10192), d = 10, its = 54
i = 9873 (of 10192), d = 10, its = 53
i = 9874 (of 10192), d = 10, its = 52
i = 9875 (of 10192), d = 10, its = 52
i = 9876 (of 10192), d = 10, its = 55
i = 9877 (of 10192), d = 10, its = 50
i = 9878 (of 10192), d = 10, its = 53
i = 9879 (of 10192), d = 10, its = 41
i = 9880 (of 10192), d = 10, its = 50
i = 9881 (of 10192), d = 1.2986, its = 6
i = 9882 (of 10192), d = 1.69077, its = 6
i = 9883 (of 10192), d = 10, its = 51
i = 9884 (of 10192), d = 10, its = 52
i = 9885 (of 10192), d = 10, its = 54
i = 9886 (of 10192), d = 10, its = 37
i = 9887 (of 10192), d = 2.43793, its = 5
i = 9888 (of 10192), d = 10, its = 53
i = 9889 (of 10192), d = 10, its = 52
i = 9890 (of 10192), d = 1.88879, its = 4
i = 9891 (of 10192), d = 10, its = 53
i = 9892 (of 10192), d = 2.08944, its = 4
i = 9893 (of 10192), d = 0.968332, its = 17
i = 9894 (of 10192), d = 10, its = 59
i = 9895 (of 10192), d = 3.08603, its = 6
i = 9896 (of 10192), d = 10, its = 54
i = 9897 (of 10192), d = 10, its = 53
i = 9898 (of 10192), d = 10, its = 47
i = 9899 (of 10192), d = 10, its = 53
i = 9900 (of 10192), d = 10, its = 54
i = 9901 (of 10192), d = 10, its = 42
i = 9902 (of 10192), d = 9.06353, its = 9
i = 9903 (of 10192), d = 10, its = 49
i = 9904 (of 10192), d = 10, its = 37
i = 9905 (of 10192), d = 10, its = 47
i = 9906 (of 10192), d = 10, its = 41
i = 9907 (of 10192), d = 10, its = 48
i = 9908 (of 10192), d = 10, its = 52
i = 9909 (of 10192), d = 10, its = 56
i = 9910 (of 10192), d = 10, its = 47
i = 9911 (of 10192), d = 10, its = 54
i = 9912 (of 10192), d = 10, its = 52
i = 9913 (of 10192), d = 10, its = 51
i = 9914 (of 10192), d = 10, its = 40
i = 9915 (of 10192), d = 1.20606, its = 19
i = 9916 (of 10192), d = 10, its = 56
i = 9917 (of 10192), d = 10, its = 37
i = 9918 (of 10192), d = 10, its = 48
i = 9919 (of 10192), d = 10, its = 57
i = 9920 (of 10192), d = 10, its = 49
i = 9921 (of 10192), d = 10, its = 51
i = 9922 (of 10192), d = 10, its = 55
i = 9923 (of 10192), d = 10, its = 40
i = 9924 (of 10192), d = 10, its = 53
i = 9925 (of 10192), d = 4.26536, its = 7
i = 9926 (of 10192), d = 5.56791, its = 8
i = 9927 (of 10192), d = 10, its = 37
i = 9928 (of 10192), d = 1.9803, its = 3
i = 9929 (of 10192), d = 10, its = 53
i = 9930 (of 10192), d = 1.87202, its = 4
i = 9931 (of 10192), d = 3.60272, its = 7
i = 9932 (of 10192), d = 0.774583, its = 18
i = 9933 (of 10192), d = 10, its = 54
i = 9934 (of 10192), d = 10, its = 39
i = 9935 (of 10192), d = 10, its = 49
i = 9936 (of 10192), d = 1.68832, its = 5
i = 9937 (of 10192), d = 10, its = 37
i = 9938 (of 10192), d = 10, its = 50
i = 9939 (of 10192), d = 10, its = 53
i = 9940 (of 10192), d = 10, its = 40
i = 9941 (of 10192), d = 10, its = 53
i = 9942 (of 10192), d = 10, its = 52
i = 9943 (of 10192), d = 10, its = 53
i = 9944 (of 10192), d = 10, its = 55
i = 9945 (of 10192), d = 10, its = 37
i = 9946 (of 10192), d = 10, its = 55
i = 9947 (of 10192), d = 10, its = 46
i = 9948 (of 10192), d = 10, its = 47
i = 9949 (of 10192), d = 1.61048, its = 7
i = 9950 (of 10192), d = 1.8659, its = 4
i = 9951 (of 10192), d = 10, its = 40
i = 9952 (of 10192), d = 10, its = 51
i = 9953 (of 10192), d = 1.98684, its = 3
i = 9954 (of 10192), d = 0.995355, its = 18
i = 9955 (of 10192), d = 10, its = 50
i = 9956 (of 10192), d = 10, its = 53
i = 9957 (of 10192), d = 10, its = 53
i = 9958 (of 10192), d = 10, its = 52
i = 9959 (of 10192), d = 10, its = 37
i = 9960 (of 10192), d = 10, its = 51
i = 9961 (of 10192), d = 10, its = 50
i = 9962 (of 10192), d = 10, its = 50
i = 9963 (of 10192), d = 10, its = 48
i = 9964 (of 10192), d = 10, its = 57
i = 9965 (of 10192), d = 2.14145, its = 4
i = 9966 (of 10192), d = 4.5568, its = 8
i = 9967 (of 10192), d = 10, its = 54
i = 9968 (of 10192), d = 10, its = 50
i = 9969 (of 10192), d = 10, its = 55
i = 9970 (of 10192), d = 10, its = 40
i = 9971 (of 10192), d = 10, its = 55
i = 9972 (of 10192), d = 1.50403, its = 8
i = 9973 (of 10192), d = 10, its = 37
i = 9974 (of 10192), d = 10, its = 39
i = 9975 (of 10192), d = 0.418962, its = 18
i = 9976 (of 10192), d = 10, its = 59
i = 9977 (of 10192), d = 10, its = 51
i = 9978 (of 10192), d = 10, its = 59
i = 9979 (of 10192), d = 4.99379, its = 8
i = 9980 (of 10192), d = 0.817256, its = 19
i = 9981 (of 10192), d = 0.993048, its = 17
i = 9982 (of 10192), d = 10, its = 53
i = 9983 (of 10192), d = 1.71293, its = 5
i = 9984 (of 10192), d = 10, its = 41
i = 9985 (of 10192), d = 10, its = 56
i = 9986 (of 10192), d = 10, its = 47
i = 9987 (of 10192), d = 10, its = 54
i = 9988 (of 10192), d = 0.947043, its = 19
i = 9989 (of 10192), d = 10, its = 55
i = 9990 (of 10192), d = 2.32849, its = 5
i = 9991 (of 10192), d = 10, its = 37
i = 9992 (of 10192), d = 2.08983, its = 4
i = 9993 (of 10192), d = 10, its = 58
i = 9994 (of 10192), d = 1.36683, its = 6
i = 9995 (of 10192), d = 10, its = 48
i = 9996 (of 10192), d = 10, its = 37
i = 9997 (of 10192), d = 10, its = 53
i = 9998 (of 10192), d = 10, its = 56
i = 9999 (of 10192), d = 10, its = 55
i = 10000 (of 10192), d = 0.748005, its = 22
i = 10001 (of 10192), d = 1.49227, its = 7
i = 10002 (of 10192), d = 2.01281, its = 3
i = 10003 (of 10192), d = 1.10392, its = 5
i = 10004 (of 10192), d = 10, its = 58
i = 10005 (of 10192), d = 10, its = 37
i = 10006 (of 10192), d = 10, its = 53
i = 10007 (of 10192), d = 2.49293, its = 5
i = 10008 (of 10192), d = 10, its = 53
i = 10009 (of 10192), d = 0.886999, its = 18
i = 10010 (of 10192), d = 10, its = 50
i = 10011 (of 10192), d = 10, its = 54
i = 10012 (of 10192), d = 10, its = 54
i = 10013 (of 10192), d = 10, its = 40
i = 10014 (of 10192), d = 1.47662, its = 7
i = 10015 (of 10192), d = 10, its = 48
i = 10016 (of 10192), d = 10, its = 40
i = 10017 (of 10192), d = 1.44898, its = 6
i = 10018 (of 10192), d = 10, its = 54
i = 10019 (of 10192), d = 10, its = 57
i = 10020 (of 10192), d = 10, its = 49
i = 10021 (of 10192), d = 5.7715, its = 7
i = 10022 (of 10192), d = 3.1585, its = 7
i = 10023 (of 10192), d = 10, its = 41
i = 10024 (of 10192), d = 10, its = 43
i = 10025 (of 10192), d = 2.06617, its = 3
i = 10026 (of 10192), d = 10, its = 41
i = 10027 (of 10192), d = 10, its = 56
i = 10028 (of 10192), d = 10, its = 51
i = 10029 (of 10192), d = 1.59379, its = 6
i = 10030 (of 10192), d = 10, its = 52
i = 10031 (of 10192), d = 10, its = 41
i = 10032 (of 10192), d = 10, its = 37
i = 10033 (of 10192), d = 10, its = 54
i = 10034 (of 10192), d = 10, its = 37
i = 10035 (of 10192), d = 10, its = 53
i = 10036 (of 10192), d = 10, its = 51
i = 10037 (of 10192), d = 10, its = 58
i = 10038 (of 10192), d = 10, its = 55
i = 10039 (of 10192), d = 1.88429, its = 4
i = 10040 (of 10192), d = 10, its = 53
i = 10041 (of 10192), d = 0.897008, its = 18
i = 10042 (of 10192), d = 10, its = 51
i = 10043 (of 10192), d = 10, its = 55
i = 10044 (of 10192), d = 10, its = 55
i = 10045 (of 10192), d = 10, its = 37
i = 10046 (of 10192), d = 10, its = 55
i = 10047 (of 10192), d = 10, its = 52
i = 10048 (of 10192), d = 10, its = 37
i = 10049 (of 10192), d = 10, its = 41
i = 10050 (of 10192), d = 10, its = 52
i = 10051 (of 10192), d = 10, its = 53
i = 10052 (of 10192), d = 10, its = 50
i = 10053 (of 10192), d = 10, its = 42
i = 10054 (of 10192), d = 10, its = 54
i = 10055 (of 10192), d = 10, its = 53
i = 10056 (of 10192), d = 10, its = 40
i = 10057 (of 10192), d = 10, its = 52
i = 10058 (of 10192), d = 10, its = 54
i = 10059 (of 10192), d = 10, its = 51
i = 10060 (of 10192), d = 10, its = 50
i = 10061 (of 10192), d = 10, its = 37
i = 10062 (of 10192), d = 10, its = 52
i = 10063 (of 10192), d = 2.17112, its = 4
i = 10064 (of 10192), d = 10, its = 57
i = 10065 (of 10192), d = 4.13023, its = 7
i = 10066 (of 10192), d = 10, its = 38
i = 10067 (of 10192), d = 0.971903, its = 23
i = 10068 (of 10192), d = 1.84063, its = 4
i = 10069 (of 10192), d = 2.52499, its = 6
i = 10070 (of 10192), d = 10, its = 38
i = 10071 (of 10192), d = 10, its = 52
i = 10072 (of 10192), d = 3.07135, its = 6
i = 10073 (of 10192), d = 1.53974, its = 6
i = 10074 (of 10192), d = 10, its = 37
i = 10075 (of 10192), d = 10, its = 37
i = 10076 (of 10192), d = 1.58743, its = 8
i = 10077 (of 10192), d = 10, its = 58
i = 10078 (of 10192), d = 10, its = 40
i = 10079 (of 10192), d = 1.13758, its = 20
i = 10080 (of 10192), d = 10, its = 39
i = 10081 (of 10192), d = 2.63287, its = 5
i = 10082 (of 10192), d = 10, its = 52
i = 10083 (of 10192), d = 10, its = 55
i = 10084 (of 10192), d = 4.55167, its = 7
i = 10085 (of 10192), d = 10, its = 37
i = 10086 (of 10192), d = 1.46664, its = 8
i = 10087 (of 10192), d = 10, its = 59
i = 10088 (of 10192), d = 10, its = 52
i = 10089 (of 10192), d = 10, its = 54
i = 10090 (of 10192), d = 10, its = 57
i = 10091 (of 10192), d = 10, its = 52
i = 10092 (of 10192), d = 10, its = 39
i = 10093 (of 10192), d = 10, its = 40
i = 10094 (of 10192), d = 10, its = 53
i = 10095 (of 10192), d = 10, its = 43
i = 10096 (of 10192), d = 2.23673, its = 4
i = 10097 (of 10192), d = 10, its = 56
i = 10098 (of 10192), d = 10, its = 56
i = 10099 (of 10192), d = 10, its = 37
i = 10100 (of 10192), d = 10, its = 50
i = 10101 (of 10192), d = 10, its = 41
i = 10102 (of 10192), d = 10, its = 54
i = 10103 (of 10192), d = 1.46686, its = 7
i = 10104 (of 10192), d = 2.11249, its = 4
i = 10105 (of 10192), d = 10, its = 37
i = 10106 (of 10192), d = 10, its = 51
i = 10107 (of 10192), d = 10, its = 54
i = 10108 (of 10192), d = 10, its = 52
i = 10109 (of 10192), d = 10, its = 53
i = 10110 (of 10192), d = 1.74493, its = 5
i = 10111 (of 10192), d = 10, its = 50
i = 10112 (of 10192), d = 10, its = 51
i = 10113 (of 10192), d = 2.43459, its = 5
i = 10114 (of 10192), d = 10, its = 56
i = 10115 (of 10192), d = 10, its = 51
i = 10116 (of 10192), d = 3.38664, its = 8
i = 10117 (of 10192), d = 10, its = 50
i = 10118 (of 10192), d = 10, its = 41
i = 10119 (of 10192), d = 1.81008, its = 4
i = 10120 (of 10192), d = 2.08944, its = 4
i = 10121 (of 10192), d = 2.68817, its = 6
i = 10122 (of 10192), d = 0.941199, its = 19
i = 10123 (of 10192), d = 10, its = 56
i = 10124 (of 10192), d = 2.29553, its = 4
i = 10125 (of 10192), d = 10, its = 41
i = 10126 (of 10192), d = 10, its = 40
i = 10127 (of 10192), d = 4.3458, its = 8
i = 10128 (of 10192), d = 10, its = 43
i = 10129 (of 10192), d = 10, its = 52
i = 10130 (of 10192), d = 7.77805, its = 19
i = 10131 (of 10192), d = 10, its = 52
i = 10132 (of 10192), d = 10, its = 49
i = 10133 (of 10192), d = 10, its = 51
i = 10134 (of 10192), d = 10, its = 55
i = 10135 (of 10192), d = 10, its = 51
i = 10136 (of 10192), d = 10, its = 55
i = 10137 (of 10192), d = 2.56284, its = 5
i = 10138 (of 10192), d = 10, its = 40
i = 10139 (of 10192), d = 10, its = 55
i = 10140 (of 10192), d = 2.75769, its = 6
i = 10141 (of 10192), d = 10, its = 56
i = 10142 (of 10192), d = 10, its = 37
i = 10143 (of 10192), d = 1.39144, its = 7
i = 10144 (of 10192), d = 10, its = 40
i = 10145 (of 10192), d = 10, its = 51
i = 10146 (of 10192), d = 10, its = 52
i = 10147 (of 10192), d = 2.82832, its = 6
i = 10148 (of 10192), d = 1.90162, its = 4
i = 10149 (of 10192), d = 10, its = 37
i = 10150 (of 10192), d = 1.83774, its = 5
i = 10151 (of 10192), d = 1.49859, its = 7
i = 10152 (of 10192), d = 6.2142, its = 7
i = 10153 (of 10192), d = 10, its = 52
i = 10154 (of 10192), d = 10, its = 50
i = 10155 (of 10192), d = 1.87352, its = 4
i = 10156 (of 10192), d = 2.01522, its = 3
i = 10157 (of 10192), d = 10, its = 53
i = 10158 (of 10192), d = 10, its = 37
i = 10159 (of 10192), d = 10, its = 53
i = 10160 (of 10192), d = 1.90749, its = 4
i = 10161 (of 10192), d = 10, its = 37
i = 10162 (of 10192), d = 2.50584, its = 6
i = 10163 (of 10192), d = 10, its = 54
i = 10164 (of 10192), d = 2.03863, its = 3
i = 10165 (of 10192), d = 2.66548, its = 5
i = 10166 (of 10192), d = 10, its = 37
i = 10167 (of 10192), d = 1.77357, its = 4
i = 10168 (of 10192), d = 1.67679, its = 5
i = 10169 (of 10192), d = 0.716185, its = 16
i = 10170 (of 10192), d = 10, its = 50
i = 10171 (of 10192), d = 1.47971, its = 7
i = 10172 (of 10192), d = 0.832271, its = 17
i = 10173 (of 10192), d = 10, its = 52
i = 10174 (of 10192), d = 10, its = 52
i = 10175 (of 10192), d = 10, its = 53
i = 10176 (of 10192), d = 10, its = 53
i = 10177 (of 10192), d = 10, its = 38
i = 10178 (of 10192), d = 5.64728, its = 8
i = 10179 (of 10192), d = 10, its = 50
i = 10180 (of 10192), d = 10, its = 37
i = 10181 (of 10192), d = 10, its = 54
i = 10182 (of 10192), d = 10, its = 54
i = 10183 (of 10192), d = 10, its = 57
i = 10184 (of 10192), d = 5.47871, its = 7
i = 10185 (of 10192), d = 4.98465, its = 8
i = 10186 (of 10192), d = 10, its = 37
i = 10187 (of 10192), d = 0.908427, its = 18
i = 10188 (of 10192), d = 10, its = 37
i = 10189 (of 10192), d = 1.53747, its = 6
i = 10190 (of 10192), d = 10, its = 41
i = 10191 (of 10192), d = 3.23562, its = 7
i = 10192 (of 10192), d = 1.21739, its = 6
[1] "Fri Feb 09 07:50:37 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 07:51:31 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 07:51:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 07:51:41 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 07:51:50 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Fri Feb 09 07:52:05 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=8; decay=6.29e-05
# weights:  97
initial  value 8565.875449 
iter  10 value 619.740047
iter  20 value 306.833266
iter  30 value 168.383206
iter  40 value 121.322397
iter  50 value 98.682753
iter  60 value 90.067191
iter  70 value 87.580808
iter  80 value 86.582612
iter  90 value 86.106838
iter 100 value 85.814881
final  value 85.814881 
stopped after 100 iterations
# weights:  97
initial  value 98909.918100 
iter  10 value 632.258834
iter  20 value 338.295960
iter  30 value 172.287561
iter  40 value 102.329940
iter  50 value 91.099216
iter  60 value 88.020580
iter  70 value 87.168835
iter  80 value 86.793267
iter  90 value 86.619574
iter 100 value 86.485947
final  value 86.485947 
stopped after 100 iterations
# weights:  97
initial  value 21131.255649 
iter  10 value 1001.563236
iter  20 value 407.405138
iter  30 value 239.841331
iter  40 value 157.613242
iter  50 value 110.705253
iter  60 value 94.281268
iter  70 value 88.134684
iter  80 value 86.151735
iter  90 value 85.468768
iter 100 value 85.322477
final  value 85.322477 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0651; time: 0.5 min
[Tune-x] 2: size=7; decay=1.15e-05
# weights:  85
initial  value 11089.234667 
iter  10 value 619.708342
iter  20 value 216.723591
iter  30 value 150.358109
iter  40 value 118.322650
iter  50 value 97.927201
iter  60 value 90.055759
iter  70 value 86.854098
iter  80 value 85.928631
iter  90 value 85.466571
iter 100 value 85.383363
final  value 85.383363 
stopped after 100 iterations
# weights:  85
initial  value 1634.204978 
iter  10 value 458.940493
iter  20 value 183.785094
iter  30 value 134.249745
iter  40 value 107.689125
iter  50 value 94.165265
iter  60 value 89.245706
iter  70 value 87.550460
iter  80 value 86.728712
iter  90 value 86.513358
iter 100 value 86.414878
final  value 86.414878 
stopped after 100 iterations
# weights:  85
initial  value 26237.882963 
iter  10 value 1688.370477
iter  20 value 1661.035252
iter  30 value 1267.292832
iter  40 value 830.679611
iter  50 value 503.510747
iter  60 value 496.334538
iter  70 value 444.459289
iter  80 value 332.543840
iter  90 value 206.113465
iter 100 value 103.745484
final  value 103.745484 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0672; time: 0.4 min
[Tune-x] 3: size=4; decay=6.24e-05
# weights:  49
initial  value 27644.355079 
iter  10 value 402.827420
iter  20 value 171.680731
iter  30 value 133.367328
iter  40 value 115.737870
iter  50 value 96.688598
iter  60 value 90.014168
iter  70 value 86.829521
iter  80 value 85.794129
iter  90 value 85.608031
iter 100 value 85.536774
final  value 85.536774 
stopped after 100 iterations
# weights:  49
initial  value 17475.998669 
iter  10 value 775.639415
iter  20 value 498.050870
iter  30 value 419.999404
iter  40 value 217.714512
iter  50 value 155.124117
iter  60 value 136.654815
iter  70 value 115.712998
iter  80 value 104.423895
iter  90 value 94.403729
iter 100 value 88.491403
final  value 88.491403 
stopped after 100 iterations
# weights:  49
initial  value 17340.782664 
iter  10 value 631.439256
iter  20 value 325.542191
iter  30 value 175.020032
iter  40 value 94.192412
iter  50 value 87.002876
iter  60 value 85.955245
iter  70 value 85.757323
iter  80 value 85.689879
iter  90 value 85.437678
iter 100 value 85.378006
final  value 85.378006 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0653; time: 0.3 min
[Tune-x] 4: size=4; decay=0.00234
# weights:  49
initial  value 1970.468927 
iter  10 value 498.495364
iter  20 value 195.074965
iter  30 value 154.325049
iter  40 value 123.510412
iter  50 value 108.061050
iter  60 value 96.004344
iter  70 value 89.076179
iter  80 value 86.733886
iter  90 value 86.406568
iter 100 value 86.241325
final  value 86.241325 
stopped after 100 iterations
# weights:  49
initial  value 67268.346630 
iter  10 value 1014.113373
iter  20 value 410.188289
iter  30 value 244.420156
iter  40 value 146.796651
iter  50 value 121.921891
iter  60 value 110.368076
iter  70 value 103.259104
iter  80 value 101.681832
iter  90 value 98.034568
iter 100 value 91.772704
final  value 91.772704 
stopped after 100 iterations
# weights:  49
initial  value 7910.690691 
iter  10 value 450.447706
iter  20 value 164.810152
iter  30 value 128.852231
iter  40 value 108.341099
iter  50 value 91.618774
iter  60 value 86.756338
iter  70 value 85.988788
iter  80 value 85.860943
iter  90 value 85.826516
iter 100 value 85.785929
final  value 85.785929 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0654; time: 0.3 min
[Tune-x] 5: size=2; decay=0.00227
# weights:  25
initial  value 15886.710783 
iter  10 value 414.483415
iter  20 value 226.071508
iter  30 value 170.961360
iter  40 value 110.871174
iter  50 value 98.774687
iter  60 value 93.466314
iter  70 value 90.903802
iter  80 value 87.068917
iter  90 value 85.950884
iter 100 value 85.858836
final  value 85.858836 
stopped after 100 iterations
# weights:  25
initial  value 32903.174698 
iter  10 value 625.478682
iter  20 value 497.159939
iter  30 value 410.225155
iter  40 value 299.393559
iter  50 value 146.067578
iter  60 value 123.739055
iter  70 value 115.975830
iter  80 value 107.873282
iter  90 value 104.094898
iter 100 value 103.704709
final  value 103.704709 
stopped after 100 iterations
# weights:  25
initial  value 36969.414302 
iter  10 value 427.835204
iter  20 value 333.613947
iter  30 value 186.057798
iter  40 value 146.577885
iter  50 value 135.586677
iter  60 value 120.791246
iter  70 value 104.904257
iter  80 value 94.105892
iter  90 value 86.757120
iter 100 value 86.472540
final  value 86.472540 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.067; time: 0.2 min
[Tune-x] 6: size=8; decay=0.00309
# weights:  97
initial  value 1703.130568 
iter  10 value 296.004600
iter  20 value 134.372150
iter  30 value 110.211764
iter  40 value 95.796017
iter  50 value 90.804160
iter  60 value 88.237868
iter  70 value 86.916339
iter  80 value 86.306326
iter  90 value 86.104651
iter 100 value 86.026512
final  value 86.026512 
stopped after 100 iterations
# weights:  97
initial  value 28018.042570 
final  value 1706.814693 
converged
# weights:  97
initial  value 3316.852649 
iter  10 value 363.794350
iter  20 value 127.056951
iter  30 value 107.753031
iter  40 value 99.235408
iter  50 value 93.356288
iter  60 value 88.390117
iter  70 value 86.928323
iter  80 value 86.099199
iter  90 value 85.794070
iter 100 value 85.689827
final  value 85.689827 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.175; time: 0.4 min
[Tune-x] 7: size=2; decay=0.000138
# weights:  25
initial  value 24574.797296 
iter  10 value 926.236351
iter  20 value 465.492923
iter  30 value 257.434422
iter  40 value 175.080934
iter  50 value 146.204613
iter  60 value 135.486261
iter  70 value 116.062980
iter  80 value 101.954433
iter  90 value 89.867010
iter 100 value 86.511380
final  value 86.511380 
stopped after 100 iterations
# weights:  25
initial  value 5627.308751 
iter  10 value 502.782290
iter  20 value 354.129695
iter  30 value 309.100079
iter  40 value 196.940649
iter  50 value 112.728687
iter  60 value 100.100101
iter  70 value 94.853210
iter  80 value 90.228918
iter  90 value 89.149450
iter 100 value 87.812849
final  value 87.812849 
stopped after 100 iterations
# weights:  25
initial  value 9275.597028 
iter  10 value 370.941980
iter  20 value 286.649653
iter  30 value 168.818933
iter  40 value 111.051059
iter  50 value 91.527180
iter  60 value 89.447547
iter  70 value 87.743917
iter  80 value 86.774072
iter  90 value 86.392198
iter 100 value 86.074995
final  value 86.074995 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0654; time: 0.2 min
[Tune-x] 8: size=6; decay=0.0017
# weights:  73
initial  value 2136.345392 
iter  10 value 442.526598
iter  20 value 213.048935
iter  30 value 133.326403
iter  40 value 98.110728
iter  50 value 88.881218
iter  60 value 86.858623
iter  70 value 86.056592
iter  80 value 85.921857
iter  90 value 85.817118
iter 100 value 85.753384
final  value 85.753384 
stopped after 100 iterations
# weights:  73
initial  value 6460.875097 
iter  10 value 394.680244
iter  20 value 138.118646
iter  30 value 110.701781
iter  40 value 95.977557
iter  50 value 91.291076
iter  60 value 87.911587
iter  70 value 87.207171
iter  80 value 87.057050
iter  90 value 86.975816
iter 100 value 86.886725
final  value 86.886725 
stopped after 100 iterations
# weights:  73
initial  value 23085.854259 
iter  10 value 498.489941
iter  20 value 199.841169
iter  30 value 139.833306
iter  40 value 109.513258
iter  50 value 99.728040
iter  60 value 91.271770
iter  70 value 86.800954
iter  80 value 86.153635
iter  90 value 85.964971
iter 100 value 85.855484
final  value 85.855484 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.065; time: 0.4 min
[Tune-x] 9: size=20; decay=0.0145
# weights:  241
initial  value 4804.039377 
iter  10 value 371.093252
iter  20 value 220.395129
iter  30 value 124.207815
iter  40 value 102.281323
iter  50 value 94.035946
iter  60 value 91.018441
iter  70 value 89.959518
iter  80 value 88.948247
iter  90 value 88.279059
iter 100 value 87.816521
final  value 87.816521 
stopped after 100 iterations
# weights:  241
initial  value 17940.800776 
iter  10 value 360.214832
iter  20 value 163.734863
iter  30 value 113.631210
iter  40 value 99.894163
iter  50 value 93.535409
iter  60 value 90.060977
iter  70 value 88.902675
iter  80 value 88.420545
iter  90 value 88.152767
iter 100 value 87.975010
final  value 87.975010 
stopped after 100 iterations
# weights:  241
initial  value 54998.788597 
iter  10 value 547.147805
iter  20 value 189.213594
iter  30 value 113.320890
iter  40 value 97.566418
iter  50 value 89.962740
iter  60 value 88.571279
iter  70 value 87.724719
iter  80 value 87.323313
iter  90 value 87.038350
iter 100 value 86.875761
final  value 86.875761 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0651; time: 1.6 min
[Tune-x] 10: size=12; decay=0.000808
# weights:  145
initial  value 6503.003934 
iter  10 value 462.200136
iter  20 value 162.450452
iter  30 value 114.648246
iter  40 value 92.768086
iter  50 value 88.156259
iter  60 value 86.282706
iter  70 value 85.720810
iter  80 value 85.468361
iter  90 value 85.388038
iter 100 value 85.321234
final  value 85.321234 
stopped after 100 iterations
# weights:  145
initial  value 6856.585494 
iter  10 value 474.135877
iter  20 value 149.338166
iter  30 value 109.870019
iter  40 value 93.001754
iter  50 value 89.008692
iter  60 value 87.444688
iter  70 value 86.798815
iter  80 value 86.504168
iter  90 value 86.429314
iter 100 value 86.399524
final  value 86.399524 
stopped after 100 iterations
# weights:  145
initial  value 2224.001957 
iter  10 value 389.159724
iter  20 value 134.111441
iter  30 value 106.471334
iter  40 value 92.857162
iter  50 value 88.310757
iter  60 value 86.685095
iter  70 value 86.035807
iter  80 value 85.735423
iter  90 value 85.521986
iter 100 value 85.394749
final  value 85.394749 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0651; time: 0.9 min
[Tune-x] 11: size=8; decay=0.0764
# weights:  97
initial  value 113941.062345 
iter  10 value 503.908383
iter  20 value 228.012464
iter  30 value 161.123257
iter  40 value 114.409399
iter  50 value 102.447397
iter  60 value 96.956261
iter  70 value 94.270047
iter  80 value 93.454211
iter  90 value 92.833565
iter 100 value 92.364886
final  value 92.364886 
stopped after 100 iterations
# weights:  97
initial  value 5526.013018 
iter  10 value 461.701719
iter  20 value 202.189897
iter  30 value 153.563243
iter  40 value 125.088883
iter  50 value 110.680324
iter  60 value 100.166240
iter  70 value 97.042868
iter  80 value 95.731742
iter  90 value 94.773164
iter 100 value 93.943389
final  value 93.943389 
stopped after 100 iterations
# weights:  97
initial  value 2770.932206 
iter  10 value 511.138281
iter  20 value 132.739416
iter  30 value 103.768553
iter  40 value 95.211387
iter  50 value 92.468987
iter  60 value 91.797007
iter  70 value 91.285212
iter  80 value 90.841668
iter  90 value 90.323647
iter 100 value 89.911352
final  value 89.911352 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0653; time: 0.6 min
[Tune-x] 12: size=3; decay=0.0425
# weights:  37
initial  value 2906.372630 
iter  10 value 606.854033
iter  20 value 333.231355
iter  30 value 168.109673
iter  40 value 137.151143
iter  50 value 101.844300
iter  60 value 94.652867
iter  70 value 93.176243
iter  80 value 92.442558
iter  90 value 90.914936
iter 100 value 89.714192
final  value 89.714192 
stopped after 100 iterations
# weights:  37
initial  value 26075.186238 
iter  10 value 474.532116
iter  20 value 246.217381
iter  30 value 190.502298
iter  40 value 151.904354
iter  50 value 108.276477
iter  60 value 99.535139
iter  70 value 97.584004
iter  80 value 96.358219
iter  90 value 94.587954
iter 100 value 93.216606
final  value 93.216606 
stopped after 100 iterations
# weights:  37
initial  value 3067.792208 
iter  10 value 797.706730
iter  20 value 414.527210
iter  30 value 257.840136
iter  40 value 150.923945
iter  50 value 117.519959
iter  60 value 104.145012
iter  70 value 97.009998
iter  80 value 94.716903
iter  90 value 92.652493
iter 100 value 91.487134
final  value 91.487134 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0654; time: 0.2 min
[Tune-x] 13: size=19; decay=0.0399
# weights:  229
initial  value 4972.896542 
iter  10 value 427.786410
iter  20 value 167.367577
iter  30 value 123.157021
iter  40 value 102.899133
iter  50 value 94.669817
iter  60 value 92.715627
iter  70 value 91.031990
iter  80 value 90.224636
iter  90 value 89.493962
iter 100 value 89.056129
final  value 89.056129 
stopped after 100 iterations
# weights:  229
initial  value 11984.018993 
iter  10 value 422.780822
iter  20 value 168.671291
iter  30 value 122.956162
iter  40 value 107.889664
iter  50 value 98.758216
iter  60 value 95.251287
iter  70 value 93.337969
iter  80 value 92.059385
iter  90 value 91.373996
iter 100 value 90.911888
final  value 90.911888 
stopped after 100 iterations
# weights:  229
initial  value 16105.217259 
iter  10 value 421.681168
iter  20 value 133.387035
iter  30 value 103.779919
iter  40 value 96.046646
iter  50 value 92.589692
iter  60 value 90.504744
iter  70 value 89.662229
iter  80 value 88.807172
iter  90 value 88.470914
iter 100 value 88.226061
final  value 88.226061 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0652; time: 1.5 min
[Tune-x] 14: size=16; decay=0.00805
# weights:  193
initial  value 28074.672190 
iter  10 value 575.799259
iter  20 value 159.810519
iter  30 value 112.569210
iter  40 value 95.314326
iter  50 value 91.415065
iter  60 value 89.303059
iter  70 value 88.400887
iter  80 value 87.770890
iter  90 value 87.328597
iter 100 value 86.949202
final  value 86.949202 
stopped after 100 iterations
# weights:  193
initial  value 2757.879356 
iter  10 value 468.407246
iter  20 value 160.972052
iter  30 value 115.089190
iter  40 value 98.243636
iter  50 value 92.633547
iter  60 value 89.746796
iter  70 value 88.402443
iter  80 value 87.992646
iter  90 value 87.704927
iter 100 value 87.504788
final  value 87.504788 
stopped after 100 iterations
# weights:  193
initial  value 101020.924722 
iter  10 value 596.154770
iter  20 value 141.253051
iter  30 value 107.173423
iter  40 value 95.194337
iter  50 value 90.915669
iter  60 value 88.651926
iter  70 value 87.427443
iter  80 value 86.837177
iter  90 value 86.490624
iter 100 value 86.325138
final  value 86.325138 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0653; time: 1.3 min
[Tune-x] 15: size=20; decay=1.07e-05
# weights:  241
initial  value 22319.154732 
iter  10 value 435.499114
iter  20 value 179.325257
iter  30 value 123.595447
iter  40 value 99.476368
iter  50 value 90.132468
iter  60 value 86.695398
iter  70 value 85.691117
iter  80 value 85.435590
iter  90 value 85.259490
iter 100 value 85.130612
final  value 85.130612 
stopped after 100 iterations
# weights:  241
initial  value 33189.281646 
iter  10 value 380.428181
iter  20 value 169.635680
iter  30 value 112.086251
iter  40 value 99.056828
iter  50 value 90.751579
iter  60 value 88.197230
iter  70 value 86.967681
iter  80 value 86.527612
iter  90 value 86.344914
iter 100 value 86.198399
final  value 86.198399 
stopped after 100 iterations
# weights:  241
initial  value 26858.520259 
iter  10 value 497.744833
iter  20 value 286.874200
iter  30 value 171.659622
iter  40 value 116.083633
iter  50 value 96.124536
iter  60 value 89.843768
iter  70 value 87.289274
iter  80 value 86.163419
iter  90 value 85.614803
iter 100 value 85.297422
final  value 85.297422 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0651; time: 1.6 min
[Tune-x] 16: size=11; decay=0.00141
# weights:  133
initial  value 2227.171583 
iter  10 value 450.090046
iter  20 value 195.317128
iter  30 value 144.968626
iter  40 value 122.268449
iter  50 value 101.486114
iter  60 value 90.384968
iter  70 value 87.746643
iter  80 value 86.473376
iter  90 value 85.905209
iter 100 value 85.702644
final  value 85.702644 
stopped after 100 iterations
# weights:  133
initial  value 4943.199740 
iter  10 value 322.474944
iter  20 value 175.758922
iter  30 value 125.660578
iter  40 value 102.471821
iter  50 value 92.734796
iter  60 value 88.434600
iter  70 value 87.338060
iter  80 value 86.769224
iter  90 value 86.550650
iter 100 value 86.436204
final  value 86.436204 
stopped after 100 iterations
# weights:  133
initial  value 13247.494504 
iter  10 value 808.772725
iter  20 value 314.666231
iter  30 value 168.288558
iter  40 value 121.174943
iter  50 value 106.313042
iter  60 value 97.654362
iter  70 value 92.479993
iter  80 value 89.233319
iter  90 value 87.539581
iter 100 value 87.088325
final  value 87.088325 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0652; time: 0.8 min
[Tune-x] 17: size=2; decay=0.0214
# weights:  25
initial  value 11120.933611 
iter  10 value 500.122098
iter  20 value 335.829439
iter  30 value 213.249692
iter  40 value 146.544172
iter  50 value 124.345707
iter  60 value 117.872767
iter  70 value 107.070198
iter  80 value 96.638338
iter  90 value 88.785906
iter 100 value 88.285210
final  value 88.285210 
stopped after 100 iterations
# weights:  25
initial  value 46617.933672 
iter  10 value 744.665445
iter  20 value 405.722820
iter  30 value 290.830075
iter  40 value 170.915158
iter  50 value 91.634565
iter  60 value 89.398796
iter  70 value 89.331684
iter  80 value 89.200087
iter  90 value 89.013391
iter 100 value 88.831266
final  value 88.831266 
stopped after 100 iterations
# weights:  25
initial  value 23140.314688 
iter  10 value 933.990354
iter  20 value 606.440702
iter  30 value 441.447485
iter  40 value 238.895954
iter  50 value 178.607059
iter  60 value 148.638898
iter  70 value 130.778009
iter  80 value 109.293539
iter  90 value 90.764145
iter 100 value 88.581325
final  value 88.581325 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0652; time: 0.2 min
[Tune-x] 18: size=4; decay=0.000866
# weights:  49
initial  value 2294.071787 
iter  10 value 607.583279
iter  20 value 292.713879
iter  30 value 214.927931
iter  40 value 136.905147
iter  50 value 113.019951
iter  60 value 94.619373
iter  70 value 91.934100
iter  80 value 89.052348
iter  90 value 88.198570
iter 100 value 87.716321
final  value 87.716321 
stopped after 100 iterations
# weights:  49
initial  value 28055.753562 
iter  10 value 577.196019
iter  20 value 277.891921
iter  30 value 190.574265
iter  40 value 154.026279
iter  50 value 105.958888
iter  60 value 96.833056
iter  70 value 90.159855
iter  80 value 88.025374
iter  90 value 87.324917
iter 100 value 87.035749
final  value 87.035749 
stopped after 100 iterations
# weights:  49
initial  value 20275.607488 
iter  10 value 566.404522
iter  20 value 201.514812
iter  30 value 158.264086
iter  40 value 136.218037
iter  50 value 121.449275
iter  60 value 106.547115
iter  70 value 95.944008
iter  80 value 92.377414
iter  90 value 89.155339
iter 100 value 87.694374
final  value 87.694374 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0654; time: 0.3 min
[Tune-x] 19: size=20; decay=0.00161
# weights:  241
initial  value 81831.847921 
iter  10 value 334.157272
iter  20 value 118.739809
iter  30 value 101.700920
iter  40 value 89.798292
iter  50 value 87.103307
iter  60 value 86.108632
iter  70 value 85.678153
iter  80 value 85.468224
iter  90 value 85.317787
iter 100 value 85.233508
final  value 85.233508 
stopped after 100 iterations
# weights:  241
initial  value 32352.639688 
iter  10 value 543.349575
iter  20 value 186.897481
iter  30 value 126.181627
iter  40 value 101.938786
iter  50 value 92.136685
iter  60 value 89.043837
iter  70 value 87.747811
iter  80 value 87.065337
iter  90 value 86.773166
iter 100 value 86.581674
final  value 86.581674 
stopped after 100 iterations
# weights:  241
initial  value 20076.941758 
iter  10 value 472.956836
iter  20 value 229.495488
iter  30 value 131.769887
iter  40 value 100.614518
iter  50 value 89.324593
iter  60 value 86.537192
iter  70 value 85.965950
iter  80 value 85.551389
iter  90 value 85.429440
iter 100 value 85.359280
final  value 85.359280 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0652; time: 1.6 min
[Tune-x] 20: size=16; decay=1.67
# weights:  193
initial  value 1848.710432 
iter  10 value 525.338008
iter  20 value 248.638879
iter  30 value 223.569257
iter  40 value 203.267725
iter  50 value 182.398645
iter  60 value 162.138670
iter  70 value 154.840191
iter  80 value 150.761123
iter  90 value 147.768353
iter 100 value 145.019236
final  value 145.019236 
stopped after 100 iterations
# weights:  193
initial  value 3604.644923 
iter  10 value 628.131981
iter  20 value 384.208654
iter  30 value 306.381413
iter  40 value 264.301851
iter  50 value 210.769831
iter  60 value 184.484640
iter  70 value 174.645473
iter  80 value 167.454468
iter  90 value 160.531621
iter 100 value 156.034655
final  value 156.034655 
stopped after 100 iterations
# weights:  193
initial  value 12111.017476 
iter  10 value 543.161603
iter  20 value 240.891132
iter  30 value 209.808663
iter  40 value 186.149287
iter  50 value 170.043461
iter  60 value 158.171675
iter  70 value 154.818655
iter  80 value 150.195792
iter  90 value 146.310994
iter 100 value 144.006007
final  value 144.006007 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0688; time: 1.4 min
[Tune] Result: size=6; decay=0.0017 : rmse.test.rmse=0.065
# weights:  73
initial  value 3888.143183 
iter  10 value 705.895089
iter  20 value 317.991046
iter  30 value 211.077516
iter  40 value 150.408883
iter  50 value 138.464021
iter  60 value 131.183276
iter  70 value 129.867405
iter  80 value 129.534118
iter  90 value 129.338141
iter 100 value 129.185311
final  value 129.185311 
stopped after 100 iterations
[1] "Fri Feb 09 08:06:57 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081Warning in train(allmodel, regr.task) :
  Could not train learner regr.nodeHarvest: Error : cannot allocate vector of size 252.2 Mb

[1] "Fri Feb 09 08:10:45 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 08:10:52 2018"
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 08:11:56 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
<simpleError: cannot allocate vector of size 233.2 Mb>
Warning in train(allmodel, regr.task) :
  Could not train learner regr.randomForestSRC: Error in randomForestSRC::rfsrc(f, data = getTaskData(.task, .subset),  : 
  An error has occurred in the grow algorithm.  Please turn trace on for further analysis.

[1] "Fri Feb 09 08:12:03 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3 1 to 10   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=4; min.node.size=2
[Tune-y] 1: rmse.test.rmse=0.0683; time: 3.4 min
[Tune-x] 2: mtry=4; min.node.size=1
[Tune-y] 2: rmse.test.rmse=0.0684; time: 3.1 min
[Tune-x] 3: mtry=2; min.node.size=2
[Tune-y] 3: rmse.test.rmse=0.089; time: 1.4 min
[Tune-x] 4: mtry=2; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0892; time: 1.4 min
[Tune-x] 5: mtry=1; min.node.size=4
[Tune-y] 5: rmse.test.rmse=0.183; time: 0.6 min
[Tune-x] 6: mtry=4; min.node.size=5
[Tune-y] 6: rmse.test.rmse=0.0676; time: 2.7 min
[Tune-x] 7: mtry=1; min.node.size=2
[Tune-y] 7: rmse.test.rmse=0.181; time: 0.6 min
[Tune-x] 8: mtry=3; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0687; time: 2.2 min
[Tune-x] 9: mtry=10; min.node.size=6
[Tune-y] 9: rmse.test.rmse=0.07; time: 5.0 min
[Tune-x] 10: mtry=6; min.node.size=4
[Tune-y] 10: rmse.test.rmse=0.0696; time: 3.8 min
[Tune-x] 11: mtry=4; min.node.size=7
[Tune-y] 11: rmse.test.rmse=0.0672; time: 2.6 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0892; time: 1.3 min
[Tune-x] 13: mtry=10; min.node.size=7
[Tune-y] 13: rmse.test.rmse=0.0695; time: 4.9 min
[Tune-x] 14: mtry=8; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.07; time: 4.5 min
[Tune-x] 15: mtry=10; min.node.size=1
[Tune-y] 15: rmse.test.rmse=0.0729; time: 6.7 min
[Tune-x] 16: mtry=6; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0696; time: 3.9 min
[Tune-x] 17: mtry=1; min.node.size=6
[Tune-y] 17: rmse.test.rmse=0.181; time: 0.6 min
[Tune-x] 18: mtry=2; min.node.size=4
[Tune-y] 18: rmse.test.rmse=0.0895; time: 1.4 min
[Tune-x] 19: mtry=10; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0711; time: 5.6 min
[Tune-x] 20: mtry=8; min.node.size=9
[Tune-y] 20: rmse.test.rmse=0.0684; time: 3.9 min
[Tune] Result: mtry=4; min.node.size=7 : rmse.test.rmse=0.0672
[1] "Fri Feb 09 09:13:08 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Fri Feb 09 09:13:16 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.0114; maxdepth=6; minbucket=20; minsplit=5
[Tune-y] 1: rmse.test.rmse=0.0993; time: 0.0 min
[Tune-x] 2: cp=0.00291; maxdepth=6; minbucket=12; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.0855; time: 0.0 min
[Tune-x] 3: cp=0.00139; maxdepth=13; minbucket=23; minsplit=24
[Tune-y] 3: rmse.test.rmse=0.0737; time: 0.0 min
[Tune-x] 4: cp=0.00186; maxdepth=8; minbucket=17; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.0777; time: 0.0 min
[Tune-x] 5: cp=0.739; maxdepth=17; minbucket=31; minsplit=19
[Tune-y] 5: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 6: cp=0.0151; maxdepth=21; minbucket=10; minsplit=32
[Tune-y] 6: rmse.test.rmse=0.109; time: 0.0 min
[Tune-x] 7: cp=0.598; maxdepth=19; minbucket=39; minsplit=27
[Tune-y] 7: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 8: cp=0.84; maxdepth=3; minbucket=29; minsplit=21
[Tune-y] 8: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 9: cp=0.00145; maxdepth=18; minbucket=12; minsplit=19
[Tune-y] 9: rmse.test.rmse=0.0745; time: 0.0 min
[Tune-x] 10: cp=0.931; maxdepth=13; minbucket=40; minsplit=45
[Tune-y] 10: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 11: cp=0.351; maxdepth=16; minbucket=46; minsplit=15
[Tune-y] 11: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 12: cp=0.0111; maxdepth=3; minbucket=12; minsplit=5
[Tune-y] 12: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 13: cp=0.0786; maxdepth=14; minbucket=40; minsplit=29
[Tune-y] 13: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 14: cp=0.00115; maxdepth=13; minbucket=10; minsplit=41
[Tune-y] 14: rmse.test.rmse=0.0722; time: 0.0 min
[Tune-x] 15: cp=0.0027; maxdepth=23; minbucket=18; minsplit=27
[Tune-y] 15: rmse.test.rmse=0.0855; time: 0.0 min
[Tune-x] 16: cp=0.177; maxdepth=30; minbucket=7; minsplit=48
[Tune-y] 16: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 17: cp=0.0119; maxdepth=30; minbucket=40; minsplit=39
[Tune-y] 17: rmse.test.rmse=0.101; time: 0.0 min
[Tune-x] 18: cp=0.00346; maxdepth=19; minbucket=50; minsplit=15
[Tune-y] 18: rmse.test.rmse=0.0855; time: 0.0 min
[Tune-x] 19: cp=0.00333; maxdepth=28; minbucket=12; minsplit=13
[Tune-y] 19: rmse.test.rmse=0.0855; time: 0.0 min
[Tune-x] 20: cp=0.00111; maxdepth=23; minbucket=32; minsplit=24
[Tune-y] 20: rmse.test.rmse=0.0722; time: 0.0 min
[Tune] Result: cp=0.00111; maxdepth=23; minbucket=32; minsplit=24 : rmse.test.rmse=0.0722
[1] "Fri Feb 09 09:13:37 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:13:42 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.rvm: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 09:13:49 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.69100 0.23100 0.07750 0.02590 0.00868
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 8 
Runtime: 1.769051 mins 

 Values of predicted responses: 
   index             3 
   lambda      0.07746 
    Y 1         0.7765 
    Y 2         0.6725 
    Y 3        0.09914 
    Y 4         0.6575 
    Y 5         0.7288 
[1] "Fri Feb 09 09:15:40 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=84; max_depth=2; eta=0.202; gamma=0.0999; colsample_bytree=0.363; min_child_weight=2.65; subsample=0.373
[Tune-y] 1: rmse.test.rmse=0.0978; time: 0.0 min
[Tune-x] 2: nrounds=107; max_depth=1; eta=0.236; gamma=3.94; colsample_bytree=0.466; min_child_weight=1.86; subsample=0.393
[Tune-y] 2: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 3: nrounds=48; max_depth=4; eta=0.574; gamma=5.27; colsample_bytree=0.528; min_child_weight=6.36; subsample=0.547
[Tune-y] 3: rmse.test.rmse=0.0932; time: 0.0 min
[Tune-x] 4: nrounds=483; max_depth=2; eta=0.363; gamma=9.26; colsample_bytree=0.54; min_child_weight=15.1; subsample=0.613
[Tune-y] 4: rmse.test.rmse=0.106; time: 0.2 min
[Tune-x] 5: nrounds=3.44e+03; max_depth=1; eta=0.325; gamma=3.58; colsample_bytree=0.323; min_child_weight=11.1; subsample=0.376
[Tune-y] 5: rmse.test.rmse=0.16; time: 0.8 min
[Tune-x] 6: nrounds=69; max_depth=10; eta=0.221; gamma=7.73; colsample_bytree=0.648; min_child_weight=17; subsample=0.6
[Tune-y] 6: rmse.test.rmse= 0.1; time: 0.1 min
[Tune-x] 7: nrounds=2.33e+03; max_depth=3; eta=0.211; gamma=0.292; colsample_bytree=0.364; min_child_weight=0.119; subsample=0.725
[Tune-y] 7: rmse.test.rmse=0.0672; time: 1.0 min
[Tune-x] 8: nrounds=128; max_depth=8; eta=0.325; gamma=0.24; colsample_bytree=0.454; min_child_weight=2.27; subsample=0.846
[Tune-y] 8: rmse.test.rmse=0.0677; time: 0.1 min
[Tune-x] 9: nrounds=24; max_depth=8; eta=0.183; gamma=5; colsample_bytree=0.6; min_child_weight=19.4; subsample=0.293
[Tune-y] 9: rmse.test.rmse=0.107; time: 0.0 min
[Tune-x] 10: nrounds=2.92e+03; max_depth=4; eta=0.6; gamma=7.73; colsample_bytree=0.603; min_child_weight=3.65; subsample=0.695
[Tune-y] 10: rmse.test.rmse=0.0883; time: 1.7 min
[Tune-x] 11: nrounds=3.8e+03; max_depth=3; eta=0.107; gamma=8.95; colsample_bytree=0.366; min_child_weight=3.82; subsample=0.264
[Tune-y] 11: rmse.test.rmse=0.129; time: 1.5 min
[Tune-x] 12: nrounds=742; max_depth=7; eta=0.252; gamma=7.44; colsample_bytree=0.43; min_child_weight=6.56; subsample=0.704
[Tune-y] 12: rmse.test.rmse=0.0938; time: 0.6 min
[Tune-x] 13: nrounds=33; max_depth=4; eta=0.067; gamma=4.68; colsample_bytree=0.667; min_child_weight=13.9; subsample=0.669
[Tune-y] 13: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 14: nrounds=1.43e+03; max_depth=3; eta=0.426; gamma=7.6; colsample_bytree=0.51; min_child_weight=16.6; subsample=0.648
[Tune-y] 14: rmse.test.rmse=0.0951; time: 0.7 min
[Tune-x] 15: nrounds=21; max_depth=5; eta=0.166; gamma=5.5; colsample_bytree=0.481; min_child_weight=12.2; subsample=0.404
[Tune-y] 15: rmse.test.rmse=0.145; time: 0.0 min
[Tune-x] 16: nrounds=61; max_depth=9; eta=0.54; gamma=6.82; colsample_bytree=0.392; min_child_weight=3.06; subsample=0.741
[Tune-y] 16: rmse.test.rmse=0.0955; time: 0.0 min
[Tune-x] 17: nrounds=3.18e+03; max_depth=9; eta=0.0883; gamma=0.147; colsample_bytree=0.465; min_child_weight=17.2; subsample=0.765
[Tune-y] 17: rmse.test.rmse=0.0663; time: 2.9 min
[Tune-x] 18: nrounds=19; max_depth=6; eta=0.187; gamma=9.74; colsample_bytree=0.682; min_child_weight=16.2; subsample=0.713
[Tune-y] 18: rmse.test.rmse=0.115; time: 0.0 min
[Tune-x] 19: nrounds=383; max_depth=8; eta=0.505; gamma=5.03; colsample_bytree=0.441; min_child_weight=10.7; subsample=0.355
[Tune-y] 19: rmse.test.rmse=0.0982; time: 0.3 min
[Tune-x] 20: nrounds=59; max_depth=1; eta=0.124; gamma=8.72; colsample_bytree=0.566; min_child_weight=1.73; subsample=0.502
[Tune-y] 20: rmse.test.rmse=0.172; time: 0.0 min
[Tune] Result: nrounds=3.18e+03; max_depth=9; eta=0.0883; gamma=0.147; colsample_bytree=0.465; min_child_weight=17.2; subsample=0.765 : rmse.test.rmse=0.0663
[1] "Fri Feb 09 09:27:02 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 09:27:08 2018"
<simpleError: cannot allocate vector of size 233.2 Mb>
Error in randomForestSRC::rfsrc(getTaskFormula(task), data = getTaskData(task),  : 
  An error has occurred in the grow algorithm.  Please turn trace on for further analysis.
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
**GROW** @depth 0: [7,0.499375], n=(20388,10188)
**GROW** @depth 1: [1,0], n=(10156,10272)
**PRUNE** @depth 1: [7,0]
**GROW** @depth 1: [5,0.498895], n=(10134,5052)
**GROW** @depth 1: [4,0.498015], n=(10178,5212)
**GROW** @depth 2: [4,0], n=(5108,5070)
**GROW** @depth 2: [6,0.499917], n=(3440,1638)
**GROW** @depth 2: [7,0.499375], n=(6688,3420)
**GROW** @depth 3: [3,0.499714], n=(3352,1718)
**GROW** @depth 3: [5,0], n=(3440,3284)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(3440,3284,3410,1669,3383,3473,3326,3379,5212)
**GROW** @depth 3: [4,0], n=(1782,1691)
**GROW** @depth 4: [6,0], n=(1725,1715)
**GROW** @depth 5: [7,0.499375], n=(1164,561)
**GROW** @depth 4: [3,0], n=(1782,1615)
**GROW** @depth 4: [2,0], n=(1129,2147)
**GROW** @depth 6: [7,0.499375], n=(574,561)
**GROW** @depth 3: [5,0], n=(1638,1772)
**GROW** @depth 3: [6,0.499917], n=(1135,534)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(590,574,561,1715,3284,1638,1772,1135,2215,1702,1157,1141,2304,2147,1711,1718,5212)

Sampling @ nn=0 pred locs:
**GROW** @depth 4: [2,0.502101], n=(1162,556)
**GROW** @depth 4: [6,0], n=(1676,1608)
**GROW** @depth 4: [6,0], n=(559,576)
**GROW** @depth 5: [2,0.502101], n=(1191,1113)
**GROW** @depth 3: [7,0.499375], n=(1124,578)
**GROW** @depth 6: [4,0], n=(602,589)
**GROW** @depth 4: [6,0], n=(562,545)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(590,574,561,1715,1676,1608,1638,1772,1101,1142,562,545,1124,578,1157,1141,602,589,1051,2209,1711,1162,556,5212)
**GROW** @depth 5: [2,0], n=(576,586)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(590,574,561,1715,1676,1608,1638,1772,1101,1142,562,545,1124,578,1157,1141,602,589,1051,2209,1141,1185,547,556,5212)
**GROW** @depth 4: [7,0.499375], n=(1119,519)
**GROW** @depth 5: [7,0], n=(559,542)
**GROW** @depth 5: [4,0], n=(599,586)
**GROW** @depth 2: [3,0.499714], n=(3418,1794)
**GROW** @depth 5: [7,0.499375], n=(1114,562)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(590,574,561,1715,1114,562,1608,1119,519,1772,559,542,1142,562,545,1124,578,1157,1141,602,589,1051,2209,1141,599,586,547,556,3418,1794)
**GROW** @depth 5: [7,0.499375], n=(1132,583)
**GROW** @depth 5: [3,0], n=(569,572)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(590,574,561,1132,583,1114,562,1608,1119,519,1772,559,542,1142,562,545,1124,578,1157,569,572,602,589,1051,2209,1141,599,586,547,556,3418,1794)
**GROW** @depth 6: [7,0], n=(569,545)
**GROW** @depth 3: [2,0], n=(1137,2281)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(590,574,561,1132,583,569,545,562,1608,1119,519,1772,559,542,1142,562,545,1124,578,1169,557,572,602,589,1051,2209,1141,599,586,547,556,1137,613,3462)
Grow: 9.019%, Prune: 0.3145%, Change: 1.473%, Swap: 13.11%

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.bcart: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 09:39:55 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 09:40:00 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.blm: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 09:42:06 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Number of parameters (weights and biases) to estimate: 24 
Nguyen-Widrow method
Scaling factor= 0.7000159 
gamma= 23.4691 	 alpha= 0.0638 	 beta= 78.1045 
[1] "Fri Feb 09 09:42:30 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:42:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

burn in:
r=1000 d=[0]; n=30576
r=2000 d=[0]; n=30576

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=30576
r=2000 d=[0]; mh=1 n=30576
r=3000 d=[0]; mh=1 n=30576
r=4000 d=[0]; mh=1 n=30576
r=5000 d=[0]; mh=1 n=30576
Grow: 0%, 

Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.btlm: Error : cannot allocate vector of size 792.5 Mb

[1] "Fri Feb 09 09:52:57 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:05 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:19 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:26 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:34 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:39 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:46 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:53:51 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.gausspr: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 09:53:59 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=33; interaction.depth=1; shrinkage=0.476; n.minobsinnode=7
[Tune-y] 1: rmse.test.rmse=2.39; time: 0.0 min
[Tune-x] 2: n.trees=272; interaction.depth=5; shrinkage=0.465; n.minobsinnode=15
[Tune-y] 2: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 3: n.trees=48; interaction.depth=8; shrinkage=0.598; n.minobsinnode=17
[Tune-y] 3: rmse.test.rmse=1.02; time: 0.1 min
[Tune-x] 4: n.trees=73; interaction.depth=4; shrinkage=0.468; n.minobsinnode=6
[Tune-y] 4: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 5: n.trees=29; interaction.depth=2; shrinkage=0.54; n.minobsinnode=16
[Tune-y] 5: rmse.test.rmse= 1.1; time: 0.0 min
[Tune-x] 6: n.trees=28; interaction.depth=8; shrinkage=0.305; n.minobsinnode=13
[Tune-y] 6: rmse.test.rmse=   1; time: 0.1 min
[Tune-x] 7: n.trees=160; interaction.depth=4; shrinkage=0.587; n.minobsinnode=12
[Tune-y] 7: rmse.test.rmse=1.01; time: 0.2 min
[Tune-x] 8: n.trees=163; interaction.depth=8; shrinkage=0.118; n.minobsinnode=24
[Tune-y] 8: rmse.test.rmse=   1; time: 0.4 min
[Tune-x] 9: n.trees=130; interaction.depth=8; shrinkage=0.575; n.minobsinnode=19
[Tune-y] 9: rmse.test.rmse=1.02; time: 0.3 min
[Tune-x] 10: n.trees=68; interaction.depth=4; shrinkage=0.27; n.minobsinnode=5
[Tune-y] 10: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 11: n.trees=176; interaction.depth=3; shrinkage=0.391; n.minobsinnode=15
[Tune-y] 11: rmse.test.rmse=   1; time: 0.2 min
[Tune-x] 12: n.trees=320; interaction.depth=8; shrinkage=0.4; n.minobsinnode=18
[Tune-y] 12: rmse.test.rmse=1.03; time: 0.8 min
[Tune-x] 13: n.trees=265; interaction.depth=7; shrinkage=0.545; n.minobsinnode=23
[Tune-y] 13: rmse.test.rmse=1.03; time: 0.6 min
[Tune-x] 14: n.trees=103; interaction.depth=1; shrinkage=0.0369; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=2.84; time: 0.1 min
[Tune-x] 15: n.trees=479; interaction.depth=6; shrinkage=0.207; n.minobsinnode=22
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.9 min
[Tune-x] 16: n.trees=30; interaction.depth=1; shrinkage=0.456; n.minobsinnode=17
[Tune-y] 16: rmse.test.rmse= 2.4; time: 0.0 min
[Tune-x] 17: n.trees=21; interaction.depth=5; shrinkage=0.0566; n.minobsinnode=8
[Tune-y] 17: rmse.test.rmse=2.08; time: 0.0 min
[Tune-x] 18: n.trees=28; interaction.depth=2; shrinkage=0.514; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=1.11; time: 0.0 min
[Tune-x] 19: n.trees=176; interaction.depth=8; shrinkage=0.447; n.minobsinnode=20
[Tune-y] 19: rmse.test.rmse=1.02; time: 0.4 min
[Tune-x] 20: n.trees=65; interaction.depth=8; shrinkage=0.412; n.minobsinnode=20
[Tune-y] 20: rmse.test.rmse=1.01; time: 0.2 min
[Tune] Result: n.trees=163; interaction.depth=8; shrinkage=0.118; n.minobsinnode=24 : rmse.test.rmse=   1
[1] "Fri Feb 09 09:59:17 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 09:59:22 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.261; lambda=0.00197
[Tune-y] 1: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 2: alpha=0.793; lambda=0.00279
[Tune-y] 2: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 3: alpha=0.717; lambda=0.0672
[Tune-y] 3: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 4: alpha=0.775; lambda=0.0732
[Tune-y] 4: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 5: alpha=0.341; lambda=1.13
[Tune-y] 5: rmse.test.rmse=2.73; time: 0.0 min
[Tune-x] 6: alpha=0.996; lambda=0.207
[Tune-y] 6: rmse.test.rmse=2.44; time: 0.0 min
[Tune-x] 7: alpha=0.433; lambda=0.0147
[Tune-y] 7: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 8: alpha=0.779; lambda=0.00157
[Tune-y] 8: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 9: alpha=0.23; lambda=0.00286
[Tune-y] 9: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 10: alpha=0.9; lambda=0.158
[Tune-y] 10: rmse.test.rmse=2.41; time: 0.0 min
[Tune-x] 11: alpha=0.223; lambda=0.695
[Tune-y] 11: rmse.test.rmse=2.49; time: 0.0 min
[Tune-x] 12: alpha=0.507; lambda=0.0351
[Tune-y] 12: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 13: alpha=0.603; lambda=0.024
[Tune-y] 13: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 14: alpha=0.977; lambda=0.0258
[Tune-y] 14: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 15: alpha=0.606; lambda=1.16
[Tune-y] 15: rmse.test.rmse=3.01; time: 0.0 min
[Tune-x] 16: alpha=0.195; lambda=4.8
[Tune-y] 16: rmse.test.rmse=3.65; time: 0.0 min
[Tune-x] 17: alpha=0.558; lambda=0.547
[Tune-y] 17: rmse.test.rmse=2.55; time: 0.0 min
[Tune-x] 18: alpha=0.958; lambda=0.59
[Tune-y] 18: rmse.test.rmse=2.77; time: 0.0 min
[Tune-x] 19: alpha=0.416; lambda=0.0194
[Tune-y] 19: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 20: alpha=0.449; lambda=0.0012
[Tune-y] 20: rmse.test.rmse=2.38; time: 0.0 min
[Tune] Result: alpha=0.793; lambda=0.00279 : rmse.test.rmse=2.38
[1] "Fri Feb 09 09:59:36 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=====================                                                 |  30%  |                                                                              |===================================                                   |  50%  |                                                                              |=================================================                     |  70%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 10:00:32 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |============================================================          |  86%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 10:00:46 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   2%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 10:00:57 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |=======                                                               |  10%  |                                                                              |==========                                                            |  14%  |                                                                              |==============                                                        |  20%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |========================                                              |  34%  |                                                                              |===========================                                           |  38%  |                                                                              |===============================                                       |  44%  |                                                                              |===================================                                   |  50%  |                                                                              |=======================================                               |  56%  |                                                                              |==========================================                            |  60%  |                                                                              |==============================================                        |  66%  |                                                                              |==================================================                    |  72%  |                                                                              |=======================================================               |  78%  |                                                                              |===========================================================           |  84%  |                                                                              |==============================================================        |  88%  |                                                                              |==================================================================    |  94%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 10:01:28 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 10:02:14 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
i = 1 (of 10192), d = 10, its = 52
i = 2 (of 10192), d = 3.88729, its = 7
i = 3 (of 10192), d = 10, its = 52
i = 4 (of 10192), d = 1.04924, its = 19
i = 5 (of 10192), d = 10, its = 41
i = 6 (of 10192), d = 10, its = 54
i = 7 (of 10192), d = 10, its = 46
i = 8 (of 10192), d = 2.20836, its = 4
i = 9 (of 10192), d = 10, its = 55
i = 10 (of 10192), d = 10, its = 52
i = 11 (of 10192), d = 10, its = 50
i = 12 (of 10192), d = 10, its = 52
i = 13 (of 10192), d = 10, its = 50
i = 14 (of 10192), d = 10, its = 55
i = 15 (of 10192), d = 10, its = 38
i = 16 (of 10192), d = 10, its = 57
i = 17 (of 10192), d = 10, its = 37
i = 18 (of 10192), d = 10, its = 40
i = 19 (of 10192), d = 10, its = 53
i = 20 (of 10192), d = 10, its = 52
i = 21 (of 10192), d = 10, its = 37
i = 22 (of 10192), d = 0.772473, its = 20
i = 23 (of 10192), d = 10, its = 41
i = 24 (of 10192), d = 10, its = 37
i = 25 (of 10192), d = 10, its = 53
i = 26 (of 10192), d = 10, its = 37
i = 27 (of 10192), d = 10, its = 54
i = 28 (of 10192), d = 1.0691, its = 16
i = 29 (of 10192), d = 0.915524, its = 21
i = 30 (of 10192), d = 10, its = 53
i = 31 (of 10192), d = 10, its = 50
i = 32 (of 10192), d = 2.24843, its = 4
i = 33 (of 10192), d = 10, its = 53
i = 34 (of 10192), d = 4.5896, its = 7
i = 35 (of 10192), d = 1.20927, its = 18
i = 36 (of 10192), d = 10, its = 52
i = 37 (of 10192), d = 1.57733, its = 6
i = 38 (of 10192), d = 10, its = 37
i = 39 (of 10192), d = 10, its = 55
i = 40 (of 10192), d = 10, its = 52
i = 41 (of 10192), d = 10, its = 53
i = 42 (of 10192), d = 10, its = 50
i = 43 (of 10192), d = 10, its = 54
i = 44 (of 10192), d = 10, its = 50
i = 45 (of 10192), d = 10, its = 52
i = 46 (of 10192), d = 10, its = 52
i = 47 (of 10192), d = 1.13944, its = 17
i = 48 (of 10192), d = 10, its = 37
i = 49 (of 10192), d = 10, its = 55
i = 50 (of 10192), d = 10, its = 52
i = 51 (of 10192), d = 10, its = 39
i = 52 (of 10192), d = 10, its = 48
i = 53 (of 10192), d = 10, its = 51
i = 54 (of 10192), d = 0.851723, its = 16
i = 55 (of 10192), d = 10, its = 52
i = 56 (of 10192), d = 1.95913, its = 3
i = 57 (of 10192), d = 10, its = 53
i = 58 (of 10192), d = 10, its = 50
i = 59 (of 10192), d = 10, its = 55
i = 60 (of 10192), d = 10, its = 56
i = 61 (of 10192), d = 10, its = 37
i = 62 (of 10192), d = 10, its = 43
i = 63 (of 10192), d = 10, its = 48
i = 64 (of 10192), d = 1.52854, its = 6
i = 65 (of 10192), d = 1.33914, its = 6
i = 66 (of 10192), d = 10, its = 50
i = 67 (of 10192), d = 1.90923, its = 4
i = 68 (of 10192), d = 10, its = 40
i = 69 (of 10192), d = 10, its = 52
i = 70 (of 10192), d = 10, its = 55
i = 71 (of 10192), d = 10, its = 52
i = 72 (of 10192), d = 10, its = 49
i = 73 (of 10192), d = 10, its = 37
i = 74 (of 10192), d = 10, its = 60
i = 75 (of 10192), d = 10, its = 53
i = 76 (of 10192), d = 10, its = 54
i = 77 (of 10192), d = 10, its = 37
i = 78 (of 10192), d = 10, its = 55
i = 79 (of 10192), d = 3.67287, its = 6
i = 80 (of 10192), d = 10, its = 38
i = 81 (of 10192), d = 10, its = 52
i = 82 (of 10192), d = 2.9998, its = 6
i = 83 (of 10192), d = 1.10325, its = 17
i = 84 (of 10192), d = 10, its = 41
i = 85 (of 10192), d = 10, its = 39
i = 86 (of 10192), d = 10, its = 47
i = 87 (of 10192), d = 10, its = 57
i = 88 (of 10192), d = 2.04061, its = 3
i = 89 (of 10192), d = 10, its = 57
i = 90 (of 10192), d = 10, its = 41
i = 91 (of 10192), d = 10, its = 37
i = 92 (of 10192), d = 2.27825, its = 4
i = 93 (of 10192), d = 0.579701, its = 22
i = 94 (of 10192), d = 10, its = 55
i = 95 (of 10192), d = 10, its = 53
i = 96 (of 10192), d = 1.83013, its = 4
i = 97 (of 10192), d = 10, its = 40
i = 98 (of 10192), d = 4.45512, its = 9
i = 99 (of 10192), d = 10, its = 53
i = 100 (of 10192), d = 10, its = 53
i = 101 (of 10192), d = 10, its = 40
i = 102 (of 10192), d = 10, its = 37
i = 103 (of 10192), d = 10, its = 37
i = 104 (of 10192), d = 10, its = 57
i = 105 (of 10192), d = 10, its = 54
i = 106 (of 10192), d = 10, its = 53
i = 107 (of 10192), d = 10, its = 53
i = 108 (of 10192), d = 2.22235, its = 4
i = 109 (of 10192), d = 10, its = 51
i = 110 (of 10192), d = 10, its = 51
i = 111 (of 10192), d = 10, its = 37
i = 112 (of 10192), d = 10, its = 51
i = 113 (of 10192), d = 10, its = 52
i = 114 (of 10192), d = 2.12527, its = 4
i = 115 (of 10192), d = 10, its = 55
i = 116 (of 10192), d = 10, its = 53
i = 117 (of 10192), d = 10, its = 49
i = 118 (of 10192), d = 10, its = 53
i = 119 (of 10192), d = 10, its = 51
i = 120 (of 10192), d = 10, its = 37
i = 121 (of 10192), d = 10, its = 53
i = 122 (of 10192), d = 10, its = 56
i = 123 (of 10192), d = 10, its = 52
i = 124 (of 10192), d = 0.917576, its = 21
i = 125 (of 10192), d = 10, its = 53
i = 126 (of 10192), d = 10, its = 57
i = 127 (of 10192), d = 10, its = 54
i = 128 (of 10192), d = 10, its = 51
i = 129 (of 10192), d = 10, its = 37
i = 130 (of 10192), d = 2.65622, its = 5
i = 131 (of 10192), d = 10, its = 50
i = 132 (of 10192), d = 10, its = 56
i = 133 (of 10192), d = 2.37192, its = 5
i = 134 (of 10192), d = 10, its = 52
i = 135 (of 10192), d = 10, its = 56
i = 136 (of 10192), d = 10, its = 54
i = 137 (of 10192), d = 10, its = 46
i = 138 (of 10192), d = 10, its = 37
i = 139 (of 10192), d = 10, its = 52
i = 140 (of 10192), d = 10, its = 51
i = 141 (of 10192), d = 10, its = 50
i = 142 (of 10192), d = 10, its = 54
i = 143 (of 10192), d = 10, its = 37
i = 144 (of 10192), d = 1.46151, its = 7
i = 145 (of 10192), d = 10, its = 52
i = 146 (of 10192), d = 10, its = 52
i = 147 (of 10192), d = 10, its = 53
i = 148 (of 10192), d = 10, its = 50
i = 149 (of 10192), d = 10, its = 55
i = 150 (of 10192), d = 10, its = 52
i = 151 (of 10192), d = 10, its = 49
i = 152 (of 10192), d = 10, its = 58
i = 153 (of 10192), d = 0.568051, its = 17
i = 154 (of 10192), d = 10, its = 41
i = 155 (of 10192), d = 10, its = 54
i = 156 (of 10192), d = 10, its = 54
i = 157 (of 10192), d = 10, its = 59
i = 158 (of 10192), d = 10, its = 37
i = 159 (of 10192), d = 2.5364, its = 5
i = 160 (of 10192), d = 10, its = 51
i = 161 (of 10192), d = 3.32521, its = 6
i = 162 (of 10192), d = 10, its = 56
i = 163 (of 10192), d = 10, its = 51
i = 164 (of 10192), d = 10, its = 37
i = 165 (of 10192), d = 10, its = 56
i = 166 (of 10192), d = 10, its = 49
i = 167 (of 10192), d = 10, its = 53
i = 168 (of 10192), d = 10, its = 41
i = 169 (of 10192), d = 10, its = 37
i = 170 (of 10192), d = 10, its = 51
i = 171 (of 10192), d = 10, its = 54
i = 172 (of 10192), d = 10, its = 53
i = 173 (of 10192), d = 10, its = 37
i = 174 (of 10192), d = 10, its = 40
i = 175 (of 10192), d = 10, its = 52
i = 176 (of 10192), d = 10, its = 37
i = 177 (of 10192), d = 10, its = 40
i = 178 (of 10192), d = 1.57817, its = 6
i = 179 (of 10192), d = 10, its = 53
i = 180 (of 10192), d = 2.19654, its = 4
i = 181 (of 10192), d = 10, its = 37
i = 182 (of 10192), d = 10, its = 49
i = 183 (of 10192), d = 3.13488, its = 6
i = 184 (of 10192), d = 2.38223, its = 5
i = 185 (of 10192), d = 10, its = 52
i = 186 (of 10192), d = 10, its = 41
i = 187 (of 10192), d = 10, its = 41
i = 188 (of 10192), d = 5.04891, its = 8
i = 189 (of 10192), d = 2.93374, its = 6
i = 190 (of 10192), d = 0.895263, its = 24
i = 191 (of 10192), d = 10, its = 51
i = 192 (of 10192), d = 10, its = 54
i = 193 (of 10192), d = 10, its = 57
i = 194 (of 10192), d = 10, its = 54
i = 195 (of 10192), d = 10, its = 37
i = 196 (of 10192), d = 1.23671, its = 6
i = 197 (of 10192), d = 1.45239, its = 7
i = 198 (of 10192), d = 10, its = 58
i = 199 (of 10192), d = 10, its = 49
i = 200 (of 10192), d = 10, its = 54
i = 201 (of 10192), d = 10, its = 48
i = 202 (of 10192), d = 1.91782, its = 5
i = 203 (of 10192), d = 10, its = 52
i = 204 (of 10192), d = 10, its = 49
i = 205 (of 10192), d = 10, its = 54
i = 206 (of 10192), d = 10, its = 55
i = 207 (of 10192), d = 10, its = 39
i = 208 (of 10192), d = 10, its = 53
i = 209 (of 10192), d = 0.858121, its = 20
i = 210 (of 10192), d = 10, its = 46
i = 211 (of 10192), d = 10, its = 54
i = 212 (of 10192), d = 1.48311, its = 6
i = 213 (of 10192), d = 10, its = 54
i = 214 (of 10192), d = 1.43205, its = 5
i = 215 (of 10192), d = 10, its = 37
i = 216 (of 10192), d = 10, its = 55
i = 217 (of 10192), d = 10, its = 51
i = 218 (of 10192), d = 10, its = 52
i = 219 (of 10192), d = 10, its = 57
i = 220 (of 10192), d = 1.38436, its = 6
i = 221 (of 10192), d = 3.85214, its = 7
i = 222 (of 10192), d = 10, its = 39
i = 223 (of 10192), d = 0.710203, its = 19
i = 224 (of 10192), d = 10, its = 53
i = 225 (of 10192), d = 1.69596, its = 6
i = 226 (of 10192), d = 10, its = 37
i = 227 (of 10192), d = 10, its = 44
i = 228 (of 10192), d = 10, its = 60
i = 229 (of 10192), d = 10, its = 56
i = 230 (of 10192), d = 10, its = 49
i = 231 (of 10192), d = 0.873797, its = 23
i = 232 (of 10192), d = 10, its = 57
i = 233 (of 10192), d = 10, its = 51
i = 234 (of 10192), d = 1.877, its = 8
i = 235 (of 10192), d = 6.78044, its = 9
i = 236 (of 10192), d = 3.20849, its = 6
i = 237 (of 10192), d = 10, its = 53
i = 238 (of 10192), d = 10, its = 51
i = 239 (of 10192), d = 10, its = 39
i = 240 (of 10192), d = 10, its = 53
i = 241 (of 10192), d = 10, its = 37
i = 242 (of 10192), d = 10, its = 43
i = 243 (of 10192), d = 10, its = 37
i = 244 (of 10192), d = 2.36768, its = 6
i = 245 (of 10192), d = 10, its = 52
i = 246 (of 10192), d = 10, its = 52
i = 247 (of 10192), d = 10, its = 50
i = 248 (of 10192), d = 2.84526, its = 6
i = 249 (of 10192), d = 10, its = 39
i = 250 (of 10192), d = 10, its = 40
i = 251 (of 10192), d = 0.765506, its = 18
i = 252 (of 10192), d = 10, its = 37
i = 253 (of 10192), d = 10, its = 55
i = 254 (of 10192), d = 10, its = 55
i = 255 (of 10192), d = 10, its = 51
i = 256 (of 10192), d = 10, its = 54
i = 257 (of 10192), d = 10, its = 40
i = 258 (of 10192), d = 10, its = 37
i = 259 (of 10192), d = 10, its = 55
i = 260 (of 10192), d = 10, its = 53
i = 261 (of 10192), d = 10, its = 49
i = 262 (of 10192), d = 10, its = 39
i = 263 (of 10192), d = 10, its = 54
i = 264 (of 10192), d = 1.74956, its = 5
i = 265 (of 10192), d = 10, its = 55
i = 266 (of 10192), d = 10, its = 38
i = 267 (of 10192), d = 3.11822, its = 7
i = 268 (of 10192), d = 10, its = 50
i = 269 (of 10192), d = 10, its = 37
i = 270 (of 10192), d = 10, its = 58
i = 271 (of 10192), d = 10, its = 56
i = 272 (of 10192), d = 10, its = 50
i = 273 (of 10192), d = 0.861774, its = 17
i = 274 (of 10192), d = 10, its = 53
i = 275 (of 10192), d = 1.40235, its = 7
i = 276 (of 10192), d = 10, its = 50
i = 277 (of 10192), d = 10, its = 42
i = 278 (of 10192), d = 10, its = 55
i = 279 (of 10192), d = 10, its = 37
i = 280 (of 10192), d = 2.50626, its = 5
i = 281 (of 10192), d = 10, its = 56
i = 282 (of 10192), d = 1.10839, its = 21
i = 283 (of 10192), d = 10, its = 55
i = 284 (of 10192), d = 10, its = 50
i = 285 (of 10192), d = 1.51106, its = 6
i = 286 (of 10192), d = 2.10842, its = 4
i = 287 (of 10192), d = 4.6927, its = 8
i = 288 (of 10192), d = 10, its = 37
i = 289 (of 10192), d = 10, its = 52
i = 290 (of 10192), d = 10, its = 51
i = 291 (of 10192), d = 1.1037, its = 18
i = 292 (of 10192), d = 10, its = 54
i = 293 (of 10192), d = 10, its = 52
i = 294 (of 10192), d = 10, its = 52
i = 295 (of 10192), d = 10, its = 54
i = 296 (of 10192), d = 10, its = 40
i = 297 (of 10192), d = 10, its = 51
i = 298 (of 10192), d = 10, its = 44
i = 299 (of 10192), d = 3.07618, its = 7
i = 300 (of 10192), d = 3.29225, its = 6
i = 301 (of 10192), d = 10, its = 53
i = 302 (of 10192), d = 2.19409, its = 4
i = 303 (of 10192), d = 10, its = 51
i = 304 (of 10192), d = 4.07257, its = 8
i = 305 (of 10192), d = 10, its = 53
i = 306 (of 10192), d = 10, its = 52
i = 307 (of 10192), d = 10, its = 49
i = 308 (of 10192), d = 10, its = 52
i = 309 (of 10192), d = 10, its = 37
i = 310 (of 10192), d = 10, its = 40
i = 311 (of 10192), d = 10, its = 56
i = 312 (of 10192), d = 10, its = 51
i = 313 (of 10192), d = 10, its = 37
i = 314 (of 10192), d = 10, its = 55
i = 315 (of 10192), d = 1.06401, its = 19
i = 316 (of 10192), d = 2.48218, its = 5
i = 317 (of 10192), d = 10, its = 54
i = 318 (of 10192), d = 10, its = 53
i = 319 (of 10192), d = 10, its = 37
i = 320 (of 10192), d = 10, its = 54
i = 321 (of 10192), d = 10, its = 49
i = 322 (of 10192), d = 10, its = 52
i = 323 (of 10192), d = 10, its = 42
i = 324 (of 10192), d = 10, its = 52
i = 325 (of 10192), d = 10, its = 54
i = 326 (of 10192), d = 10, its = 39
i = 327 (of 10192), d = 10, its = 40
i = 328 (of 10192), d = 10, its = 40
i = 329 (of 10192), d = 10, its = 53
i = 330 (of 10192), d = 10, its = 49
i = 331 (of 10192), d = 10, its = 55
i = 332 (of 10192), d = 2.76402, its = 6
i = 333 (of 10192), d = 10, its = 51
i = 334 (of 10192), d = 10, its = 37
i = 335 (of 10192), d = 10, its = 54
i = 336 (of 10192), d = 10, its = 54
i = 337 (of 10192), d = 10, its = 51
i = 338 (of 10192), d = 10, its = 50
i = 339 (of 10192), d = 10, its = 52
i = 340 (of 10192), d = 10, its = 53
i = 341 (of 10192), d = 10, its = 52
i = 342 (of 10192), d = 10, its = 53
i = 343 (of 10192), d = 1.75426, its = 5
i = 344 (of 10192), d = 2.3226, its = 4
i = 345 (of 10192), d = 10, its = 54
i = 346 (of 10192), d = 10, its = 50
i = 347 (of 10192), d = 10, its = 38
i = 348 (of 10192), d = 10, its = 53
i = 349 (of 10192), d = 10, its = 41
i = 350 (of 10192), d = 2.5353, its = 5
i = 351 (of 10192), d = 10, its = 52
i = 352 (of 10192), d = 10, its = 46
i = 353 (of 10192), d = 10, its = 51
i = 354 (of 10192), d = 10, its = 46
i = 355 (of 10192), d = 10, its = 37
i = 356 (of 10192), d = 10, its = 58
i = 357 (of 10192), d = 2.32622, its = 4
i = 358 (of 10192), d = 1.66717, its = 6
i = 359 (of 10192), d = 2.39945, its = 5
i = 360 (of 10192), d = 10, its = 37
i = 361 (of 10192), d = 10, its = 40
i = 362 (of 10192), d = 10, its = 37
i = 363 (of 10192), d = 10, its = 37
i = 364 (of 10192), d = 10, its = 53
i = 365 (of 10192), d = 10, its = 51
i = 366 (of 10192), d = 10, its = 45
i = 367 (of 10192), d = 10, its = 56
i = 368 (of 10192), d = 10, its = 50
i = 369 (of 10192), d = 10, its = 52
i = 370 (of 10192), d = 10, its = 49
i = 371 (of 10192), d = 10, its = 51
i = 372 (of 10192), d = 10, its = 51
i = 373 (of 10192), d = 10, its = 60
i = 374 (of 10192), d = 10, its = 37
i = 375 (of 10192), d = 1.29785, its = 8
i = 376 (of 10192), d = 2.14934, its = 4
i = 377 (of 10192), d = 10, its = 55
i = 378 (of 10192), d = 0.695247, its = 19
i = 379 (of 10192), d = 10, its = 52
i = 380 (of 10192), d = 10, its = 37
i = 381 (of 10192), d = 10, its = 49
i = 382 (of 10192), d = 10, its = 40
i = 383 (of 10192), d = 1.563, its = 6
i = 384 (of 10192), d = 10, its = 56
i = 385 (of 10192), d = 9.96295, its = 8
i = 386 (of 10192), d = 10, its = 49
i = 387 (of 10192), d = 10, its = 52
i = 388 (of 10192), d = 10, its = 56
i = 389 (of 10192), d = 10, its = 54
i = 390 (of 10192), d = 10, its = 56
i = 391 (of 10192), d = 10, its = 38
i = 392 (of 10192), d = 10, its = 40
i = 393 (of 10192), d = 10, its = 50
i = 394 (of 10192), d = 10, its = 52
i = 395 (of 10192), d = 10, its = 57
i = 396 (of 10192), d = 10, its = 50
i = 397 (of 10192), d = 0.591373, its = 19
i = 398 (of 10192), d = 8.18167, its = 8
i = 399 (of 10192), d = 10, its = 54
i = 400 (of 10192), d = 0.865521, its = 17
i = 401 (of 10192), d = 10, its = 51
i = 402 (of 10192), d = 10, its = 54
i = 403 (of 10192), d = 10, its = 52
i = 404 (of 10192), d = 10, its = 37
i = 405 (of 10192), d = 10, its = 49
i = 406 (of 10192), d = 10, its = 37
i = 407 (of 10192), d = 10, its = 52
i = 408 (of 10192), d = 10, its = 52
i = 409 (of 10192), d = 0.927751, its = 19
i = 410 (of 10192), d = 10, its = 52
i = 411 (of 10192), d = 10, its = 40
i = 412 (of 10192), d = 10, its = 53
i = 413 (of 10192), d = 10, its = 43
i = 414 (of 10192), d = 10, its = 51
i = 415 (of 10192), d = 10, its = 54
i = 416 (of 10192), d = 10, its = 53
i = 417 (of 10192), d = 10, its = 52
i = 418 (of 10192), d = 10, its = 47
i = 419 (of 10192), d = 10, its = 40
i = 420 (of 10192), d = 10, its = 51
i = 421 (of 10192), d = 10, its = 51
i = 422 (of 10192), d = 10, its = 37
i = 423 (of 10192), d = 10, its = 51
i = 424 (of 10192), d = 10, its = 48
i = 425 (of 10192), d = 10, its = 50
i = 426 (of 10192), d = 10, its = 54
i = 427 (of 10192), d = 10, its = 53
i = 428 (of 10192), d = 0.533045, its = 17
i = 429 (of 10192), d = 2.16819, its = 4
i = 430 (of 10192), d = 10, its = 55
i = 431 (of 10192), d = 1.94187, its = 3
i = 432 (of 10192), d = 10, its = 52
i = 433 (of 10192), d = 0.697736, its = 18
i = 434 (of 10192), d = 10, its = 46
i = 435 (of 10192), d = 10, its = 51
i = 436 (of 10192), d = 10, its = 53
i = 437 (of 10192), d = 0.58994, its = 17
i = 438 (of 10192), d = 3.23974, its = 6
i = 439 (of 10192), d = 10, its = 52
i = 440 (of 10192), d = 10, its = 55
i = 441 (of 10192), d = 10, its = 40
i = 442 (of 10192), d = 1.41522, its = 6
i = 443 (of 10192), d = 10, its = 37
i = 444 (of 10192), d = 10, its = 54
i = 445 (of 10192), d = 0.765506, its = 18
i = 446 (of 10192), d = 10, its = 52
i = 447 (of 10192), d = 10, its = 40
i = 448 (of 10192), d = 10, its = 50
i = 449 (of 10192), d = 1.13529, its = 22
i = 450 (of 10192), d = 10, its = 37
i = 451 (of 10192), d = 10, its = 52
i = 452 (of 10192), d = 3.05923, its = 6
i = 453 (of 10192), d = 10, its = 55
i = 454 (of 10192), d = 10, its = 51
i = 455 (of 10192), d = 10, its = 52
i = 456 (of 10192), d = 10, its = 54
i = 457 (of 10192), d = 10, its = 52
i = 458 (of 10192), d = 0.740488, its = 21
i = 459 (of 10192), d = 10, its = 51
i = 460 (of 10192), d = 10, its = 40
i = 461 (of 10192), d = 10, its = 51
i = 462 (of 10192), d = 10, its = 56
i = 463 (of 10192), d = 2.81108, its = 5
i = 464 (of 10192), d = 10, its = 58
i = 465 (of 10192), d = 2.07502, its = 4
i = 466 (of 10192), d = 10, its = 53
i = 467 (of 10192), d = 10, its = 54
i = 468 (of 10192), d = 10, its = 51
i = 469 (of 10192), d = 10, its = 42
i = 470 (of 10192), d = 10, its = 51
i = 471 (of 10192), d = 10, its = 37
i = 472 (of 10192), d = 10, its = 49
i = 473 (of 10192), d = 1.42421, its = 7
i = 474 (of 10192), d = 10, its = 55
i = 475 (of 10192), d = 10, its = 37
i = 476 (of 10192), d = 0.565109, its = 18
i = 477 (of 10192), d = 9.54474, its = 8
i = 478 (of 10192), d = 10, its = 37
i = 479 (of 10192), d = 10, its = 58
i = 480 (of 10192), d = 8.33418, its = 9
i = 481 (of 10192), d = 10, its = 37
i = 482 (of 10192), d = 1.68401, its = 5
i = 483 (of 10192), d = 10, its = 37
i = 484 (of 10192), d = 10, its = 55
i = 485 (of 10192), d = 10, its = 40
i = 486 (of 10192), d = 10, its = 42
i = 487 (of 10192), d = 0.917434, its = 18
i = 488 (of 10192), d = 10, its = 54
i = 489 (of 10192), d = 3.40879, its = 6
i = 490 (of 10192), d = 1.83425, its = 4
i = 491 (of 10192), d = 10, its = 54
i = 492 (of 10192), d = 10, its = 40
i = 493 (of 10192), d = 10, its = 57
i = 494 (of 10192), d = 10, its = 53
i = 495 (of 10192), d = 10, its = 41
i = 496 (of 10192), d = 10, its = 51
i = 497 (of 10192), d = 10, its = 55
i = 498 (of 10192), d = 10, its = 50
i = 499 (of 10192), d = 1.89511, its = 4
i = 500 (of 10192), d = 10, its = 40
i = 501 (of 10192), d = 9.69604, its = 9
i = 502 (of 10192), d = 10, its = 54
i = 503 (of 10192), d = 10, its = 47
i = 504 (of 10192), d = 10, its = 37
i = 505 (of 10192), d = 10, its = 56
i = 506 (of 10192), d = 1.59361, its = 6
i = 507 (of 10192), d = 10, its = 52
i = 508 (of 10192), d = 1.28158, its = 6
i = 509 (of 10192), d = 3.33814, its = 6
i = 510 (of 10192), d = 10, its = 56
i = 511 (of 10192), d = 10, its = 58
i = 512 (of 10192), d = 10, its = 54
i = 513 (of 10192), d = 10, its = 55
i = 514 (of 10192), d = 10, its = 56
i = 515 (of 10192), d = 1.74174, its = 5
i = 516 (of 10192), d = 10, its = 37
i = 517 (of 10192), d = 10, its = 50
i = 518 (of 10192), d = 10, its = 58
i = 519 (of 10192), d = 1.75421, its = 5
i = 520 (of 10192), d = 10, its = 53
i = 521 (of 10192), d = 10, its = 53
i = 522 (of 10192), d = 1.68985, its = 5
i = 523 (of 10192), d = 10, its = 54
i = 524 (of 10192), d = 10, its = 53
i = 525 (of 10192), d = 2.16664, its = 4
i = 526 (of 10192), d = 10, its = 52
i = 527 (of 10192), d = 10, its = 37
i = 528 (of 10192), d = 0.596289, its = 20
i = 529 (of 10192), d = 2.93179, its = 6
i = 530 (of 10192), d = 1.55923, its = 6
i = 531 (of 10192), d = 1.13049, its = 20
i = 532 (of 10192), d = 0.829951, its = 23
i = 533 (of 10192), d = 2.94808, its = 6
i = 534 (of 10192), d = 10, its = 52
i = 535 (of 10192), d = 10, its = 51
i = 536 (of 10192), d = 10, its = 52
i = 537 (of 10192), d = 10, its = 51
i = 538 (of 10192), d = 10, its = 40
i = 539 (of 10192), d = 10, its = 56
i = 540 (of 10192), d = 10, its = 37
i = 541 (of 10192), d = 10, its = 54
i = 542 (of 10192), d = 0.910784, its = 18
i = 543 (of 10192), d = 10, its = 54
i = 544 (of 10192), d = 10, its = 52
i = 545 (of 10192), d = 10, its = 51
i = 546 (of 10192), d = 10, its = 40
i = 547 (of 10192), d = 10, its = 51
i = 548 (of 10192), d = 10, its = 37
i = 549 (of 10192), d = 10, its = 54
i = 550 (of 10192), d = 4.59661, its = 7
i = 551 (of 10192), d = 10, its = 37
i = 552 (of 10192), d = 10, its = 55
i = 553 (of 10192), d = 10, its = 55
i = 554 (of 10192), d = 10, its = 56
i = 555 (of 10192), d = 10, its = 54
i = 556 (of 10192), d = 10, its = 55
i = 557 (of 10192), d = 10, its = 53
i = 558 (of 10192), d = 10, its = 54
i = 559 (of 10192), d = 10, its = 42
i = 560 (of 10192), d = 10, its = 41
i = 561 (of 10192), d = 10, its = 40
i = 562 (of 10192), d = 10, its = 53
i = 563 (of 10192), d = 10, its = 55
i = 564 (of 10192), d = 1.9612, its = 3
i = 565 (of 10192), d = 1.86195, its = 4
i = 566 (of 10192), d = 10, its = 55
i = 567 (of 10192), d = 10, its = 42
i = 568 (of 10192), d = 1.88004, its = 4
i = 569 (of 10192), d = 1.61451, its = 6
i = 570 (of 10192), d = 10, its = 54
i = 571 (of 10192), d = 0.77639, its = 18
i = 572 (of 10192), d = 10, its = 41
i = 573 (of 10192), d = 10, its = 52
i = 574 (of 10192), d = 10, its = 56
i = 575 (of 10192), d = 10, its = 53
i = 576 (of 10192), d = 1.75948, its = 5
i = 577 (of 10192), d = 10, its = 54
i = 578 (of 10192), d = 10, its = 48
i = 579 (of 10192), d = 2.75108, its = 6
i = 580 (of 10192), d = 1.53254, its = 6
i = 581 (of 10192), d = 10, its = 54
i = 582 (of 10192), d = 2.58636, its = 5
i = 583 (of 10192), d = 10, its = 53
i = 584 (of 10192), d = 10, its = 55
i = 585 (of 10192), d = 1.53259, its = 6
i = 586 (of 10192), d = 10, its = 51
i = 587 (of 10192), d = 10, its = 40
i = 588 (of 10192), d = 1.76434, its = 5
i = 589 (of 10192), d = 1.78645, its = 5
i = 590 (of 10192), d = 1.70567, its = 5
i = 591 (of 10192), d = 10, its = 56
i = 592 (of 10192), d = 10, its = 51
i = 593 (of 10192), d = 1.2837, its = 21
i = 594 (of 10192), d = 10, its = 54
i = 595 (of 10192), d = 10, its = 55
i = 596 (of 10192), d = 10, its = 55
i = 597 (of 10192), d = 10, its = 37
i = 598 (of 10192), d = 10, its = 51
i = 599 (of 10192), d = 1.15984, its = 19
i = 600 (of 10192), d = 10, its = 50
i = 601 (of 10192), d = 10, its = 55
i = 602 (of 10192), d = 1.99504, its = 2
i = 603 (of 10192), d = 10, its = 52
i = 604 (of 10192), d = 10, its = 54
i = 605 (of 10192), d = 10, its = 45
i = 606 (of 10192), d = 2.19642, its = 4
i = 607 (of 10192), d = 10, its = 46
i = 608 (of 10192), d = 10, its = 53
i = 609 (of 10192), d = 10, its = 54
i = 610 (of 10192), d = 10, its = 41
i = 611 (of 10192), d = 10, its = 41
i = 612 (of 10192), d = 10, its = 52
i = 613 (of 10192), d = 10, its = 54
i = 614 (of 10192), d = 10, its = 37
i = 615 (of 10192), d = 10, its = 50
i = 616 (of 10192), d = 10, its = 56
i = 617 (of 10192), d = 1.57962, its = 7
i = 618 (of 10192), d = 10, its = 54
i = 619 (of 10192), d = 10, its = 56
i = 620 (of 10192), d = 10, its = 37
i = 621 (of 10192), d = 10, its = 53
i = 622 (of 10192), d = 10, its = 39
i = 623 (of 10192), d = 10, its = 37
i = 624 (of 10192), d = 2.4208, its = 5
i = 625 (of 10192), d = 10, its = 56
i = 626 (of 10192), d = 2.5927, its = 5
i = 627 (of 10192), d = 10, its = 37
i = 628 (of 10192), d = 10, its = 50
i = 629 (of 10192), d = 1.83546, its = 4
i = 630 (of 10192), d = 10, its = 53
i = 631 (of 10192), d = 2.39131, its = 5
i = 632 (of 10192), d = 10, its = 48
i = 633 (of 10192), d = 2.27611, its = 4
i = 634 (of 10192), d = 10, its = 37
i = 635 (of 10192), d = 10, its = 56
i = 636 (of 10192), d = 10, its = 55
i = 637 (of 10192), d = 10, its = 47
i = 638 (of 10192), d = 0.788497, its = 16
i = 639 (of 10192), d = 10, its = 54
i = 640 (of 10192), d = 1.29705, its = 6
i = 641 (of 10192), d = 10, its = 50
i = 642 (of 10192), d = 10, its = 57
i = 643 (of 10192), d = 10, its = 53
i = 644 (of 10192), d = 10, its = 52
i = 645 (of 10192), d = 10, its = 53
i = 646 (of 10192), d = 2.29593, its = 4
i = 647 (of 10192), d = 10, its = 39
i = 648 (of 10192), d = 10, its = 37
i = 649 (of 10192), d = 10, its = 52
i = 650 (of 10192), d = 10, its = 51
i = 651 (of 10192), d = 10, its = 39
i = 652 (of 10192), d = 0.544453, its = 16
i = 653 (of 10192), d = 10, its = 37
i = 654 (of 10192), d = 10, its = 41
i = 655 (of 10192), d = 3.14112, its = 6
i = 656 (of 10192), d = 10, its = 50
i = 657 (of 10192), d = 1.90518, its = 4
i = 658 (of 10192), d = 1.48104, its = 6
i = 659 (of 10192), d = 10, its = 52
i = 660 (of 10192), d = 10, its = 37
i = 661 (of 10192), d = 1.00929, its = 20
i = 662 (of 10192), d = 10, its = 57
i = 663 (of 10192), d = 1.60088, its = 6
i = 664 (of 10192), d = 0.494169, its = 18
i = 665 (of 10192), d = 10, its = 37
i = 666 (of 10192), d = 0.694465, its = 17
i = 667 (of 10192), d = 1.59212, its = 7
i = 668 (of 10192), d = 0.693593, its = 19
i = 669 (of 10192), d = 10, its = 55
i = 670 (of 10192), d = 10, its = 54
i = 671 (of 10192), d = 1.60659, its = 6
i = 672 (of 10192), d = 0.387748, its = 17
i = 673 (of 10192), d = 10, its = 53
i = 674 (of 10192), d = 1.74805, its = 5
i = 675 (of 10192), d = 10, its = 53
i = 676 (of 10192), d = 10, its = 52
i = 677 (of 10192), d = 10, its = 55
i = 678 (of 10192), d = 1.35368, its = 6
i = 679 (of 10192), d = 10, its = 50
i = 680 (of 10192), d = 10, its = 50
i = 681 (of 10192), d = 10, its = 53
i = 682 (of 10192), d = 10, its = 50
i = 683 (of 10192), d = 2.0393, its = 3
i = 684 (of 10192), d = 10, its = 37
i = 685 (of 10192), d = 10, its = 48
i = 686 (of 10192), d = 10, its = 55
i = 687 (of 10192), d = 10, its = 56
i = 688 (of 10192), d = 10, its = 50
i = 689 (of 10192), d = 1.19721, its = 7
i = 690 (of 10192), d = 10, its = 48
i = 691 (of 10192), d = 8.70463, its = 9
i = 692 (of 10192), d = 10, its = 54
i = 693 (of 10192), d = 0.479938, its = 17
i = 694 (of 10192), d = 10, its = 55
i = 695 (of 10192), d = 10, its = 55
i = 696 (of 10192), d = 2.14085, its = 4
i = 697 (of 10192), d = 0.674069, its = 16
i = 698 (of 10192), d = 10, its = 40
i = 699 (of 10192), d = 10, its = 56
i = 700 (of 10192), d = 10, its = 37
i = 701 (of 10192), d = 10, its = 52
i = 702 (of 10192), d = 10, its = 49
i = 703 (of 10192), d = 1.05555, its = 22
i = 704 (of 10192), d = 4.87861, its = 7
i = 705 (of 10192), d = 10, its = 54
i = 706 (of 10192), d = 10, its = 53
i = 707 (of 10192), d = 10, its = 43
i = 708 (of 10192), d = 1.46048, its = 6
i = 709 (of 10192), d = 3.41723, its = 6
i = 710 (of 10192), d = 10, its = 52
i = 711 (of 10192), d = 10, its = 50
i = 712 (of 10192), d = 10, its = 51
i = 713 (of 10192), d = 10, its = 37
i = 714 (of 10192), d = 10, its = 39
i = 715 (of 10192), d = 10, its = 53
i = 716 (of 10192), d = 10, its = 50
i = 717 (of 10192), d = 10, its = 50
i = 718 (of 10192), d = 4.5589, its = 7
i = 719 (of 10192), d = 10, its = 49
i = 720 (of 10192), d = 1.77699, its = 7
i = 721 (of 10192), d = 0.285125, its = 23
i = 722 (of 10192), d = 10, its = 37
i = 723 (of 10192), d = 10, its = 40
i = 724 (of 10192), d = 10, its = 52
i = 725 (of 10192), d = 10, its = 37
i = 726 (of 10192), d = 10, its = 55
i = 727 (of 10192), d = 10, its = 53
i = 728 (of 10192), d = 10, its = 49
i = 729 (of 10192), d = 10, its = 40
i = 730 (of 10192), d = 10, its = 52
i = 731 (of 10192), d = 10, its = 54
i = 732 (of 10192), d = 10, its = 55
i = 733 (of 10192), d = 10, its = 54
i = 734 (of 10192), d = 10, its = 53
i = 735 (of 10192), d = 0.549913, its = 16
i = 736 (of 10192), d = 10, its = 40
i = 737 (of 10192), d = 10, its = 52
i = 738 (of 10192), d = 10, its = 39
i = 739 (of 10192), d = 10, its = 40
i = 740 (of 10192), d = 10, its = 44
i = 741 (of 10192), d = 10, its = 49
i = 742 (of 10192), d = 0.557803, its = 18
i = 743 (of 10192), d = 10, its = 53
i = 744 (of 10192), d = 10, its = 48
i = 745 (of 10192), d = 10, its = 56
i = 746 (of 10192), d = 10, its = 39
i = 747 (of 10192), d = 10, its = 37
i = 748 (of 10192), d = 10, its = 53
i = 749 (of 10192), d = 10, its = 51
i = 750 (of 10192), d = 10, its = 52
i = 751 (of 10192), d = 1.3894, its = 6
i = 752 (of 10192), d = 1.8817, its = 4
i = 753 (of 10192), d = 10, its = 52
i = 754 (of 10192), d = 10, its = 55
i = 755 (of 10192), d = 10, its = 41
i = 756 (of 10192), d = 10, its = 50
i = 757 (of 10192), d = 1.90947, its = 4
i = 758 (of 10192), d = 10, its = 37
i = 759 (of 10192), d = 10, its = 43
i = 760 (of 10192), d = 1.7171, its = 5
i = 761 (of 10192), d = 1.40191, its = 7
i = 762 (of 10192), d = 10, its = 52
i = 763 (of 10192), d = 10, its = 52
i = 764 (of 10192), d = 10, its = 37
i = 765 (of 10192), d = 3.7227, its = 7
i = 766 (of 10192), d = 1.05388, its = 20
i = 767 (of 10192), d = 10, its = 52
i = 768 (of 10192), d = 7.79599, its = 8
i = 769 (of 10192), d = 10, its = 37
i = 770 (of 10192), d = 10, its = 54
i = 771 (of 10192), d = 10, its = 53
i = 772 (of 10192), d = 10, its = 53
i = 773 (of 10192), d = 10, its = 48
i = 774 (of 10192), d = 10, its = 49
i = 775 (of 10192), d = 10, its = 52
i = 776 (of 10192), d = 10, its = 37
i = 777 (of 10192), d = 0.938765, its = 22
i = 778 (of 10192), d = 10, its = 41
i = 779 (of 10192), d = 10, its = 51
i = 780 (of 10192), d = 10, its = 41
i = 781 (of 10192), d = 10, its = 50
i = 782 (of 10192), d = 2.43801, its = 5
i = 783 (of 10192), d = 10, its = 57
i = 784 (of 10192), d = 3.48072, its = 6
i = 785 (of 10192), d = 10, its = 49
i = 786 (of 10192), d = 1.36177, its = 6
i = 787 (of 10192), d = 10, its = 53
i = 788 (of 10192), d = 10, its = 52
i = 789 (of 10192), d = 10, its = 55
i = 790 (of 10192), d = 10, its = 51
i = 791 (of 10192), d = 10, its = 37
i = 792 (of 10192), d = 0.740266, its = 16
i = 793 (of 10192), d = 10, its = 53
i = 794 (of 10192), d = 10, its = 52
i = 795 (of 10192), d = 1.08611, its = 19
i = 796 (of 10192), d = 10, its = 53
i = 797 (of 10192), d = 10, its = 41
i = 798 (of 10192), d = 10, its = 53
i = 799 (of 10192), d = 10, its = 50
i = 800 (of 10192), d = 1.00318, its = 17
i = 801 (of 10192), d = 2.21896, its = 4
i = 802 (of 10192), d = 0.284111, its = 17
i = 803 (of 10192), d = 5.50611, its = 8
i = 804 (of 10192), d = 10, its = 55
i = 805 (of 10192), d = 2.83665, its = 6
i = 806 (of 10192), d = 1.09756, its = 21
i = 807 (of 10192), d = 10, its = 41
i = 808 (of 10192), d = 10, its = 55
i = 809 (of 10192), d = 10, its = 54
i = 810 (of 10192), d = 3.56339, its = 7
i = 811 (of 10192), d = 10, its = 40
i = 812 (of 10192), d = 1.42856, its = 7
i = 813 (of 10192), d = 10, its = 56
i = 814 (of 10192), d = 10, its = 54
i = 815 (of 10192), d = 10, its = 56
i = 816 (of 10192), d = 10, its = 41
i = 817 (of 10192), d = 10, its = 53
i = 818 (of 10192), d = 1.70912, its = 5
i = 819 (of 10192), d = 10, its = 37
i = 820 (of 10192), d = 10, its = 56
i = 821 (of 10192), d = 10, its = 40
i = 822 (of 10192), d = 10, its = 53
i = 823 (of 10192), d = 0.709427, its = 17
i = 824 (of 10192), d = 10, its = 57
i = 825 (of 10192), d = 1.70936, its = 5
i = 826 (of 10192), d = 10, its = 53
i = 827 (of 10192), d = 10, its = 37
i = 828 (of 10192), d = 10, its = 37
i = 829 (of 10192), d = 10, its = 49
i = 830 (of 10192), d = 1.33691, its = 6
i = 831 (of 10192), d = 10, its = 39
i = 832 (of 10192), d = 10, its = 56
i = 833 (of 10192), d = 2.75826, its = 6
i = 834 (of 10192), d = 10, its = 55
i = 835 (of 10192), d = 5.49781, its = 10
i = 836 (of 10192), d = 10, its = 55
i = 837 (of 10192), d = 10, its = 37
i = 838 (of 10192), d = 2.19097, its = 4
i = 839 (of 10192), d = 2.24215, its = 5
i = 840 (of 10192), d = 10, its = 40
i = 841 (of 10192), d = 10, its = 52
i = 842 (of 10192), d = 10, its = 52
i = 843 (of 10192), d = 10, its = 53
i = 844 (of 10192), d = 10, its = 39
i = 845 (of 10192), d = 10, its = 37
i = 846 (of 10192), d = 10, its = 53
i = 847 (of 10192), d = 10, its = 57
i = 848 (of 10192), d = 10, its = 53
i = 849 (of 10192), d = 10, its = 55
i = 850 (of 10192), d = 10, its = 41
i = 851 (of 10192), d = 10, its = 37
i = 852 (of 10192), d = 9.81616, its = 9
i = 853 (of 10192), d = 10, its = 52
i = 854 (of 10192), d = 1.93328, its = 4
i = 855 (of 10192), d = 10, its = 53
i = 856 (of 10192), d = 10, its = 53
i = 857 (of 10192), d = 10, its = 52
i = 858 (of 10192), d = 2.39633, its = 5
i = 859 (of 10192), d = 10, its = 37
i = 860 (of 10192), d = 1.62965, its = 7
i = 861 (of 10192), d = 10, its = 37
i = 862 (of 10192), d = 10, its = 56
i = 863 (of 10192), d = 10, its = 54
i = 864 (of 10192), d = 10, its = 38
i = 865 (of 10192), d = 10, its = 52
i = 866 (of 10192), d = 10, its = 51
i = 867 (of 10192), d = 2.67331, its = 6
i = 868 (of 10192), d = 10, its = 53
i = 869 (of 10192), d = 10, its = 50
i = 870 (of 10192), d = 10, its = 41
i = 871 (of 10192), d = 10, its = 40
i = 872 (of 10192), d = 10, its = 52
i = 873 (of 10192), d = 10, its = 49
i = 874 (of 10192), d = 10, its = 54
i = 875 (of 10192), d = 10, its = 52
i = 876 (of 10192), d = 10, its = 53
i = 877 (of 10192), d = 1.66562, its = 5
i = 878 (of 10192), d = 10, its = 52
i = 879 (of 10192), d = 10, its = 55
i = 880 (of 10192), d = 10, its = 17
i = 881 (of 10192), d = 10, its = 51
i = 882 (of 10192), d = 10, its = 53
i = 883 (of 10192), d = 10, its = 37
i = 884 (of 10192), d = 10, its = 50
i = 885 (of 10192), d = 2.46439, its = 5
i = 886 (of 10192), d = 10, its = 42
i = 887 (of 10192), d = 10, its = 55
i = 888 (of 10192), d = 10, its = 48
i = 889 (of 10192), d = 10, its = 40
i = 890 (of 10192), d = 4.13788, its = 6
i = 891 (of 10192), d = 10, its = 39
i = 892 (of 10192), d = 10, its = 52
i = 893 (of 10192), d = 10, its = 51
i = 894 (of 10192), d = 10, its = 50
i = 895 (of 10192), d = 0.673202, its = 23
i = 896 (of 10192), d = 2.49218, its = 6
i = 897 (of 10192), d = 1.90518, its = 4
i = 898 (of 10192), d = 10, its = 37
i = 899 (of 10192), d = 10, its = 49
i = 900 (of 10192), d = 10, its = 53
i = 901 (of 10192), d = 10, its = 37
i = 902 (of 10192), d = 10, its = 40
i = 903 (of 10192), d = 10, its = 37
i = 904 (of 10192), d = 0.511131, its = 21
i = 905 (of 10192), d = 10, its = 39
i = 906 (of 10192), d = 2.46353, its = 5
i = 907 (of 10192), d = 10, its = 51
i = 908 (of 10192), d = 10, its = 54
i = 909 (of 10192), d = 0.284111, its = 17
i = 910 (of 10192), d = 10, its = 55
i = 911 (of 10192), d = 2.53746, its = 5
i = 912 (of 10192), d = 10, its = 39
i = 913 (of 10192), d = 2.1661, its = 4
i = 914 (of 10192), d = 10, its = 45
i = 915 (of 10192), d = 0.629716, its = 21
i = 916 (of 10192), d = 10, its = 37
i = 917 (of 10192), d = 10, its = 42
i = 918 (of 10192), d = 10, its = 49
i = 919 (of 10192), d = 10, its = 21
i = 920 (of 10192), d = 4.13346, its = 6
i = 921 (of 10192), d = 3.34293, its = 7
i = 922 (of 10192), d = 2.11402, its = 4
i = 923 (of 10192), d = 10, its = 56
i = 924 (of 10192), d = 10, its = 51
i = 925 (of 10192), d = 10, its = 52
i = 926 (of 10192), d = 2.79994, its = 5
i = 927 (of 10192), d = 10, its = 53
i = 928 (of 10192), d = 10, its = 55
i = 929 (of 10192), d = 10, its = 51
i = 930 (of 10192), d = 0.486739, its = 17
i = 931 (of 10192), d = 10, its = 54
i = 932 (of 10192), d = 10, its = 53
i = 933 (of 10192), d = 10, its = 37
i = 934 (of 10192), d = 0.501911, its = 17
i = 935 (of 10192), d = 10, its = 39
i = 936 (of 10192), d = 10, its = 55
i = 937 (of 10192), d = 1.64675, its = 6
i = 938 (of 10192), d = 10, its = 37
i = 939 (of 10192), d = 10, its = 52
i = 940 (of 10192), d = 2.40758, its = 5
i = 941 (of 10192), d = 1.48945, its = 6
i = 942 (of 10192), d = 10, its = 51
i = 943 (of 10192), d = 10, its = 37
i = 944 (of 10192), d = 10, its = 37
i = 945 (of 10192), d = 10, its = 48
i = 946 (of 10192), d = 10, its = 53
i = 947 (of 10192), d = 1.86042, its = 4
i = 948 (of 10192), d = 10, its = 48
i = 949 (of 10192), d = 10, its = 53
i = 950 (of 10192), d = 10, its = 58
i = 951 (of 10192), d = 10, its = 16
i = 952 (of 10192), d = 10, its = 50
i = 953 (of 10192), d = 2.5965, its = 6
i = 954 (of 10192), d = 10, its = 55
i = 955 (of 10192), d = 10, its = 40
i = 956 (of 10192), d = 10, its = 55
i = 957 (of 10192), d = 1.69625, its = 5
i = 958 (of 10192), d = 10, its = 51
i = 959 (of 10192), d = 1.91766, its = 4
i = 960 (of 10192), d = 10, its = 41
i = 961 (of 10192), d = 10, its = 53
i = 962 (of 10192), d = 10, its = 56
i = 963 (of 10192), d = 10, its = 53
i = 964 (of 10192), d = 10, its = 40
i = 965 (of 10192), d = 10, its = 54
i = 966 (of 10192), d = 10, its = 50
i = 967 (of 10192), d = 10, its = 51
i = 968 (of 10192), d = 1.57411, its = 6
i = 969 (of 10192), d = 10, its = 56
i = 970 (of 10192), d = 10, its = 37
i = 971 (of 10192), d = 10, its = 55
i = 972 (of 10192), d = 2.36018, its = 5
i = 973 (of 10192), d = 10, its = 55
i = 974 (of 10192), d = 1.45215, its = 6
i = 975 (of 10192), d = 10, its = 53
i = 976 (of 10192), d = 2.06414, its = 3
i = 977 (of 10192), d = 10, its = 51
i = 978 (of 10192), d = 10, its = 51
i = 979 (of 10192), d = 10, its = 41
i = 980 (of 10192), d = 10, its = 41
i = 981 (of 10192), d = 10, its = 52
i = 982 (of 10192), d = 10, its = 50
i = 983 (of 10192), d = 10, its = 51
i = 984 (of 10192), d = 10, its = 37
i = 985 (of 10192), d = 1.53563, its = 6
i = 986 (of 10192), d = 2.64284, its = 5
i = 987 (of 10192), d = 10, its = 54
i = 988 (of 10192), d = 10, its = 52
i = 989 (of 10192), d = 10, its = 39
i = 990 (of 10192), d = 10, its = 41
i = 991 (of 10192), d = 10, its = 55
i = 992 (of 10192), d = 10, its = 37
i = 993 (of 10192), d = 10, its = 53
i = 994 (of 10192), d = 10, its = 54
i = 995 (of 10192), d = 4.6813, its = 8
i = 996 (of 10192), d = 10, its = 54
i = 997 (of 10192), d = 0.456562, its = 18
i = 998 (of 10192), d = 10, its = 37
i = 999 (of 10192), d = 0.884605, its = 18
i = 1000 (of 10192), d = 10, its = 54
i = 1001 (of 10192), d = 10, its = 49
i = 1002 (of 10192), d = 10, its = 53
i = 1003 (of 10192), d = 0.852437, its = 17
i = 1004 (of 10192), d = 10, its = 51
i = 1005 (of 10192), d = 10, its = 53
i = 1006 (of 10192), d = 2.51445, its = 5
i = 1007 (of 10192), d = 10, its = 53
i = 1008 (of 10192), d = 10, its = 40
i = 1009 (of 10192), d = 10, its = 40
i = 1010 (of 10192), d = 0.434346, its = 19
i = 1011 (of 10192), d = 10, its = 37
i = 1012 (of 10192), d = 3.21952, its = 6
i = 1013 (of 10192), d = 1.47695, its = 6
i = 1014 (of 10192), d = 10, its = 37
i = 1015 (of 10192), d = 10, its = 50
i = 1016 (of 10192), d = 10, its = 51
i = 1017 (of 10192), d = 10, its = 55
i = 1018 (of 10192), d = 5.40282, its = 8
i = 1019 (of 10192), d = 10, its = 37
i = 1020 (of 10192), d = 10, its = 37
i = 1021 (of 10192), d = 2.24264, its = 4
i = 1022 (of 10192), d = 0.75679, its = 17
i = 1023 (of 10192), d = 3.79583, its = 7
i = 1024 (of 10192), d = 10, its = 40
i = 1025 (of 10192), d = 10, its = 42
i = 1026 (of 10192), d = 2.6599, its = 5
i = 1027 (of 10192), d = 10, its = 52
i = 1028 (of 10192), d = 10, its = 57
i = 1029 (of 10192), d = 10, its = 38
i = 1030 (of 10192), d = 10, its = 49
i = 1031 (of 10192), d = 10, its = 53
i = 1032 (of 10192), d = 10, its = 39
i = 1033 (of 10192), d = 10, its = 50
i = 1034 (of 10192), d = 2.40279, its = 5
i = 1035 (of 10192), d = 10, its = 51
i = 1036 (of 10192), d = 10, its = 42
i = 1037 (of 10192), d = 10, its = 52
i = 1038 (of 10192), d = 0.693938, its = 20
i = 1039 (of 10192), d = 10, its = 57
i = 1040 (of 10192), d = 10, its = 52
i = 1041 (of 10192), d = 10, its = 41
i = 1042 (of 10192), d = 0.715491, its = 17
i = 1043 (of 10192), d = 10, its = 53
i = 1044 (of 10192), d = 2.54348, its = 5
i = 1045 (of 10192), d = 1.06115, its = 25
i = 1046 (of 10192), d = 1.3487, its = 6
i = 1047 (of 10192), d = 10, its = 54
i = 1048 (of 10192), d = 10, its = 52
i = 1049 (of 10192), d = 10, its = 40
i = 1050 (of 10192), d = 10, its = 51
i = 1051 (of 10192), d = 10, its = 57
i = 1052 (of 10192), d = 10, its = 53
i = 1053 (of 10192), d = 10, its = 54
i = 1054 (of 10192), d = 10, its = 40
i = 1055 (of 10192), d = 10, its = 53
i = 1056 (of 10192), d = 2.39141, its = 5
i = 1057 (of 10192), d = 10, its = 41
i = 1058 (of 10192), d = 10, its = 48
i = 1059 (of 10192), d = 1.12441, its = 7
i = 1060 (of 10192), d = 2.64891, its = 5
i = 1061 (of 10192), d = 10, its = 54
i = 1062 (of 10192), d = 10, its = 52
i = 1063 (of 10192), d = 0.925052, its = 18
i = 1064 (of 10192), d = 10, its = 56
i = 1065 (of 10192), d = 10, its = 57
i = 1066 (of 10192), d = 10, its = 47
i = 1067 (of 10192), d = 1.16662, its = 25
i = 1068 (of 10192), d = 10, its = 37
i = 1069 (of 10192), d = 2.91137, its = 6
i = 1070 (of 10192), d = 10, its = 49
i = 1071 (of 10192), d = 10, its = 49
i = 1072 (of 10192), d = 10, its = 52
i = 1073 (of 10192), d = 10, its = 39
i = 1074 (of 10192), d = 1.70994, its = 5
i = 1075 (of 10192), d = 0.539758, its = 24
i = 1076 (of 10192), d = 10, its = 53
i = 1077 (of 10192), d = 10, its = 37
i = 1078 (of 10192), d = 10, its = 50
i = 1079 (of 10192), d = 10, its = 51
i = 1080 (of 10192), d = 0.504396, its = 18
i = 1081 (of 10192), d = 10, its = 51
i = 1082 (of 10192), d = 10, its = 48
i = 1083 (of 10192), d = 10, its = 55
i = 1084 (of 10192), d = 1.30967, its = 6
i = 1085 (of 10192), d = 10, its = 50
i = 1086 (of 10192), d = 10, its = 54
i = 1087 (of 10192), d = 10, its = 57
i = 1088 (of 10192), d = 3.40159, its = 6
i = 1089 (of 10192), d = 10, its = 53
i = 1090 (of 10192), d = 10, its = 52
i = 1091 (of 10192), d = 10, its = 39
i = 1092 (of 10192), d = 10, its = 58
i = 1093 (of 10192), d = 1.21204, its = 6
i = 1094 (of 10192), d = 3.52899, its = 8
i = 1095 (of 10192), d = 1.21236, its = 7
i = 1096 (of 10192), d = 1.00077, its = 24
i = 1097 (of 10192), d = 10, its = 43
i = 1098 (of 10192), d = 10, its = 51
i = 1099 (of 10192), d = 10, its = 37
i = 1100 (of 10192), d = 10, its = 52
i = 1101 (of 10192), d = 10, its = 55
i = 1102 (of 10192), d = 10, its = 52
i = 1103 (of 10192), d = 10, its = 37
i = 1104 (of 10192), d = 10, its = 51
i = 1105 (of 10192), d = 2.30533, its = 4
i = 1106 (of 10192), d = 3.6031, its = 7
i = 1107 (of 10192), d = 0.692327, its = 18
i = 1108 (of 10192), d = 10, its = 52
i = 1109 (of 10192), d = 1.281, its = 6
i = 1110 (of 10192), d = 10, its = 50
i = 1111 (of 10192), d = 0.843501, its = 20
i = 1112 (of 10192), d = 1.93862, its = 4
i = 1113 (of 10192), d = 3.72618, its = 7
i = 1114 (of 10192), d = 10, its = 52
i = 1115 (of 10192), d = 10, its = 37
i = 1116 (of 10192), d = 10, its = 58
i = 1117 (of 10192), d = 10, its = 50
i = 1118 (of 10192), d = 10, its = 50
i = 1119 (of 10192), d = 10, its = 51
i = 1120 (of 10192), d = 10, its = 54
i = 1121 (of 10192), d = 10, its = 52
i = 1122 (of 10192), d = 3.43001, its = 6
i = 1123 (of 10192), d = 10, its = 50
i = 1124 (of 10192), d = 10, its = 56
i = 1125 (of 10192), d = 10, its = 37
i = 1126 (of 10192), d = 0.382978, its = 21
i = 1127 (of 10192), d = 10, its = 55
i = 1128 (of 10192), d = 10, its = 55
i = 1129 (of 10192), d = 10, its = 37
i = 1130 (of 10192), d = 10, its = 37
i = 1131 (of 10192), d = 10, its = 55
i = 1132 (of 10192), d = 10, its = 52
i = 1133 (of 10192), d = 10, its = 52
i = 1134 (of 10192), d = 10, its = 51
i = 1135 (of 10192), d = 2.74035, its = 5
i = 1136 (of 10192), d = 0.436699, its = 19
i = 1137 (of 10192), d = 10, its = 55
i = 1138 (of 10192), d = 10, its = 54
i = 1139 (of 10192), d = 10, its = 51
i = 1140 (of 10192), d = 10, its = 37
i = 1141 (of 10192), d = 3.9921, its = 7
i = 1142 (of 10192), d = 10, its = 53
i = 1143 (of 10192), d = 1.42421, its = 7
i = 1144 (of 10192), d = 10, its = 51
i = 1145 (of 10192), d = 10, its = 50
i = 1146 (of 10192), d = 10, its = 37
i = 1147 (of 10192), d = 10, its = 50
i = 1148 (of 10192), d = 10, its = 48
i = 1149 (of 10192), d = 0.887518, its = 18
i = 1150 (of 10192), d = 10, its = 57
i = 1151 (of 10192), d = 10, its = 54
i = 1152 (of 10192), d = 10, its = 52
i = 1153 (of 10192), d = 2.06675, its = 4
i = 1154 (of 10192), d = 10, its = 37
i = 1155 (of 10192), d = 1.92965, its = 4
i = 1156 (of 10192), d = 3.37626, its = 6
i = 1157 (of 10192), d = 1.10726, its = 23
i = 1158 (of 10192), d = 0.442599, its = 19
i = 1159 (of 10192), d = 10, its = 57
i = 1160 (of 10192), d = 10, its = 56
i = 1161 (of 10192), d = 1.12469, its = 7
i = 1162 (of 10192), d = 10, its = 52
i = 1163 (of 10192), d = 10, its = 54
i = 1164 (of 10192), d = 10, its = 54
i = 1165 (of 10192), d = 10, its = 52
i = 1166 (of 10192), d = 10, its = 54
i = 1167 (of 10192), d = 10, its = 40
i = 1168 (of 10192), d = 10, its = 52
i = 1169 (of 10192), d = 1.67127, its = 5
i = 1170 (of 10192), d = 10, its = 54
i = 1171 (of 10192), d = 10, its = 48
i = 1172 (of 10192), d = 2.02906, its = 3
i = 1173 (of 10192), d = 10, its = 39
i = 1174 (of 10192), d = 10, its = 52
i = 1175 (of 10192), d = 1.84867, its = 4
i = 1176 (of 10192), d = 10, its = 53
i = 1177 (of 10192), d = 2.1747, its = 4
i = 1178 (of 10192), d = 2.39606, its = 5
i = 1179 (of 10192), d = 0.697983, its = 19
i = 1180 (of 10192), d = 10, its = 41
i = 1181 (of 10192), d = 1.79376, its = 5
i = 1182 (of 10192), d = 10, its = 51
i = 1183 (of 10192), d = 10, its = 50
i = 1184 (of 10192), d = 10, its = 53
i = 1185 (of 10192), d = 10, its = 37
i = 1186 (of 10192), d = 10, its = 37
i = 1187 (of 10192), d = 1.80952, its = 5
i = 1188 (of 10192), d = 2.3352, its = 5
i = 1189 (of 10192), d = 10, its = 37
i = 1190 (of 10192), d = 1.92062, its = 4
i = 1191 (of 10192), d = 2.79258, its = 6
i = 1192 (of 10192), d = 10, its = 52
i = 1193 (of 10192), d = 1.63801, its = 7
i = 1194 (of 10192), d = 10, its = 52
i = 1195 (of 10192), d = 10, its = 47
i = 1196 (of 10192), d = 1.70994, its = 5
i = 1197 (of 10192), d = 10, its = 47
i = 1198 (of 10192), d = 1.48158, its = 6
i = 1199 (of 10192), d = 10, its = 49
i = 1200 (of 10192), d = 1.44535, its = 7
i = 1201 (of 10192), d = 10, its = 37
i = 1202 (of 10192), d = 10, its = 51
i = 1203 (of 10192), d = 10, its = 40
i = 1204 (of 10192), d = 10, its = 53
i = 1205 (of 10192), d = 10, its = 37
i = 1206 (of 10192), d = 10, its = 57
i = 1207 (of 10192), d = 10, its = 53
i = 1208 (of 10192), d = 1.9898, its = 3
i = 1209 (of 10192), d = 10, its = 53
i = 1210 (of 10192), d = 10, its = 50
i = 1211 (of 10192), d = 10, its = 53
i = 1212 (of 10192), d = 10, its = 38
i = 1213 (of 10192), d = 10, its = 52
i = 1214 (of 10192), d = 10, its = 52
i = 1215 (of 10192), d = 1.71888, its = 6
i = 1216 (of 10192), d = 2.19642, its = 4
i = 1217 (of 10192), d = 3.02572, its = 7
i = 1218 (of 10192), d = 10, its = 50
i = 1219 (of 10192), d = 10, its = 51
i = 1220 (of 10192), d = 10, its = 54
i = 1221 (of 10192), d = 10, its = 50
i = 1222 (of 10192), d = 2.13574, its = 4
i = 1223 (of 10192), d = 10, its = 49
i = 1224 (of 10192), d = 2.601, its = 5
i = 1225 (of 10192), d = 10, its = 51
i = 1226 (of 10192), d = 10, its = 57
i = 1227 (of 10192), d = 10, its = 53
i = 1228 (of 10192), d = 0.743468, its = 17
i = 1229 (of 10192), d = 10, its = 37
i = 1230 (of 10192), d = 10, its = 37
i = 1231 (of 10192), d = 1.73133, its = 5
i = 1232 (of 10192), d = 1.77022, its = 5
i = 1233 (of 10192), d = 10, its = 52
i = 1234 (of 10192), d = 10, its = 48
i = 1235 (of 10192), d = 10, its = 54
i = 1236 (of 10192), d = 10, its = 41
i = 1237 (of 10192), d = 10, its = 53
i = 1238 (of 10192), d = 10, its = 37
i = 1239 (of 10192), d = 10, its = 37
i = 1240 (of 10192), d = 10, its = 37
i = 1241 (of 10192), d = 3.51787, its = 7
i = 1242 (of 10192), d = 1.13696, its = 21
i = 1243 (of 10192), d = 10, its = 37
i = 1244 (of 10192), d = 10, its = 51
i = 1245 (of 10192), d = 10, its = 56
i = 1246 (of 10192), d = 2.55112, its = 5
i = 1247 (of 10192), d = 3.20488, its = 7
i = 1248 (of 10192), d = 10, its = 37
i = 1249 (of 10192), d = 10, its = 51
i = 1250 (of 10192), d = 10, its = 53
i = 1251 (of 10192), d = 10, its = 55
i = 1252 (of 10192), d = 10, its = 49
i = 1253 (of 10192), d = 10, its = 52
i = 1254 (of 10192), d = 0.488511, its = 17
i = 1255 (of 10192), d = 4.6813, its = 8
i = 1256 (of 10192), d = 10, its = 41
i = 1257 (of 10192), d = 10, its = 58
i = 1258 (of 10192), d = 2.17641, its = 5
i = 1259 (of 10192), d = 10, its = 52
i = 1260 (of 10192), d = 10, its = 37
i = 1261 (of 10192), d = 10, its = 52
i = 1262 (of 10192), d = 10, its = 42
i = 1263 (of 10192), d = 10, its = 58
i = 1264 (of 10192), d = 10, its = 52
i = 1265 (of 10192), d = 10, its = 56
i = 1266 (of 10192), d = 10, its = 52
i = 1267 (of 10192), d = 10, its = 54
i = 1268 (of 10192), d = 3.82484, its = 7
i = 1269 (of 10192), d = 10, its = 47
i = 1270 (of 10192), d = 0.810311, its = 16
i = 1271 (of 10192), d = 1.19851, its = 18
i = 1272 (of 10192), d = 3.64531, its = 6
i = 1273 (of 10192), d = 10, its = 50
i = 1274 (of 10192), d = 2.80175, its = 6
i = 1275 (of 10192), d = 7.72532, its = 8
i = 1276 (of 10192), d = 2.65319, its = 5
i = 1277 (of 10192), d = 0.548245, its = 23
i = 1278 (of 10192), d = 0.818619, its = 19
i = 1279 (of 10192), d = 10, its = 50
i = 1280 (of 10192), d = 10, its = 53
i = 1281 (of 10192), d = 10, its = 53
i = 1282 (of 10192), d = 10, its = 52
i = 1283 (of 10192), d = 10, its = 53
i = 1284 (of 10192), d = 10, its = 50
i = 1285 (of 10192), d = 3.76655, its = 7
i = 1286 (of 10192), d = 10, its = 53
i = 1287 (of 10192), d = 10, its = 55
i = 1288 (of 10192), d = 10, its = 37
i = 1289 (of 10192), d = 10, its = 49
i = 1290 (of 10192), d = 1.99483, its = 2
i = 1291 (of 10192), d = 10, its = 51
i = 1292 (of 10192), d = 6.11737, its = 8
i = 1293 (of 10192), d = 0.490846, its = 20
i = 1294 (of 10192), d = 10, its = 52
i = 1295 (of 10192), d = 1.83802, its = 6
i = 1296 (of 10192), d = 10, its = 53
i = 1297 (of 10192), d = 0.614063, its = 18
i = 1298 (of 10192), d = 10, its = 53
i = 1299 (of 10192), d = 10, its = 50
i = 1300 (of 10192), d = 10, its = 52
i = 1301 (of 10192), d = 10, its = 37
i = 1302 (of 10192), d = 10, its = 53
i = 1303 (of 10192), d = 10, its = 39
i = 1304 (of 10192), d = 3.04028, its = 6
i = 1305 (of 10192), d = 10, its = 54
i = 1306 (of 10192), d = 10, its = 54
i = 1307 (of 10192), d = 10, its = 37
i = 1308 (of 10192), d = 0.745705, its = 21
i = 1309 (of 10192), d = 1.52376, its = 6
i = 1310 (of 10192), d = 10, its = 53
i = 1311 (of 10192), d = 10, its = 53
i = 1312 (of 10192), d = 10, its = 54
i = 1313 (of 10192), d = 10, its = 53
i = 1314 (of 10192), d = 10, its = 39
i = 1315 (of 10192), d = 10, its = 49
i = 1316 (of 10192), d = 3.85947, its = 7
i = 1317 (of 10192), d = 2.22092, its = 5
i = 1318 (of 10192), d = 10, its = 41
i = 1319 (of 10192), d = 10, its = 37
i = 1320 (of 10192), d = 1.57021, its = 6
i = 1321 (of 10192), d = 2.22532, its = 5
i = 1322 (of 10192), d = 3.25204, its = 6
i = 1323 (of 10192), d = 10, its = 55
i = 1324 (of 10192), d = 10, its = 49
i = 1325 (of 10192), d = 10, its = 37
i = 1326 (of 10192), d = 10, its = 37
i = 1327 (of 10192), d = 1.80416, its = 4
i = 1328 (of 10192), d = 0.881052, its = 24
i = 1329 (of 10192), d = 10, its = 53
i = 1330 (of 10192), d = 1.58142, its = 6
i = 1331 (of 10192), d = 7.44314, its = 19
i = 1332 (of 10192), d = 10, its = 52
i = 1333 (of 10192), d = 10, its = 51
i = 1334 (of 10192), d = 10, its = 52
i = 1335 (of 10192), d = 10, its = 52
i = 1336 (of 10192), d = 10, its = 55
i = 1337 (of 10192), d = 0.928943, its = 19
i = 1338 (of 10192), d = 10, its = 37
i = 1339 (of 10192), d = 10, its = 52
i = 1340 (of 10192), d = 10, its = 54
i = 1341 (of 10192), d = 7.615, its = 8
i = 1342 (of 10192), d = 10, its = 50
i = 1343 (of 10192), d = 2.28479, its = 4
i = 1344 (of 10192), d = 1.23606, its = 5
i = 1345 (of 10192), d = 10, its = 50
i = 1346 (of 10192), d = 10, its = 37
i = 1347 (of 10192), d = 10, its = 52
i = 1348 (of 10192), d = 1.82295, its = 4
i = 1349 (of 10192), d = 10, its = 43
i = 1350 (of 10192), d = 10, its = 37
i = 1351 (of 10192), d = 10, its = 40
i = 1352 (of 10192), d = 10, its = 54
i = 1353 (of 10192), d = 10, its = 57
i = 1354 (of 10192), d = 10, its = 38
i = 1355 (of 10192), d = 10, its = 53
i = 1356 (of 10192), d = 10, its = 53
i = 1357 (of 10192), d = 3.89256, its = 7
i = 1358 (of 10192), d = 10, its = 39
i = 1359 (of 10192), d = 10, its = 37
i = 1360 (of 10192), d = 10, its = 51
i = 1361 (of 10192), d = 10, its = 50
i = 1362 (of 10192), d = 10, its = 37
i = 1363 (of 10192), d = 10, its = 53
i = 1364 (of 10192), d = 10, its = 54
i = 1365 (of 10192), d = 10, its = 56
i = 1366 (of 10192), d = 10, its = 50
i = 1367 (of 10192), d = 10, its = 57
i = 1368 (of 10192), d = 10, its = 56
i = 1369 (of 10192), d = 10, its = 52
i = 1370 (of 10192), d = 10, its = 37
i = 1371 (of 10192), d = 10, its = 56
i = 1372 (of 10192), d = 10, its = 51
i = 1373 (of 10192), d = 10, its = 39
i = 1374 (of 10192), d = 10, its = 56
i = 1375 (of 10192), d = 4.72719, its = 8
i = 1376 (of 10192), d = 10, its = 37
i = 1377 (of 10192), d = 10, its = 56
i = 1378 (of 10192), d = 2.69406, its = 5
i = 1379 (of 10192), d = 10, its = 52
i = 1380 (of 10192), d = 1.73835, its = 5
i = 1381 (of 10192), d = 10, its = 51
i = 1382 (of 10192), d = 10, its = 50
i = 1383 (of 10192), d = 10, its = 56
i = 1384 (of 10192), d = 10, its = 56
i = 1385 (of 10192), d = 10, its = 37
i = 1386 (of 10192), d = 1.48631, its = 7
i = 1387 (of 10192), d = 10, its = 52
i = 1388 (of 10192), d = 10, its = 56
i = 1389 (of 10192), d = 4.47177, its = 7
i = 1390 (of 10192), d = 10, its = 56
i = 1391 (of 10192), d = 0.633879, its = 18
i = 1392 (of 10192), d = 10, its = 51
i = 1393 (of 10192), d = 10, its = 49
i = 1394 (of 10192), d = 10, its = 41
i = 1395 (of 10192), d = 10, its = 43
i = 1396 (of 10192), d = 10, its = 50
i = 1397 (of 10192), d = 10, its = 50
i = 1398 (of 10192), d = 0.48446, its = 17
i = 1399 (of 10192), d = 10, its = 37
i = 1400 (of 10192), d = 10, its = 40
i = 1401 (of 10192), d = 2.17098, its = 4
i = 1402 (of 10192), d = 10, its = 52
i = 1403 (of 10192), d = 10, its = 38
i = 1404 (of 10192), d = 10, its = 37
i = 1405 (of 10192), d = 1.15161, its = 20
i = 1406 (of 10192), d = 10, its = 57
i = 1407 (of 10192), d = 10, its = 50
i = 1408 (of 10192), d = 10, its = 37
i = 1409 (of 10192), d = 10, its = 46
i = 1410 (of 10192), d = 2.52644, its = 5
i = 1411 (of 10192), d = 10, its = 52
i = 1412 (of 10192), d = 10, its = 51
i = 1413 (of 10192), d = 10, its = 40
i = 1414 (of 10192), d = 10, its = 37
i = 1415 (of 10192), d = 10, its = 54
i = 1416 (of 10192), d = 10, its = 52
i = 1417 (of 10192), d = 1.7768, its = 5
i = 1418 (of 10192), d = 10, its = 53
i = 1419 (of 10192), d = 10, its = 52
i = 1420 (of 10192), d = 10, its = 38
i = 1421 (of 10192), d = 10, its = 54
i = 1422 (of 10192), d = 2.75511, its = 6
i = 1423 (of 10192), d = 10, its = 37
i = 1424 (of 10192), d = 10, its = 52
i = 1425 (of 10192), d = 10, its = 42
i = 1426 (of 10192), d = 2.47328, its = 5
i = 1427 (of 10192), d = 1.46805, its = 6
i = 1428 (of 10192), d = 10, its = 52
i = 1429 (of 10192), d = 10, its = 54
i = 1430 (of 10192), d = 10, its = 52
i = 1431 (of 10192), d = 6.36711, its = 8
i = 1432 (of 10192), d = 10, its = 51
i = 1433 (of 10192), d = 10, its = 50
i = 1434 (of 10192), d = 10, its = 53
i = 1435 (of 10192), d = 10, its = 53
i = 1436 (of 10192), d = 10, its = 40
i = 1437 (of 10192), d = 10, its = 51
i = 1438 (of 10192), d = 10, its = 51
i = 1439 (of 10192), d = 10, its = 50
i = 1440 (of 10192), d = 10, its = 50
i = 1441 (of 10192), d = 4.27168, its = 7
i = 1442 (of 10192), d = 10, its = 37
i = 1443 (of 10192), d = 0.514825, its = 18
i = 1444 (of 10192), d = 10, its = 49
i = 1445 (of 10192), d = 10, its = 52
i = 1446 (of 10192), d = 10, its = 52
i = 1447 (of 10192), d = 4.24413, its = 9
i = 1448 (of 10192), d = 10, its = 52
i = 1449 (of 10192), d = 10, its = 50
i = 1450 (of 10192), d = 10, its = 53
i = 1451 (of 10192), d = 10, its = 37
i = 1452 (of 10192), d = 10, its = 54
i = 1453 (of 10192), d = 10, its = 37
i = 1454 (of 10192), d = 10, its = 50
i = 1455 (of 10192), d = 10, its = 50
i = 1456 (of 10192), d = 10, its = 53
i = 1457 (of 10192), d = 10, its = 38
i = 1458 (of 10192), d = 10, its = 51
i = 1459 (of 10192), d = 10, its = 37
i = 1460 (of 10192), d = 10, its = 51
i = 1461 (of 10192), d = 10, its = 53
i = 1462 (of 10192), d = 10, its = 56
i = 1463 (of 10192), d = 10, its = 37
i = 1464 (of 10192), d = 10, its = 52
i = 1465 (of 10192), d = 2.52528, its = 5
i = 1466 (of 10192), d = 10, its = 55
i = 1467 (of 10192), d = 10, its = 55
i = 1468 (of 10192), d = 1.94952, its = 4
i = 1469 (of 10192), d = 10, its = 43
i = 1470 (of 10192), d = 10, its = 51
i = 1471 (of 10192), d = 10, its = 54
i = 1472 (of 10192), d = 10, its = 54
i = 1473 (of 10192), d = 10, its = 56
i = 1474 (of 10192), d = 10, its = 56
i = 1475 (of 10192), d = 2.32645, its = 4
i = 1476 (of 10192), d = 10, its = 38
i = 1477 (of 10192), d = 10, its = 57
i = 1478 (of 10192), d = 10, its = 53
i = 1479 (of 10192), d = 2.63063, its = 5
i = 1480 (of 10192), d = 5.09988, its = 9
i = 1481 (of 10192), d = 1.79421, its = 4
i = 1482 (of 10192), d = 10, its = 38
i = 1483 (of 10192), d = 3.47986, its = 7
i = 1484 (of 10192), d = 10, its = 49
i = 1485 (of 10192), d = 0.632513, its = 19
i = 1486 (of 10192), d = 10, its = 52
i = 1487 (of 10192), d = 10, its = 52
i = 1488 (of 10192), d = 1.96776, its = 3
i = 1489 (of 10192), d = 10, its = 37
i = 1490 (of 10192), d = 10, its = 52
i = 1491 (of 10192), d = 10, its = 49
i = 1492 (of 10192), d = 10, its = 55
i = 1493 (of 10192), d = 1.93328, its = 4
i = 1494 (of 10192), d = 10, its = 39
i = 1495 (of 10192), d = 10, its = 53
i = 1496 (of 10192), d = 1.74743, its = 5
i = 1497 (of 10192), d = 10, its = 51
i = 1498 (of 10192), d = 0.984012, its = 16
i = 1499 (of 10192), d = 10, its = 40
i = 1500 (of 10192), d = 10, its = 54
i = 1501 (of 10192), d = 10, its = 37
i = 1502 (of 10192), d = 10, its = 50
i = 1503 (of 10192), d = 10, its = 50
i = 1504 (of 10192), d = 10, its = 54
i = 1505 (of 10192), d = 10, its = 57
i = 1506 (of 10192), d = 0.823535, its = 20
i = 1507 (of 10192), d = 10, its = 37
i = 1508 (of 10192), d = 10, its = 53
i = 1509 (of 10192), d = 0.981284, its = 21
i = 1510 (of 10192), d = 10, its = 56
i = 1511 (of 10192), d = 10, its = 37
i = 1512 (of 10192), d = 10, its = 51
i = 1513 (of 10192), d = 3.46959, its = 7
i = 1514 (of 10192), d = 10, its = 48
i = 1515 (of 10192), d = 10, its = 50
i = 1516 (of 10192), d = 10, its = 53
i = 1517 (of 10192), d = 2.42343, its = 5
i = 1518 (of 10192), d = 1.74732, its = 5
i = 1519 (of 10192), d = 10, its = 51
i = 1520 (of 10192), d = 10, its = 49
i = 1521 (of 10192), d = 10, its = 53
i = 1522 (of 10192), d = 1.72486, its = 5
i = 1523 (of 10192), d = 10, its = 56
i = 1524 (of 10192), d = 1.98769, its = 3
i = 1525 (of 10192), d = 10, its = 42
i = 1526 (of 10192), d = 10, its = 47
i = 1527 (of 10192), d = 10, its = 47
i = 1528 (of 10192), d = 2.62331, its = 5
i = 1529 (of 10192), d = 10, its = 56
i = 1530 (of 10192), d = 1.52527, its = 7
i = 1531 (of 10192), d = 10, its = 51
i = 1532 (of 10192), d = 10, its = 37
i = 1533 (of 10192), d = 10, its = 53
i = 1534 (of 10192), d = 10, its = 51
i = 1535 (of 10192), d = 10, its = 53
i = 1536 (of 10192), d = 1.42975, its = 7
i = 1537 (of 10192), d = 10, its = 42
i = 1538 (of 10192), d = 1.82759, its = 4
i = 1539 (of 10192), d = 10, its = 41
i = 1540 (of 10192), d = 1.87911, its = 4
i = 1541 (of 10192), d = 10, its = 56
i = 1542 (of 10192), d = 1.95172, its = 3
i = 1543 (of 10192), d = 10, its = 54
i = 1544 (of 10192), d = 10, its = 37
i = 1545 (of 10192), d = 10, its = 54
i = 1546 (of 10192), d = 10, its = 51
i = 1547 (of 10192), d = 10, its = 52
i = 1548 (of 10192), d = 10, its = 52
i = 1549 (of 10192), d = 10, its = 42
i = 1550 (of 10192), d = 10, its = 37
i = 1551 (of 10192), d = 0.775423, its = 18
i = 1552 (of 10192), d = 10, its = 37
i = 1553 (of 10192), d = 2.6837, its = 6
i = 1554 (of 10192), d = 1.77473, its = 6
i = 1555 (of 10192), d = 10, its = 37
i = 1556 (of 10192), d = 10, its = 54
i = 1557 (of 10192), d = 0.495692, its = 17
i = 1558 (of 10192), d = 10, its = 40
i = 1559 (of 10192), d = 10, its = 54
i = 1560 (of 10192), d = 10, its = 51
i = 1561 (of 10192), d = 10, its = 52
i = 1562 (of 10192), d = 0.751355, its = 20
i = 1563 (of 10192), d = 1.33767, its = 8
i = 1564 (of 10192), d = 10, its = 49
i = 1565 (of 10192), d = 1.62537, its = 6
i = 1566 (of 10192), d = 10, its = 54
i = 1567 (of 10192), d = 10, its = 51
i = 1568 (of 10192), d = 10, its = 41
i = 1569 (of 10192), d = 1.50715, its = 7
i = 1570 (of 10192), d = 2.12617, its = 4
i = 1571 (of 10192), d = 10, its = 50
i = 1572 (of 10192), d = 10, its = 54
i = 1573 (of 10192), d = 2.35019, its = 5
i = 1574 (of 10192), d = 10, its = 39
i = 1575 (of 10192), d = 2.45447, its = 5
i = 1576 (of 10192), d = 1.73043, its = 5
i = 1577 (of 10192), d = 10, its = 40
i = 1578 (of 10192), d = 10, its = 39
i = 1579 (of 10192), d = 10, its = 37
i = 1580 (of 10192), d = 10, its = 37
i = 1581 (of 10192), d = 10, its = 54
i = 1582 (of 10192), d = 1.68508, its = 6
i = 1583 (of 10192), d = 1.29562, its = 8
i = 1584 (of 10192), d = 10, its = 60
i = 1585 (of 10192), d = 10, its = 42
i = 1586 (of 10192), d = 10, its = 37
i = 1587 (of 10192), d = 0.94731, its = 25
i = 1588 (of 10192), d = 10, its = 56
i = 1589 (of 10192), d = 10, its = 53
i = 1590 (of 10192), d = 2.14247, its = 4
i = 1591 (of 10192), d = 0.565109, its = 18
i = 1592 (of 10192), d = 10, its = 50
i = 1593 (of 10192), d = 10, its = 53
i = 1594 (of 10192), d = 10, its = 50
i = 1595 (of 10192), d = 10, its = 39
i = 1596 (of 10192), d = 3.36658, its = 6
i = 1597 (of 10192), d = 10, its = 53
i = 1598 (of 10192), d = 3.03153, its = 6
i = 1599 (of 10192), d = 10, its = 54
i = 1600 (of 10192), d = 2.83228, its = 6
i = 1601 (of 10192), d = 2.56908, its = 5
i = 1602 (of 10192), d = 1.26156, its = 6
i = 1603 (of 10192), d = 10, its = 51
i = 1604 (of 10192), d = 10, its = 53
i = 1605 (of 10192), d = 10, its = 41
i = 1606 (of 10192), d = 10, its = 52
i = 1607 (of 10192), d = 2.38895, its = 5
i = 1608 (of 10192), d = 10, its = 54
i = 1609 (of 10192), d = 10, its = 50
i = 1610 (of 10192), d = 10, its = 59
i = 1611 (of 10192), d = 10, its = 50
i = 1612 (of 10192), d = 10, its = 49
i = 1613 (of 10192), d = 10, its = 37
i = 1614 (of 10192), d = 10, its = 37
i = 1615 (of 10192), d = 10, its = 54
i = 1616 (of 10192), d = 10, its = 52
i = 1617 (of 10192), d = 1.15327, its = 6
i = 1618 (of 10192), d = 2.40775, its = 6
i = 1619 (of 10192), d = 10, its = 54
i = 1620 (of 10192), d = 1.68499, its = 6
i = 1621 (of 10192), d = 10, its = 50
i = 1622 (of 10192), d = 10, its = 51
i = 1623 (of 10192), d = 1.26523, its = 8
i = 1624 (of 10192), d = 0.835479, its = 20
i = 1625 (of 10192), d = 10, its = 37
i = 1626 (of 10192), d = 10, its = 53
i = 1627 (of 10192), d = 10, its = 37
i = 1628 (of 10192), d = 2.05215, its = 3
i = 1629 (of 10192), d = 10, its = 49
i = 1630 (of 10192), d = 10, its = 52
i = 1631 (of 10192), d = 10, its = 51
i = 1632 (of 10192), d = 10, its = 37
i = 1633 (of 10192), d = 10, its = 54
i = 1634 (of 10192), d = 1.37098, its = 6
i = 1635 (of 10192), d = 1.49346, its = 6
i = 1636 (of 10192), d = 2.96068, its = 5
i = 1637 (of 10192), d = 10, its = 37
i = 1638 (of 10192), d = 2.71532, its = 6
i = 1639 (of 10192), d = 2.41205, its = 5
i = 1640 (of 10192), d = 0.432006, its = 19
i = 1641 (of 10192), d = 10, its = 40
i = 1642 (of 10192), d = 10, its = 53
i = 1643 (of 10192), d = 10, its = 41
i = 1644 (of 10192), d = 10, its = 37
i = 1645 (of 10192), d = 10, its = 55
i = 1646 (of 10192), d = 10, its = 51
i = 1647 (of 10192), d = 10, its = 50
i = 1648 (of 10192), d = 10, its = 51
i = 1649 (of 10192), d = 10, its = 57
i = 1650 (of 10192), d = 10, its = 43
i = 1651 (of 10192), d = 1.93869, its = 3
i = 1652 (of 10192), d = 1.08697, its = 25
i = 1653 (of 10192), d = 0.903697, its = 20
i = 1654 (of 10192), d = 2.05158, its = 3
i = 1655 (of 10192), d = 10, its = 39
i = 1656 (of 10192), d = 1.30289, its = 7
i = 1657 (of 10192), d = 10, its = 51
i = 1658 (of 10192), d = 10, its = 53
i = 1659 (of 10192), d = 2.16895, its = 4
i = 1660 (of 10192), d = 10, its = 56
i = 1661 (of 10192), d = 10, its = 53
i = 1662 (of 10192), d = 10, its = 38
i = 1663 (of 10192), d = 2.26964, its = 5
i = 1664 (of 10192), d = 10, its = 52
i = 1665 (of 10192), d = 10, its = 40
i = 1666 (of 10192), d = 10, its = 51
i = 1667 (of 10192), d = 1.76725, its = 4
i = 1668 (of 10192), d = 10, its = 51
i = 1669 (of 10192), d = 10, its = 50
i = 1670 (of 10192), d = 10, its = 54
i = 1671 (of 10192), d = 2.59436, its = 6
i = 1672 (of 10192), d = 10, its = 48
i = 1673 (of 10192), d = 10, its = 50
i = 1674 (of 10192), d = 10, its = 55
i = 1675 (of 10192), d = 10, its = 52
i = 1676 (of 10192), d = 10, its = 52
i = 1677 (of 10192), d = 10, its = 42
i = 1678 (of 10192), d = 10, its = 41
i = 1679 (of 10192), d = 0.559619, its = 18
i = 1680 (of 10192), d = 1.88436, its = 4
i = 1681 (of 10192), d = 10, its = 37
i = 1682 (of 10192), d = 10, its = 52
i = 1683 (of 10192), d = 10, its = 53
i = 1684 (of 10192), d = 10, its = 49
i = 1685 (of 10192), d = 1.83658, its = 4
i = 1686 (of 10192), d = 10, its = 50
i = 1687 (of 10192), d = 10, its = 48
i = 1688 (of 10192), d = 10, its = 51
i = 1689 (of 10192), d = 10, its = 51
i = 1690 (of 10192), d = 10, its = 37
i = 1691 (of 10192), d = 2.33402, its = 4
i = 1692 (of 10192), d = 3.2484, its = 7
i = 1693 (of 10192), d = 10, its = 37
i = 1694 (of 10192), d = 10, its = 40
i = 1695 (of 10192), d = 10, its = 43
i = 1696 (of 10192), d = 1.35356, its = 6
i = 1697 (of 10192), d = 0.912026, its = 20
i = 1698 (of 10192), d = 10, its = 37
i = 1699 (of 10192), d = 10, its = 37
i = 1700 (of 10192), d = 10, its = 52
i = 1701 (of 10192), d = 1.94501, its = 3
i = 1702 (of 10192), d = 10, its = 54
i = 1703 (of 10192), d = 10, its = 56
i = 1704 (of 10192), d = 3.08293, its = 5
i = 1705 (of 10192), d = 10, its = 55
i = 1706 (of 10192), d = 10, its = 37
i = 1707 (of 10192), d = 10, its = 57
i = 1708 (of 10192), d = 10, its = 51
i = 1709 (of 10192), d = 1.65506, its = 6
i = 1710 (of 10192), d = 10, its = 46
i = 1711 (of 10192), d = 10, its = 54
i = 1712 (of 10192), d = 10, its = 55
i = 1713 (of 10192), d = 10, its = 37
i = 1714 (of 10192), d = 10, its = 52
i = 1715 (of 10192), d = 10, its = 51
i = 1716 (of 10192), d = 10, its = 37
i = 1717 (of 10192), d = 1.73862, its = 5
i = 1718 (of 10192), d = 10, its = 53
i = 1719 (of 10192), d = 10, its = 52
i = 1720 (of 10192), d = 10, its = 55
i = 1721 (of 10192), d = 10, its = 49
i = 1722 (of 10192), d = 10, its = 37
i = 1723 (of 10192), d = 10, its = 57
i = 1724 (of 10192), d = 10, its = 37
i = 1725 (of 10192), d = 1.45593, its = 7
i = 1726 (of 10192), d = 2.74374, its = 6
i = 1727 (of 10192), d = 10, its = 37
i = 1728 (of 10192), d = 3.58234, its = 7
i = 1729 (of 10192), d = 10, its = 37
i = 1730 (of 10192), d = 10, its = 55
i = 1731 (of 10192), d = 10, its = 57
i = 1732 (of 10192), d = 0.358228, its = 18
i = 1733 (of 10192), d = 1.23265, its = 18
i = 1734 (of 10192), d = 10, its = 53
i = 1735 (of 10192), d = 10, its = 40
i = 1736 (of 10192), d = 2.93398, its = 6
i = 1737 (of 10192), d = 0.60061, its = 18
i = 1738 (of 10192), d = 10, its = 53
i = 1739 (of 10192), d = 10, its = 40
i = 1740 (of 10192), d = 0.699937, its = 18
i = 1741 (of 10192), d = 10, its = 50
i = 1742 (of 10192), d = 10, its = 37
i = 1743 (of 10192), d = 10, its = 37
i = 1744 (of 10192), d = 4.02859, its = 7
i = 1745 (of 10192), d = 0.81517, its = 26
i = 1746 (of 10192), d = 10, its = 51
i = 1747 (of 10192), d = 0.85664, its = 18
i = 1748 (of 10192), d = 10, its = 55
i = 1749 (of 10192), d = 10, its = 55
i = 1750 (of 10192), d = 10, its = 37
i = 1751 (of 10192), d = 7.95457, its = 9
i = 1752 (of 10192), d = 1.08176, its = 17
i = 1753 (of 10192), d = 10, its = 55
i = 1754 (of 10192), d = 10, its = 37
i = 1755 (of 10192), d = 4.8021, its = 7
i = 1756 (of 10192), d = 0.793589, its = 16
i = 1757 (of 10192), d = 10, its = 41
i = 1758 (of 10192), d = 10, its = 37
i = 1759 (of 10192), d = 0.755586, its = 24
i = 1760 (of 10192), d = 10, its = 40
i = 1761 (of 10192), d = 10, its = 40
i = 1762 (of 10192), d = 10, its = 51
i = 1763 (of 10192), d = 10, its = 50
i = 1764 (of 10192), d = 10, its = 53
i = 1765 (of 10192), d = 10, its = 49
i = 1766 (of 10192), d = 10, its = 55
i = 1767 (of 10192), d = 2.09014, its = 4
i = 1768 (of 10192), d = 10, its = 21
i = 1769 (of 10192), d = 10, its = 52
i = 1770 (of 10192), d = 10, its = 37
i = 1771 (of 10192), d = 10, its = 55
i = 1772 (of 10192), d = 3.47188, its = 7
i = 1773 (of 10192), d = 2.79149, its = 6
i = 1774 (of 10192), d = 10, its = 54
i = 1775 (of 10192), d = 10, its = 51
i = 1776 (of 10192), d = 10, its = 50
i = 1777 (of 10192), d = 10, its = 37
i = 1778 (of 10192), d = 10, its = 37
i = 1779 (of 10192), d = 10, its = 52
i = 1780 (of 10192), d = 10, its = 37
i = 1781 (of 10192), d = 10, its = 57
i = 1782 (of 10192), d = 2.84125, its = 5
i = 1783 (of 10192), d = 10, its = 55
i = 1784 (of 10192), d = 10, its = 42
i = 1785 (of 10192), d = 1.55777, its = 7
i = 1786 (of 10192), d = 10, its = 52
i = 1787 (of 10192), d = 1.71994, its = 5
i = 1788 (of 10192), d = 2.13227, its = 4
i = 1789 (of 10192), d = 10, its = 47
i = 1790 (of 10192), d = 10, its = 53
i = 1791 (of 10192), d = 10, its = 39
i = 1792 (of 10192), d = 10, its = 37
i = 1793 (of 10192), d = 10, its = 57
i = 1794 (of 10192), d = 10, its = 37
i = 1795 (of 10192), d = 10, its = 37
i = 1796 (of 10192), d = 1.70671, its = 6
i = 1797 (of 10192), d = 10, its = 54
i = 1798 (of 10192), d = 10, its = 52
i = 1799 (of 10192), d = 10, its = 49
i = 1800 (of 10192), d = 2.39364, its = 5
i = 1801 (of 10192), d = 10, its = 48
i = 1802 (of 10192), d = 10, its = 52
i = 1803 (of 10192), d = 1.77002, its = 5
i = 1804 (of 10192), d = 2.34923, its = 6
i = 1805 (of 10192), d = 2.32516, its = 5
i = 1806 (of 10192), d = 10, its = 37
i = 1807 (of 10192), d = 10, its = 58
i = 1808 (of 10192), d = 2.91958, its = 6
i = 1809 (of 10192), d = 10, its = 39
i = 1810 (of 10192), d = 1.06592, its = 18
i = 1811 (of 10192), d = 10, its = 56
i = 1812 (of 10192), d = 0.873416, its = 21
i = 1813 (of 10192), d = 1.04917, its = 16
i = 1814 (of 10192), d = 1.72242, its = 6
i = 1815 (of 10192), d = 10, its = 37
i = 1816 (of 10192), d = 10, its = 40
i = 1817 (of 10192), d = 10, its = 51
i = 1818 (of 10192), d = 10, its = 55
i = 1819 (of 10192), d = 10, its = 53
i = 1820 (of 10192), d = 10, its = 56
i = 1821 (of 10192), d = 1.8616, its = 4
i = 1822 (of 10192), d = 10, its = 40
i = 1823 (of 10192), d = 10, its = 50
i = 1824 (of 10192), d = 10, its = 54
i = 1825 (of 10192), d = 10, its = 51
i = 1826 (of 10192), d = 10, its = 53
i = 1827 (of 10192), d = 10, its = 49
i = 1828 (of 10192), d = 1.78309, its = 5
i = 1829 (of 10192), d = 1.69191, its = 6
i = 1830 (of 10192), d = 2.53902, its = 5
i = 1831 (of 10192), d = 10, its = 52
i = 1832 (of 10192), d = 10, its = 59
i = 1833 (of 10192), d = 10, its = 41
i = 1834 (of 10192), d = 10, its = 53
i = 1835 (of 10192), d = 7.80453, its = 9
i = 1836 (of 10192), d = 10, its = 55
i = 1837 (of 10192), d = 1.772, its = 5
i = 1838 (of 10192), d = 10, its = 37
i = 1839 (of 10192), d = 10, its = 53
i = 1840 (of 10192), d = 10, its = 59
i = 1841 (of 10192), d = 10, its = 53
i = 1842 (of 10192), d = 10, its = 54
i = 1843 (of 10192), d = 10, its = 51
i = 1844 (of 10192), d = 2.06616, its = 3
i = 1845 (of 10192), d = 10, its = 55
i = 1846 (of 10192), d = 10, its = 20
i = 1847 (of 10192), d = 10, its = 51
i = 1848 (of 10192), d = 10, its = 55
i = 1849 (of 10192), d = 10, its = 48
i = 1850 (of 10192), d = 10, its = 37
i = 1851 (of 10192), d = 10, its = 40
i = 1852 (of 10192), d = 10, its = 55
i = 1853 (of 10192), d = 10, its = 52
i = 1854 (of 10192), d = 10, its = 37
i = 1855 (of 10192), d = 10, its = 37
i = 1856 (of 10192), d = 10, its = 50
i = 1857 (of 10192), d = 10, its = 48
i = 1858 (of 10192), d = 10, its = 52
i = 1859 (of 10192), d = 10, its = 48
i = 1860 (of 10192), d = 10, its = 54
i = 1861 (of 10192), d = 1.48101, its = 6
i = 1862 (of 10192), d = 2.30673, its = 4
i = 1863 (of 10192), d = 2.2361, its = 4
i = 1864 (of 10192), d = 10, its = 37
i = 1865 (of 10192), d = 10, its = 50
i = 1866 (of 10192), d = 0.472939, its = 19
i = 1867 (of 10192), d = 3.20738, its = 7
i = 1868 (of 10192), d = 10, its = 53
i = 1869 (of 10192), d = 2.13074, its = 4
i = 1870 (of 10192), d = 10, its = 51
i = 1871 (of 10192), d = 10, its = 55
i = 1872 (of 10192), d = 1.55023, its = 6
i = 1873 (of 10192), d = 10, its = 50
i = 1874 (of 10192), d = 10, its = 52
i = 1875 (of 10192), d = 1.36172, its = 6
i = 1876 (of 10192), d = 0.528602, its = 17
i = 1877 (of 10192), d = 10, its = 55
i = 1878 (of 10192), d = 0.881187, its = 19
i = 1879 (of 10192), d = 10, its = 38
i = 1880 (of 10192), d = 10, its = 54
i = 1881 (of 10192), d = 0.948172, its = 24
i = 1882 (of 10192), d = 10, its = 55
i = 1883 (of 10192), d = 10, its = 54
i = 1884 (of 10192), d = 10, its = 37
i = 1885 (of 10192), d = 10, its = 53
i = 1886 (of 10192), d = 1.64817, its = 5
i = 1887 (of 10192), d = 10, its = 40
i = 1888 (of 10192), d = 3.10929, its = 6
i = 1889 (of 10192), d = 10, its = 54
i = 1890 (of 10192), d = 10, its = 46
i = 1891 (of 10192), d = 10, its = 54
i = 1892 (of 10192), d = 1.27792, its = 7
i = 1893 (of 10192), d = 10, its = 56
i = 1894 (of 10192), d = 1.51883, its = 6
i = 1895 (of 10192), d = 10, its = 56
i = 1896 (of 10192), d = 10, its = 51
i = 1897 (of 10192), d = 1.07869, its = 19
i = 1898 (of 10192), d = 10, its = 50
i = 1899 (of 10192), d = 10, its = 58
i = 1900 (of 10192), d = 10, its = 55
i = 1901 (of 10192), d = 10, its = 52
i = 1902 (of 10192), d = 10, its = 58
i = 1903 (of 10192), d = 10, its = 37
i = 1904 (of 10192), d = 1.6813, its = 5
i = 1905 (of 10192), d = 10, its = 51
i = 1906 (of 10192), d = 2.39261, its = 5
i = 1907 (of 10192), d = 10, its = 50
i = 1908 (of 10192), d = 3.47986, its = 7
i = 1909 (of 10192), d = 10, its = 54
i = 1910 (of 10192), d = 10, its = 52
i = 1911 (of 10192), d = 10, its = 55
i = 1912 (of 10192), d = 10, its = 55
i = 1913 (of 10192), d = 10, its = 50
i = 1914 (of 10192), d = 1.75325, its = 5
i = 1915 (of 10192), d = 10, its = 54
i = 1916 (of 10192), d = 10, its = 53
i = 1917 (of 10192), d = 10, its = 53
i = 1918 (of 10192), d = 8.4988, its = 35
i = 1919 (of 10192), d = 10, its = 39
i = 1920 (of 10192), d = 0.81429, its = 16
i = 1921 (of 10192), d = 1.77291, its = 5
i = 1922 (of 10192), d = 10, its = 55
i = 1923 (of 10192), d = 10, its = 50
i = 1924 (of 10192), d = 10, its = 37
i = 1925 (of 10192), d = 10, its = 37
i = 1926 (of 10192), d = 10, its = 51
i = 1927 (of 10192), d = 10, its = 40
i = 1928 (of 10192), d = 10, its = 56
i = 1929 (of 10192), d = 10, its = 39
i = 1930 (of 10192), d = 10, its = 37
i = 1931 (of 10192), d = 10, its = 50
i = 1932 (of 10192), d = 1.92743, its = 4
i = 1933 (of 10192), d = 10, its = 57
i = 1934 (of 10192), d = 10, its = 56
i = 1935 (of 10192), d = 10, its = 37
i = 1936 (of 10192), d = 1.95889, its = 3
i = 1937 (of 10192), d = 1.90543, its = 4
i = 1938 (of 10192), d = 10, its = 51
i = 1939 (of 10192), d = 1.20017, its = 17
i = 1940 (of 10192), d = 10, its = 42
i = 1941 (of 10192), d = 10, its = 55
i = 1942 (of 10192), d = 10, its = 40
i = 1943 (of 10192), d = 0.745705, its = 21
i = 1944 (of 10192), d = 10, its = 54
i = 1945 (of 10192), d = 1.70479, its = 5
i = 1946 (of 10192), d = 8.35008, its = 8
i = 1947 (of 10192), d = 10, its = 55
i = 1948 (of 10192), d = 10, its = 53
i = 1949 (of 10192), d = 10, its = 51
i = 1950 (of 10192), d = 10, its = 51
i = 1951 (of 10192), d = 3.45912, its = 7
i = 1952 (of 10192), d = 0.538908, its = 16
i = 1953 (of 10192), d = 10, its = 37
i = 1954 (of 10192), d = 1.97122, its = 3
i = 1955 (of 10192), d = 10, its = 37
i = 1956 (of 10192), d = 10, its = 55
i = 1957 (of 10192), d = 10, its = 40
i = 1958 (of 10192), d = 2.85419, its = 6
i = 1959 (of 10192), d = 3.24266, its = 7
i = 1960 (of 10192), d = 10, its = 55
i = 1961 (of 10192), d = 10, its = 55
i = 1962 (of 10192), d = 10, its = 51
i = 1963 (of 10192), d = 10, its = 37
i = 1964 (of 10192), d = 1.47638, its = 7
i = 1965 (of 10192), d = 1.06333, its = 18
i = 1966 (of 10192), d = 3.06218, its = 6
i = 1967 (of 10192), d = 10, its = 54
i = 1968 (of 10192), d = 0.788642, its = 23
i = 1969 (of 10192), d = 10, its = 38
i = 1970 (of 10192), d = 10, its = 53
i = 1971 (of 10192), d = 2.41205, its = 5
i = 1972 (of 10192), d = 10, its = 53
i = 1973 (of 10192), d = 10, its = 37
i = 1974 (of 10192), d = 2.47483, its = 5
i = 1975 (of 10192), d = 10, its = 56
i = 1976 (of 10192), d = 10, its = 53
i = 1977 (of 10192), d = 0.747187, its = 16
i = 1978 (of 10192), d = 10, its = 41
i = 1979 (of 10192), d = 10, its = 50
i = 1980 (of 10192), d = 10, its = 57
i = 1981 (of 10192), d = 10, its = 52
i = 1982 (of 10192), d = 10, its = 53
i = 1983 (of 10192), d = 2.33461, its = 5
i = 1984 (of 10192), d = 10, its = 37
i = 1985 (of 10192), d = 10, its = 57
i = 1986 (of 10192), d = 10, its = 51
i = 1987 (of 10192), d = 10, its = 54
i = 1988 (of 10192), d = 10, its = 53
i = 1989 (of 10192), d = 10, its = 41
i = 1990 (of 10192), d = 10, its = 54
i = 1991 (of 10192), d = 10, its = 55
i = 1992 (of 10192), d = 10, its = 56
i = 1993 (of 10192), d = 10, its = 50
i = 1994 (of 10192), d = 1.49625, its = 7
i = 1995 (of 10192), d = 10, its = 37
i = 1996 (of 10192), d = 10, its = 55
i = 1997 (of 10192), d = 10, its = 43
i = 1998 (of 10192), d = 10, its = 52
i = 1999 (of 10192), d = 10, its = 53
i = 2000 (of 10192), d = 10, its = 51
i = 2001 (of 10192), d = 4.58016, its = 7
i = 2002 (of 10192), d = 0.689074, its = 17
i = 2003 (of 10192), d = 1.98775, its = 3
i = 2004 (of 10192), d = 10, its = 52
i = 2005 (of 10192), d = 10, its = 39
i = 2006 (of 10192), d = 2.30749, its = 5
i = 2007 (of 10192), d = 10, its = 54
i = 2008 (of 10192), d = 10, its = 40
i = 2009 (of 10192), d = 10, its = 53
i = 2010 (of 10192), d = 10, its = 55
i = 2011 (of 10192), d = 10, its = 53
i = 2012 (of 10192), d = 7.54388, its = 8
i = 2013 (of 10192), d = 0.829103, its = 20
i = 2014 (of 10192), d = 3.91597, its = 6
i = 2015 (of 10192), d = 10, its = 53
i = 2016 (of 10192), d = 10, its = 50
i = 2017 (of 10192), d = 10, its = 37
i = 2018 (of 10192), d = 1.50074, its = 6
i = 2019 (of 10192), d = 2.72895, its = 6
i = 2020 (of 10192), d = 1.00205, its = 19
i = 2021 (of 10192), d = 10, its = 39
i = 2022 (of 10192), d = 2.1915, its = 4
i = 2023 (of 10192), d = 2.86095, its = 8
i = 2024 (of 10192), d = 10, its = 59
i = 2025 (of 10192), d = 10, its = 40
i = 2026 (of 10192), d = 10, its = 56
i = 2027 (of 10192), d = 10, its = 52
i = 2028 (of 10192), d = 10, its = 50
i = 2029 (of 10192), d = 10, its = 52
i = 2030 (of 10192), d = 1.79212, its = 5
i = 2031 (of 10192), d = 2.27658, its = 5
i = 2032 (of 10192), d = 10, its = 40
i = 2033 (of 10192), d = 10, its = 53
i = 2034 (of 10192), d = 10, its = 56
i = 2035 (of 10192), d = 10, its = 38
i = 2036 (of 10192), d = 1.05648, its = 21
i = 2037 (of 10192), d = 3.87016, its = 7
i = 2038 (of 10192), d = 10, its = 51
i = 2039 (of 10192), d = 10, its = 59
i = 2040 (of 10192), d = 10, its = 39
i = 2041 (of 10192), d = 10, its = 37
i = 2042 (of 10192), d = 10, its = 50
i = 2043 (of 10192), d = 10, its = 37
i = 2044 (of 10192), d = 10, its = 56
i = 2045 (of 10192), d = 10, its = 50
i = 2046 (of 10192), d = 10, its = 53
i = 2047 (of 10192), d = 10, its = 40
i = 2048 (of 10192), d = 10, its = 37
i = 2049 (of 10192), d = 10, its = 40
i = 2050 (of 10192), d = 3.46947, its = 6
i = 2051 (of 10192), d = 10, its = 53
i = 2052 (of 10192), d = 10, its = 37
i = 2053 (of 10192), d = 1.00811, its = 22
i = 2054 (of 10192), d = 2.90916, its = 6
i = 2055 (of 10192), d = 10, its = 41
i = 2056 (of 10192), d = 10, its = 49
i = 2057 (of 10192), d = 10, its = 40
i = 2058 (of 10192), d = 1.62201, its = 6
i = 2059 (of 10192), d = 10, its = 57
i = 2060 (of 10192), d = 10, its = 54
i = 2061 (of 10192), d = 10, its = 53
i = 2062 (of 10192), d = 10, its = 51
i = 2063 (of 10192), d = 10, its = 37
i = 2064 (of 10192), d = 3.26557, its = 6
i = 2065 (of 10192), d = 10, its = 37
i = 2066 (of 10192), d = 10, its = 40
i = 2067 (of 10192), d = 10, its = 51
i = 2068 (of 10192), d = 10, its = 55
i = 2069 (of 10192), d = 10, its = 51
i = 2070 (of 10192), d = 10, its = 39
i = 2071 (of 10192), d = 10, its = 52
i = 2072 (of 10192), d = 10, its = 51
i = 2073 (of 10192), d = 10, its = 49
i = 2074 (of 10192), d = 10, its = 50
i = 2075 (of 10192), d = 0.843501, its = 20
i = 2076 (of 10192), d = 10, its = 40
i = 2077 (of 10192), d = 10, its = 40
i = 2078 (of 10192), d = 2.95251, its = 6
i = 2079 (of 10192), d = 10, its = 52
i = 2080 (of 10192), d = 10, its = 54
i = 2081 (of 10192), d = 10, its = 41
i = 2082 (of 10192), d = 10, its = 37
i = 2083 (of 10192), d = 10, its = 40
i = 2084 (of 10192), d = 0.837427, its = 24
i = 2085 (of 10192), d = 10, its = 52
i = 2086 (of 10192), d = 10, its = 52
i = 2087 (of 10192), d = 1.46782, its = 6
i = 2088 (of 10192), d = 10, its = 56
i = 2089 (of 10192), d = 10, its = 52
i = 2090 (of 10192), d = 10, its = 40
i = 2091 (of 10192), d = 10, its = 37
i = 2092 (of 10192), d = 10, its = 37
i = 2093 (of 10192), d = 10, its = 52
i = 2094 (of 10192), d = 10, its = 37
i = 2095 (of 10192), d = 10, its = 41
i = 2096 (of 10192), d = 10, its = 52
i = 2097 (of 10192), d = 10, its = 53
i = 2098 (of 10192), d = 10, its = 37
i = 2099 (of 10192), d = 10, its = 53
i = 2100 (of 10192), d = 10, its = 54
i = 2101 (of 10192), d = 10, its = 51
i = 2102 (of 10192), d = 3.17344, its = 6
i = 2103 (of 10192), d = 10, its = 39
i = 2104 (of 10192), d = 10, its = 52
i = 2105 (of 10192), d = 2.42748, its = 5
i = 2106 (of 10192), d = 10, its = 50
i = 2107 (of 10192), d = 0.911808, its = 22
i = 2108 (of 10192), d = 0.539758, its = 24
i = 2109 (of 10192), d = 10, its = 56
i = 2110 (of 10192), d = 10, its = 50
i = 2111 (of 10192), d = 10, its = 42
i = 2112 (of 10192), d = 10, its = 53
i = 2113 (of 10192), d = 10, its = 51
i = 2114 (of 10192), d = 10, its = 42
i = 2115 (of 10192), d = 10, its = 52
i = 2116 (of 10192), d = 2.1484, its = 4
i = 2117 (of 10192), d = 10, its = 55
i = 2118 (of 10192), d = 10, its = 55
i = 2119 (of 10192), d = 10, its = 51
i = 2120 (of 10192), d = 10, its = 54
i = 2121 (of 10192), d = 0.624791, its = 22
i = 2122 (of 10192), d = 10, its = 55
i = 2123 (of 10192), d = 10, its = 37
i = 2124 (of 10192), d = 10, its = 48
i = 2125 (of 10192), d = 10, its = 37
i = 2126 (of 10192), d = 10, its = 52
i = 2127 (of 10192), d = 10, its = 55
i = 2128 (of 10192), d = 10, its = 52
i = 2129 (of 10192), d = 10, its = 40
i = 2130 (of 10192), d = 2.06153, its = 4
i = 2131 (of 10192), d = 1.7651, its = 5
i = 2132 (of 10192), d = 10, its = 41
i = 2133 (of 10192), d = 10, its = 37
i = 2134 (of 10192), d = 10, its = 55
i = 2135 (of 10192), d = 1.281, its = 6
i = 2136 (of 10192), d = 10, its = 52
i = 2137 (of 10192), d = 10, its = 55
i = 2138 (of 10192), d = 1.69191, its = 6
i = 2139 (of 10192), d = 10, its = 52
i = 2140 (of 10192), d = 10, its = 47
i = 2141 (of 10192), d = 10, its = 50
i = 2142 (of 10192), d = 10, its = 41
i = 2143 (of 10192), d = 10, its = 51
i = 2144 (of 10192), d = 1.98648, its = 3
i = 2145 (of 10192), d = 10, its = 41
i = 2146 (of 10192), d = 10, its = 41
i = 2147 (of 10192), d = 3.80773, its = 7
i = 2148 (of 10192), d = 3.31074, its = 8
i = 2149 (of 10192), d = 10, its = 50
i = 2150 (of 10192), d = 2.21169, its = 4
i = 2151 (of 10192), d = 4.0271, its = 7
i = 2152 (of 10192), d = 10, its = 49
i = 2153 (of 10192), d = 10, its = 51
i = 2154 (of 10192), d = 10, its = 49
i = 2155 (of 10192), d = 10, its = 56
i = 2156 (of 10192), d = 10, its = 54
i = 2157 (of 10192), d = 3.76137, its = 6
i = 2158 (of 10192), d = 2.01984, its = 3
i = 2159 (of 10192), d = 10, its = 49
i = 2160 (of 10192), d = 10, its = 55
i = 2161 (of 10192), d = 0.425276, its = 18
i = 2162 (of 10192), d = 10, its = 37
i = 2163 (of 10192), d = 0.751355, its = 20
i = 2164 (of 10192), d = 10, its = 51
i = 2165 (of 10192), d = 10, its = 49
i = 2166 (of 10192), d = 1.92899, its = 3
i = 2167 (of 10192), d = 10, its = 37
i = 2168 (of 10192), d = 10, its = 56
i = 2169 (of 10192), d = 10, its = 52
i = 2170 (of 10192), d = 10, its = 51
i = 2171 (of 10192), d = 10, its = 52
i = 2172 (of 10192), d = 10, its = 49
i = 2173 (of 10192), d = 10, its = 54
i = 2174 (of 10192), d = 10, its = 50
i = 2175 (of 10192), d = 10, its = 58
i = 2176 (of 10192), d = 10, its = 46
i = 2177 (of 10192), d = 10, its = 52
i = 2178 (of 10192), d = 1.88136, its = 4
i = 2179 (of 10192), d = 10, its = 37
i = 2180 (of 10192), d = 10, its = 38
i = 2181 (of 10192), d = 10, its = 37
i = 2182 (of 10192), d = 10, its = 53
i = 2183 (of 10192), d = 2.38853, its = 5
i = 2184 (of 10192), d = 10, its = 37
i = 2185 (of 10192), d = 10, its = 50
i = 2186 (of 10192), d = 10, its = 58
i = 2187 (of 10192), d = 10, its = 39
i = 2188 (of 10192), d = 1.00031, its = 17
i = 2189 (of 10192), d = 10, its = 55
i = 2190 (of 10192), d = 10, its = 55
i = 2191 (of 10192), d = 10, its = 52
i = 2192 (of 10192), d = 2.59758, its = 6
i = 2193 (of 10192), d = 10, its = 55
i = 2194 (of 10192), d = 10, its = 37
i = 2195 (of 10192), d = 10, its = 55
i = 2196 (of 10192), d = 2.48438, its = 5
i = 2197 (of 10192), d = 10, its = 39
i = 2198 (of 10192), d = 0.851266, its = 18
i = 2199 (of 10192), d = 10, its = 52
i = 2200 (of 10192), d = 10, its = 52
i = 2201 (of 10192), d = 10, its = 52
i = 2202 (of 10192), d = 10, its = 48
i = 2203 (of 10192), d = 10, its = 47
i = 2204 (of 10192), d = 10, its = 51
i = 2205 (of 10192), d = 10, its = 53
i = 2206 (of 10192), d = 10, its = 52
i = 2207 (of 10192), d = 1.65852, its = 6
i = 2208 (of 10192), d = 10, its = 50
i = 2209 (of 10192), d = 10, its = 54
i = 2210 (of 10192), d = 10, its = 37
i = 2211 (of 10192), d = 1.57034, its = 6
i = 2212 (of 10192), d = 10, its = 54
i = 2213 (of 10192), d = 0.580657, its = 17
i = 2214 (of 10192), d = 10, its = 52
i = 2215 (of 10192), d = 10, its = 37
i = 2216 (of 10192), d = 10, its = 54
i = 2217 (of 10192), d = 0.944663, its = 24
i = 2218 (of 10192), d = 10, its = 52
i = 2219 (of 10192), d = 1.80455, its = 7
i = 2220 (of 10192), d = 10, its = 37
i = 2221 (of 10192), d = 10, its = 51
i = 2222 (of 10192), d = 10, its = 52
i = 2223 (of 10192), d = 10, its = 37
i = 2224 (of 10192), d = 10, its = 37
i = 2225 (of 10192), d = 10, its = 49
i = 2226 (of 10192), d = 10, its = 57
i = 2227 (of 10192), d = 10, its = 54
i = 2228 (of 10192), d = 10, its = 55
i = 2229 (of 10192), d = 10, its = 49
i = 2230 (of 10192), d = 0.657156, its = 22
i = 2231 (of 10192), d = 10, its = 51
i = 2232 (of 10192), d = 10, its = 55
i = 2233 (of 10192), d = 10, its = 52
i = 2234 (of 10192), d = 10, its = 52
i = 2235 (of 10192), d = 6.28754, its = 11
i = 2236 (of 10192), d = 10, its = 51
i = 2237 (of 10192), d = 10, its = 51
i = 2238 (of 10192), d = 10, its = 37
i = 2239 (of 10192), d = 0.741668, its = 17
i = 2240 (of 10192), d = 10, its = 55
i = 2241 (of 10192), d = 0.538044, its = 17
i = 2242 (of 10192), d = 7.68478, its = 9
i = 2243 (of 10192), d = 10, its = 39
i = 2244 (of 10192), d = 1.78984, its = 5
i = 2245 (of 10192), d = 10, its = 49
i = 2246 (of 10192), d = 10, its = 40
i = 2247 (of 10192), d = 2.07517, its = 3
i = 2248 (of 10192), d = 10, its = 50
i = 2249 (of 10192), d = 10, its = 53
i = 2250 (of 10192), d = 10, its = 37
i = 2251 (of 10192), d = 1.09638, its = 18
i = 2252 (of 10192), d = 1.82454, its = 4
i = 2253 (of 10192), d = 10, its = 49
i = 2254 (of 10192), d = 1.00467, its = 19
i = 2255 (of 10192), d = 2.25348, its = 5
i = 2256 (of 10192), d = 10, its = 51
i = 2257 (of 10192), d = 1.77671, its = 5
i = 2258 (of 10192), d = 10, its = 40
i = 2259 (of 10192), d = 10, its = 54
i = 2260 (of 10192), d = 3.21222, its = 6
i = 2261 (of 10192), d = 10, its = 37
i = 2262 (of 10192), d = 2.06926, its = 3
i = 2263 (of 10192), d = 10, its = 56
i = 2264 (of 10192), d = 10, its = 50
i = 2265 (of 10192), d = 10, its = 51
i = 2266 (of 10192), d = 10, its = 49
i = 2267 (of 10192), d = 10, its = 54
i = 2268 (of 10192), d = 10, its = 53
i = 2269 (of 10192), d = 10, its = 55
i = 2270 (of 10192), d = 10, its = 50
i = 2271 (of 10192), d = 2.58042, its = 5
i = 2272 (of 10192), d = 10, its = 53
i = 2273 (of 10192), d = 10, its = 49
i = 2274 (of 10192), d = 10, its = 51
i = 2275 (of 10192), d = 10, its = 55
i = 2276 (of 10192), d = 10, its = 50
i = 2277 (of 10192), d = 10, its = 53
i = 2278 (of 10192), d = 10, its = 52
i = 2279 (of 10192), d = 0.851138, its = 17
i = 2280 (of 10192), d = 10, its = 53
i = 2281 (of 10192), d = 10, its = 48
i = 2282 (of 10192), d = 10, its = 49
i = 2283 (of 10192), d = 10, its = 54
i = 2284 (of 10192), d = 0.771312, its = 21
i = 2285 (of 10192), d = 10, its = 37
i = 2286 (of 10192), d = 10, its = 44
i = 2287 (of 10192), d = 1.62273, its = 6
i = 2288 (of 10192), d = 10, its = 54
i = 2289 (of 10192), d = 10, its = 54
i = 2290 (of 10192), d = 10, its = 56
i = 2291 (of 10192), d = 10, its = 52
i = 2292 (of 10192), d = 10, its = 53
i = 2293 (of 10192), d = 10, its = 37
i = 2294 (of 10192), d = 10, its = 51
i = 2295 (of 10192), d = 1.72467, its = 5
i = 2296 (of 10192), d = 2.45555, its = 5
i = 2297 (of 10192), d = 10, its = 53
i = 2298 (of 10192), d = 10, its = 59
i = 2299 (of 10192), d = 10, its = 37
i = 2300 (of 10192), d = 3.0684, its = 6
i = 2301 (of 10192), d = 10, its = 56
i = 2302 (of 10192), d = 1.73687, its = 5
i = 2303 (of 10192), d = 10, its = 54
i = 2304 (of 10192), d = 0.824938, its = 17
i = 2305 (of 10192), d = 10, its = 37
i = 2306 (of 10192), d = 4.26145, its = 7
i = 2307 (of 10192), d = 10, its = 53
i = 2308 (of 10192), d = 0.79493, its = 21
i = 2309 (of 10192), d = 10, its = 51
i = 2310 (of 10192), d = 10, its = 41
i = 2311 (of 10192), d = 10, its = 54
i = 2312 (of 10192), d = 10, its = 52
i = 2313 (of 10192), d = 10, its = 51
i = 2314 (of 10192), d = 10, its = 58
i = 2315 (of 10192), d = 10, its = 53
i = 2316 (of 10192), d = 10, its = 51
i = 2317 (of 10192), d = 10, its = 51
i = 2318 (of 10192), d = 1.85727, its = 4
i = 2319 (of 10192), d = 10, its = 56
i = 2320 (of 10192), d = 1.07802, its = 19
i = 2321 (of 10192), d = 3.18926, its = 6
i = 2322 (of 10192), d = 2.38289, its = 5
i = 2323 (of 10192), d = 10, its = 52
i = 2324 (of 10192), d = 10, its = 49
i = 2325 (of 10192), d = 10, its = 52
i = 2326 (of 10192), d = 10, its = 39
i = 2327 (of 10192), d = 10, its = 51
i = 2328 (of 10192), d = 10, its = 51
i = 2329 (of 10192), d = 2.41779, its = 5
i = 2330 (of 10192), d = 10, its = 38
i = 2331 (of 10192), d = 10, its = 37
i = 2332 (of 10192), d = 10, its = 40
i = 2333 (of 10192), d = 10, its = 55
i = 2334 (of 10192), d = 10, its = 53
i = 2335 (of 10192), d = 10, its = 52
i = 2336 (of 10192), d = 7.49743, its = 6
i = 2337 (of 10192), d = 10, its = 53
i = 2338 (of 10192), d = 2.05448, its = 4
i = 2339 (of 10192), d = 1.72894, its = 5
i = 2340 (of 10192), d = 1.95789, its = 3
i = 2341 (of 10192), d = 10, its = 51
i = 2342 (of 10192), d = 10, its = 44
i = 2343 (of 10192), d = 0.950062, its = 22
i = 2344 (of 10192), d = 10, its = 40
i = 2345 (of 10192), d = 10, its = 55
i = 2346 (of 10192), d = 10, its = 37
i = 2347 (of 10192), d = 10, its = 60
i = 2348 (of 10192), d = 10, its = 37
i = 2349 (of 10192), d = 10, its = 54
i = 2350 (of 10192), d = 10, its = 51
i = 2351 (of 10192), d = 10, its = 52
i = 2352 (of 10192), d = 10, its = 54
i = 2353 (of 10192), d = 10, its = 56
i = 2354 (of 10192), d = 10, its = 51
i = 2355 (of 10192), d = 10, its = 49
i = 2356 (of 10192), d = 10, its = 51
i = 2357 (of 10192), d = 1.85844, its = 5
i = 2358 (of 10192), d = 10, its = 50
i = 2359 (of 10192), d = 10, its = 52
i = 2360 (of 10192), d = 10, its = 44
i = 2361 (of 10192), d = 10, its = 40
i = 2362 (of 10192), d = 10, its = 37
i = 2363 (of 10192), d = 10, its = 57
i = 2364 (of 10192), d = 1.07065, its = 20
i = 2365 (of 10192), d = 1.82481, its = 5
i = 2366 (of 10192), d = 10, its = 37
i = 2367 (of 10192), d = 10, its = 52
i = 2368 (of 10192), d = 2.06564, its = 3
i = 2369 (of 10192), d = 10, its = 50
i = 2370 (of 10192), d = 10, its = 56
i = 2371 (of 10192), d = 1.58405, its = 6
i = 2372 (of 10192), d = 10, its = 54
i = 2373 (of 10192), d = 10, its = 37
i = 2374 (of 10192), d = 10, its = 50
i = 2375 (of 10192), d = 1.7245, its = 5
i = 2376 (of 10192), d = 9.07421, its = 8
i = 2377 (of 10192), d = 10, its = 40
i = 2378 (of 10192), d = 10, its = 54
i = 2379 (of 10192), d = 10, its = 57
i = 2380 (of 10192), d = 10, its = 43
i = 2381 (of 10192), d = 1.39097, its = 6
i = 2382 (of 10192), d = 10, its = 37
i = 2383 (of 10192), d = 2.11343, its = 4
i = 2384 (of 10192), d = 10, its = 54
i = 2385 (of 10192), d = 10, its = 38
i = 2386 (of 10192), d = 2.03229, its = 3
i = 2387 (of 10192), d = 10, its = 44
i = 2388 (of 10192), d = 10, its = 48
i = 2389 (of 10192), d = 10, its = 51
i = 2390 (of 10192), d = 10, its = 51
i = 2391 (of 10192), d = 10, its = 50
i = 2392 (of 10192), d = 10, its = 37
i = 2393 (of 10192), d = 0.998498, its = 17
i = 2394 (of 10192), d = 10, its = 53
i = 2395 (of 10192), d = 10, its = 43
i = 2396 (of 10192), d = 0.774395, its = 19
i = 2397 (of 10192), d = 2.12326, its = 4
i = 2398 (of 10192), d = 10, its = 37
i = 2399 (of 10192), d = 10, its = 37
i = 2400 (of 10192), d = 1.58182, its = 6
i = 2401 (of 10192), d = 10, its = 50
i = 2402 (of 10192), d = 10, its = 57
i = 2403 (of 10192), d = 10, its = 48
i = 2404 (of 10192), d = 10, its = 49
i = 2405 (of 10192), d = 1.57817, its = 6
i = 2406 (of 10192), d = 10, its = 49
i = 2407 (of 10192), d = 10, its = 52
i = 2408 (of 10192), d = 0.8365, its = 21
i = 2409 (of 10192), d = 10, its = 37
i = 2410 (of 10192), d = 10, its = 40
i = 2411 (of 10192), d = 10, its = 52
i = 2412 (of 10192), d = 10, its = 53
i = 2413 (of 10192), d = 10, its = 55
i = 2414 (of 10192), d = 10, its = 37
i = 2415 (of 10192), d = 10, its = 54
i = 2416 (of 10192), d = 0.358285, its = 18
i = 2417 (of 10192), d = 10, its = 54
i = 2418 (of 10192), d = 3.24294, its = 7
i = 2419 (of 10192), d = 10, its = 56
i = 2420 (of 10192), d = 10, its = 53
i = 2421 (of 10192), d = 10, its = 50
i = 2422 (of 10192), d = 10, its = 52
i = 2423 (of 10192), d = 2.2134, its = 5
i = 2424 (of 10192), d = 10, its = 56
i = 2425 (of 10192), d = 10, its = 40
i = 2426 (of 10192), d = 10, its = 54
i = 2427 (of 10192), d = 10, its = 55
i = 2428 (of 10192), d = 10, its = 37
i = 2429 (of 10192), d = 5.08582, its = 7
i = 2430 (of 10192), d = 10, its = 40
i = 2431 (of 10192), d = 10, its = 54
i = 2432 (of 10192), d = 10, its = 54
i = 2433 (of 10192), d = 10, its = 50
i = 2434 (of 10192), d = 1.76517, its = 5
i = 2435 (of 10192), d = 10, its = 50
i = 2436 (of 10192), d = 10, its = 53
i = 2437 (of 10192), d = 10, its = 37
i = 2438 (of 10192), d = 10, its = 37
i = 2439 (of 10192), d = 10, its = 37
i = 2440 (of 10192), d = 10, its = 37
i = 2441 (of 10192), d = 10, its = 37
i = 2442 (of 10192), d = 0.353976, its = 18
i = 2443 (of 10192), d = 10, its = 54
i = 2444 (of 10192), d = 10, its = 50
i = 2445 (of 10192), d = 10, its = 56
i = 2446 (of 10192), d = 10, its = 55
i = 2447 (of 10192), d = 10, its = 52
i = 2448 (of 10192), d = 10, its = 37
i = 2449 (of 10192), d = 10, its = 53
i = 2450 (of 10192), d = 10, its = 52
i = 2451 (of 10192), d = 10, its = 37
i = 2452 (of 10192), d = 10, its = 37
i = 2453 (of 10192), d = 10, its = 37
i = 2454 (of 10192), d = 10, its = 37
i = 2455 (of 10192), d = 10, its = 48
i = 2456 (of 10192), d = 10, its = 51
i = 2457 (of 10192), d = 10, its = 49
i = 2458 (of 10192), d = 10, its = 37
i = 2459 (of 10192), d = 2.91902, its = 6
i = 2460 (of 10192), d = 10, its = 51
i = 2461 (of 10192), d = 10, its = 53
i = 2462 (of 10192), d = 1.75122, its = 6
i = 2463 (of 10192), d = 10, its = 52
i = 2464 (of 10192), d = 1.98949, its = 3
i = 2465 (of 10192), d = 10, its = 52
i = 2466 (of 10192), d = 10, its = 37
i = 2467 (of 10192), d = 10, its = 50
i = 2468 (of 10192), d = 0.469012, its = 18
i = 2469 (of 10192), d = 10, its = 50
i = 2470 (of 10192), d = 10, its = 40
i = 2471 (of 10192), d = 10, its = 40
i = 2472 (of 10192), d = 10, its = 41
i = 2473 (of 10192), d = 10, its = 52
i = 2474 (of 10192), d = 10, its = 49
i = 2475 (of 10192), d = 0.647553, its = 17
i = 2476 (of 10192), d = 10, its = 37
i = 2477 (of 10192), d = 10, its = 52
i = 2478 (of 10192), d = 10, its = 50
i = 2479 (of 10192), d = 10, its = 56
i = 2480 (of 10192), d = 1.2033, its = 26
i = 2481 (of 10192), d = 10, its = 40
i = 2482 (of 10192), d = 10, its = 50
i = 2483 (of 10192), d = 2.83235, its = 7
i = 2484 (of 10192), d = 10, its = 54
i = 2485 (of 10192), d = 10, its = 56
i = 2486 (of 10192), d = 10, its = 44
i = 2487 (of 10192), d = 10, its = 55
i = 2488 (of 10192), d = 10, its = 53
i = 2489 (of 10192), d = 2.30426, its = 4
i = 2490 (of 10192), d = 10, its = 51
i = 2491 (of 10192), d = 10, its = 40
i = 2492 (of 10192), d = 10, its = 40
i = 2493 (of 10192), d = 10, its = 47
i = 2494 (of 10192), d = 2.20843, its = 4
i = 2495 (of 10192), d = 2.50447, its = 6
i = 2496 (of 10192), d = 0.85664, its = 18
i = 2497 (of 10192), d = 10, its = 51
i = 2498 (of 10192), d = 10, its = 51
i = 2499 (of 10192), d = 1.43205, its = 5
i = 2500 (of 10192), d = 10, its = 55
i = 2501 (of 10192), d = 2.53411, its = 5
i = 2502 (of 10192), d = 1.09192, its = 25
i = 2503 (of 10192), d = 5.4033, its = 8
i = 2504 (of 10192), d = 10, its = 50
i = 2505 (of 10192), d = 10, its = 37
i = 2506 (of 10192), d = 1.70327, its = 5
i = 2507 (of 10192), d = 10, its = 37
i = 2508 (of 10192), d = 0.809334, its = 17
i = 2509 (of 10192), d = 10, its = 37
i = 2510 (of 10192), d = 10, its = 39
i = 2511 (of 10192), d = 10, its = 37
i = 2512 (of 10192), d = 1.18935, its = 7
i = 2513 (of 10192), d = 10, its = 61
i = 2514 (of 10192), d = 10, its = 40
i = 2515 (of 10192), d = 0.284111, its = 17
i = 2516 (of 10192), d = 10, its = 54
i = 2517 (of 10192), d = 10, its = 37
i = 2518 (of 10192), d = 10, its = 50
i = 2519 (of 10192), d = 10, its = 37
i = 2520 (of 10192), d = 10, its = 53
i = 2521 (of 10192), d = 10, its = 54
i = 2522 (of 10192), d = 10, its = 52
i = 2523 (of 10192), d = 10, its = 49
i = 2524 (of 10192), d = 10, its = 55
i = 2525 (of 10192), d = 10, its = 49
i = 2526 (of 10192), d = 10, its = 47
i = 2527 (of 10192), d = 10, its = 55
i = 2528 (of 10192), d = 1.00205, its = 19
i = 2529 (of 10192), d = 10, its = 50
i = 2530 (of 10192), d = 0.883653, its = 17
i = 2531 (of 10192), d = 10, its = 53
i = 2532 (of 10192), d = 10, its = 54
i = 2533 (of 10192), d = 10, its = 56
i = 2534 (of 10192), d = 1.66369, its = 5
i = 2535 (of 10192), d = 10, its = 53
i = 2536 (of 10192), d = 10, its = 53
i = 2537 (of 10192), d = 2.74116, its = 6
i = 2538 (of 10192), d = 10, its = 50
i = 2539 (of 10192), d = 2.79118, its = 7
i = 2540 (of 10192), d = 10, its = 38
i = 2541 (of 10192), d = 0.522204, its = 18
i = 2542 (of 10192), d = 10, its = 37
i = 2543 (of 10192), d = 10, its = 40
i = 2544 (of 10192), d = 10, its = 48
i = 2545 (of 10192), d = 10, its = 52
i = 2546 (of 10192), d = 2.45417, its = 5
i = 2547 (of 10192), d = 10, its = 51
i = 2548 (of 10192), d = 1.34106, its = 6
i = 2549 (of 10192), d = 10, its = 51
i = 2550 (of 10192), d = 0.522203, its = 18
i = 2551 (of 10192), d = 10, its = 54
i = 2552 (of 10192), d = 10, its = 54
i = 2553 (of 10192), d = 10, its = 50
i = 2554 (of 10192), d = 4.00035, its = 7
i = 2555 (of 10192), d = 10, its = 56
i = 2556 (of 10192), d = 10, its = 37
i = 2557 (of 10192), d = 10, its = 53
i = 2558 (of 10192), d = 10, its = 53
i = 2559 (of 10192), d = 10, its = 49
i = 2560 (of 10192), d = 10, its = 53
i = 2561 (of 10192), d = 1.78781, its = 5
i = 2562 (of 10192), d = 10, its = 39
i = 2563 (of 10192), d = 10, its = 38
i = 2564 (of 10192), d = 10, its = 51
i = 2565 (of 10192), d = 10, its = 52
i = 2566 (of 10192), d = 10, its = 52
i = 2567 (of 10192), d = 1.95298, its = 3
i = 2568 (of 10192), d = 2.05288, its = 3
i = 2569 (of 10192), d = 10, its = 37
i = 2570 (of 10192), d = 10, its = 54
i = 2571 (of 10192), d = 10, its = 52
i = 2572 (of 10192), d = 10, its = 55
i = 2573 (of 10192), d = 0.856117, its = 16
i = 2574 (of 10192), d = 10, its = 37
i = 2575 (of 10192), d = 10, its = 53
i = 2576 (of 10192), d = 10, its = 37
i = 2577 (of 10192), d = 10, its = 37
i = 2578 (of 10192), d = 10, its = 37
i = 2579 (of 10192), d = 10, its = 56
i = 2580 (of 10192), d = 10, its = 55
i = 2581 (of 10192), d = 10, its = 53
i = 2582 (of 10192), d = 2.15783, its = 4
i = 2583 (of 10192), d = 10, its = 54
i = 2584 (of 10192), d = 10, its = 48
i = 2585 (of 10192), d = 10, its = 52
i = 2586 (of 10192), d = 10, its = 49
i = 2587 (of 10192), d = 1.08198, its = 20
i = 2588 (of 10192), d = 10, its = 37
i = 2589 (of 10192), d = 10, its = 41
i = 2590 (of 10192), d = 10, its = 52
i = 2591 (of 10192), d = 10, its = 54
i = 2592 (of 10192), d = 10, its = 55
i = 2593 (of 10192), d = 10, its = 52
i = 2594 (of 10192), d = 3.83061, its = 6
i = 2595 (of 10192), d = 10, its = 51
i = 2596 (of 10192), d = 10, its = 52
i = 2597 (of 10192), d = 1.90636, its = 4
i = 2598 (of 10192), d = 10, its = 55
i = 2599 (of 10192), d = 10, its = 51
i = 2600 (of 10192), d = 10, its = 37
i = 2601 (of 10192), d = 10, its = 52
i = 2602 (of 10192), d = 0.677842, its = 16
i = 2603 (of 10192), d = 1.09268, its = 20
i = 2604 (of 10192), d = 3.06376, its = 7
i = 2605 (of 10192), d = 10, its = 51
i = 2606 (of 10192), d = 1.13655, its = 18
i = 2607 (of 10192), d = 10, its = 53
i = 2608 (of 10192), d = 10, its = 52
i = 2609 (of 10192), d = 10, its = 53
i = 2610 (of 10192), d = 1.26589, its = 23
i = 2611 (of 10192), d = 3.60707, its = 7
i = 2612 (of 10192), d = 10, its = 52
i = 2613 (of 10192), d = 4.02119, its = 6
i = 2614 (of 10192), d = 10, its = 51
i = 2615 (of 10192), d = 10, its = 42
i = 2616 (of 10192), d = 10, its = 56
i = 2617 (of 10192), d = 10, its = 55
i = 2618 (of 10192), d = 10, its = 48
i = 2619 (of 10192), d = 10, its = 37
i = 2620 (of 10192), d = 2.89828, its = 5
i = 2621 (of 10192), d = 2.09419, its = 4
i = 2622 (of 10192), d = 10, its = 46
i = 2623 (of 10192), d = 10, its = 54
i = 2624 (of 10192), d = 2.39131, its = 5
i = 2625 (of 10192), d = 10, its = 52
i = 2626 (of 10192), d = 0.826068, its = 18
i = 2627 (of 10192), d = 10, its = 37
i = 2628 (of 10192), d = 10, its = 37
i = 2629 (of 10192), d = 10, its = 48
i = 2630 (of 10192), d = 0.80032, its = 17
i = 2631 (of 10192), d = 10, its = 54
i = 2632 (of 10192), d = 2.77782, its = 5
i = 2633 (of 10192), d = 10, its = 55
i = 2634 (of 10192), d = 10, its = 41
i = 2635 (of 10192), d = 10, its = 52
i = 2636 (of 10192), d = 10, its = 53
i = 2637 (of 10192), d = 10, its = 54
i = 2638 (of 10192), d = 10, its = 51
i = 2639 (of 10192), d = 10, its = 42
i = 2640 (of 10192), d = 10, its = 40
i = 2641 (of 10192), d = 10, its = 59
i = 2642 (of 10192), d = 10, its = 40
i = 2643 (of 10192), d = 10, its = 52
i = 2644 (of 10192), d = 2.07573, its = 4
i = 2645 (of 10192), d = 10, its = 39
i = 2646 (of 10192), d = 10, its = 40
i = 2647 (of 10192), d = 2.20558, its = 4
i = 2648 (of 10192), d = 10, its = 39
i = 2649 (of 10192), d = 0.541829, its = 20
i = 2650 (of 10192), d = 10, its = 55
i = 2651 (of 10192), d = 10, its = 53
i = 2652 (of 10192), d = 10, its = 40
i = 2653 (of 10192), d = 10, its = 52
i = 2654 (of 10192), d = 10, its = 53
i = 2655 (of 10192), d = 1.94718, its = 3
i = 2656 (of 10192), d = 10, its = 55
i = 2657 (of 10192), d = 10, its = 53
i = 2658 (of 10192), d = 10, its = 51
i = 2659 (of 10192), d = 10, its = 51
i = 2660 (of 10192), d = 10, its = 56
i = 2661 (of 10192), d = 10, its = 52
i = 2662 (of 10192), d = 10, its = 52
i = 2663 (of 10192), d = 2.10462, its = 4
i = 2664 (of 10192), d = 10, its = 49
i = 2665 (of 10192), d = 10, its = 51
i = 2666 (of 10192), d = 10, its = 53
i = 2667 (of 10192), d = 10, its = 37
i = 2668 (of 10192), d = 10, its = 52
i = 2669 (of 10192), d = 10, its = 49
i = 2670 (of 10192), d = 10, its = 52
i = 2671 (of 10192), d = 10, its = 55
i = 2672 (of 10192), d = 10, its = 54
i = 2673 (of 10192), d = 10, its = 40
i = 2674 (of 10192), d = 10, its = 37
i = 2675 (of 10192), d = 10, its = 47
i = 2676 (of 10192), d = 1.3546, its = 6
i = 2677 (of 10192), d = 10, its = 52
i = 2678 (of 10192), d = 0.913846, its = 23
i = 2679 (of 10192), d = 0.404517, its = 24
i = 2680 (of 10192), d = 10, its = 55
i = 2681 (of 10192), d = 10, its = 43
i = 2682 (of 10192), d = 10, its = 55
i = 2683 (of 10192), d = 10, its = 51
i = 2684 (of 10192), d = 10, its = 50
i = 2685 (of 10192), d = 10, its = 37
i = 2686 (of 10192), d = 1.41535, its = 6
i = 2687 (of 10192), d = 10, its = 37
i = 2688 (of 10192), d = 10, its = 53
i = 2689 (of 10192), d = 0.478615, its = 18
i = 2690 (of 10192), d = 10, its = 54
i = 2691 (of 10192), d = 1.50655, its = 6
i = 2692 (of 10192), d = 10, its = 52
i = 2693 (of 10192), d = 10, its = 56
i = 2694 (of 10192), d = 3.23547, its = 6
i = 2695 (of 10192), d = 10, its = 54
i = 2696 (of 10192), d = 10, its = 53
i = 2697 (of 10192), d = 1.20303, its = 6
i = 2698 (of 10192), d = 10, its = 55
i = 2699 (of 10192), d = 10, its = 55
i = 2700 (of 10192), d = 10, its = 40
i = 2701 (of 10192), d = 10, its = 50
i = 2702 (of 10192), d = 10, its = 52
i = 2703 (of 10192), d = 10, its = 50
i = 2704 (of 10192), d = 10, its = 54
i = 2705 (of 10192), d = 10, its = 53
i = 2706 (of 10192), d = 2.36662, its = 5
i = 2707 (of 10192), d = 10, its = 51
i = 2708 (of 10192), d = 10, its = 52
i = 2709 (of 10192), d = 10, its = 37
i = 2710 (of 10192), d = 2.46358, its = 5
i = 2711 (of 10192), d = 10, its = 58
i = 2712 (of 10192), d = 10, its = 41
i = 2713 (of 10192), d = 10, its = 37
i = 2714 (of 10192), d = 10, its = 38
i = 2715 (of 10192), d = 10, its = 51
i = 2716 (of 10192), d = 1.61316, its = 6
i = 2717 (of 10192), d = 10, its = 52
i = 2718 (of 10192), d = 10, its = 52
i = 2719 (of 10192), d = 2.84548, its = 5
i = 2720 (of 10192), d = 10, its = 47
i = 2721 (of 10192), d = 10, its = 56
i = 2722 (of 10192), d = 10, its = 58
i = 2723 (of 10192), d = 10, its = 37
i = 2724 (of 10192), d = 2.48103, its = 5
i = 2725 (of 10192), d = 2.16875, its = 5
i = 2726 (of 10192), d = 10, its = 43
i = 2727 (of 10192), d = 1.80075, its = 6
i = 2728 (of 10192), d = 2.84073, its = 7
i = 2729 (of 10192), d = 2.17495, its = 5
i = 2730 (of 10192), d = 10, its = 58
i = 2731 (of 10192), d = 1.00928, its = 17
i = 2732 (of 10192), d = 1.00465, its = 22
i = 2733 (of 10192), d = 10, its = 53
i = 2734 (of 10192), d = 4.07333, its = 7
i = 2735 (of 10192), d = 10, its = 54
i = 2736 (of 10192), d = 9.28927, its = 11
i = 2737 (of 10192), d = 0.494169, its = 18
i = 2738 (of 10192), d = 1.74736, its = 5
i = 2739 (of 10192), d = 3.37727, its = 8
i = 2740 (of 10192), d = 1.88209, its = 4
i = 2741 (of 10192), d = 10, its = 37
i = 2742 (of 10192), d = 10, its = 52
i = 2743 (of 10192), d = 10, its = 57
i = 2744 (of 10192), d = 2.53825, its = 5
i = 2745 (of 10192), d = 1.00928, its = 17
i = 2746 (of 10192), d = 3.50995, its = 9
i = 2747 (of 10192), d = 10, its = 37
i = 2748 (of 10192), d = 10, its = 52
i = 2749 (of 10192), d = 10, its = 39
i = 2750 (of 10192), d = 1.01079, its = 18
i = 2751 (of 10192), d = 10, its = 56
i = 2752 (of 10192), d = 10, its = 49
i = 2753 (of 10192), d = 10, its = 52
i = 2754 (of 10192), d = 10, its = 51
i = 2755 (of 10192), d = 10, its = 51
i = 2756 (of 10192), d = 10, its = 40
i = 2757 (of 10192), d = 3.858, its = 8
i = 2758 (of 10192), d = 10, its = 52
i = 2759 (of 10192), d = 10, its = 53
i = 2760 (of 10192), d = 1.72652, its = 5
i = 2761 (of 10192), d = 10, its = 54
i = 2762 (of 10192), d = 10, its = 52
i = 2763 (of 10192), d = 10, its = 53
i = 2764 (of 10192), d = 10, its = 54
i = 2765 (of 10192), d = 10, its = 37
i = 2766 (of 10192), d = 2.06414, its = 3
i = 2767 (of 10192), d = 10, its = 53
i = 2768 (of 10192), d = 3.56349, its = 7
i = 2769 (of 10192), d = 0.794494, its = 21
i = 2770 (of 10192), d = 2.74003, its = 5
i = 2771 (of 10192), d = 10, its = 55
i = 2772 (of 10192), d = 10, its = 51
i = 2773 (of 10192), d = 10, its = 51
i = 2774 (of 10192), d = 10, its = 37
i = 2775 (of 10192), d = 10, its = 54
i = 2776 (of 10192), d = 10, its = 51
i = 2777 (of 10192), d = 10, its = 39
i = 2778 (of 10192), d = 10, its = 37
i = 2779 (of 10192), d = 10, its = 51
i = 2780 (of 10192), d = 10, its = 54
i = 2781 (of 10192), d = 10, its = 49
i = 2782 (of 10192), d = 10, its = 54
i = 2783 (of 10192), d = 10, its = 54
i = 2784 (of 10192), d = 1.77786, its = 5
i = 2785 (of 10192), d = 10, its = 54
i = 2786 (of 10192), d = 3.37764, its = 7
i = 2787 (of 10192), d = 10, its = 54
i = 2788 (of 10192), d = 3.56022, its = 6
i = 2789 (of 10192), d = 2.22899, its = 4
i = 2790 (of 10192), d = 10, its = 52
i = 2791 (of 10192), d = 10, its = 52
i = 2792 (of 10192), d = 10, its = 37
i = 2793 (of 10192), d = 3.41217, its = 6
i = 2794 (of 10192), d = 2.07764, its = 3
i = 2795 (of 10192), d = 10, its = 50
i = 2796 (of 10192), d = 10, its = 50
i = 2797 (of 10192), d = 10, its = 56
i = 2798 (of 10192), d = 10, its = 53
i = 2799 (of 10192), d = 1.97889, its = 3
i = 2800 (of 10192), d = 10, its = 54
i = 2801 (of 10192), d = 8.50866, its = 8
i = 2802 (of 10192), d = 10, its = 40
i = 2803 (of 10192), d = 10, its = 53
i = 2804 (of 10192), d = 1.08278, its = 23
i = 2805 (of 10192), d = 10, its = 50
i = 2806 (of 10192), d = 2.03928, its = 3
i = 2807 (of 10192), d = 10, its = 55
i = 2808 (of 10192), d = 10, its = 56
i = 2809 (of 10192), d = 10, its = 56
i = 2810 (of 10192), d = 10, its = 50
i = 2811 (of 10192), d = 10, its = 53
i = 2812 (of 10192), d = 10, its = 51
i = 2813 (of 10192), d = 10, its = 56
i = 2814 (of 10192), d = 10, its = 55
i = 2815 (of 10192), d = 4.02119, its = 6
i = 2816 (of 10192), d = 10, its = 51
i = 2817 (of 10192), d = 10, its = 37
i = 2818 (of 10192), d = 10, its = 51
i = 2819 (of 10192), d = 0.991175, its = 24
i = 2820 (of 10192), d = 10, its = 53
i = 2821 (of 10192), d = 2.56778, its = 5
i = 2822 (of 10192), d = 2.67217, its = 5
i = 2823 (of 10192), d = 10, its = 52
i = 2824 (of 10192), d = 1.15771, its = 24
i = 2825 (of 10192), d = 1.83417, its = 5
i = 2826 (of 10192), d = 10, its = 53
i = 2827 (of 10192), d = 10, its = 51
i = 2828 (of 10192), d = 10, its = 53
i = 2829 (of 10192), d = 2.16911, its = 4
i = 2830 (of 10192), d = 10, its = 37
i = 2831 (of 10192), d = 10, its = 56
i = 2832 (of 10192), d = 10, its = 54
i = 2833 (of 10192), d = 10, its = 54
i = 2834 (of 10192), d = 10, its = 37
i = 2835 (of 10192), d = 2.57875, its = 5
i = 2836 (of 10192), d = 0.711313, its = 17
i = 2837 (of 10192), d = 1.13532, its = 23
i = 2838 (of 10192), d = 10, its = 37
i = 2839 (of 10192), d = 10, its = 54
i = 2840 (of 10192), d = 10, its = 37
i = 2841 (of 10192), d = 10, its = 52
i = 2842 (of 10192), d = 10, its = 39
i = 2843 (of 10192), d = 0.643083, its = 20
i = 2844 (of 10192), d = 1.65547, its = 6
i = 2845 (of 10192), d = 6.99906, its = 21
i = 2846 (of 10192), d = 10, its = 55
i = 2847 (of 10192), d = 10, its = 52
i = 2848 (of 10192), d = 10, its = 50
i = 2849 (of 10192), d = 10, its = 44
i = 2850 (of 10192), d = 10, its = 40
i = 2851 (of 10192), d = 10, its = 52
i = 2852 (of 10192), d = 3.70706, its = 7
i = 2853 (of 10192), d = 10, its = 54
i = 2854 (of 10192), d = 10, its = 52
i = 2855 (of 10192), d = 10, its = 53
i = 2856 (of 10192), d = 10, its = 55
i = 2857 (of 10192), d = 10, its = 52
i = 2858 (of 10192), d = 2.52358, its = 5
i = 2859 (of 10192), d = 10, its = 49
i = 2860 (of 10192), d = 10, its = 53
i = 2861 (of 10192), d = 10, its = 56
i = 2862 (of 10192), d = 10, its = 51
i = 2863 (of 10192), d = 0.861595, its = 20
i = 2864 (of 10192), d = 2.51451, its = 5
i = 2865 (of 10192), d = 10, its = 55
i = 2866 (of 10192), d = 10, its = 37
i = 2867 (of 10192), d = 10, its = 55
i = 2868 (of 10192), d = 10, its = 53
i = 2869 (of 10192), d = 10, its = 54
i = 2870 (of 10192), d = 10, its = 48
i = 2871 (of 10192), d = 10, its = 37
i = 2872 (of 10192), d = 10, its = 53
i = 2873 (of 10192), d = 10, its = 56
i = 2874 (of 10192), d = 0.634587, its = 19
i = 2875 (of 10192), d = 10, its = 54
i = 2876 (of 10192), d = 10, its = 50
i = 2877 (of 10192), d = 1.39505, its = 6
i = 2878 (of 10192), d = 10, its = 52
i = 2879 (of 10192), d = 10, its = 51
i = 2880 (of 10192), d = 1.75948, its = 5
i = 2881 (of 10192), d = 10, its = 37
i = 2882 (of 10192), d = 10, its = 49
i = 2883 (of 10192), d = 2.27538, its = 5
i = 2884 (of 10192), d = 10, its = 54
i = 2885 (of 10192), d = 10, its = 53
i = 2886 (of 10192), d = 10, its = 37
i = 2887 (of 10192), d = 10, its = 53
i = 2888 (of 10192), d = 10, its = 57
i = 2889 (of 10192), d = 10, its = 53
i = 2890 (of 10192), d = 10, its = 40
i = 2891 (of 10192), d = 10, its = 54
i = 2892 (of 10192), d = 10, its = 41
i = 2893 (of 10192), d = 10, its = 52
i = 2894 (of 10192), d = 10, its = 41
i = 2895 (of 10192), d = 10, its = 56
i = 2896 (of 10192), d = 10, its = 38
i = 2897 (of 10192), d = 10, its = 37
i = 2898 (of 10192), d = 2.80757, its = 6
i = 2899 (of 10192), d = 10, its = 48
i = 2900 (of 10192), d = 10, its = 51
i = 2901 (of 10192), d = 10, its = 48
i = 2902 (of 10192), d = 10, its = 51
i = 2903 (of 10192), d = 2.50688, its = 6
i = 2904 (of 10192), d = 10, its = 52
i = 2905 (of 10192), d = 1.75996, its = 8
i = 2906 (of 10192), d = 10, its = 37
i = 2907 (of 10192), d = 10, its = 40
i = 2908 (of 10192), d = 10, its = 54
i = 2909 (of 10192), d = 10, its = 51
i = 2910 (of 10192), d = 10, its = 49
i = 2911 (of 10192), d = 3.40681, its = 6
i = 2912 (of 10192), d = 10, its = 56
i = 2913 (of 10192), d = 10, its = 52
i = 2914 (of 10192), d = 10, its = 51
i = 2915 (of 10192), d = 10, its = 54
i = 2916 (of 10192), d = 10, its = 43
i = 2917 (of 10192), d = 10, its = 37
i = 2918 (of 10192), d = 10, its = 49
i = 2919 (of 10192), d = 10, its = 51
i = 2920 (of 10192), d = 10, its = 52
i = 2921 (of 10192), d = 10, its = 53
i = 2922 (of 10192), d = 1.97476, its = 3
i = 2923 (of 10192), d = 10, its = 51
i = 2924 (of 10192), d = 10, its = 37
i = 2925 (of 10192), d = 10, its = 39
i = 2926 (of 10192), d = 10, its = 54
i = 2927 (of 10192), d = 0.641483, its = 20
i = 2928 (of 10192), d = 10, its = 55
i = 2929 (of 10192), d = 2.46098, its = 5
i = 2930 (of 10192), d = 10, its = 54
i = 2931 (of 10192), d = 10, its = 44
i = 2932 (of 10192), d = 10, its = 37
i = 2933 (of 10192), d = 10, its = 44
i = 2934 (of 10192), d = 10, its = 37
i = 2935 (of 10192), d = 10, its = 55
i = 2936 (of 10192), d = 10, its = 53
i = 2937 (of 10192), d = 10, its = 55
i = 2938 (of 10192), d = 0.727862, its = 19
i = 2939 (of 10192), d = 10, its = 53
i = 2940 (of 10192), d = 10, its = 50
i = 2941 (of 10192), d = 10, its = 49
i = 2942 (of 10192), d = 1.66006, its = 6
i = 2943 (of 10192), d = 10, its = 52
i = 2944 (of 10192), d = 10, its = 37
i = 2945 (of 10192), d = 10, its = 53
i = 2946 (of 10192), d = 10, its = 50
i = 2947 (of 10192), d = 10, its = 51
i = 2948 (of 10192), d = 2.37192, its = 5
i = 2949 (of 10192), d = 10, its = 53
i = 2950 (of 10192), d = 10, its = 51
i = 2951 (of 10192), d = 10, its = 57
i = 2952 (of 10192), d = 10, its = 56
i = 2953 (of 10192), d = 2.0938, its = 4
i = 2954 (of 10192), d = 1.24513, its = 20
i = 2955 (of 10192), d = 10, its = 39
i = 2956 (of 10192), d = 10, its = 53
i = 2957 (of 10192), d = 1.33782, its = 6
i = 2958 (of 10192), d = 1.63552, its = 5
i = 2959 (of 10192), d = 10, its = 53
i = 2960 (of 10192), d = 10, its = 52
i = 2961 (of 10192), d = 10, its = 58
i = 2962 (of 10192), d = 10, its = 54
i = 2963 (of 10192), d = 10, its = 37
i = 2964 (of 10192), d = 10, its = 51
i = 2965 (of 10192), d = 10, its = 40
i = 2966 (of 10192), d = 10, its = 52
i = 2967 (of 10192), d = 10, its = 49
i = 2968 (of 10192), d = 2.07208, its = 4
i = 2969 (of 10192), d = 0.940245, its = 19
i = 2970 (of 10192), d = 3.39275, its = 6
i = 2971 (of 10192), d = 10, its = 53
i = 2972 (of 10192), d = 10, its = 51
i = 2973 (of 10192), d = 10, its = 37
i = 2974 (of 10192), d = 10, its = 50
i = 2975 (of 10192), d = 10, its = 38
i = 2976 (of 10192), d = 10, its = 56
i = 2977 (of 10192), d = 0.726228, its = 18
i = 2978 (of 10192), d = 10, its = 54
i = 2979 (of 10192), d = 10, its = 52
i = 2980 (of 10192), d = 10, its = 52
i = 2981 (of 10192), d = 10, its = 37
i = 2982 (of 10192), d = 10, its = 37
i = 2983 (of 10192), d = 10, its = 40
i = 2984 (of 10192), d = 10, its = 53
i = 2985 (of 10192), d = 10, its = 51
i = 2986 (of 10192), d = 1.54223, its = 6
i = 2987 (of 10192), d = 10, its = 37
i = 2988 (of 10192), d = 1.50005, its = 6
i = 2989 (of 10192), d = 10, its = 37
i = 2990 (of 10192), d = 10, its = 49
i = 2991 (of 10192), d = 10, its = 38
i = 2992 (of 10192), d = 10, its = 39
i = 2993 (of 10192), d = 10, its = 52
i = 2994 (of 10192), d = 10, its = 53
i = 2995 (of 10192), d = 10, its = 50
i = 2996 (of 10192), d = 2.45111, its = 5
i = 2997 (of 10192), d = 10, its = 57
i = 2998 (of 10192), d = 10, its = 37
i = 2999 (of 10192), d = 10, its = 51
i = 3000 (of 10192), d = 0.980436, its = 18
i = 3001 (of 10192), d = 10, its = 54
i = 3002 (of 10192), d = 3.2484, its = 7
i = 3003 (of 10192), d = 1.81217, its = 5
i = 3004 (of 10192), d = 10, its = 54
i = 3005 (of 10192), d = 10, its = 52
i = 3006 (of 10192), d = 10, its = 51
i = 3007 (of 10192), d = 10, its = 37
i = 3008 (of 10192), d = 3.15782, its = 6
i = 3009 (of 10192), d = 10, its = 50
i = 3010 (of 10192), d = 10, its = 48
i = 3011 (of 10192), d = 10, its = 37
i = 3012 (of 10192), d = 10, its = 57
i = 3013 (of 10192), d = 10, its = 50
i = 3014 (of 10192), d = 10, its = 40
i = 3015 (of 10192), d = 10, its = 51
i = 3016 (of 10192), d = 0.738221, its = 16
i = 3017 (of 10192), d = 1.78955, its = 5
i = 3018 (of 10192), d = 10, its = 51
i = 3019 (of 10192), d = 10, its = 53
i = 3020 (of 10192), d = 10, its = 55
i = 3021 (of 10192), d = 10, its = 37
i = 3022 (of 10192), d = 10, its = 41
i = 3023 (of 10192), d = 1.60312, its = 6
i = 3024 (of 10192), d = 0.85009, its = 18
i = 3025 (of 10192), d = 2.28782, its = 5
i = 3026 (of 10192), d = 10, its = 37
i = 3027 (of 10192), d = 0.89624, its = 22
i = 3028 (of 10192), d = 10, its = 50
i = 3029 (of 10192), d = 10, its = 51
i = 3030 (of 10192), d = 8.60128, its = 25
i = 3031 (of 10192), d = 10, its = 56
i = 3032 (of 10192), d = 10, its = 56
i = 3033 (of 10192), d = 10, its = 52
i = 3034 (of 10192), d = 2.1313, its = 4
i = 3035 (of 10192), d = 10, its = 49
i = 3036 (of 10192), d = 10, its = 37
i = 3037 (of 10192), d = 10, its = 54
i = 3038 (of 10192), d = 2.85022, its = 6
i = 3039 (of 10192), d = 10, its = 54
i = 3040 (of 10192), d = 10, its = 37
i = 3041 (of 10192), d = 2.09117, its = 4
i = 3042 (of 10192), d = 0.571764, its = 17
i = 3043 (of 10192), d = 10, its = 37
i = 3044 (of 10192), d = 10, its = 52
i = 3045 (of 10192), d = 2.17271, its = 5
i = 3046 (of 10192), d = 10, its = 56
i = 3047 (of 10192), d = 10, its = 40
i = 3048 (of 10192), d = 10, its = 37
i = 3049 (of 10192), d = 0.579701, its = 22
i = 3050 (of 10192), d = 10, its = 52
i = 3051 (of 10192), d = 10, its = 49
i = 3052 (of 10192), d = 10, its = 55
i = 3053 (of 10192), d = 10, its = 48
i = 3054 (of 10192), d = 10, its = 52
i = 3055 (of 10192), d = 2.9481, its = 5
i = 3056 (of 10192), d = 1.49083, its = 7
i = 3057 (of 10192), d = 5.35519, its = 7
i = 3058 (of 10192), d = 4.29187, its = 8
i = 3059 (of 10192), d = 10, its = 40
i = 3060 (of 10192), d = 10, its = 51
i = 3061 (of 10192), d = 10, its = 37
i = 3062 (of 10192), d = 10, its = 40
i = 3063 (of 10192), d = 10, its = 56
i = 3064 (of 10192), d = 2.26453, its = 4
i = 3065 (of 10192), d = 10, its = 55
i = 3066 (of 10192), d = 10, its = 52
i = 3067 (of 10192), d = 10, its = 57
i = 3068 (of 10192), d = 10, its = 54
i = 3069 (of 10192), d = 10, its = 56
i = 3070 (of 10192), d = 10, its = 55
i = 3071 (of 10192), d = 10, its = 41
i = 3072 (of 10192), d = 10, its = 52
i = 3073 (of 10192), d = 10, its = 52
i = 3074 (of 10192), d = 10, its = 54
i = 3075 (of 10192), d = 0.621208, its = 18
i = 3076 (of 10192), d = 10, its = 49
i = 3077 (of 10192), d = 10, its = 54
i = 3078 (of 10192), d = 10, its = 37
i = 3079 (of 10192), d = 10, its = 54
i = 3080 (of 10192), d = 10, its = 40
i = 3081 (of 10192), d = 2.01984, its = 3
i = 3082 (of 10192), d = 10, its = 48
i = 3083 (of 10192), d = 10, its = 51
i = 3084 (of 10192), d = 0.830959, its = 20
i = 3085 (of 10192), d = 10, its = 54
i = 3086 (of 10192), d = 2.01984, its = 3
i = 3087 (of 10192), d = 10, its = 37
i = 3088 (of 10192), d = 0.81429, its = 16
i = 3089 (of 10192), d = 10, its = 55
i = 3090 (of 10192), d = 10, its = 40
i = 3091 (of 10192), d = 1.21648, its = 6
i = 3092 (of 10192), d = 3.10004, its = 6
i = 3093 (of 10192), d = 10, its = 53
i = 3094 (of 10192), d = 10, its = 53
i = 3095 (of 10192), d = 10, its = 52
i = 3096 (of 10192), d = 10, its = 40
i = 3097 (of 10192), d = 10, its = 44
i = 3098 (of 10192), d = 10, its = 39
i = 3099 (of 10192), d = 10, its = 53
i = 3100 (of 10192), d = 10, its = 54
i = 3101 (of 10192), d = 10, its = 39
i = 3102 (of 10192), d = 10, its = 51
i = 3103 (of 10192), d = 2.27366, its = 4
i = 3104 (of 10192), d = 10, its = 54
i = 3105 (of 10192), d = 10, its = 52
i = 3106 (of 10192), d = 10, its = 40
i = 3107 (of 10192), d = 2.2133, its = 4
i = 3108 (of 10192), d = 2.52316, its = 5
i = 3109 (of 10192), d = 10, its = 50
i = 3110 (of 10192), d = 0.732451, its = 17
i = 3111 (of 10192), d = 10, its = 39
i = 3112 (of 10192), d = 10, its = 52
i = 3113 (of 10192), d = 10, its = 50
i = 3114 (of 10192), d = 10, its = 40
i = 3115 (of 10192), d = 10, its = 37
i = 3116 (of 10192), d = 1.44155, its = 6
i = 3117 (of 10192), d = 10, its = 54
i = 3118 (of 10192), d = 1.96966, its = 3
i = 3119 (of 10192), d = 10, its = 52
i = 3120 (of 10192), d = 10, its = 40
i = 3121 (of 10192), d = 0.441069, its = 18
i = 3122 (of 10192), d = 1.59728, its = 6
i = 3123 (of 10192), d = 3.57513, its = 6
i = 3124 (of 10192), d = 3.72738, its = 7
i = 3125 (of 10192), d = 10, its = 37
i = 3126 (of 10192), d = 10, its = 54
i = 3127 (of 10192), d = 2.07573, its = 4
i = 3128 (of 10192), d = 5.34197, its = 8
i = 3129 (of 10192), d = 1.01814, its = 17
i = 3130 (of 10192), d = 10, its = 50
i = 3131 (of 10192), d = 2.20509, its = 4
i = 3132 (of 10192), d = 10, its = 54
i = 3133 (of 10192), d = 10, its = 40
i = 3134 (of 10192), d = 10, its = 53
i = 3135 (of 10192), d = 10, its = 50
i = 3136 (of 10192), d = 10, its = 50
i = 3137 (of 10192), d = 1.45442, its = 6
i = 3138 (of 10192), d = 10, its = 55
i = 3139 (of 10192), d = 10, its = 49
i = 3140 (of 10192), d = 5.18989, its = 7
i = 3141 (of 10192), d = 10, its = 51
i = 3142 (of 10192), d = 10, its = 53
i = 3143 (of 10192), d = 10, its = 43
i = 3144 (of 10192), d = 10, its = 52
i = 3145 (of 10192), d = 10, its = 53
i = 3146 (of 10192), d = 10, its = 55
i = 3147 (of 10192), d = 10, its = 48
i = 3148 (of 10192), d = 10, its = 54
i = 3149 (of 10192), d = 10, its = 52
i = 3150 (of 10192), d = 10, its = 54
i = 3151 (of 10192), d = 10, its = 39
i = 3152 (of 10192), d = 10, its = 50
i = 3153 (of 10192), d = 0.746052, its = 17
i = 3154 (of 10192), d = 2.11577, its = 4
i = 3155 (of 10192), d = 10, its = 42
i = 3156 (of 10192), d = 10, its = 50
i = 3157 (of 10192), d = 10, its = 39
i = 3158 (of 10192), d = 10, its = 53
i = 3159 (of 10192), d = 10, its = 51
i = 3160 (of 10192), d = 10, its = 51
i = 3161 (of 10192), d = 10, its = 50
i = 3162 (of 10192), d = 10, its = 50
i = 3163 (of 10192), d = 10, its = 53
i = 3164 (of 10192), d = 3.906, its = 6
i = 3165 (of 10192), d = 10, its = 53
i = 3166 (of 10192), d = 10, its = 52
i = 3167 (of 10192), d = 10, its = 37
i = 3168 (of 10192), d = 1.65674, its = 5
i = 3169 (of 10192), d = 2.76615, its = 6
i = 3170 (of 10192), d = 2.09814, its = 4
i = 3171 (of 10192), d = 10, its = 52
i = 3172 (of 10192), d = 10, its = 54
i = 3173 (of 10192), d = 2.85859, its = 6
i = 3174 (of 10192), d = 10, its = 37
i = 3175 (of 10192), d = 10, its = 40
i = 3176 (of 10192), d = 1.67271, its = 5
i = 3177 (of 10192), d = 10, its = 49
i = 3178 (of 10192), d = 10, its = 40
i = 3179 (of 10192), d = 3.68925, its = 6
i = 3180 (of 10192), d = 10, its = 53
i = 3181 (of 10192), d = 10, its = 51
i = 3182 (of 10192), d = 10, its = 51
i = 3183 (of 10192), d = 10, its = 37
i = 3184 (of 10192), d = 10, its = 55
i = 3185 (of 10192), d = 10, its = 50
i = 3186 (of 10192), d = 10, its = 48
i = 3187 (of 10192), d = 0.64337, its = 20
i = 3188 (of 10192), d = 1.63294, its = 6
i = 3189 (of 10192), d = 10, its = 51
i = 3190 (of 10192), d = 1.05362, its = 18
i = 3191 (of 10192), d = 3.10512, its = 6
i = 3192 (of 10192), d = 10, its = 56
i = 3193 (of 10192), d = 1.80562, its = 5
i = 3194 (of 10192), d = 10, its = 58
i = 3195 (of 10192), d = 2.70343, its = 5
i = 3196 (of 10192), d = 10, its = 49
i = 3197 (of 10192), d = 10, its = 52
i = 3198 (of 10192), d = 10, its = 51
i = 3199 (of 10192), d = 1.0122, its = 20
i = 3200 (of 10192), d = 10, its = 48
i = 3201 (of 10192), d = 10, its = 50
i = 3202 (of 10192), d = 10, its = 37
i = 3203 (of 10192), d = 10, its = 53
i = 3204 (of 10192), d = 10, its = 49
i = 3205 (of 10192), d = 10, its = 56
i = 3206 (of 10192), d = 1.98324, its = 3
i = 3207 (of 10192), d = 10, its = 53
i = 3208 (of 10192), d = 10, its = 39
i = 3209 (of 10192), d = 10, its = 37
i = 3210 (of 10192), d = 10, its = 48
i = 3211 (of 10192), d = 10, its = 53
i = 3212 (of 10192), d = 10, its = 56
i = 3213 (of 10192), d = 10, its = 54
i = 3214 (of 10192), d = 1.54101, its = 6
i = 3215 (of 10192), d = 10, its = 55
i = 3216 (of 10192), d = 10, its = 41
i = 3217 (of 10192), d = 10, its = 55
i = 3218 (of 10192), d = 10, its = 53
i = 3219 (of 10192), d = 10, its = 58
i = 3220 (of 10192), d = 10, its = 37
i = 3221 (of 10192), d = 10, its = 55
i = 3222 (of 10192), d = 3.61481, its = 7
i = 3223 (of 10192), d = 10, its = 53
i = 3224 (of 10192), d = 10, its = 37
i = 3225 (of 10192), d = 10, its = 50
i = 3226 (of 10192), d = 10, its = 37
i = 3227 (of 10192), d = 10, its = 51
i = 3228 (of 10192), d = 10, its = 57
i = 3229 (of 10192), d = 10, its = 41
i = 3230 (of 10192), d = 10, its = 57
i = 3231 (of 10192), d = 10, its = 49
i = 3232 (of 10192), d = 10, its = 51
i = 3233 (of 10192), d = 10, its = 54
i = 3234 (of 10192), d = 10, its = 50
i = 3235 (of 10192), d = 10, its = 42
i = 3236 (of 10192), d = 10, its = 37
i = 3237 (of 10192), d = 1.26334, its = 21
i = 3238 (of 10192), d = 10, its = 53
i = 3239 (of 10192), d = 10, its = 37
i = 3240 (of 10192), d = 10, its = 57
i = 3241 (of 10192), d = 10, its = 55
i = 3242 (of 10192), d = 10, its = 40
i = 3243 (of 10192), d = 0.861595, its = 20
i = 3244 (of 10192), d = 10, its = 41
i = 3245 (of 10192), d = 10, its = 51
i = 3246 (of 10192), d = 10, its = 51
i = 3247 (of 10192), d = 10, its = 52
i = 3248 (of 10192), d = 10, its = 48
i = 3249 (of 10192), d = 10, its = 51
i = 3250 (of 10192), d = 2.12378, its = 4
i = 3251 (of 10192), d = 10, its = 55
i = 3252 (of 10192), d = 10, its = 37
i = 3253 (of 10192), d = 10, its = 49
i = 3254 (of 10192), d = 10, its = 37
i = 3255 (of 10192), d = 10, its = 54
i = 3256 (of 10192), d = 1.97062, its = 3
i = 3257 (of 10192), d = 1.12237, its = 27
i = 3258 (of 10192), d = 10, its = 54
i = 3259 (of 10192), d = 1.28057, its = 6
i = 3260 (of 10192), d = 1.85204, its = 4
i = 3261 (of 10192), d = 10, its = 55
i = 3262 (of 10192), d = 10, its = 50
i = 3263 (of 10192), d = 10, its = 52
i = 3264 (of 10192), d = 10, its = 50
i = 3265 (of 10192), d = 0.88443, its = 18
i = 3266 (of 10192), d = 10, its = 55
i = 3267 (of 10192), d = 2.12341, its = 4
i = 3268 (of 10192), d = 10, its = 52
i = 3269 (of 10192), d = 10, its = 48
i = 3270 (of 10192), d = 10, its = 52
i = 3271 (of 10192), d = 2.46628, its = 5
i = 3272 (of 10192), d = 1.36493, its = 6
i = 3273 (of 10192), d = 10, its = 37
i = 3274 (of 10192), d = 10, its = 49
i = 3275 (of 10192), d = 10, its = 37
i = 3276 (of 10192), d = 10, its = 40
i = 3277 (of 10192), d = 10, its = 52
i = 3278 (of 10192), d = 0.452568, its = 21
i = 3279 (of 10192), d = 10, its = 54
i = 3280 (of 10192), d = 10, its = 51
i = 3281 (of 10192), d = 10, its = 37
i = 3282 (of 10192), d = 10, its = 48
i = 3283 (of 10192), d = 10, its = 40
i = 3284 (of 10192), d = 10, its = 57
i = 3285 (of 10192), d = 0.480689, its = 25
i = 3286 (of 10192), d = 10, its = 37
i = 3287 (of 10192), d = 2.37263, its = 5
i = 3288 (of 10192), d = 10, its = 37
i = 3289 (of 10192), d = 10, its = 52
i = 3290 (of 10192), d = 10, its = 52
i = 3291 (of 10192), d = 10, its = 49
i = 3292 (of 10192), d = 10, its = 40
i = 3293 (of 10192), d = 10, its = 54
i = 3294 (of 10192), d = 2.7842, its = 6
i = 3295 (of 10192), d = 0.722056, its = 19
i = 3296 (of 10192), d = 10, its = 49
i = 3297 (of 10192), d = 10, its = 53
i = 3298 (of 10192), d = 10, its = 51
i = 3299 (of 10192), d = 10, its = 50
i = 3300 (of 10192), d = 10, its = 35
i = 3301 (of 10192), d = 10, its = 56
i = 3302 (of 10192), d = 1.11121, its = 19
i = 3303 (of 10192), d = 10, its = 52
i = 3304 (of 10192), d = 10, its = 54
i = 3305 (of 10192), d = 0.711763, its = 24
i = 3306 (of 10192), d = 0.562449, its = 18
i = 3307 (of 10192), d = 10, its = 52
i = 3308 (of 10192), d = 10, its = 56
i = 3309 (of 10192), d = 10, its = 50
i = 3310 (of 10192), d = 10, its = 56
i = 3311 (of 10192), d = 10, its = 53
i = 3312 (of 10192), d = 10, its = 55
i = 3313 (of 10192), d = 10, its = 57
i = 3314 (of 10192), d = 2.04029, its = 3
i = 3315 (of 10192), d = 10, its = 54
i = 3316 (of 10192), d = 1.69764, its = 6
i = 3317 (of 10192), d = 10, its = 54
i = 3318 (of 10192), d = 10, its = 53
i = 3319 (of 10192), d = 1.81874, its = 5
i = 3320 (of 10192), d = 3.10112, its = 6
i = 3321 (of 10192), d = 10, its = 52
i = 3322 (of 10192), d = 10, its = 48
i = 3323 (of 10192), d = 10, its = 50
i = 3324 (of 10192), d = 1.8698, its = 4
i = 3325 (of 10192), d = 1.13486, its = 8
i = 3326 (of 10192), d = 2.13866, its = 4
i = 3327 (of 10192), d = 10, its = 55
i = 3328 (of 10192), d = 10, its = 53
i = 3329 (of 10192), d = 2.00805, its = 3
i = 3330 (of 10192), d = 0.35744, its = 20
i = 3331 (of 10192), d = 10, its = 54
i = 3332 (of 10192), d = 10, its = 52
i = 3333 (of 10192), d = 10, its = 53
i = 3334 (of 10192), d = 10, its = 37
i = 3335 (of 10192), d = 10, its = 37
i = 3336 (of 10192), d = 10, its = 37
i = 3337 (of 10192), d = 10, its = 53
i = 3338 (of 10192), d = 10, its = 53
i = 3339 (of 10192), d = 10, its = 50
i = 3340 (of 10192), d = 10, its = 54
i = 3341 (of 10192), d = 10, its = 50
i = 3342 (of 10192), d = 10, its = 40
i = 3343 (of 10192), d = 10, its = 52
i = 3344 (of 10192), d = 10, its = 40
i = 3345 (of 10192), d = 10, its = 37
i = 3346 (of 10192), d = 10, its = 41
i = 3347 (of 10192), d = 2.11422, its = 4
i = 3348 (of 10192), d = 10, its = 53
i = 3349 (of 10192), d = 10, its = 52
i = 3350 (of 10192), d = 10, its = 37
i = 3351 (of 10192), d = 1.16056, its = 20
i = 3352 (of 10192), d = 10, its = 55
i = 3353 (of 10192), d = 10, its = 53
i = 3354 (of 10192), d = 10, its = 55
i = 3355 (of 10192), d = 10, its = 40
i = 3356 (of 10192), d = 10, its = 40
i = 3357 (of 10192), d = 2.83641, its = 5
i = 3358 (of 10192), d = 10, its = 37
i = 3359 (of 10192), d = 0.843094, its = 25
i = 3360 (of 10192), d = 0.89061, its = 19
i = 3361 (of 10192), d = 10, its = 39
i = 3362 (of 10192), d = 10, its = 51
i = 3363 (of 10192), d = 10, its = 51
i = 3364 (of 10192), d = 10, its = 37
i = 3365 (of 10192), d = 10, its = 56
i = 3366 (of 10192), d = 10, its = 52
i = 3367 (of 10192), d = 10, its = 51
i = 3368 (of 10192), d = 10, its = 38
i = 3369 (of 10192), d = 10, its = 56
i = 3370 (of 10192), d = 10, its = 37
i = 3371 (of 10192), d = 10, its = 45
i = 3372 (of 10192), d = 10, its = 40
i = 3373 (of 10192), d = 10, its = 53
i = 3374 (of 10192), d = 10, its = 37
i = 3375 (of 10192), d = 10, its = 52
i = 3376 (of 10192), d = 2.36267, its = 5
i = 3377 (of 10192), d = 10, its = 41
i = 3378 (of 10192), d = 10, its = 39
i = 3379 (of 10192), d = 10, its = 55
i = 3380 (of 10192), d = 10, its = 50
i = 3381 (of 10192), d = 2.88783, its = 6
i = 3382 (of 10192), d = 10, its = 41
i = 3383 (of 10192), d = 3.41559, its = 6
i = 3384 (of 10192), d = 10, its = 50
i = 3385 (of 10192), d = 10, its = 51
i = 3386 (of 10192), d = 10, its = 53
i = 3387 (of 10192), d = 2.21379, its = 4
i = 3388 (of 10192), d = 8.03702, its = 18
i = 3389 (of 10192), d = 2.79149, its = 6
i = 3390 (of 10192), d = 10, its = 41
i = 3391 (of 10192), d = 2.19098, its = 4
i = 3392 (of 10192), d = 10, its = 55
i = 3393 (of 10192), d = 10, its = 51
i = 3394 (of 10192), d = 0.387744, its = 17
i = 3395 (of 10192), d = 10, its = 55
i = 3396 (of 10192), d = 10, its = 54
i = 3397 (of 10192), d = 10, its = 54
i = 3398 (of 10192), d = 10, its = 49
i = 3399 (of 10192), d = 10, its = 53
i = 3400 (of 10192), d = 10, its = 53
i = 3401 (of 10192), d = 10, its = 49
i = 3402 (of 10192), d = 10, its = 37
i = 3403 (of 10192), d = 10, its = 41
i = 3404 (of 10192), d = 10, its = 39
i = 3405 (of 10192), d = 10, its = 50
i = 3406 (of 10192), d = 10, its = 55
i = 3407 (of 10192), d = 10, its = 51
i = 3408 (of 10192), d = 10, its = 52
i = 3409 (of 10192), d = 3.3616, its = 7
i = 3410 (of 10192), d = 1.64003, its = 5
i = 3411 (of 10192), d = 10, its = 37
i = 3412 (of 10192), d = 2.14413, its = 4
i = 3413 (of 10192), d = 0.81429, its = 16
i = 3414 (of 10192), d = 10, its = 37
i = 3415 (of 10192), d = 10, its = 42
i = 3416 (of 10192), d = 1.71186, its = 5
i = 3417 (of 10192), d = 10, its = 39
i = 3418 (of 10192), d = 10, its = 51
i = 3419 (of 10192), d = 10, its = 50
i = 3420 (of 10192), d = 10, its = 54
i = 3421 (of 10192), d = 1.34358, its = 6
i = 3422 (of 10192), d = 10, its = 51
i = 3423 (of 10192), d = 10, its = 48
i = 3424 (of 10192), d = 10, its = 37
i = 3425 (of 10192), d = 10, its = 38
i = 3426 (of 10192), d = 10, its = 57
i = 3427 (of 10192), d = 10, its = 54
i = 3428 (of 10192), d = 10, its = 55
i = 3429 (of 10192), d = 10, its = 43
i = 3430 (of 10192), d = 10, its = 58
i = 3431 (of 10192), d = 10, its = 37
i = 3432 (of 10192), d = 10, its = 54
i = 3433 (of 10192), d = 10, its = 48
i = 3434 (of 10192), d = 1.43436, its = 6
i = 3435 (of 10192), d = 10, its = 56
i = 3436 (of 10192), d = 10, its = 57
i = 3437 (of 10192), d = 2.53817, its = 5
i = 3438 (of 10192), d = 2.10641, its = 4
i = 3439 (of 10192), d = 2.67482, its = 6
i = 3440 (of 10192), d = 10, its = 52
i = 3441 (of 10192), d = 10, its = 37
i = 3442 (of 10192), d = 10, its = 52
i = 3443 (of 10192), d = 10, its = 40
i = 3444 (of 10192), d = 10, its = 49
i = 3445 (of 10192), d = 10, its = 37
i = 3446 (of 10192), d = 2.0999, its = 4
i = 3447 (of 10192), d = 0.744739, its = 17
i = 3448 (of 10192), d = 10, its = 54
i = 3449 (of 10192), d = 10, its = 53
i = 3450 (of 10192), d = 10, its = 52
i = 3451 (of 10192), d = 10, its = 52
i = 3452 (of 10192), d = 10, its = 40
i = 3453 (of 10192), d = 10, its = 37
i = 3454 (of 10192), d = 2.84791, its = 6
i = 3455 (of 10192), d = 10, its = 37
i = 3456 (of 10192), d = 10, its = 56
i = 3457 (of 10192), d = 10, its = 37
i = 3458 (of 10192), d = 2.89055, its = 5
i = 3459 (of 10192), d = 3.51812, its = 6
i = 3460 (of 10192), d = 10, its = 55
i = 3461 (of 10192), d = 10, its = 37
i = 3462 (of 10192), d = 10, its = 55
i = 3463 (of 10192), d = 1.69588, its = 6
i = 3464 (of 10192), d = 10, its = 55
i = 3465 (of 10192), d = 10, its = 53
i = 3466 (of 10192), d = 10, its = 37
i = 3467 (of 10192), d = 10, its = 51
i = 3468 (of 10192), d = 10, its = 56
i = 3469 (of 10192), d = 2.62517, its = 5
i = 3470 (of 10192), d = 10, its = 52
i = 3471 (of 10192), d = 10, its = 53
i = 3472 (of 10192), d = 10, its = 53
i = 3473 (of 10192), d = 10, its = 54
i = 3474 (of 10192), d = 0.529778, its = 18
i = 3475 (of 10192), d = 10, its = 56
i = 3476 (of 10192), d = 10, its = 53
i = 3477 (of 10192), d = 0.364366, its = 17
i = 3478 (of 10192), d = 10, its = 51
i = 3479 (of 10192), d = 10, its = 53
i = 3480 (of 10192), d = 10, its = 51
i = 3481 (of 10192), d = 0.88041, its = 19
i = 3482 (of 10192), d = 10, its = 53
i = 3483 (of 10192), d = 10, its = 50
i = 3484 (of 10192), d = 10, its = 52
i = 3485 (of 10192), d = 10, its = 37
i = 3486 (of 10192), d = 1.57909, its = 6
i = 3487 (of 10192), d = 10, its = 47
i = 3488 (of 10192), d = 1.40235, its = 7
i = 3489 (of 10192), d = 10, its = 43
i = 3490 (of 10192), d = 10, its = 40
i = 3491 (of 10192), d = 2.50757, its = 6
i = 3492 (of 10192), d = 10, its = 37
i = 3493 (of 10192), d = 10, its = 54
i = 3494 (of 10192), d = 10, its = 53
i = 3495 (of 10192), d = 10, its = 56
i = 3496 (of 10192), d = 10, its = 53
i = 3497 (of 10192), d = 1.01246, its = 17
i = 3498 (of 10192), d = 10, its = 55
i = 3499 (of 10192), d = 10, its = 56
i = 3500 (of 10192), d = 10, its = 55
i = 3501 (of 10192), d = 10, its = 37
i = 3502 (of 10192), d = 10, its = 51
i = 3503 (of 10192), d = 10, its = 52
i = 3504 (of 10192), d = 10, its = 52
i = 3505 (of 10192), d = 10, its = 52
i = 3506 (of 10192), d = 10, its = 57
i = 3507 (of 10192), d = 10, its = 40
i = 3508 (of 10192), d = 10, its = 37
i = 3509 (of 10192), d = 10, its = 37
i = 3510 (of 10192), d = 10, its = 52
i = 3511 (of 10192), d = 10, its = 40
i = 3512 (of 10192), d = 10, its = 37
i = 3513 (of 10192), d = 10, its = 54
i = 3514 (of 10192), d = 10, its = 55
i = 3515 (of 10192), d = 10, its = 37
i = 3516 (of 10192), d = 2.55077, its = 6
i = 3517 (of 10192), d = 10, its = 56
i = 3518 (of 10192), d = 10, its = 52
i = 3519 (of 10192), d = 0.324598, its = 19
i = 3520 (of 10192), d = 10, its = 52
i = 3521 (of 10192), d = 10, its = 56
i = 3522 (of 10192), d = 2.48851, its = 6
i = 3523 (of 10192), d = 10, its = 55
i = 3524 (of 10192), d = 10, its = 54
i = 3525 (of 10192), d = 10, its = 53
i = 3526 (of 10192), d = 1.0097, its = 20
i = 3527 (of 10192), d = 10, its = 44
i = 3528 (of 10192), d = 10, its = 42
i = 3529 (of 10192), d = 10, its = 55
i = 3530 (of 10192), d = 10, its = 54
i = 3531 (of 10192), d = 10, its = 37
i = 3532 (of 10192), d = 10, its = 56
i = 3533 (of 10192), d = 10, its = 53
i = 3534 (of 10192), d = 1.15791, its = 5
i = 3535 (of 10192), d = 10, its = 53
i = 3536 (of 10192), d = 1.56128, its = 6
i = 3537 (of 10192), d = 9.68944, its = 8
i = 3538 (of 10192), d = 10, its = 40
i = 3539 (of 10192), d = 10, its = 53
i = 3540 (of 10192), d = 2.07934, its = 4
i = 3541 (of 10192), d = 10, its = 41
i = 3542 (of 10192), d = 10, its = 53
i = 3543 (of 10192), d = 10, its = 37
i = 3544 (of 10192), d = 10, its = 51
i = 3545 (of 10192), d = 10, its = 53
i = 3546 (of 10192), d = 10, its = 52
i = 3547 (of 10192), d = 0.285881, its = 18
i = 3548 (of 10192), d = 10, its = 53
i = 3549 (of 10192), d = 10, its = 55
i = 3550 (of 10192), d = 4.75387, its = 9
i = 3551 (of 10192), d = 1.92701, its = 4
i = 3552 (of 10192), d = 10, its = 57
i = 3553 (of 10192), d = 10, its = 56
i = 3554 (of 10192), d = 10, its = 50
i = 3555 (of 10192), d = 10, its = 52
i = 3556 (of 10192), d = 2.44337, its = 5
i = 3557 (of 10192), d = 10, its = 40
i = 3558 (of 10192), d = 0.578644, its = 17
i = 3559 (of 10192), d = 1.59114, its = 7
i = 3560 (of 10192), d = 10, its = 49
i = 3561 (of 10192), d = 2.5906, its = 5
i = 3562 (of 10192), d = 10, its = 40
i = 3563 (of 10192), d = 0.6841, its = 20
i = 3564 (of 10192), d = 10, its = 49
i = 3565 (of 10192), d = 10, its = 54
i = 3566 (of 10192), d = 10, its = 50
i = 3567 (of 10192), d = 10, its = 52
i = 3568 (of 10192), d = 0.816158, its = 17
i = 3569 (of 10192), d = 10, its = 51
i = 3570 (of 10192), d = 3.64531, its = 6
i = 3571 (of 10192), d = 6.28213, its = 8
i = 3572 (of 10192), d = 2.75218, its = 6
i = 3573 (of 10192), d = 10, its = 37
i = 3574 (of 10192), d = 1.92675, its = 4
i = 3575 (of 10192), d = 10, its = 53
i = 3576 (of 10192), d = 10, its = 53
i = 3577 (of 10192), d = 10, its = 56
i = 3578 (of 10192), d = 10, its = 52
i = 3579 (of 10192), d = 10, its = 43
i = 3580 (of 10192), d = 3.08095, its = 6
i = 3581 (of 10192), d = 10, its = 57
i = 3582 (of 10192), d = 10, its = 55
i = 3583 (of 10192), d = 10, its = 40
i = 3584 (of 10192), d = 10, its = 41
i = 3585 (of 10192), d = 2.73942, its = 6
i = 3586 (of 10192), d = 10, its = 55
i = 3587 (of 10192), d = 10, its = 51
i = 3588 (of 10192), d = 10, its = 51
i = 3589 (of 10192), d = 10, its = 56
i = 3590 (of 10192), d = 10, its = 52
i = 3591 (of 10192), d = 1.64217, its = 5
i = 3592 (of 10192), d = 10, its = 51
i = 3593 (of 10192), d = 10, its = 53
i = 3594 (of 10192), d = 10, its = 40
i = 3595 (of 10192), d = 10, its = 49
i = 3596 (of 10192), d = 0.633925, its = 26
i = 3597 (of 10192), d = 10, its = 53
i = 3598 (of 10192), d = 2.12414, its = 5
i = 3599 (of 10192), d = 10, its = 53
i = 3600 (of 10192), d = 10, its = 37
i = 3601 (of 10192), d = 3.66643, its = 8
i = 3602 (of 10192), d = 10, its = 52
i = 3603 (of 10192), d = 10, its = 52
i = 3604 (of 10192), d = 1.47795, its = 6
i = 3605 (of 10192), d = 10, its = 39
i = 3606 (of 10192), d = 10, its = 52
i = 3607 (of 10192), d = 10, its = 51
i = 3608 (of 10192), d = 10, its = 40
i = 3609 (of 10192), d = 10, its = 54
i = 3610 (of 10192), d = 10, its = 49
i = 3611 (of 10192), d = 10, its = 48
i = 3612 (of 10192), d = 10, its = 38
i = 3613 (of 10192), d = 10, its = 53
i = 3614 (of 10192), d = 1.61548, its = 6
i = 3615 (of 10192), d = 10, its = 38
i = 3616 (of 10192), d = 10, its = 57
i = 3617 (of 10192), d = 10, its = 52
i = 3618 (of 10192), d = 10, its = 55
i = 3619 (of 10192), d = 10, its = 55
i = 3620 (of 10192), d = 0.688785, its = 18
i = 3621 (of 10192), d = 0.724367, its = 23
i = 3622 (of 10192), d = 10, its = 46
i = 3623 (of 10192), d = 10, its = 52
i = 3624 (of 10192), d = 2.32191, its = 5
i = 3625 (of 10192), d = 10, its = 52
i = 3626 (of 10192), d = 10, its = 57
i = 3627 (of 10192), d = 10, its = 41
i = 3628 (of 10192), d = 10, its = 51
i = 3629 (of 10192), d = 10, its = 37
i = 3630 (of 10192), d = 10, its = 54
i = 3631 (of 10192), d = 10, its = 48
i = 3632 (of 10192), d = 10, its = 55
i = 3633 (of 10192), d = 1.52936, its = 6
i = 3634 (of 10192), d = 10, its = 42
i = 3635 (of 10192), d = 10, its = 54
i = 3636 (of 10192), d = 10, its = 51
i = 3637 (of 10192), d = 10, its = 55
i = 3638 (of 10192), d = 0.85664, its = 18
i = 3639 (of 10192), d = 10, its = 37
i = 3640 (of 10192), d = 10, its = 41
i = 3641 (of 10192), d = 1.52581, its = 7
i = 3642 (of 10192), d = 10, its = 52
i = 3643 (of 10192), d = 10, its = 51
i = 3644 (of 10192), d = 1.10562, its = 22
i = 3645 (of 10192), d = 10, its = 55
i = 3646 (of 10192), d = 10, its = 52
i = 3647 (of 10192), d = 10, its = 53
i = 3648 (of 10192), d = 10, its = 37
i = 3649 (of 10192), d = 10, its = 37
i = 3650 (of 10192), d = 10, its = 37
i = 3651 (of 10192), d = 10, its = 53
i = 3652 (of 10192), d = 10, its = 38
i = 3653 (of 10192), d = 10, its = 53
i = 3654 (of 10192), d = 0.625898, its = 23
i = 3655 (of 10192), d = 10, its = 51
i = 3656 (of 10192), d = 10, its = 52
i = 3657 (of 10192), d = 1.74221, its = 5
i = 3658 (of 10192), d = 1.73225, its = 5
i = 3659 (of 10192), d = 2.11677, its = 4
i = 3660 (of 10192), d = 10, its = 53
i = 3661 (of 10192), d = 10, its = 52
i = 3662 (of 10192), d = 10, its = 41
i = 3663 (of 10192), d = 10, its = 48
i = 3664 (of 10192), d = 2.28401, its = 4
i = 3665 (of 10192), d = 10, its = 49
i = 3666 (of 10192), d = 10, its = 37
i = 3667 (of 10192), d = 1.06997, its = 27
i = 3668 (of 10192), d = 1.51382, its = 7
i = 3669 (of 10192), d = 10, its = 54
i = 3670 (of 10192), d = 10, its = 54
i = 3671 (of 10192), d = 1.53689, its = 7
i = 3672 (of 10192), d = 1.70994, its = 5
i = 3673 (of 10192), d = 1.2856, its = 6
i = 3674 (of 10192), d = 10, its = 37
i = 3675 (of 10192), d = 10, its = 37
i = 3676 (of 10192), d = 10, its = 44
i = 3677 (of 10192), d = 2.35654, its = 5
i = 3678 (of 10192), d = 10, its = 53
i = 3679 (of 10192), d = 10, its = 37
i = 3680 (of 10192), d = 10, its = 37
i = 3681 (of 10192), d = 0.633803, its = 21
i = 3682 (of 10192), d = 10, its = 56
i = 3683 (of 10192), d = 10, its = 54
i = 3684 (of 10192), d = 10, its = 52
i = 3685 (of 10192), d = 10, its = 53
i = 3686 (of 10192), d = 10, its = 51
i = 3687 (of 10192), d = 10, its = 52
i = 3688 (of 10192), d = 10, its = 40
i = 3689 (of 10192), d = 10, its = 52
i = 3690 (of 10192), d = 10, its = 55
i = 3691 (of 10192), d = 1.00927, its = 21
i = 3692 (of 10192), d = 10, its = 53
i = 3693 (of 10192), d = 3.47063, its = 7
i = 3694 (of 10192), d = 10, its = 55
i = 3695 (of 10192), d = 10, its = 52
i = 3696 (of 10192), d = 10, its = 55
i = 3697 (of 10192), d = 10, its = 50
i = 3698 (of 10192), d = 2.33712, its = 5
i = 3699 (of 10192), d = 10, its = 55
i = 3700 (of 10192), d = 10, its = 40
i = 3701 (of 10192), d = 10, its = 51
i = 3702 (of 10192), d = 10, its = 57
i = 3703 (of 10192), d = 3.86556, its = 6
i = 3704 (of 10192), d = 10, its = 54
i = 3705 (of 10192), d = 10, its = 37
i = 3706 (of 10192), d = 10, its = 52
i = 3707 (of 10192), d = 10, its = 58
i = 3708 (of 10192), d = 10, its = 48
i = 3709 (of 10192), d = 3.26117, its = 7
i = 3710 (of 10192), d = 1.25148, its = 7
i = 3711 (of 10192), d = 10, its = 37
i = 3712 (of 10192), d = 10, its = 52
i = 3713 (of 10192), d = 3.87444, its = 7
i = 3714 (of 10192), d = 10, its = 51
i = 3715 (of 10192), d = 10, its = 41
i = 3716 (of 10192), d = 10, its = 49
i = 3717 (of 10192), d = 10, its = 52
i = 3718 (of 10192), d = 10, its = 52
i = 3719 (of 10192), d = 10, its = 53
i = 3720 (of 10192), d = 2.02645, its = 3
i = 3721 (of 10192), d = 10, its = 37
i = 3722 (of 10192), d = 10, its = 53
i = 3723 (of 10192), d = 10, its = 37
i = 3724 (of 10192), d = 10, its = 49
i = 3725 (of 10192), d = 10, its = 37
i = 3726 (of 10192), d = 10, its = 52
i = 3727 (of 10192), d = 10, its = 55
i = 3728 (of 10192), d = 1.76578, its = 5
i = 3729 (of 10192), d = 10, its = 55
i = 3730 (of 10192), d = 10, its = 55
i = 3731 (of 10192), d = 2.63235, its = 5
i = 3732 (of 10192), d = 10, its = 44
i = 3733 (of 10192), d = 10, its = 56
i = 3734 (of 10192), d = 10, its = 51
i = 3735 (of 10192), d = 10, its = 53
i = 3736 (of 10192), d = 10, its = 37
i = 3737 (of 10192), d = 10, its = 54
i = 3738 (of 10192), d = 10, its = 59
i = 3739 (of 10192), d = 10, its = 39
i = 3740 (of 10192), d = 2.2408, its = 4
i = 3741 (of 10192), d = 10, its = 52
i = 3742 (of 10192), d = 10, its = 54
i = 3743 (of 10192), d = 10, its = 53
i = 3744 (of 10192), d = 10, its = 55
i = 3745 (of 10192), d = 10, its = 38
i = 3746 (of 10192), d = 10, its = 51
i = 3747 (of 10192), d = 3.20038, its = 6
i = 3748 (of 10192), d = 10, its = 54
i = 3749 (of 10192), d = 10, its = 37
i = 3750 (of 10192), d = 3.29658, its = 6
i = 3751 (of 10192), d = 10, its = 55
i = 3752 (of 10192), d = 10, its = 51
i = 3753 (of 10192), d = 10, its = 37
i = 3754 (of 10192), d = 10, its = 37
i = 3755 (of 10192), d = 10, its = 54
i = 3756 (of 10192), d = 10, its = 56
i = 3757 (of 10192), d = 10, its = 52
i = 3758 (of 10192), d = 2.24215, its = 5
i = 3759 (of 10192), d = 1.00355, its = 18
i = 3760 (of 10192), d = 10, its = 53
i = 3761 (of 10192), d = 1.81642, its = 6
i = 3762 (of 10192), d = 10, its = 55
i = 3763 (of 10192), d = 10, its = 54
i = 3764 (of 10192), d = 10, its = 52
i = 3765 (of 10192), d = 10, its = 54
i = 3766 (of 10192), d = 10, its = 52
i = 3767 (of 10192), d = 10, its = 52
i = 3768 (of 10192), d = 10, its = 53
i = 3769 (of 10192), d = 10, its = 54
i = 3770 (of 10192), d = 10, its = 56
i = 3771 (of 10192), d = 1.54223, its = 6
i = 3772 (of 10192), d = 1.39823, its = 7
i = 3773 (of 10192), d = 10, its = 51
i = 3774 (of 10192), d = 1.42297, its = 6
i = 3775 (of 10192), d = 0.27578, its = 28
i = 3776 (of 10192), d = 4.02859, its = 7
i = 3777 (of 10192), d = 10, its = 57
i = 3778 (of 10192), d = 1.94248, its = 3
i = 3779 (of 10192), d = 10, its = 54
i = 3780 (of 10192), d = 10, its = 57
i = 3781 (of 10192), d = 3.04216, its = 7
i = 3782 (of 10192), d = 2.66532, its = 5
i = 3783 (of 10192), d = 10, its = 41
i = 3784 (of 10192), d = 2.70724, its = 6
i = 3785 (of 10192), d = 10, its = 52
i = 3786 (of 10192), d = 1.29785, its = 8
i = 3787 (of 10192), d = 10, its = 40
i = 3788 (of 10192), d = 10, its = 50
i = 3789 (of 10192), d = 10, its = 53
i = 3790 (of 10192), d = 10, its = 51
i = 3791 (of 10192), d = 0.853082, its = 25
i = 3792 (of 10192), d = 10, its = 43
i = 3793 (of 10192), d = 10, its = 52
i = 3794 (of 10192), d = 10, its = 57
i = 3795 (of 10192), d = 10, its = 51
i = 3796 (of 10192), d = 1.19636, its = 7
i = 3797 (of 10192), d = 2.20658, its = 4
i = 3798 (of 10192), d = 10, its = 57
i = 3799 (of 10192), d = 10, its = 41
i = 3800 (of 10192), d = 10, its = 37
i = 3801 (of 10192), d = 10, its = 37
i = 3802 (of 10192), d = 1.57602, its = 6
i = 3803 (of 10192), d = 10, its = 51
i = 3804 (of 10192), d = 10, its = 46
i = 3805 (of 10192), d = 10, its = 53
i = 3806 (of 10192), d = 10, its = 54
i = 3807 (of 10192), d = 10, its = 52
i = 3808 (of 10192), d = 10, its = 37
i = 3809 (of 10192), d = 10, its = 51
i = 3810 (of 10192), d = 0.901567, its = 18
i = 3811 (of 10192), d = 10, its = 51
i = 3812 (of 10192), d = 10, its = 58
i = 3813 (of 10192), d = 10, its = 52
i = 3814 (of 10192), d = 10, its = 39
i = 3815 (of 10192), d = 10, its = 51
i = 3816 (of 10192), d = 6.03143, its = 9
i = 3817 (of 10192), d = 3.47072, its = 6
i = 3818 (of 10192), d = 0.861126, its = 23
i = 3819 (of 10192), d = 10, its = 37
i = 3820 (of 10192), d = 10, its = 37
i = 3821 (of 10192), d = 10, its = 37
i = 3822 (of 10192), d = 10, its = 52
i = 3823 (of 10192), d = 10, its = 56
i = 3824 (of 10192), d = 0.908763, its = 18
i = 3825 (of 10192), d = 1.61436, its = 6
i = 3826 (of 10192), d = 10, its = 55
i = 3827 (of 10192), d = 10, its = 51
i = 3828 (of 10192), d = 10, its = 55
i = 3829 (of 10192), d = 10, its = 55
i = 3830 (of 10192), d = 10, its = 51
i = 3831 (of 10192), d = 5.98065, its = 8
i = 3832 (of 10192), d = 10, its = 37
i = 3833 (of 10192), d = 10, its = 53
i = 3834 (of 10192), d = 10, its = 40
i = 3835 (of 10192), d = 10, its = 56
i = 3836 (of 10192), d = 10, its = 37
i = 3837 (of 10192), d = 10, its = 50
i = 3838 (of 10192), d = 10, its = 52
i = 3839 (of 10192), d = 10, its = 37
i = 3840 (of 10192), d = 10, its = 37
i = 3841 (of 10192), d = 3.28872, its = 6
i = 3842 (of 10192), d = 10, its = 37
i = 3843 (of 10192), d = 10, its = 50
i = 3844 (of 10192), d = 10, its = 41
i = 3845 (of 10192), d = 1.4614, its = 6
i = 3846 (of 10192), d = 0.998967, its = 16
i = 3847 (of 10192), d = 10, its = 57
i = 3848 (of 10192), d = 10, its = 52
i = 3849 (of 10192), d = 10, its = 53
i = 3850 (of 10192), d = 10, its = 53
i = 3851 (of 10192), d = 10, its = 54
i = 3852 (of 10192), d = 10, its = 54
i = 3853 (of 10192), d = 10, its = 53
i = 3854 (of 10192), d = 10, its = 51
i = 3855 (of 10192), d = 10, its = 37
i = 3856 (of 10192), d = 10, its = 52
i = 3857 (of 10192), d = 2.79709, its = 5
i = 3858 (of 10192), d = 10, its = 53
i = 3859 (of 10192), d = 10, its = 56
i = 3860 (of 10192), d = 10, its = 51
i = 3861 (of 10192), d = 10, its = 52
i = 3862 (of 10192), d = 10, its = 41
i = 3863 (of 10192), d = 10, its = 39
i = 3864 (of 10192), d = 10, its = 56
i = 3865 (of 10192), d = 10, its = 43
i = 3866 (of 10192), d = 10, its = 54
i = 3867 (of 10192), d = 0.88443, its = 18
i = 3868 (of 10192), d = 10, its = 51
i = 3869 (of 10192), d = 10, its = 52
i = 3870 (of 10192), d = 0.996839, its = 18
i = 3871 (of 10192), d = 4.02721, its = 7
i = 3872 (of 10192), d = 10, its = 53
i = 3873 (of 10192), d = 10, its = 53
i = 3874 (of 10192), d = 10, its = 40
i = 3875 (of 10192), d = 10, its = 53
i = 3876 (of 10192), d = 10, its = 50
i = 3877 (of 10192), d = 1.68517, its = 6
i = 3878 (of 10192), d = 10, its = 40
i = 3879 (of 10192), d = 10, its = 57
i = 3880 (of 10192), d = 10, its = 55
i = 3881 (of 10192), d = 1.41306, its = 6
i = 3882 (of 10192), d = 10, its = 43
i = 3883 (of 10192), d = 10, its = 55
i = 3884 (of 10192), d = 10, its = 56
i = 3885 (of 10192), d = 10, its = 54
i = 3886 (of 10192), d = 10, its = 42
i = 3887 (of 10192), d = 10, its = 49
i = 3888 (of 10192), d = 10, its = 42
i = 3889 (of 10192), d = 10, its = 37
i = 3890 (of 10192), d = 10, its = 53
i = 3891 (of 10192), d = 1.51975, its = 7
i = 3892 (of 10192), d = 10, its = 40
i = 3893 (of 10192), d = 10, its = 54
i = 3894 (of 10192), d = 10, its = 52
i = 3895 (of 10192), d = 10, its = 52
i = 3896 (of 10192), d = 10, its = 53
i = 3897 (of 10192), d = 10, its = 52
i = 3898 (of 10192), d = 10, its = 54
i = 3899 (of 10192), d = 10, its = 51
i = 3900 (of 10192), d = 10, its = 53
i = 3901 (of 10192), d = 10, its = 54
i = 3902 (of 10192), d = 9.28206, its = 5
i = 3903 (of 10192), d = 10, its = 41
i = 3904 (of 10192), d = 10, its = 53
i = 3905 (of 10192), d = 10, its = 57
i = 3906 (of 10192), d = 2.21441, its = 4
i = 3907 (of 10192), d = 10, its = 54
i = 3908 (of 10192), d = 10, its = 50
i = 3909 (of 10192), d = 0.932845, its = 23
i = 3910 (of 10192), d = 10, its = 58
i = 3911 (of 10192), d = 1.79889, its = 4
i = 3912 (of 10192), d = 10, its = 51
i = 3913 (of 10192), d = 3.36084, its = 7
i = 3914 (of 10192), d = 10, its = 55
i = 3915 (of 10192), d = 10, its = 37
i = 3916 (of 10192), d = 2.54857, its = 5
i = 3917 (of 10192), d = 10, its = 55
i = 3918 (of 10192), d = 10, its = 41
i = 3919 (of 10192), d = 10, its = 54
i = 3920 (of 10192), d = 2.06293, its = 4
i = 3921 (of 10192), d = 10, its = 49
i = 3922 (of 10192), d = 10, its = 50
i = 3923 (of 10192), d = 10, its = 50
i = 3924 (of 10192), d = 10, its = 37
i = 3925 (of 10192), d = 10, its = 52
i = 3926 (of 10192), d = 10, its = 41
i = 3927 (of 10192), d = 10, its = 51
i = 3928 (of 10192), d = 10, its = 51
i = 3929 (of 10192), d = 0.820039, its = 17
i = 3930 (of 10192), d = 10, its = 54
i = 3931 (of 10192), d = 10, its = 37
i = 3932 (of 10192), d = 10, its = 52
i = 3933 (of 10192), d = 1.06751, its = 21
i = 3934 (of 10192), d = 10, its = 42
i = 3935 (of 10192), d = 10, its = 39
i = 3936 (of 10192), d = 10, its = 37
i = 3937 (of 10192), d = 1.43728, its = 20
i = 3938 (of 10192), d = 1.23975, its = 5
i = 3939 (of 10192), d = 10, its = 53
i = 3940 (of 10192), d = 10, its = 40
i = 3941 (of 10192), d = 10, its = 51
i = 3942 (of 10192), d = 10, its = 55
i = 3943 (of 10192), d = 10, its = 51
i = 3944 (of 10192), d = 10, its = 37
i = 3945 (of 10192), d = 0.432856, its = 20
i = 3946 (of 10192), d = 10, its = 41
i = 3947 (of 10192), d = 10, its = 57
i = 3948 (of 10192), d = 1.99979, its = 2
i = 3949 (of 10192), d = 1.85854, its = 4
i = 3950 (of 10192), d = 10, its = 52
i = 3951 (of 10192), d = 10, its = 50
i = 3952 (of 10192), d = 10, its = 54
i = 3953 (of 10192), d = 10, its = 54
i = 3954 (of 10192), d = 10, its = 52
i = 3955 (of 10192), d = 10, its = 41
i = 3956 (of 10192), d = 2.1381, its = 4
i = 3957 (of 10192), d = 10, its = 51
i = 3958 (of 10192), d = 10, its = 41
i = 3959 (of 10192), d = 10, its = 38
i = 3960 (of 10192), d = 0.869819, its = 17
i = 3961 (of 10192), d = 10, its = 51
i = 3962 (of 10192), d = 10, its = 43
i = 3963 (of 10192), d = 5.26615, its = 7
i = 3964 (of 10192), d = 10, its = 39
i = 3965 (of 10192), d = 10, its = 37
i = 3966 (of 10192), d = 10, its = 37
i = 3967 (of 10192), d = 1.4419, its = 8
i = 3968 (of 10192), d = 10, its = 56
i = 3969 (of 10192), d = 4.00207, its = 8
i = 3970 (of 10192), d = 10, its = 37
i = 3971 (of 10192), d = 10, its = 53
i = 3972 (of 10192), d = 2.60207, its = 5
i = 3973 (of 10192), d = 3.97413, its = 6
i = 3974 (of 10192), d = 10, its = 49
i = 3975 (of 10192), d = 4.69821, its = 7
i = 3976 (of 10192), d = 10, its = 56
i = 3977 (of 10192), d = 2.48716, its = 5
i = 3978 (of 10192), d = 0.696589, its = 18
i = 3979 (of 10192), d = 1.63417, its = 6
i = 3980 (of 10192), d = 10, its = 54
i = 3981 (of 10192), d = 10, its = 42
i = 3982 (of 10192), d = 0.522203, its = 18
i = 3983 (of 10192), d = 10, its = 52
i = 3984 (of 10192), d = 1.40955, its = 6
i = 3985 (of 10192), d = 10, its = 54
i = 3986 (of 10192), d = 6.47839, its = 8
i = 3987 (of 10192), d = 10, its = 54
i = 3988 (of 10192), d = 2.73219, its = 5
i = 3989 (of 10192), d = 10, its = 37
i = 3990 (of 10192), d = 10, its = 37
i = 3991 (of 10192), d = 10, its = 50
i = 3992 (of 10192), d = 2.47612, its = 5
i = 3993 (of 10192), d = 10, its = 37
i = 3994 (of 10192), d = 10, its = 37
i = 3995 (of 10192), d = 10, its = 52
i = 3996 (of 10192), d = 10, its = 40
i = 3997 (of 10192), d = 10, its = 41
i = 3998 (of 10192), d = 10, its = 54
i = 3999 (of 10192), d = 10, its = 54
i = 4000 (of 10192), d = 0.973938, its = 16
i = 4001 (of 10192), d = 10, its = 37
i = 4002 (of 10192), d = 10, its = 40
i = 4003 (of 10192), d = 10, its = 37
i = 4004 (of 10192), d = 10, its = 54
i = 4005 (of 10192), d = 10, its = 39
i = 4006 (of 10192), d = 10, its = 53
i = 4007 (of 10192), d = 10, its = 50
i = 4008 (of 10192), d = 10, its = 48
i = 4009 (of 10192), d = 10, its = 40
i = 4010 (of 10192), d = 10, its = 46
i = 4011 (of 10192), d = 10, its = 50
i = 4012 (of 10192), d = 10, its = 37
i = 4013 (of 10192), d = 10, its = 51
i = 4014 (of 10192), d = 10, its = 56
i = 4015 (of 10192), d = 10, its = 52
i = 4016 (of 10192), d = 10, its = 50
i = 4017 (of 10192), d = 10, its = 41
i = 4018 (of 10192), d = 10, its = 51
i = 4019 (of 10192), d = 10, its = 51
i = 4020 (of 10192), d = 10, its = 55
i = 4021 (of 10192), d = 10, its = 52
i = 4022 (of 10192), d = 10, its = 54
i = 4023 (of 10192), d = 10, its = 55
i = 4024 (of 10192), d = 10, its = 53
i = 4025 (of 10192), d = 10, its = 49
i = 4026 (of 10192), d = 10, its = 37
i = 4027 (of 10192), d = 10, its = 49
i = 4028 (of 10192), d = 10, its = 50
i = 4029 (of 10192), d = 10, its = 52
i = 4030 (of 10192), d = 10, its = 38
i = 4031 (of 10192), d = 10, its = 55
i = 4032 (of 10192), d = 10, its = 37
i = 4033 (of 10192), d = 2.96335, its = 6
i = 4034 (of 10192), d = 2.62667, its = 5
i = 4035 (of 10192), d = 2.13227, its = 4
i = 4036 (of 10192), d = 10, its = 50
i = 4037 (of 10192), d = 10, its = 44
i = 4038 (of 10192), d = 10, its = 37
i = 4039 (of 10192), d = 10, its = 55
i = 4040 (of 10192), d = 1.14176, its = 20
i = 4041 (of 10192), d = 1.55096, its = 6
i = 4042 (of 10192), d = 10, its = 40
i = 4043 (of 10192), d = 10, its = 42
i = 4044 (of 10192), d = 10, its = 54
i = 4045 (of 10192), d = 10, its = 50
i = 4046 (of 10192), d = 10, its = 52
i = 4047 (of 10192), d = 10, its = 50
i = 4048 (of 10192), d = 10, its = 57
i = 4049 (of 10192), d = 2.14247, its = 4
i = 4050 (of 10192), d = 10, its = 54
i = 4051 (of 10192), d = 3.83418, its = 7
i = 4052 (of 10192), d = 10, its = 37
i = 4053 (of 10192), d = 10, its = 39
i = 4054 (of 10192), d = 4.06195, its = 7
i = 4055 (of 10192), d = 10, its = 54
i = 4056 (of 10192), d = 10, its = 52
i = 4057 (of 10192), d = 1.01937, its = 16
i = 4058 (of 10192), d = 10, its = 55
i = 4059 (of 10192), d = 2.37552, its = 5
i = 4060 (of 10192), d = 10, its = 51
i = 4061 (of 10192), d = 10, its = 37
i = 4062 (of 10192), d = 10, its = 52
i = 4063 (of 10192), d = 2.831, its = 6
i = 4064 (of 10192), d = 0.647553, its = 17
i = 4065 (of 10192), d = 10, its = 37
i = 4066 (of 10192), d = 10, its = 39
i = 4067 (of 10192), d = 10, its = 57
i = 4068 (of 10192), d = 10, its = 51
i = 4069 (of 10192), d = 10, its = 50
i = 4070 (of 10192), d = 1.77434, its = 4
i = 4071 (of 10192), d = 0.881052, its = 24
i = 4072 (of 10192), d = 10, its = 54
i = 4073 (of 10192), d = 10, its = 54
i = 4074 (of 10192), d = 10, its = 39
i = 4075 (of 10192), d = 1.04917, its = 16
i = 4076 (of 10192), d = 10, its = 43
i = 4077 (of 10192), d = 1.69646, its = 6
i = 4078 (of 10192), d = 10, its = 37
i = 4079 (of 10192), d = 10, its = 41
i = 4080 (of 10192), d = 10, its = 51
i = 4081 (of 10192), d = 10, its = 54
i = 4082 (of 10192), d = 10, its = 51
i = 4083 (of 10192), d = 10, its = 52
i = 4084 (of 10192), d = 10, its = 54
i = 4085 (of 10192), d = 10, its = 51
i = 4086 (of 10192), d = 10, its = 53
i = 4087 (of 10192), d = 10, its = 51
i = 4088 (of 10192), d = 10, its = 58
i = 4089 (of 10192), d = 10, its = 54
i = 4090 (of 10192), d = 10, its = 37
i = 4091 (of 10192), d = 10, its = 41
i = 4092 (of 10192), d = 10, its = 37
i = 4093 (of 10192), d = 10, its = 54
i = 4094 (of 10192), d = 0.906592, its = 18
i = 4095 (of 10192), d = 10, its = 48
i = 4096 (of 10192), d = 1.78309, its = 5
i = 4097 (of 10192), d = 10, its = 55
i = 4098 (of 10192), d = 10, its = 52
i = 4099 (of 10192), d = 10, its = 50
i = 4100 (of 10192), d = 10, its = 53
i = 4101 (of 10192), d = 1.90359, its = 5
i = 4102 (of 10192), d = 4.26989, its = 7
i = 4103 (of 10192), d = 10, its = 37
i = 4104 (of 10192), d = 10, its = 53
i = 4105 (of 10192), d = 10, its = 52
i = 4106 (of 10192), d = 10, its = 49
i = 4107 (of 10192), d = 1.67733, its = 6
i = 4108 (of 10192), d = 10, its = 54
i = 4109 (of 10192), d = 10, its = 49
i = 4110 (of 10192), d = 10, its = 54
i = 4111 (of 10192), d = 10, its = 52
i = 4112 (of 10192), d = 2.72408, its = 6
i = 4113 (of 10192), d = 10, its = 37
i = 4114 (of 10192), d = 10, its = 58
i = 4115 (of 10192), d = 10, its = 52
i = 4116 (of 10192), d = 10, its = 40
i = 4117 (of 10192), d = 10, its = 52
i = 4118 (of 10192), d = 10, its = 54
i = 4119 (of 10192), d = 10, its = 37
i = 4120 (of 10192), d = 0.818619, its = 19
i = 4121 (of 10192), d = 10, its = 52
i = 4122 (of 10192), d = 10, its = 40
i = 4123 (of 10192), d = 10, its = 54
i = 4124 (of 10192), d = 10, its = 53
i = 4125 (of 10192), d = 1.46252, its = 8
i = 4126 (of 10192), d = 10, its = 54
i = 4127 (of 10192), d = 10, its = 56
i = 4128 (of 10192), d = 10, its = 54
i = 4129 (of 10192), d = 10, its = 53
i = 4130 (of 10192), d = 5.49532, its = 9
i = 4131 (of 10192), d = 1.53059, its = 7
i = 4132 (of 10192), d = 10, its = 52
i = 4133 (of 10192), d = 10, its = 37
i = 4134 (of 10192), d = 0.870552, its = 21
i = 4135 (of 10192), d = 10, its = 54
i = 4136 (of 10192), d = 10, its = 52
i = 4137 (of 10192), d = 10, its = 55
i = 4138 (of 10192), d = 10, its = 52
i = 4139 (of 10192), d = 10, its = 53
i = 4140 (of 10192), d = 1.85684, its = 4
i = 4141 (of 10192), d = 10, its = 50
i = 4142 (of 10192), d = 10, its = 54
i = 4143 (of 10192), d = 10, its = 42
i = 4144 (of 10192), d = 10, its = 37
i = 4145 (of 10192), d = 10, its = 54
i = 4146 (of 10192), d = 10, its = 40
i = 4147 (of 10192), d = 0.553244, its = 16
i = 4148 (of 10192), d = 10, its = 53
i = 4149 (of 10192), d = 10, its = 58
i = 4150 (of 10192), d = 10, its = 53
i = 4151 (of 10192), d = 10, its = 52
i = 4152 (of 10192), d = 10, its = 37
i = 4153 (of 10192), d = 10, its = 37
i = 4154 (of 10192), d = 10, its = 47
i = 4155 (of 10192), d = 10, its = 41
i = 4156 (of 10192), d = 0.925052, its = 18
i = 4157 (of 10192), d = 0.239569, its = 22
i = 4158 (of 10192), d = 10, its = 51
i = 4159 (of 10192), d = 10, its = 37
i = 4160 (of 10192), d = 10, its = 53
i = 4161 (of 10192), d = 10, its = 40
i = 4162 (of 10192), d = 10, its = 50
i = 4163 (of 10192), d = 2.36924, its = 5
i = 4164 (of 10192), d = 2.35981, its = 5
i = 4165 (of 10192), d = 10, its = 53
i = 4166 (of 10192), d = 10, its = 52
i = 4167 (of 10192), d = 10, its = 57
i = 4168 (of 10192), d = 10, its = 40
i = 4169 (of 10192), d = 2.45481, its = 5
i = 4170 (of 10192), d = 10, its = 53
i = 4171 (of 10192), d = 10, its = 52
i = 4172 (of 10192), d = 10, its = 57
i = 4173 (of 10192), d = 10, its = 58
i = 4174 (of 10192), d = 10, its = 51
i = 4175 (of 10192), d = 10, its = 52
i = 4176 (of 10192), d = 10, its = 48
i = 4177 (of 10192), d = 1.65445, its = 5
i = 4178 (of 10192), d = 10, its = 40
i = 4179 (of 10192), d = 10, its = 37
i = 4180 (of 10192), d = 10, its = 50
i = 4181 (of 10192), d = 10, its = 53
i = 4182 (of 10192), d = 10, its = 52
i = 4183 (of 10192), d = 10, its = 40
i = 4184 (of 10192), d = 10, its = 57
i = 4185 (of 10192), d = 10, its = 53
i = 4186 (of 10192), d = 0.816891, its = 17
i = 4187 (of 10192), d = 2.07944, its = 4
i = 4188 (of 10192), d = 10, its = 54
i = 4189 (of 10192), d = 0.315668, its = 20
i = 4190 (of 10192), d = 10, its = 50
i = 4191 (of 10192), d = 10, its = 50
i = 4192 (of 10192), d = 10, its = 54
i = 4193 (of 10192), d = 10, its = 42
i = 4194 (of 10192), d = 10, its = 50
i = 4195 (of 10192), d = 3.19201, its = 6
i = 4196 (of 10192), d = 1.9034, its = 4
i = 4197 (of 10192), d = 10, its = 41
i = 4198 (of 10192), d = 10, its = 57
i = 4199 (of 10192), d = 10, its = 54
i = 4200 (of 10192), d = 10, its = 37
i = 4201 (of 10192), d = 1.96966, its = 3
i = 4202 (of 10192), d = 10, its = 55
i = 4203 (of 10192), d = 10, its = 53
i = 4204 (of 10192), d = 10, its = 37
i = 4205 (of 10192), d = 10, its = 53
i = 4206 (of 10192), d = 10, its = 53
i = 4207 (of 10192), d = 10, its = 37
i = 4208 (of 10192), d = 2.29408, its = 5
i = 4209 (of 10192), d = 10, its = 37
i = 4210 (of 10192), d = 10, its = 37
i = 4211 (of 10192), d = 10, its = 37
i = 4212 (of 10192), d = 10, its = 40
i = 4213 (of 10192), d = 10, its = 51
i = 4214 (of 10192), d = 10, its = 57
i = 4215 (of 10192), d = 10, its = 57
i = 4216 (of 10192), d = 10, its = 51
i = 4217 (of 10192), d = 10, its = 51
i = 4218 (of 10192), d = 3.06253, its = 6
i = 4219 (of 10192), d = 10, its = 40
i = 4220 (of 10192), d = 10, its = 52
i = 4221 (of 10192), d = 10, its = 50
i = 4222 (of 10192), d = 10, its = 39
i = 4223 (of 10192), d = 10, its = 37
i = 4224 (of 10192), d = 10, its = 48
i = 4225 (of 10192), d = 2.4933, its = 5
i = 4226 (of 10192), d = 10, its = 54
i = 4227 (of 10192), d = 10, its = 37
i = 4228 (of 10192), d = 0.410957, its = 18
i = 4229 (of 10192), d = 10, its = 38
i = 4230 (of 10192), d = 10, its = 42
i = 4231 (of 10192), d = 10, its = 51
i = 4232 (of 10192), d = 10, its = 52
i = 4233 (of 10192), d = 10, its = 52
i = 4234 (of 10192), d = 10, its = 51
i = 4235 (of 10192), d = 10, its = 52
i = 4236 (of 10192), d = 1.62735, its = 6
i = 4237 (of 10192), d = 10, its = 56
i = 4238 (of 10192), d = 2.49378, its = 5
i = 4239 (of 10192), d = 10, its = 53
i = 4240 (of 10192), d = 10, its = 52
i = 4241 (of 10192), d = 10, its = 54
i = 4242 (of 10192), d = 3.71284, its = 7
i = 4243 (of 10192), d = 10, its = 49
i = 4244 (of 10192), d = 10, its = 59
i = 4245 (of 10192), d = 10, its = 51
i = 4246 (of 10192), d = 10, its = 54
i = 4247 (of 10192), d = 2.1484, its = 4
i = 4248 (of 10192), d = 6.27548, its = 7
i = 4249 (of 10192), d = 2.07517, its = 3
i = 4250 (of 10192), d = 10, its = 37
i = 4251 (of 10192), d = 10, its = 54
i = 4252 (of 10192), d = 10, its = 44
i = 4253 (of 10192), d = 1.76558, its = 6
i = 4254 (of 10192), d = 10, its = 49
i = 4255 (of 10192), d = 1.33861, its = 6
i = 4256 (of 10192), d = 10, its = 50
i = 4257 (of 10192), d = 10, its = 56
i = 4258 (of 10192), d = 1.63602, its = 6
i = 4259 (of 10192), d = 0.497034, its = 29
i = 4260 (of 10192), d = 10, its = 53
i = 4261 (of 10192), d = 10, its = 56
i = 4262 (of 10192), d = 10, its = 53
i = 4263 (of 10192), d = 10, its = 54
i = 4264 (of 10192), d = 10, its = 57
i = 4265 (of 10192), d = 10, its = 54
i = 4266 (of 10192), d = 10, its = 37
i = 4267 (of 10192), d = 10, its = 37
i = 4268 (of 10192), d = 10, its = 43
i = 4269 (of 10192), d = 1.09756, its = 21
i = 4270 (of 10192), d = 10, its = 37
i = 4271 (of 10192), d = 3.10885, its = 6
i = 4272 (of 10192), d = 10, its = 51
i = 4273 (of 10192), d = 10, its = 53
i = 4274 (of 10192), d = 10, its = 50
i = 4275 (of 10192), d = 10, its = 56
i = 4276 (of 10192), d = 10, its = 37
i = 4277 (of 10192), d = 10, its = 50
i = 4278 (of 10192), d = 10, its = 51
i = 4279 (of 10192), d = 10, its = 49
i = 4280 (of 10192), d = 10, its = 37
i = 4281 (of 10192), d = 10, its = 51
i = 4282 (of 10192), d = 0.875752, its = 16
i = 4283 (of 10192), d = 10, its = 37
i = 4284 (of 10192), d = 2.04253, its = 3
i = 4285 (of 10192), d = 4.49067, its = 8
i = 4286 (of 10192), d = 10, its = 40
i = 4287 (of 10192), d = 1.67367, its = 5
i = 4288 (of 10192), d = 10, its = 46
i = 4289 (of 10192), d = 10, its = 55
i = 4290 (of 10192), d = 10, its = 40
i = 4291 (of 10192), d = 10, its = 53
i = 4292 (of 10192), d = 10, its = 49
i = 4293 (of 10192), d = 10, its = 49
i = 4294 (of 10192), d = 10, its = 50
i = 4295 (of 10192), d = 10, its = 40
i = 4296 (of 10192), d = 10, its = 37
i = 4297 (of 10192), d = 10, its = 54
i = 4298 (of 10192), d = 10, its = 37
i = 4299 (of 10192), d = 10, its = 40
i = 4300 (of 10192), d = 10, its = 54
i = 4301 (of 10192), d = 10, its = 54
i = 4302 (of 10192), d = 10, its = 37
i = 4303 (of 10192), d = 1.13549, its = 20
i = 4304 (of 10192), d = 10, its = 48
i = 4305 (of 10192), d = 0.341445, its = 18
i = 4306 (of 10192), d = 10, its = 52
i = 4307 (of 10192), d = 10, its = 52
i = 4308 (of 10192), d = 1.7991, its = 5
i = 4309 (of 10192), d = 10, its = 37
i = 4310 (of 10192), d = 10, its = 51
i = 4311 (of 10192), d = 10, its = 50
i = 4312 (of 10192), d = 10, its = 51
i = 4313 (of 10192), d = 10, its = 54
i = 4314 (of 10192), d = 2.11287, its = 4
i = 4315 (of 10192), d = 5.04446, its = 7
i = 4316 (of 10192), d = 10, its = 37
i = 4317 (of 10192), d = 10, its = 52
i = 4318 (of 10192), d = 10, its = 52
i = 4319 (of 10192), d = 10, its = 42
i = 4320 (of 10192), d = 10, its = 49
i = 4321 (of 10192), d = 10, its = 41
i = 4322 (of 10192), d = 1.47272, its = 6
i = 4323 (of 10192), d = 10, its = 53
i = 4324 (of 10192), d = 10, its = 47
i = 4325 (of 10192), d = 2.26714, its = 5
i = 4326 (of 10192), d = 10, its = 37
i = 4327 (of 10192), d = 10, its = 49
i = 4328 (of 10192), d = 10, its = 56
i = 4329 (of 10192), d = 10, its = 58
i = 4330 (of 10192), d = 10, its = 55
i = 4331 (of 10192), d = 10, its = 37
i = 4332 (of 10192), d = 10, its = 52
i = 4333 (of 10192), d = 0.514825, its = 18
i = 4334 (of 10192), d = 10, its = 51
i = 4335 (of 10192), d = 0.957836, its = 20
i = 4336 (of 10192), d = 10, its = 52
i = 4337 (of 10192), d = 1.45566, its = 6
i = 4338 (of 10192), d = 10, its = 52
i = 4339 (of 10192), d = 0.895263, its = 24
i = 4340 (of 10192), d = 10, its = 53
i = 4341 (of 10192), d = 10, its = 55
i = 4342 (of 10192), d = 1.85093, its = 4
i = 4343 (of 10192), d = 10, its = 53
i = 4344 (of 10192), d = 1.75644, its = 6
i = 4345 (of 10192), d = 10, its = 37
i = 4346 (of 10192), d = 0.767301, its = 18
i = 4347 (of 10192), d = 10, its = 44
i = 4348 (of 10192), d = 10, its = 49
i = 4349 (of 10192), d = 10, its = 39
i = 4350 (of 10192), d = 0.746655, its = 19
i = 4351 (of 10192), d = 10, its = 54
i = 4352 (of 10192), d = 2.18915, its = 5
i = 4353 (of 10192), d = 10, its = 51
i = 4354 (of 10192), d = 10, its = 51
i = 4355 (of 10192), d = 10, its = 37
i = 4356 (of 10192), d = 10, its = 55
i = 4357 (of 10192), d = 10, its = 39
i = 4358 (of 10192), d = 10, its = 53
i = 4359 (of 10192), d = 10, its = 52
i = 4360 (of 10192), d = 10, its = 53
i = 4361 (of 10192), d = 10, its = 49
i = 4362 (of 10192), d = 10, its = 51
i = 4363 (of 10192), d = 10, its = 49
i = 4364 (of 10192), d = 0.671817, its = 18
i = 4365 (of 10192), d = 2.10238, its = 4
i = 4366 (of 10192), d = 10, its = 37
i = 4367 (of 10192), d = 10, its = 56
i = 4368 (of 10192), d = 1.33283, its = 6
i = 4369 (of 10192), d = 10, its = 54
i = 4370 (of 10192), d = 10, its = 40
i = 4371 (of 10192), d = 1.65286, its = 7
i = 4372 (of 10192), d = 2.13229, its = 4
i = 4373 (of 10192), d = 0.700734, its = 18
i = 4374 (of 10192), d = 10, its = 54
i = 4375 (of 10192), d = 1.90731, its = 4
i = 4376 (of 10192), d = 10, its = 53
i = 4377 (of 10192), d = 1.10594, its = 22
i = 4378 (of 10192), d = 10, its = 52
i = 4379 (of 10192), d = 10, its = 54
i = 4380 (of 10192), d = 10, its = 49
i = 4381 (of 10192), d = 10, its = 54
i = 4382 (of 10192), d = 3.49241, its = 8
i = 4383 (of 10192), d = 10, its = 54
i = 4384 (of 10192), d = 2.39836, its = 5
i = 4385 (of 10192), d = 10, its = 55
i = 4386 (of 10192), d = 1.96135, its = 3
i = 4387 (of 10192), d = 10, its = 41
i = 4388 (of 10192), d = 10, its = 55
i = 4389 (of 10192), d = 1.74931, its = 5
i = 4390 (of 10192), d = 2.34192, its = 5
i = 4391 (of 10192), d = 10, its = 53
i = 4392 (of 10192), d = 10, its = 40
i = 4393 (of 10192), d = 10, its = 37
i = 4394 (of 10192), d = 10, its = 52
i = 4395 (of 10192), d = 2.09101, its = 4
i = 4396 (of 10192), d = 4.61134, its = 8
i = 4397 (of 10192), d = 0.887299, its = 21
i = 4398 (of 10192), d = 10, its = 40
i = 4399 (of 10192), d = 0.756272, its = 26
i = 4400 (of 10192), d = 10, its = 55
i = 4401 (of 10192), d = 0.620357, its = 18
i = 4402 (of 10192), d = 3.25919, its = 7
i = 4403 (of 10192), d = 10, its = 53
i = 4404 (of 10192), d = 10, its = 50
i = 4405 (of 10192), d = 10, its = 50
i = 4406 (of 10192), d = 10, its = 54
i = 4407 (of 10192), d = 10, its = 39
i = 4408 (of 10192), d = 10, its = 55
i = 4409 (of 10192), d = 2.53411, its = 5
i = 4410 (of 10192), d = 7.17393, its = 8
i = 4411 (of 10192), d = 10, its = 43
i = 4412 (of 10192), d = 10, its = 53
i = 4413 (of 10192), d = 10, its = 55
i = 4414 (of 10192), d = 10, its = 37
i = 4415 (of 10192), d = 10, its = 37
i = 4416 (of 10192), d = 2.61294, its = 5
i = 4417 (of 10192), d = 10, its = 55
i = 4418 (of 10192), d = 10, its = 37
i = 4419 (of 10192), d = 10, its = 37
i = 4420 (of 10192), d = 10, its = 54
i = 4421 (of 10192), d = 2.07129, its = 4
i = 4422 (of 10192), d = 1.00486, its = 20
i = 4423 (of 10192), d = 10, its = 51
i = 4424 (of 10192), d = 10, its = 53
i = 4425 (of 10192), d = 10, its = 50
i = 4426 (of 10192), d = 10, its = 41
i = 4427 (of 10192), d = 10, its = 40
i = 4428 (of 10192), d = 10, its = 52
i = 4429 (of 10192), d = 10, its = 43
i = 4430 (of 10192), d = 10, its = 51
i = 4431 (of 10192), d = 10, its = 53
i = 4432 (of 10192), d = 10, its = 51
i = 4433 (of 10192), d = 1.93205, its = 4
i = 4434 (of 10192), d = 1.99495, its = 2
i = 4435 (of 10192), d = 10, its = 37
i = 4436 (of 10192), d = 10, its = 55
i = 4437 (of 10192), d = 10, its = 51
i = 4438 (of 10192), d = 10, its = 55
i = 4439 (of 10192), d = 10, its = 51
i = 4440 (of 10192), d = 0.624122, its = 18
i = 4441 (of 10192), d = 4.55817, its = 7
i = 4442 (of 10192), d = 10, its = 52
i = 4443 (of 10192), d = 10, its = 37
i = 4444 (of 10192), d = 2.41319, its = 5
i = 4445 (of 10192), d = 10, its = 37
i = 4446 (of 10192), d = 10, its = 54
i = 4447 (of 10192), d = 10, its = 55
i = 4448 (of 10192), d = 2.05059, its = 3
i = 4449 (of 10192), d = 10, its = 54
i = 4450 (of 10192), d = 10, its = 52
i = 4451 (of 10192), d = 10, its = 37
i = 4452 (of 10192), d = 10, its = 52
i = 4453 (of 10192), d = 10, its = 51
i = 4454 (of 10192), d = 2.50401, its = 5
i = 4455 (of 10192), d = 10, its = 37
i = 4456 (of 10192), d = 10, its = 37
i = 4457 (of 10192), d = 10, its = 54
i = 4458 (of 10192), d = 10, its = 53
i = 4459 (of 10192), d = 10, its = 38
i = 4460 (of 10192), d = 10, its = 52
i = 4461 (of 10192), d = 2.05985, its = 3
i = 4462 (of 10192), d = 10, its = 54
i = 4463 (of 10192), d = 10, its = 52
i = 4464 (of 10192), d = 10, its = 53
i = 4465 (of 10192), d = 10, its = 52
i = 4466 (of 10192), d = 10, its = 37
i = 4467 (of 10192), d = 10, its = 37
i = 4468 (of 10192), d = 3.09296, its = 6
i = 4469 (of 10192), d = 10, its = 50
i = 4470 (of 10192), d = 2.41976, its = 5
i = 4471 (of 10192), d = 10, its = 37
i = 4472 (of 10192), d = 10, its = 53
i = 4473 (of 10192), d = 10, its = 55
i = 4474 (of 10192), d = 10, its = 50
i = 4475 (of 10192), d = 10, its = 56
i = 4476 (of 10192), d = 10, its = 40
i = 4477 (of 10192), d = 1.85546, its = 4
i = 4478 (of 10192), d = 10, its = 56
i = 4479 (of 10192), d = 10, its = 49
i = 4480 (of 10192), d = 10, its = 52
i = 4481 (of 10192), d = 0.479436, its = 17
i = 4482 (of 10192), d = 10, its = 54
i = 4483 (of 10192), d = 10, its = 54
i = 4484 (of 10192), d = 10, its = 51
i = 4485 (of 10192), d = 10, its = 53
i = 4486 (of 10192), d = 0.865917, its = 19
i = 4487 (of 10192), d = 10, its = 50
i = 4488 (of 10192), d = 10, its = 54
i = 4489 (of 10192), d = 10, its = 52
i = 4490 (of 10192), d = 10, its = 54
i = 4491 (of 10192), d = 1.70936, its = 5
i = 4492 (of 10192), d = 10, its = 37
i = 4493 (of 10192), d = 0.68288, its = 17
i = 4494 (of 10192), d = 1.66439, its = 5
i = 4495 (of 10192), d = 2.33712, its = 5
i = 4496 (of 10192), d = 2.37531, its = 5
i = 4497 (of 10192), d = 10, its = 53
i = 4498 (of 10192), d = 1.87682, its = 4
i = 4499 (of 10192), d = 1.54198, its = 7
i = 4500 (of 10192), d = 10, its = 54
i = 4501 (of 10192), d = 10, its = 41
i = 4502 (of 10192), d = 2.41319, its = 5
i = 4503 (of 10192), d = 10, its = 52
i = 4504 (of 10192), d = 10, its = 51
i = 4505 (of 10192), d = 10, its = 37
i = 4506 (of 10192), d = 2.69421, its = 5
i = 4507 (of 10192), d = 10, its = 37
i = 4508 (of 10192), d = 10, its = 37
i = 4509 (of 10192), d = 10, its = 51
i = 4510 (of 10192), d = 10, its = 55
i = 4511 (of 10192), d = 1.0077, its = 19
i = 4512 (of 10192), d = 10, its = 55
i = 4513 (of 10192), d = 2.14372, its = 4
i = 4514 (of 10192), d = 1.93151, its = 4
i = 4515 (of 10192), d = 2.22404, its = 5
i = 4516 (of 10192), d = 1.56048, its = 7
i = 4517 (of 10192), d = 10, its = 53
i = 4518 (of 10192), d = 10, its = 57
i = 4519 (of 10192), d = 10, its = 21
i = 4520 (of 10192), d = 10, its = 51
i = 4521 (of 10192), d = 10, its = 56
i = 4522 (of 10192), d = 10, its = 53
i = 4523 (of 10192), d = 10, its = 55
i = 4524 (of 10192), d = 1.10953, its = 20
i = 4525 (of 10192), d = 10, its = 42
i = 4526 (of 10192), d = 10, its = 39
i = 4527 (of 10192), d = 10, its = 41
i = 4528 (of 10192), d = 10, its = 56
i = 4529 (of 10192), d = 10, its = 37
i = 4530 (of 10192), d = 10, its = 52
i = 4531 (of 10192), d = 10, its = 37
i = 4532 (of 10192), d = 10, its = 37
i = 4533 (of 10192), d = 10, its = 41
i = 4534 (of 10192), d = 4.42783, its = 7
i = 4535 (of 10192), d = 10, its = 50
i = 4536 (of 10192), d = 10, its = 56
i = 4537 (of 10192), d = 10, its = 55
i = 4538 (of 10192), d = 10, its = 55
i = 4539 (of 10192), d = 10, its = 57
i = 4540 (of 10192), d = 2.9783, its = 5
i = 4541 (of 10192), d = 1.38778, its = 6
i = 4542 (of 10192), d = 9.15254, its = 9
i = 4543 (of 10192), d = 1.08611, its = 19
i = 4544 (of 10192), d = 10, its = 37
i = 4545 (of 10192), d = 2.22114, its = 4
i = 4546 (of 10192), d = 10, its = 37
i = 4547 (of 10192), d = 10, its = 54
i = 4548 (of 10192), d = 1.2594, its = 7
i = 4549 (of 10192), d = 10, its = 37
i = 4550 (of 10192), d = 0.97019, its = 22
i = 4551 (of 10192), d = 10, its = 41
i = 4552 (of 10192), d = 10, its = 52
i = 4553 (of 10192), d = 10, its = 52
i = 4554 (of 10192), d = 10, its = 51
i = 4555 (of 10192), d = 1.56864, its = 6
i = 4556 (of 10192), d = 2.39588, its = 5
i = 4557 (of 10192), d = 2.62495, its = 5
i = 4558 (of 10192), d = 10, its = 51
i = 4559 (of 10192), d = 3.34311, its = 7
i = 4560 (of 10192), d = 10, its = 37
i = 4561 (of 10192), d = 10, its = 50
i = 4562 (of 10192), d = 10, its = 49
i = 4563 (of 10192), d = 10, its = 47
i = 4564 (of 10192), d = 10, its = 54
i = 4565 (of 10192), d = 2.45982, its = 5
i = 4566 (of 10192), d = 2.94799, its = 6
i = 4567 (of 10192), d = 10, its = 53
i = 4568 (of 10192), d = 1.42291, its = 6
i = 4569 (of 10192), d = 3.04363, its = 6
i = 4570 (of 10192), d = 10, its = 37
i = 4571 (of 10192), d = 10, its = 51
i = 4572 (of 10192), d = 10, its = 52
i = 4573 (of 10192), d = 10, its = 37
i = 4574 (of 10192), d = 0.743468, its = 17
i = 4575 (of 10192), d = 2.96545, its = 6
i = 4576 (of 10192), d = 2.87981, its = 6
i = 4577 (of 10192), d = 10, its = 55
i = 4578 (of 10192), d = 10, its = 52
i = 4579 (of 10192), d = 10, its = 37
i = 4580 (of 10192), d = 0.827251, its = 17
i = 4581 (of 10192), d = 10, its = 52
i = 4582 (of 10192), d = 2.7076, its = 5
i = 4583 (of 10192), d = 2.18587, its = 4
i = 4584 (of 10192), d = 10, its = 49
i = 4585 (of 10192), d = 1.9127, its = 4
i = 4586 (of 10192), d = 10, its = 51
i = 4587 (of 10192), d = 10, its = 39
i = 4588 (of 10192), d = 10, its = 50
i = 4589 (of 10192), d = 0.660902, its = 20
i = 4590 (of 10192), d = 10, its = 43
i = 4591 (of 10192), d = 10, its = 53
i = 4592 (of 10192), d = 0.611409, its = 22
i = 4593 (of 10192), d = 4.8767, its = 7
i = 4594 (of 10192), d = 2.04673, its = 3
i = 4595 (of 10192), d = 10, its = 55
i = 4596 (of 10192), d = 10, its = 56
i = 4597 (of 10192), d = 10, its = 37
i = 4598 (of 10192), d = 0.634587, its = 19
i = 4599 (of 10192), d = 8.70463, its = 9
i = 4600 (of 10192), d = 0.986229, its = 19
i = 4601 (of 10192), d = 10, its = 54
i = 4602 (of 10192), d = 10, its = 43
i = 4603 (of 10192), d = 10, its = 50
i = 4604 (of 10192), d = 10, its = 52
i = 4605 (of 10192), d = 10, its = 51
i = 4606 (of 10192), d = 10, its = 37
i = 4607 (of 10192), d = 0.591048, its = 25
i = 4608 (of 10192), d = 1.59357, its = 6
i = 4609 (of 10192), d = 10, its = 54
i = 4610 (of 10192), d = 10, its = 51
i = 4611 (of 10192), d = 10, its = 54
i = 4612 (of 10192), d = 10, its = 51
i = 4613 (of 10192), d = 3.5618, its = 6
i = 4614 (of 10192), d = 10, its = 55
i = 4615 (of 10192), d = 10, its = 48
i = 4616 (of 10192), d = 10, its = 42
i = 4617 (of 10192), d = 10, its = 49
i = 4618 (of 10192), d = 2.21523, its = 4
i = 4619 (of 10192), d = 10, its = 39
i = 4620 (of 10192), d = 1.68982, its = 5
i = 4621 (of 10192), d = 10, its = 50
i = 4622 (of 10192), d = 10, its = 51
i = 4623 (of 10192), d = 10, its = 55
i = 4624 (of 10192), d = 10, its = 21
i = 4625 (of 10192), d = 2.76901, its = 6
i = 4626 (of 10192), d = 2.49184, its = 5
i = 4627 (of 10192), d = 10, its = 55
i = 4628 (of 10192), d = 10, its = 39
i = 4629 (of 10192), d = 10, its = 51
i = 4630 (of 10192), d = 10, its = 48
i = 4631 (of 10192), d = 10, its = 52
i = 4632 (of 10192), d = 10, its = 55
i = 4633 (of 10192), d = 10, its = 57
i = 4634 (of 10192), d = 10, its = 53
i = 4635 (of 10192), d = 1.19321, its = 19
i = 4636 (of 10192), d = 10, its = 55
i = 4637 (of 10192), d = 10, its = 48
i = 4638 (of 10192), d = 10, its = 54
i = 4639 (of 10192), d = 10, its = 51
i = 4640 (of 10192), d = 1.91782, its = 5
i = 4641 (of 10192), d = 10, its = 53
i = 4642 (of 10192), d = 10, its = 55
i = 4643 (of 10192), d = 1.4747, its = 5
i = 4644 (of 10192), d = 10, its = 55
i = 4645 (of 10192), d = 1.34726, its = 6
i = 4646 (of 10192), d = 10, its = 39
i = 4647 (of 10192), d = 10, its = 40
i = 4648 (of 10192), d = 10, its = 56
i = 4649 (of 10192), d = 2.28281, its = 5
i = 4650 (of 10192), d = 10, its = 40
i = 4651 (of 10192), d = 10, its = 52
i = 4652 (of 10192), d = 10, its = 40
i = 4653 (of 10192), d = 10, its = 53
i = 4654 (of 10192), d = 10, its = 54
i = 4655 (of 10192), d = 1.93478, its = 4
i = 4656 (of 10192), d = 10, its = 39
i = 4657 (of 10192), d = 10, its = 52
i = 4658 (of 10192), d = 10, its = 55
i = 4659 (of 10192), d = 10, its = 49
i = 4660 (of 10192), d = 10, its = 53
i = 4661 (of 10192), d = 10, its = 47
i = 4662 (of 10192), d = 10, its = 37
i = 4663 (of 10192), d = 10, its = 49
i = 4664 (of 10192), d = 10, its = 52
i = 4665 (of 10192), d = 10, its = 37
i = 4666 (of 10192), d = 10, its = 57
i = 4667 (of 10192), d = 10, its = 52
i = 4668 (of 10192), d = 10, its = 40
i = 4669 (of 10192), d = 10, its = 49
i = 4670 (of 10192), d = 10, its = 54
i = 4671 (of 10192), d = 1.37377, its = 6
i = 4672 (of 10192), d = 10, its = 52
i = 4673 (of 10192), d = 10, its = 55
i = 4674 (of 10192), d = 1.62886, its = 6
i = 4675 (of 10192), d = 10, its = 53
i = 4676 (of 10192), d = 10, its = 51
i = 4677 (of 10192), d = 2.1716, its = 5
i = 4678 (of 10192), d = 1.48357, its = 7
i = 4679 (of 10192), d = 10, its = 53
i = 4680 (of 10192), d = 10, its = 37
i = 4681 (of 10192), d = 10, its = 55
i = 4682 (of 10192), d = 0.892535, its = 18
i = 4683 (of 10192), d = 10, its = 53
i = 4684 (of 10192), d = 10, its = 52
i = 4685 (of 10192), d = 2.50401, its = 5
i = 4686 (of 10192), d = 1.43094, its = 20
i = 4687 (of 10192), d = 10, its = 37
i = 4688 (of 10192), d = 10, its = 37
i = 4689 (of 10192), d = 10, its = 37
i = 4690 (of 10192), d = 10, its = 41
i = 4691 (of 10192), d = 10, its = 53
i = 4692 (of 10192), d = 10, its = 49
i = 4693 (of 10192), d = 10, its = 57
i = 4694 (of 10192), d = 1.8949, its = 4
i = 4695 (of 10192), d = 10, its = 40
i = 4696 (of 10192), d = 0.903903, its = 26
i = 4697 (of 10192), d = 10, its = 37
i = 4698 (of 10192), d = 1.62965, its = 7
i = 4699 (of 10192), d = 10, its = 42
i = 4700 (of 10192), d = 10, its = 37
i = 4701 (of 10192), d = 1.32204, its = 18
i = 4702 (of 10192), d = 10, its = 37
i = 4703 (of 10192), d = 0.47465, its = 21
i = 4704 (of 10192), d = 10, its = 52
i = 4705 (of 10192), d = 10, its = 54
i = 4706 (of 10192), d = 10, its = 53
i = 4707 (of 10192), d = 1.90768, its = 4
i = 4708 (of 10192), d = 0.845682, its = 18
i = 4709 (of 10192), d = 10, its = 47
i = 4710 (of 10192), d = 10, its = 39
i = 4711 (of 10192), d = 10, its = 37
i = 4712 (of 10192), d = 10, its = 37
i = 4713 (of 10192), d = 10, its = 53
i = 4714 (of 10192), d = 10, its = 37
i = 4715 (of 10192), d = 10, its = 37
i = 4716 (of 10192), d = 0.458982, its = 20
i = 4717 (of 10192), d = 10, its = 37
i = 4718 (of 10192), d = 10, its = 37
i = 4719 (of 10192), d = 10, its = 50
i = 4720 (of 10192), d = 10, its = 46
i = 4721 (of 10192), d = 6.03841, its = 7
i = 4722 (of 10192), d = 3.24395, its = 6
i = 4723 (of 10192), d = 10, its = 40
i = 4724 (of 10192), d = 10, its = 51
i = 4725 (of 10192), d = 10, its = 55
i = 4726 (of 10192), d = 10, its = 41
i = 4727 (of 10192), d = 2.4082, its = 5
i = 4728 (of 10192), d = 10, its = 57
i = 4729 (of 10192), d = 10, its = 40
i = 4730 (of 10192), d = 10, its = 51
i = 4731 (of 10192), d = 2.25718, its = 4
i = 4732 (of 10192), d = 0.841548, its = 19
i = 4733 (of 10192), d = 10, its = 51
i = 4734 (of 10192), d = 10, its = 42
i = 4735 (of 10192), d = 10, its = 37
i = 4736 (of 10192), d = 10, its = 49
i = 4737 (of 10192), d = 10, its = 40
i = 4738 (of 10192), d = 2.93867, its = 6
i = 4739 (of 10192), d = 10, its = 55
i = 4740 (of 10192), d = 10, its = 37
i = 4741 (of 10192), d = 10, its = 52
i = 4742 (of 10192), d = 10, its = 37
i = 4743 (of 10192), d = 10, its = 41
i = 4744 (of 10192), d = 10, its = 52
i = 4745 (of 10192), d = 10, its = 49
i = 4746 (of 10192), d = 10, its = 54
i = 4747 (of 10192), d = 10, its = 52
i = 4748 (of 10192), d = 10, its = 39
i = 4749 (of 10192), d = 2.46373, its = 6
i = 4750 (of 10192), d = 10, its = 37
i = 4751 (of 10192), d = 10, its = 40
i = 4752 (of 10192), d = 10, its = 52
i = 4753 (of 10192), d = 0.743729, its = 19
i = 4754 (of 10192), d = 0.539994, its = 16
i = 4755 (of 10192), d = 10, its = 50
i = 4756 (of 10192), d = 1.76423, its = 4
i = 4757 (of 10192), d = 10, its = 57
i = 4758 (of 10192), d = 10, its = 55
i = 4759 (of 10192), d = 1.56729, its = 6
i = 4760 (of 10192), d = 3.17965, its = 6
i = 4761 (of 10192), d = 10, its = 55
i = 4762 (of 10192), d = 0.868995, its = 19
i = 4763 (of 10192), d = 10, its = 37
i = 4764 (of 10192), d = 10, its = 54
i = 4765 (of 10192), d = 10, its = 37
i = 4766 (of 10192), d = 10, its = 37
i = 4767 (of 10192), d = 10, its = 58
i = 4768 (of 10192), d = 2.97324, its = 6
i = 4769 (of 10192), d = 0.932009, its = 19
i = 4770 (of 10192), d = 10, its = 57
i = 4771 (of 10192), d = 10, its = 52
i = 4772 (of 10192), d = 10, its = 53
i = 4773 (of 10192), d = 10, its = 39
i = 4774 (of 10192), d = 2.33629, its = 5
i = 4775 (of 10192), d = 0.913201, its = 24
i = 4776 (of 10192), d = 0.886473, its = 23
i = 4777 (of 10192), d = 10, its = 37
i = 4778 (of 10192), d = 0.691811, its = 19
i = 4779 (of 10192), d = 3.74404, its = 6
i = 4780 (of 10192), d = 10, its = 54
i = 4781 (of 10192), d = 3.28579, its = 6
i = 4782 (of 10192), d = 1.50675, its = 6
i = 4783 (of 10192), d = 10, its = 51
i = 4784 (of 10192), d = 10, its = 52
i = 4785 (of 10192), d = 10, its = 55
i = 4786 (of 10192), d = 10, its = 54
i = 4787 (of 10192), d = 10, its = 50
i = 4788 (of 10192), d = 10, its = 54
i = 4789 (of 10192), d = 1.41046, its = 6
i = 4790 (of 10192), d = 10, its = 53
i = 4791 (of 10192), d = 0.493118, its = 20
i = 4792 (of 10192), d = 2.27538, its = 5
i = 4793 (of 10192), d = 2.51657, its = 5
i = 4794 (of 10192), d = 10, its = 52
i = 4795 (of 10192), d = 2.97103, its = 7
i = 4796 (of 10192), d = 10, its = 52
i = 4797 (of 10192), d = 10, its = 52
i = 4798 (of 10192), d = 10, its = 39
i = 4799 (of 10192), d = 10, its = 52
i = 4800 (of 10192), d = 1.98028, its = 3
i = 4801 (of 10192), d = 10, its = 54
i = 4802 (of 10192), d = 10, its = 50
i = 4803 (of 10192), d = 3.77262, its = 7
i = 4804 (of 10192), d = 10, its = 37
i = 4805 (of 10192), d = 1.6103, its = 6
i = 4806 (of 10192), d = 10, its = 54
i = 4807 (of 10192), d = 10, its = 37
i = 4808 (of 10192), d = 10, its = 37
i = 4809 (of 10192), d = 6.64844, its = 18
i = 4810 (of 10192), d = 10, its = 37
i = 4811 (of 10192), d = 6.36711, its = 8
i = 4812 (of 10192), d = 1.75854, its = 5
i = 4813 (of 10192), d = 10, its = 46
i = 4814 (of 10192), d = 2.19409, its = 4
i = 4815 (of 10192), d = 10, its = 52
i = 4816 (of 10192), d = 10, its = 37
i = 4817 (of 10192), d = 10, its = 54
i = 4818 (of 10192), d = 3.0816, its = 7
i = 4819 (of 10192), d = 10, its = 44
i = 4820 (of 10192), d = 0.767924, its = 29
i = 4821 (of 10192), d = 0.868657, its = 17
i = 4822 (of 10192), d = 6.26545, its = 8
i = 4823 (of 10192), d = 10, its = 39
i = 4824 (of 10192), d = 10, its = 53
i = 4825 (of 10192), d = 10, its = 51
i = 4826 (of 10192), d = 10, its = 53
i = 4827 (of 10192), d = 10, its = 52
i = 4828 (of 10192), d = 1.75404, its = 5
i = 4829 (of 10192), d = 10, its = 37
i = 4830 (of 10192), d = 10, its = 54
i = 4831 (of 10192), d = 10, its = 50
i = 4832 (of 10192), d = 10, its = 37
i = 4833 (of 10192), d = 10, its = 48
i = 4834 (of 10192), d = 3.31074, its = 8
i = 4835 (of 10192), d = 10, its = 40
i = 4836 (of 10192), d = 10, its = 54
i = 4837 (of 10192), d = 1.63656, its = 6
i = 4838 (of 10192), d = 10, its = 41
i = 4839 (of 10192), d = 0.783068, its = 19
i = 4840 (of 10192), d = 10, its = 37
i = 4841 (of 10192), d = 1.9623, its = 3
i = 4842 (of 10192), d = 10, its = 37
i = 4843 (of 10192), d = 10, its = 50
i = 4844 (of 10192), d = 10, its = 54
i = 4845 (of 10192), d = 10, its = 37
i = 4846 (of 10192), d = 10, its = 56
i = 4847 (of 10192), d = 10, its = 40
i = 4848 (of 10192), d = 10, its = 52
i = 4849 (of 10192), d = 10, its = 54
i = 4850 (of 10192), d = 10, its = 48
i = 4851 (of 10192), d = 10, its = 53
i = 4852 (of 10192), d = 10, its = 43
i = 4853 (of 10192), d = 10, its = 39
i = 4854 (of 10192), d = 10, its = 55
i = 4855 (of 10192), d = 10, its = 54
i = 4856 (of 10192), d = 10, its = 48
i = 4857 (of 10192), d = 10, its = 54
i = 4858 (of 10192), d = 10, its = 39
i = 4859 (of 10192), d = 10, its = 51
i = 4860 (of 10192), d = 10, its = 59
i = 4861 (of 10192), d = 10, its = 37
i = 4862 (of 10192), d = 1.09146, its = 19
i = 4863 (of 10192), d = 10, its = 40
i = 4864 (of 10192), d = 10, its = 41
i = 4865 (of 10192), d = 10, its = 49
i = 4866 (of 10192), d = 0.394928, its = 18
i = 4867 (of 10192), d = 1.09094, its = 6
i = 4868 (of 10192), d = 1.76423, its = 4
i = 4869 (of 10192), d = 10, its = 39
i = 4870 (of 10192), d = 10, its = 50
i = 4871 (of 10192), d = 10, its = 55
i = 4872 (of 10192), d = 10, its = 51
i = 4873 (of 10192), d = 10, its = 37
i = 4874 (of 10192), d = 10, its = 53
i = 4875 (of 10192), d = 10, its = 37
i = 4876 (of 10192), d = 10, its = 50
i = 4877 (of 10192), d = 10, its = 55
i = 4878 (of 10192), d = 10, its = 49
i = 4879 (of 10192), d = 10, its = 51
i = 4880 (of 10192), d = 10, its = 39
i = 4881 (of 10192), d = 10, its = 52
i = 4882 (of 10192), d = 10, its = 50
i = 4883 (of 10192), d = 10, its = 51
i = 4884 (of 10192), d = 1.89314, its = 4
i = 4885 (of 10192), d = 10, its = 50
i = 4886 (of 10192), d = 10, its = 50
i = 4887 (of 10192), d = 1.59628, its = 6
i = 4888 (of 10192), d = 0.721036, its = 16
i = 4889 (of 10192), d = 10, its = 37
i = 4890 (of 10192), d = 10, its = 40
i = 4891 (of 10192), d = 1.88674, its = 4
i = 4892 (of 10192), d = 10, its = 37
i = 4893 (of 10192), d = 0.551581, its = 17
i = 4894 (of 10192), d = 0.681586, its = 17
i = 4895 (of 10192), d = 10, its = 51
i = 4896 (of 10192), d = 10, its = 56
i = 4897 (of 10192), d = 10, its = 54
i = 4898 (of 10192), d = 10, its = 39
i = 4899 (of 10192), d = 10, its = 53
i = 4900 (of 10192), d = 10, its = 56
i = 4901 (of 10192), d = 10, its = 53
i = 4902 (of 10192), d = 10, its = 53
i = 4903 (of 10192), d = 2.28423, its = 4
i = 4904 (of 10192), d = 10, its = 37
i = 4905 (of 10192), d = 10, its = 55
i = 4906 (of 10192), d = 1.52849, its = 6
i = 4907 (of 10192), d = 1.96024, its = 3
i = 4908 (of 10192), d = 10, its = 52
i = 4909 (of 10192), d = 10, its = 51
i = 4910 (of 10192), d = 10, its = 52
i = 4911 (of 10192), d = 2.62458, its = 6
i = 4912 (of 10192), d = 10, its = 49
i = 4913 (of 10192), d = 2.63063, its = 5
i = 4914 (of 10192), d = 1.90885, its = 4
i = 4915 (of 10192), d = 10, its = 57
i = 4916 (of 10192), d = 10, its = 51
i = 4917 (of 10192), d = 10, its = 52
i = 4918 (of 10192), d = 10, its = 55
i = 4919 (of 10192), d = 10, its = 43
i = 4920 (of 10192), d = 10, its = 44
i = 4921 (of 10192), d = 10, its = 48
i = 4922 (of 10192), d = 2.56735, its = 5
i = 4923 (of 10192), d = 10, its = 49
i = 4924 (of 10192), d = 1.47303, its = 7
i = 4925 (of 10192), d = 10, its = 54
i = 4926 (of 10192), d = 7.54388, its = 8
i = 4927 (of 10192), d = 10, its = 49
i = 4928 (of 10192), d = 10, its = 55
i = 4929 (of 10192), d = 10, its = 58
i = 4930 (of 10192), d = 1.80952, its = 5
i = 4931 (of 10192), d = 10, its = 53
i = 4932 (of 10192), d = 10, its = 50
i = 4933 (of 10192), d = 10, its = 51
i = 4934 (of 10192), d = 1.86195, its = 4
i = 4935 (of 10192), d = 0.892535, its = 18
i = 4936 (of 10192), d = 2.66081, its = 5
i = 4937 (of 10192), d = 10, its = 55
i = 4938 (of 10192), d = 10, its = 53
i = 4939 (of 10192), d = 1.29525, its = 8
i = 4940 (of 10192), d = 10, its = 50
i = 4941 (of 10192), d = 10, its = 52
i = 4942 (of 10192), d = 10, its = 54
i = 4943 (of 10192), d = 10, its = 53
i = 4944 (of 10192), d = 10, its = 37
i = 4945 (of 10192), d = 10, its = 55
i = 4946 (of 10192), d = 10, its = 50
i = 4947 (of 10192), d = 10, its = 50
i = 4948 (of 10192), d = 10, its = 37
i = 4949 (of 10192), d = 1.07502, its = 21
i = 4950 (of 10192), d = 10, its = 37
i = 4951 (of 10192), d = 10, its = 51
i = 4952 (of 10192), d = 1.97772, its = 3
i = 4953 (of 10192), d = 10, its = 49
i = 4954 (of 10192), d = 10, its = 37
i = 4955 (of 10192), d = 10, its = 39
i = 4956 (of 10192), d = 10, its = 37
i = 4957 (of 10192), d = 10, its = 40
i = 4958 (of 10192), d = 10, its = 40
i = 4959 (of 10192), d = 10, its = 54
i = 4960 (of 10192), d = 0.903697, its = 20
i = 4961 (of 10192), d = 10, its = 51
i = 4962 (of 10192), d = 10, its = 50
i = 4963 (of 10192), d = 10, its = 39
i = 4964 (of 10192), d = 1.34271, its = 6
i = 4965 (of 10192), d = 1.50655, its = 6
i = 4966 (of 10192), d = 10, its = 50
i = 4967 (of 10192), d = 10, its = 39
i = 4968 (of 10192), d = 10, its = 37
i = 4969 (of 10192), d = 10, its = 41
i = 4970 (of 10192), d = 10, its = 50
i = 4971 (of 10192), d = 10, its = 37
i = 4972 (of 10192), d = 2.46297, its = 5
i = 4973 (of 10192), d = 10, its = 40
i = 4974 (of 10192), d = 10, its = 52
i = 4975 (of 10192), d = 10, its = 49
i = 4976 (of 10192), d = 10, its = 53
i = 4977 (of 10192), d = 10, its = 53
i = 4978 (of 10192), d = 0.542639, its = 21
i = 4979 (of 10192), d = 10, its = 40
i = 4980 (of 10192), d = 10, its = 37
i = 4981 (of 10192), d = 10, its = 52
i = 4982 (of 10192), d = 1.41015, its = 6
i = 4983 (of 10192), d = 10, its = 55
i = 4984 (of 10192), d = 1.90525, its = 4
i = 4985 (of 10192), d = 10, its = 51
i = 4986 (of 10192), d = 10, its = 53
i = 4987 (of 10192), d = 0.411992, its = 20
i = 4988 (of 10192), d = 10, its = 51
i = 4989 (of 10192), d = 10, its = 42
i = 4990 (of 10192), d = 10, its = 55
i = 4991 (of 10192), d = 10, its = 37
i = 4992 (of 10192), d = 1.07272, its = 19
i = 4993 (of 10192), d = 10, its = 51
i = 4994 (of 10192), d = 10, its = 39
i = 4995 (of 10192), d = 10, its = 55
i = 4996 (of 10192), d = 3.12766, its = 6
i = 4997 (of 10192), d = 10, its = 49
i = 4998 (of 10192), d = 10, its = 53
i = 4999 (of 10192), d = 10, its = 50
i = 5000 (of 10192), d = 10, its = 54
i = 5001 (of 10192), d = 5.40282, its = 8
i = 5002 (of 10192), d = 10, its = 52
i = 5003 (of 10192), d = 10, its = 56
i = 5004 (of 10192), d = 10, its = 54
i = 5005 (of 10192), d = 10, its = 37
i = 5006 (of 10192), d = 6.58237, its = 7
i = 5007 (of 10192), d = 10, its = 50
i = 5008 (of 10192), d = 1.0097, its = 20
i = 5009 (of 10192), d = 10, its = 56
i = 5010 (of 10192), d = 10, its = 52
i = 5011 (of 10192), d = 10, its = 37
i = 5012 (of 10192), d = 0.912815, its = 26
i = 5013 (of 10192), d = 10, its = 54
i = 5014 (of 10192), d = 10, its = 56
i = 5015 (of 10192), d = 3.80876, its = 8
i = 5016 (of 10192), d = 10, its = 56
i = 5017 (of 10192), d = 10, its = 39
i = 5018 (of 10192), d = 10, its = 49
i = 5019 (of 10192), d = 10, its = 37
i = 5020 (of 10192), d = 1.28874, its = 6
i = 5021 (of 10192), d = 10, its = 55
i = 5022 (of 10192), d = 10, its = 37
i = 5023 (of 10192), d = 10, its = 40
i = 5024 (of 10192), d = 10, its = 52
i = 5025 (of 10192), d = 10, its = 37
i = 5026 (of 10192), d = 10, its = 39
i = 5027 (of 10192), d = 2.85656, its = 5
i = 5028 (of 10192), d = 2.24215, its = 5
i = 5029 (of 10192), d = 10, its = 53
i = 5030 (of 10192), d = 10, its = 37
i = 5031 (of 10192), d = 10, its = 53
i = 5032 (of 10192), d = 10, its = 39
i = 5033 (of 10192), d = 10, its = 42
i = 5034 (of 10192), d = 10, its = 56
i = 5035 (of 10192), d = 1.82349, its = 4
i = 5036 (of 10192), d = 10, its = 52
i = 5037 (of 10192), d = 10, its = 51
i = 5038 (of 10192), d = 10, its = 51
i = 5039 (of 10192), d = 3.02502, its = 7
i = 5040 (of 10192), d = 10, its = 53
i = 5041 (of 10192), d = 10, its = 53
i = 5042 (of 10192), d = 10, its = 37
i = 5043 (of 10192), d = 10, its = 50
i = 5044 (of 10192), d = 10, its = 37
i = 5045 (of 10192), d = 10, its = 50
i = 5046 (of 10192), d = 10, its = 53
i = 5047 (of 10192), d = 10, its = 51
i = 5048 (of 10192), d = 2.27071, its = 4
i = 5049 (of 10192), d = 10, its = 53
i = 5050 (of 10192), d = 10, its = 54
i = 5051 (of 10192), d = 10, its = 54
i = 5052 (of 10192), d = 10, its = 56
i = 5053 (of 10192), d = 10, its = 42
i = 5054 (of 10192), d = 10, its = 52
i = 5055 (of 10192), d = 1.07802, its = 29
i = 5056 (of 10192), d = 10, its = 37
i = 5057 (of 10192), d = 10, its = 37
i = 5058 (of 10192), d = 10, its = 37
i = 5059 (of 10192), d = 10, its = 55
i = 5060 (of 10192), d = 2.96335, its = 6
i = 5061 (of 10192), d = 10, its = 53
i = 5062 (of 10192), d = 5.93259, its = 8
i = 5063 (of 10192), d = 10, its = 37
i = 5064 (of 10192), d = 1.28961, its = 8
i = 5065 (of 10192), d = 2.32672, its = 5
i = 5066 (of 10192), d = 1.94254, its = 3
i = 5067 (of 10192), d = 10, its = 54
i = 5068 (of 10192), d = 1.01531, its = 17
i = 5069 (of 10192), d = 2.96545, its = 6
i = 5070 (of 10192), d = 10, its = 53
i = 5071 (of 10192), d = 1.59629, its = 6
i = 5072 (of 10192), d = 1.38708, its = 7
i = 5073 (of 10192), d = 10, its = 51
i = 5074 (of 10192), d = 10, its = 50
i = 5075 (of 10192), d = 10, its = 52
i = 5076 (of 10192), d = 10, its = 52
i = 5077 (of 10192), d = 2.52641, its = 5
i = 5078 (of 10192), d = 10, its = 54
i = 5079 (of 10192), d = 10, its = 51
i = 5080 (of 10192), d = 10, its = 55
i = 5081 (of 10192), d = 10, its = 53
i = 5082 (of 10192), d = 4.89445, its = 10
i = 5083 (of 10192), d = 10, its = 51
i = 5084 (of 10192), d = 10, its = 52
i = 5085 (of 10192), d = 10, its = 37
i = 5086 (of 10192), d = 10, its = 47
i = 5087 (of 10192), d = 10, its = 51
i = 5088 (of 10192), d = 10, its = 56
i = 5089 (of 10192), d = 2.78547, its = 6
i = 5090 (of 10192), d = 1.65451, its = 6
i = 5091 (of 10192), d = 10, its = 40
i = 5092 (of 10192), d = 0.66271, its = 18
i = 5093 (of 10192), d = 0.456982, its = 17
i = 5094 (of 10192), d = 1.281, its = 6
i = 5095 (of 10192), d = 0.981284, its = 21
i = 5096 (of 10192), d = 10, its = 37
i = 5097 (of 10192), d = 2.35522, its = 5
i = 5098 (of 10192), d = 10, its = 50
i = 5099 (of 10192), d = 10, its = 37
i = 5100 (of 10192), d = 10, its = 50
i = 5101 (of 10192), d = 0.692327, its = 18
i = 5102 (of 10192), d = 10, its = 54
i = 5103 (of 10192), d = 10, its = 51
i = 5104 (of 10192), d = 10, its = 53
i = 5105 (of 10192), d = 10, its = 54
i = 5106 (of 10192), d = 10, its = 51
i = 5107 (of 10192), d = 10, its = 41
i = 5108 (of 10192), d = 10, its = 53
i = 5109 (of 10192), d = 10, its = 43
i = 5110 (of 10192), d = 10, its = 52
i = 5111 (of 10192), d = 1.39745, its = 6
i = 5112 (of 10192), d = 10, its = 54
i = 5113 (of 10192), d = 1.20667, its = 18
i = 5114 (of 10192), d = 10, its = 53
i = 5115 (of 10192), d = 10, its = 50
i = 5116 (of 10192), d = 0.611356, its = 18
i = 5117 (of 10192), d = 10, its = 37
i = 5118 (of 10192), d = 0.82059, its = 17
i = 5119 (of 10192), d = 10, its = 56
i = 5120 (of 10192), d = 10, its = 52
i = 5121 (of 10192), d = 10, its = 39
i = 5122 (of 10192), d = 0.641897, its = 18
i = 5123 (of 10192), d = 10, its = 54
i = 5124 (of 10192), d = 10, its = 51
i = 5125 (of 10192), d = 10, its = 39
i = 5126 (of 10192), d = 1.39222, its = 6
i = 5127 (of 10192), d = 2.68256, its = 5
i = 5128 (of 10192), d = 10, its = 37
i = 5129 (of 10192), d = 10, its = 52
i = 5130 (of 10192), d = 10, its = 51
i = 5131 (of 10192), d = 10, its = 51
i = 5132 (of 10192), d = 2.80078, its = 5
i = 5133 (of 10192), d = 10, its = 56
i = 5134 (of 10192), d = 2.48891, its = 5
i = 5135 (of 10192), d = 10, its = 37
i = 5136 (of 10192), d = 1.01117, its = 18
i = 5137 (of 10192), d = 10, its = 37
i = 5138 (of 10192), d = 10, its = 37
i = 5139 (of 10192), d = 10, its = 50
i = 5140 (of 10192), d = 10, its = 51
i = 5141 (of 10192), d = 10, its = 55
i = 5142 (of 10192), d = 10, its = 51
i = 5143 (of 10192), d = 2.26819, its = 5
i = 5144 (of 10192), d = 10, its = 52
i = 5145 (of 10192), d = 10, its = 50
i = 5146 (of 10192), d = 2.15838, its = 4
i = 5147 (of 10192), d = 10, its = 37
i = 5148 (of 10192), d = 10, its = 52
i = 5149 (of 10192), d = 2.42748, its = 5
i = 5150 (of 10192), d = 10, its = 37
i = 5151 (of 10192), d = 10, its = 37
i = 5152 (of 10192), d = 10, its = 57
i = 5153 (of 10192), d = 0.84683, its = 26
i = 5154 (of 10192), d = 10, its = 51
i = 5155 (of 10192), d = 10, its = 56
i = 5156 (of 10192), d = 10, its = 56
i = 5157 (of 10192), d = 10, its = 51
i = 5158 (of 10192), d = 10, its = 51
i = 5159 (of 10192), d = 10, its = 53
i = 5160 (of 10192), d = 10, its = 43
i = 5161 (of 10192), d = 10, its = 54
i = 5162 (of 10192), d = 0.921392, its = 20
i = 5163 (of 10192), d = 10, its = 55
i = 5164 (of 10192), d = 10, its = 37
i = 5165 (of 10192), d = 10, its = 41
i = 5166 (of 10192), d = 10, its = 55
i = 5167 (of 10192), d = 10, its = 53
i = 5168 (of 10192), d = 0.518473, its = 22
i = 5169 (of 10192), d = 10, its = 50
i = 5170 (of 10192), d = 1.93309, its = 4
i = 5171 (of 10192), d = 10, its = 54
i = 5172 (of 10192), d = 10, its = 49
i = 5173 (of 10192), d = 10, its = 53
i = 5174 (of 10192), d = 10, its = 55
i = 5175 (of 10192), d = 10, its = 37
i = 5176 (of 10192), d = 10, its = 50
i = 5177 (of 10192), d = 10, its = 56
i = 5178 (of 10192), d = 10, its = 52
i = 5179 (of 10192), d = 10, its = 51
i = 5180 (of 10192), d = 10, its = 52
i = 5181 (of 10192), d = 3.38313, its = 6
i = 5182 (of 10192), d = 2.12971, its = 4
i = 5183 (of 10192), d = 3.04784, its = 6
i = 5184 (of 10192), d = 10, its = 54
i = 5185 (of 10192), d = 10, its = 52
i = 5186 (of 10192), d = 10, its = 39
i = 5187 (of 10192), d = 10, its = 51
i = 5188 (of 10192), d = 10, its = 40
i = 5189 (of 10192), d = 10, its = 52
i = 5190 (of 10192), d = 1.33482, its = 8
i = 5191 (of 10192), d = 10, its = 51
i = 5192 (of 10192), d = 2.37289, its = 5
i = 5193 (of 10192), d = 10, its = 54
i = 5194 (of 10192), d = 0.58191, its = 21
i = 5195 (of 10192), d = 0.538908, its = 16
i = 5196 (of 10192), d = 5.62034, its = 8
i = 5197 (of 10192), d = 10, its = 21
i = 5198 (of 10192), d = 10, its = 52
i = 5199 (of 10192), d = 0.450854, its = 17
i = 5200 (of 10192), d = 10, its = 52
i = 5201 (of 10192), d = 0.703645, its = 26
i = 5202 (of 10192), d = 10, its = 49
i = 5203 (of 10192), d = 10, its = 53
i = 5204 (of 10192), d = 10, its = 51
i = 5205 (of 10192), d = 10, its = 53
i = 5206 (of 10192), d = 10, its = 40
i = 5207 (of 10192), d = 10, its = 48
i = 5208 (of 10192), d = 10, its = 54
i = 5209 (of 10192), d = 10, its = 41
i = 5210 (of 10192), d = 10, its = 50
i = 5211 (of 10192), d = 10, its = 52
i = 5212 (of 10192), d = 10, its = 53
i = 5213 (of 10192), d = 10, its = 55
i = 5214 (of 10192), d = 10, its = 52
i = 5215 (of 10192), d = 10, its = 51
i = 5216 (of 10192), d = 0.912815, its = 26
i = 5217 (of 10192), d = 1.71509, its = 6
i = 5218 (of 10192), d = 0.725537, its = 19
i = 5219 (of 10192), d = 10, its = 54
i = 5220 (of 10192), d = 10, its = 41
i = 5221 (of 10192), d = 4.99521, its = 7
i = 5222 (of 10192), d = 10, its = 51
i = 5223 (of 10192), d = 10, its = 40
i = 5224 (of 10192), d = 0.596439, its = 19
i = 5225 (of 10192), d = 10, its = 40
i = 5226 (of 10192), d = 10, its = 52
i = 5227 (of 10192), d = 10, its = 54
i = 5228 (of 10192), d = 10, its = 54
i = 5229 (of 10192), d = 3.92768, its = 7
i = 5230 (of 10192), d = 10, its = 51
i = 5231 (of 10192), d = 3.24483, its = 8
i = 5232 (of 10192), d = 10, its = 37
i = 5233 (of 10192), d = 10, its = 50
i = 5234 (of 10192), d = 10, its = 53
i = 5235 (of 10192), d = 10, its = 53
i = 5236 (of 10192), d = 10, its = 41
i = 5237 (of 10192), d = 4.24133, its = 9
i = 5238 (of 10192), d = 10, its = 53
i = 5239 (of 10192), d = 10, its = 51
i = 5240 (of 10192), d = 10, its = 57
i = 5241 (of 10192), d = 2.48891, its = 5
i = 5242 (of 10192), d = 10, its = 52
i = 5243 (of 10192), d = 10, its = 40
i = 5244 (of 10192), d = 10, its = 51
i = 5245 (of 10192), d = 10, its = 53
i = 5246 (of 10192), d = 10, its = 54
i = 5247 (of 10192), d = 10, its = 56
i = 5248 (of 10192), d = 10, its = 49
i = 5249 (of 10192), d = 2.99406, its = 6
i = 5250 (of 10192), d = 10, its = 37
i = 5251 (of 10192), d = 10, its = 51
i = 5252 (of 10192), d = 10, its = 52
i = 5253 (of 10192), d = 10, its = 42
i = 5254 (of 10192), d = 0.297724, its = 18
i = 5255 (of 10192), d = 10, its = 37
i = 5256 (of 10192), d = 10, its = 37
i = 5257 (of 10192), d = 10, its = 55
i = 5258 (of 10192), d = 2.20127, its = 4
i = 5259 (of 10192), d = 10, its = 37
i = 5260 (of 10192), d = 10, its = 47
i = 5261 (of 10192), d = 10, its = 51
i = 5262 (of 10192), d = 10, its = 47
i = 5263 (of 10192), d = 10, its = 52
i = 5264 (of 10192), d = 10, its = 37
i = 5265 (of 10192), d = 10, its = 54
i = 5266 (of 10192), d = 10, its = 55
i = 5267 (of 10192), d = 10, its = 53
i = 5268 (of 10192), d = 2.0463, its = 3
i = 5269 (of 10192), d = 10, its = 57
i = 5270 (of 10192), d = 10, its = 49
i = 5271 (of 10192), d = 10, its = 37
i = 5272 (of 10192), d = 10, its = 53
i = 5273 (of 10192), d = 10, its = 49
i = 5274 (of 10192), d = 10, its = 51
i = 5275 (of 10192), d = 10, its = 39
i = 5276 (of 10192), d = 7.52407, its = 8
i = 5277 (of 10192), d = 10, its = 52
i = 5278 (of 10192), d = 10, its = 50
i = 5279 (of 10192), d = 10, its = 51
i = 5280 (of 10192), d = 10, its = 37
i = 5281 (of 10192), d = 10, its = 40
i = 5282 (of 10192), d = 1.48104, its = 6
i = 5283 (of 10192), d = 10, its = 54
i = 5284 (of 10192), d = 10, its = 37
i = 5285 (of 10192), d = 10, its = 59
i = 5286 (of 10192), d = 3.405, its = 7
i = 5287 (of 10192), d = 10, its = 55
i = 5288 (of 10192), d = 10, its = 56
i = 5289 (of 10192), d = 10, its = 57
i = 5290 (of 10192), d = 10, its = 53
i = 5291 (of 10192), d = 1.77786, its = 5
i = 5292 (of 10192), d = 10, its = 53
i = 5293 (of 10192), d = 0.699068, its = 18
i = 5294 (of 10192), d = 10, its = 53
i = 5295 (of 10192), d = 10, its = 51
i = 5296 (of 10192), d = 2.6127, its = 5
i = 5297 (of 10192), d = 5.88318, its = 8
i = 5298 (of 10192), d = 10, its = 52
i = 5299 (of 10192), d = 10, its = 56
i = 5300 (of 10192), d = 1.02318, its = 17
i = 5301 (of 10192), d = 10, its = 53
i = 5302 (of 10192), d = 10, its = 37
i = 5303 (of 10192), d = 10, its = 54
i = 5304 (of 10192), d = 10, its = 52
i = 5305 (of 10192), d = 10, its = 57
i = 5306 (of 10192), d = 10, its = 37
i = 5307 (of 10192), d = 1.9576, its = 3
i = 5308 (of 10192), d = 10, its = 52
i = 5309 (of 10192), d = 10, its = 41
i = 5310 (of 10192), d = 2.28813, its = 4
i = 5311 (of 10192), d = 10, its = 54
i = 5312 (of 10192), d = 10, its = 55
i = 5313 (of 10192), d = 0.672685, its = 18
i = 5314 (of 10192), d = 10, its = 55
i = 5315 (of 10192), d = 10, its = 53
i = 5316 (of 10192), d = 1.9065, its = 4
i = 5317 (of 10192), d = 10, its = 52
i = 5318 (of 10192), d = 10, its = 52
i = 5319 (of 10192), d = 10, its = 47
i = 5320 (of 10192), d = 10, its = 52
i = 5321 (of 10192), d = 10, its = 37
i = 5322 (of 10192), d = 10, its = 52
i = 5323 (of 10192), d = 3.17965, its = 6
i = 5324 (of 10192), d = 10, its = 50
i = 5325 (of 10192), d = 10, its = 51
i = 5326 (of 10192), d = 10, its = 40
i = 5327 (of 10192), d = 10, its = 44
i = 5328 (of 10192), d = 5.24788, its = 8
i = 5329 (of 10192), d = 1.60182, its = 6
i = 5330 (of 10192), d = 10, its = 41
i = 5331 (of 10192), d = 0.357279, its = 18
i = 5332 (of 10192), d = 1.80562, its = 5
i = 5333 (of 10192), d = 0.843094, its = 25
i = 5334 (of 10192), d = 10, its = 50
i = 5335 (of 10192), d = 10, its = 52
i = 5336 (of 10192), d = 10, its = 51
i = 5337 (of 10192), d = 0.698271, its = 18
i = 5338 (of 10192), d = 10, its = 49
i = 5339 (of 10192), d = 10, its = 53
i = 5340 (of 10192), d = 2.71423, its = 6
i = 5341 (of 10192), d = 10, its = 50
i = 5342 (of 10192), d = 2.06518, its = 3
i = 5343 (of 10192), d = 2.34295, its = 5
i = 5344 (of 10192), d = 10, its = 41
i = 5345 (of 10192), d = 10, its = 37
i = 5346 (of 10192), d = 10, its = 55
i = 5347 (of 10192), d = 3.0863, its = 5
i = 5348 (of 10192), d = 10, its = 55
i = 5349 (of 10192), d = 10, its = 37
i = 5350 (of 10192), d = 10, its = 56
i = 5351 (of 10192), d = 10, its = 37
i = 5352 (of 10192), d = 2.25718, its = 4
i = 5353 (of 10192), d = 10, its = 37
i = 5354 (of 10192), d = 10, its = 52
i = 5355 (of 10192), d = 10, its = 40
i = 5356 (of 10192), d = 10, its = 56
i = 5357 (of 10192), d = 10, its = 51
i = 5358 (of 10192), d = 1.60544, its = 5
i = 5359 (of 10192), d = 10, its = 40
i = 5360 (of 10192), d = 10, its = 54
i = 5361 (of 10192), d = 10, its = 48
i = 5362 (of 10192), d = 4.18617, its = 7
i = 5363 (of 10192), d = 10, its = 54
i = 5364 (of 10192), d = 10, its = 51
i = 5365 (of 10192), d = 1.96759, its = 3
i = 5366 (of 10192), d = 3.46648, its = 6
i = 5367 (of 10192), d = 10, its = 37
i = 5368 (of 10192), d = 10, its = 53
i = 5369 (of 10192), d = 1.83769, its = 4
i = 5370 (of 10192), d = 10, its = 37
i = 5371 (of 10192), d = 10, its = 51
i = 5372 (of 10192), d = 10, its = 54
i = 5373 (of 10192), d = 3.68063, its = 7
i = 5374 (of 10192), d = 0.574118, its = 19
i = 5375 (of 10192), d = 10, its = 53
i = 5376 (of 10192), d = 10, its = 52
i = 5377 (of 10192), d = 1.24548, its = 6
i = 5378 (of 10192), d = 10, its = 52
i = 5379 (of 10192), d = 10, its = 49
i = 5380 (of 10192), d = 10, its = 48
i = 5381 (of 10192), d = 10, its = 54
i = 5382 (of 10192), d = 10, its = 52
i = 5383 (of 10192), d = 1.66484, its = 6
i = 5384 (of 10192), d = 1.81155, its = 5
i = 5385 (of 10192), d = 10, its = 41
i = 5386 (of 10192), d = 2.36561, its = 4
i = 5387 (of 10192), d = 10, its = 51
i = 5388 (of 10192), d = 10, its = 40
i = 5389 (of 10192), d = 10, its = 41
i = 5390 (of 10192), d = 2.48138, its = 5
i = 5391 (of 10192), d = 1.46151, its = 7
i = 5392 (of 10192), d = 10, its = 49
i = 5393 (of 10192), d = 1.26261, its = 6
i = 5394 (of 10192), d = 10, its = 53
i = 5395 (of 10192), d = 10, its = 51
i = 5396 (of 10192), d = 10, its = 50
i = 5397 (of 10192), d = 3.46568, its = 6
i = 5398 (of 10192), d = 2.30749, its = 5
i = 5399 (of 10192), d = 0.54201, its = 16
i = 5400 (of 10192), d = 10, its = 56
i = 5401 (of 10192), d = 10, its = 37
i = 5402 (of 10192), d = 0.582343, its = 17
i = 5403 (of 10192), d = 10, its = 38
i = 5404 (of 10192), d = 0.641922, its = 18
i = 5405 (of 10192), d = 10, its = 56
i = 5406 (of 10192), d = 10, its = 40
i = 5407 (of 10192), d = 10, its = 41
i = 5408 (of 10192), d = 10, its = 41
i = 5409 (of 10192), d = 2.62641, its = 5
i = 5410 (of 10192), d = 10, its = 40
i = 5411 (of 10192), d = 10, its = 53
i = 5412 (of 10192), d = 10, its = 37
i = 5413 (of 10192), d = 10, its = 40
i = 5414 (of 10192), d = 10, its = 52
i = 5415 (of 10192), d = 10, its = 39
i = 5416 (of 10192), d = 10, its = 37
i = 5417 (of 10192), d = 10, its = 40
i = 5418 (of 10192), d = 10, its = 52
i = 5419 (of 10192), d = 10, its = 54
i = 5420 (of 10192), d = 10, its = 56
i = 5421 (of 10192), d = 10, its = 54
i = 5422 (of 10192), d = 10, its = 51
i = 5423 (of 10192), d = 10, its = 53
i = 5424 (of 10192), d = 10, its = 54
i = 5425 (of 10192), d = 10, its = 40
i = 5426 (of 10192), d = 10, its = 37
i = 5427 (of 10192), d = 10, its = 53
i = 5428 (of 10192), d = 1.9159, its = 4
i = 5429 (of 10192), d = 10, its = 56
i = 5430 (of 10192), d = 10, its = 55
i = 5431 (of 10192), d = 10, its = 52
i = 5432 (of 10192), d = 10, its = 44
i = 5433 (of 10192), d = 10, its = 37
i = 5434 (of 10192), d = 10, its = 43
i = 5435 (of 10192), d = 10, its = 51
i = 5436 (of 10192), d = 10, its = 53
i = 5437 (of 10192), d = 2.57263, its = 6
i = 5438 (of 10192), d = 10, its = 49
i = 5439 (of 10192), d = 10, its = 51
i = 5440 (of 10192), d = 6.17854, its = 9
i = 5441 (of 10192), d = 10, its = 37
i = 5442 (of 10192), d = 10, its = 54
i = 5443 (of 10192), d = 10, its = 40
i = 5444 (of 10192), d = 10, its = 52
i = 5445 (of 10192), d = 2.17641, its = 5
i = 5446 (of 10192), d = 0.800732, its = 16
i = 5447 (of 10192), d = 10, its = 52
i = 5448 (of 10192), d = 10, its = 37
i = 5449 (of 10192), d = 10, its = 49
i = 5450 (of 10192), d = 10, its = 54
i = 5451 (of 10192), d = 10, its = 56
i = 5452 (of 10192), d = 10, its = 51
i = 5453 (of 10192), d = 10, its = 51
i = 5454 (of 10192), d = 10, its = 55
i = 5455 (of 10192), d = 10, its = 53
i = 5456 (of 10192), d = 10, its = 40
i = 5457 (of 10192), d = 10, its = 37
i = 5458 (of 10192), d = 10, its = 52
i = 5459 (of 10192), d = 2.23456, its = 4
i = 5460 (of 10192), d = 0.771249, its = 20
i = 5461 (of 10192), d = 0.515135, its = 29
i = 5462 (of 10192), d = 1.60942, its = 6
i = 5463 (of 10192), d = 10, its = 54
i = 5464 (of 10192), d = 10, its = 58
i = 5465 (of 10192), d = 10, its = 51
i = 5466 (of 10192), d = 10, its = 49
i = 5467 (of 10192), d = 10, its = 55
i = 5468 (of 10192), d = 1.77278, its = 5
i = 5469 (of 10192), d = 2.15045, its = 4
i = 5470 (of 10192), d = 10, its = 49
i = 5471 (of 10192), d = 2.04039, its = 3
i = 5472 (of 10192), d = 10, its = 40
i = 5473 (of 10192), d = 2.79275, its = 5
i = 5474 (of 10192), d = 10, its = 53
i = 5475 (of 10192), d = 10, its = 54
i = 5476 (of 10192), d = 10, its = 37
i = 5477 (of 10192), d = 10, its = 53
i = 5478 (of 10192), d = 10, its = 40
i = 5479 (of 10192), d = 1.70403, its = 5
i = 5480 (of 10192), d = 0.638027, its = 18
i = 5481 (of 10192), d = 10, its = 48
i = 5482 (of 10192), d = 10, its = 51
i = 5483 (of 10192), d = 10, its = 38
i = 5484 (of 10192), d = 10, its = 37
i = 5485 (of 10192), d = 10, its = 54
i = 5486 (of 10192), d = 10, its = 44
i = 5487 (of 10192), d = 10, its = 40
i = 5488 (of 10192), d = 10, its = 52
i = 5489 (of 10192), d = 10, its = 50
i = 5490 (of 10192), d = 10, its = 42
i = 5491 (of 10192), d = 1.80262, its = 4
i = 5492 (of 10192), d = 0.425276, its = 18
i = 5493 (of 10192), d = 10, its = 42
i = 5494 (of 10192), d = 10, its = 37
i = 5495 (of 10192), d = 3.25919, its = 7
i = 5496 (of 10192), d = 10, its = 53
i = 5497 (of 10192), d = 10, its = 37
i = 5498 (of 10192), d = 10, its = 53
i = 5499 (of 10192), d = 10, its = 52
i = 5500 (of 10192), d = 10, its = 53
i = 5501 (of 10192), d = 10, its = 40
i = 5502 (of 10192), d = 3.45442, its = 6
i = 5503 (of 10192), d = 10, its = 38
i = 5504 (of 10192), d = 10, its = 47
i = 5505 (of 10192), d = 10, its = 53
i = 5506 (of 10192), d = 1.46252, its = 8
i = 5507 (of 10192), d = 10, its = 53
i = 5508 (of 10192), d = 5.53644, its = 7
i = 5509 (of 10192), d = 10, its = 56
i = 5510 (of 10192), d = 10, its = 53
i = 5511 (of 10192), d = 10, its = 50
i = 5512 (of 10192), d = 1.92454, its = 4
i = 5513 (of 10192), d = 10, its = 53
i = 5514 (of 10192), d = 10, its = 53
i = 5515 (of 10192), d = 10, its = 52
i = 5516 (of 10192), d = 10, its = 50
i = 5517 (of 10192), d = 1.87983, its = 4
i = 5518 (of 10192), d = 10, its = 52
i = 5519 (of 10192), d = 10, its = 56
i = 5520 (of 10192), d = 10, its = 51
i = 5521 (of 10192), d = 10, its = 54
i = 5522 (of 10192), d = 10, its = 52
i = 5523 (of 10192), d = 1.28991, its = 22
i = 5524 (of 10192), d = 10, its = 52
i = 5525 (of 10192), d = 1.78577, its = 4
i = 5526 (of 10192), d = 1.41138, its = 6
i = 5527 (of 10192), d = 3.3275, its = 9
i = 5528 (of 10192), d = 10, its = 37
i = 5529 (of 10192), d = 10, its = 52
i = 5530 (of 10192), d = 10, its = 50
i = 5531 (of 10192), d = 10, its = 37
i = 5532 (of 10192), d = 1.39866, its = 6
i = 5533 (of 10192), d = 10, its = 37
i = 5534 (of 10192), d = 3.87565, its = 7
i = 5535 (of 10192), d = 10, its = 53
i = 5536 (of 10192), d = 0.966779, its = 16
i = 5537 (of 10192), d = 3.1414, its = 6
i = 5538 (of 10192), d = 10, its = 51
i = 5539 (of 10192), d = 10, its = 37
i = 5540 (of 10192), d = 10, its = 37
i = 5541 (of 10192), d = 10, its = 55
i = 5542 (of 10192), d = 10, its = 37
i = 5543 (of 10192), d = 10, its = 57
i = 5544 (of 10192), d = 10, its = 54
i = 5545 (of 10192), d = 10, its = 59
i = 5546 (of 10192), d = 10, its = 54
i = 5547 (of 10192), d = 10, its = 51
i = 5548 (of 10192), d = 10, its = 51
i = 5549 (of 10192), d = 10, its = 37
i = 5550 (of 10192), d = 10, its = 42
i = 5551 (of 10192), d = 10, its = 52
i = 5552 (of 10192), d = 3.83806, its = 6
i = 5553 (of 10192), d = 10, its = 51
i = 5554 (of 10192), d = 2.1699, its = 4
i = 5555 (of 10192), d = 10, its = 49
i = 5556 (of 10192), d = 10, its = 37
i = 5557 (of 10192), d = 10, its = 40
i = 5558 (of 10192), d = 10, its = 37
i = 5559 (of 10192), d = 1.31369, its = 8
i = 5560 (of 10192), d = 10, its = 52
i = 5561 (of 10192), d = 10, its = 37
i = 5562 (of 10192), d = 2.05333, its = 3
i = 5563 (of 10192), d = 10, its = 38
i = 5564 (of 10192), d = 10, its = 50
i = 5565 (of 10192), d = 10, its = 55
i = 5566 (of 10192), d = 10, its = 38
i = 5567 (of 10192), d = 4.9423, its = 8
i = 5568 (of 10192), d = 10, its = 51
i = 5569 (of 10192), d = 7.52522, its = 9
i = 5570 (of 10192), d = 10, its = 56
i = 5571 (of 10192), d = 2.14347, its = 4
i = 5572 (of 10192), d = 2.25898, its = 5
i = 5573 (of 10192), d = 10, its = 51
i = 5574 (of 10192), d = 10, its = 54
i = 5575 (of 10192), d = 10, its = 53
i = 5576 (of 10192), d = 10, its = 37
i = 5577 (of 10192), d = 10, its = 41
i = 5578 (of 10192), d = 2.46049, its = 5
i = 5579 (of 10192), d = 10, its = 57
i = 5580 (of 10192), d = 10, its = 52
i = 5581 (of 10192), d = 10, its = 53
i = 5582 (of 10192), d = 6.35939, its = 8
i = 5583 (of 10192), d = 1.75792, its = 5
i = 5584 (of 10192), d = 10, its = 37
i = 5585 (of 10192), d = 10, its = 51
i = 5586 (of 10192), d = 1.75325, its = 5
i = 5587 (of 10192), d = 10, its = 55
i = 5588 (of 10192), d = 3.85465, its = 7
i = 5589 (of 10192), d = 10, its = 51
i = 5590 (of 10192), d = 10, its = 42
i = 5591 (of 10192), d = 10, its = 37
i = 5592 (of 10192), d = 10, its = 55
i = 5593 (of 10192), d = 0.746052, its = 17
i = 5594 (of 10192), d = 10, its = 53
i = 5595 (of 10192), d = 10, its = 55
i = 5596 (of 10192), d = 10, its = 54
i = 5597 (of 10192), d = 2.03621, its = 3
i = 5598 (of 10192), d = 10, its = 50
i = 5599 (of 10192), d = 10, its = 58
i = 5600 (of 10192), d = 1.01246, its = 17
i = 5601 (of 10192), d = 10, its = 40
i = 5602 (of 10192), d = 10, its = 56
i = 5603 (of 10192), d = 10, its = 54
i = 5604 (of 10192), d = 10, its = 54
i = 5605 (of 10192), d = 10, its = 40
i = 5606 (of 10192), d = 1.06372, its = 20
i = 5607 (of 10192), d = 0.480689, its = 25
i = 5608 (of 10192), d = 10, its = 51
i = 5609 (of 10192), d = 10, its = 53
i = 5610 (of 10192), d = 10, its = 52
i = 5611 (of 10192), d = 7.48591, its = 9
i = 5612 (of 10192), d = 10, its = 58
i = 5613 (of 10192), d = 10, its = 57
i = 5614 (of 10192), d = 0.368586, its = 19
i = 5615 (of 10192), d = 10, its = 51
i = 5616 (of 10192), d = 2.35962, its = 5
i = 5617 (of 10192), d = 10, its = 53
i = 5618 (of 10192), d = 10, its = 37
i = 5619 (of 10192), d = 10, its = 48
i = 5620 (of 10192), d = 10, its = 47
i = 5621 (of 10192), d = 10, its = 52
i = 5622 (of 10192), d = 10, its = 37
i = 5623 (of 10192), d = 10, its = 50
i = 5624 (of 10192), d = 10, its = 54
i = 5625 (of 10192), d = 1.59499, its = 6
i = 5626 (of 10192), d = 0.559053, its = 18
i = 5627 (of 10192), d = 4.30163, its = 7
i = 5628 (of 10192), d = 10, its = 54
i = 5629 (of 10192), d = 10, its = 48
i = 5630 (of 10192), d = 10, its = 53
i = 5631 (of 10192), d = 10, its = 42
i = 5632 (of 10192), d = 10, its = 57
i = 5633 (of 10192), d = 10, its = 54
i = 5634 (of 10192), d = 10, its = 54
i = 5635 (of 10192), d = 10, its = 41
i = 5636 (of 10192), d = 10, its = 58
i = 5637 (of 10192), d = 10, its = 52
i = 5638 (of 10192), d = 10, its = 54
i = 5639 (of 10192), d = 10, its = 37
i = 5640 (of 10192), d = 10, its = 51
i = 5641 (of 10192), d = 10, its = 41
i = 5642 (of 10192), d = 10, its = 52
i = 5643 (of 10192), d = 10, its = 43
i = 5644 (of 10192), d = 10, its = 44
i = 5645 (of 10192), d = 10, its = 37
i = 5646 (of 10192), d = 1.09151, its = 20
i = 5647 (of 10192), d = 10, its = 51
i = 5648 (of 10192), d = 10, its = 50
i = 5649 (of 10192), d = 10, its = 49
i = 5650 (of 10192), d = 10, its = 54
i = 5651 (of 10192), d = 2.1602, its = 4
i = 5652 (of 10192), d = 10, its = 56
i = 5653 (of 10192), d = 10, its = 54
i = 5654 (of 10192), d = 10, its = 56
i = 5655 (of 10192), d = 10, its = 52
i = 5656 (of 10192), d = 10, its = 48
i = 5657 (of 10192), d = 10, its = 40
i = 5658 (of 10192), d = 10, its = 53
i = 5659 (of 10192), d = 10, its = 56
i = 5660 (of 10192), d = 10, its = 37
i = 5661 (of 10192), d = 10, its = 44
i = 5662 (of 10192), d = 2.36373, its = 5
i = 5663 (of 10192), d = 2.161, its = 4
i = 5664 (of 10192), d = 10, its = 52
i = 5665 (of 10192), d = 10, its = 54
i = 5666 (of 10192), d = 2.62458, its = 6
i = 5667 (of 10192), d = 10, its = 52
i = 5668 (of 10192), d = 10, its = 54
i = 5669 (of 10192), d = 10, its = 38
i = 5670 (of 10192), d = 10, its = 40
i = 5671 (of 10192), d = 10, its = 49
i = 5672 (of 10192), d = 2.43897, its = 6
i = 5673 (of 10192), d = 10, its = 50
i = 5674 (of 10192), d = 10, its = 39
i = 5675 (of 10192), d = 10, its = 47
i = 5676 (of 10192), d = 0.851266, its = 18
i = 5677 (of 10192), d = 10, its = 49
i = 5678 (of 10192), d = 10, its = 54
i = 5679 (of 10192), d = 10, its = 50
i = 5680 (of 10192), d = 10, its = 37
i = 5681 (of 10192), d = 1.06333, its = 18
i = 5682 (of 10192), d = 10, its = 39
i = 5683 (of 10192), d = 10, its = 53
i = 5684 (of 10192), d = 10, its = 53
i = 5685 (of 10192), d = 10, its = 51
i = 5686 (of 10192), d = 1.80646, its = 6
i = 5687 (of 10192), d = 0.514745, its = 18
i = 5688 (of 10192), d = 10, its = 53
i = 5689 (of 10192), d = 10, its = 40
i = 5690 (of 10192), d = 2.25447, its = 5
i = 5691 (of 10192), d = 10, its = 54
i = 5692 (of 10192), d = 10, its = 51
i = 5693 (of 10192), d = 10, its = 46
i = 5694 (of 10192), d = 10, its = 50
i = 5695 (of 10192), d = 2.84205, its = 6
i = 5696 (of 10192), d = 10, its = 41
i = 5697 (of 10192), d = 10, its = 37
i = 5698 (of 10192), d = 10, its = 54
i = 5699 (of 10192), d = 10, its = 53
i = 5700 (of 10192), d = 10, its = 52
i = 5701 (of 10192), d = 10, its = 43
i = 5702 (of 10192), d = 10, its = 54
i = 5703 (of 10192), d = 10, its = 37
i = 5704 (of 10192), d = 10, its = 37
i = 5705 (of 10192), d = 10, its = 50
i = 5706 (of 10192), d = 10, its = 52
i = 5707 (of 10192), d = 0.7642, its = 18
i = 5708 (of 10192), d = 1.40192, its = 6
i = 5709 (of 10192), d = 10, its = 39
i = 5710 (of 10192), d = 10, its = 54
i = 5711 (of 10192), d = 1.69021, its = 5
i = 5712 (of 10192), d = 1.56359, its = 7
i = 5713 (of 10192), d = 10, its = 39
i = 5714 (of 10192), d = 0.467034, its = 17
i = 5715 (of 10192), d = 10, its = 37
i = 5716 (of 10192), d = 10, its = 52
i = 5717 (of 10192), d = 10, its = 37
i = 5718 (of 10192), d = 10, its = 56
i = 5719 (of 10192), d = 10, its = 53
i = 5720 (of 10192), d = 10, its = 55
i = 5721 (of 10192), d = 10, its = 54
i = 5722 (of 10192), d = 1.11561, its = 8
i = 5723 (of 10192), d = 10, its = 53
i = 5724 (of 10192), d = 1.58117, its = 6
i = 5725 (of 10192), d = 10, its = 40
i = 5726 (of 10192), d = 2.54075, its = 5
i = 5727 (of 10192), d = 10, its = 37
i = 5728 (of 10192), d = 1.99275, its = 3
i = 5729 (of 10192), d = 10, its = 54
i = 5730 (of 10192), d = 10, its = 37
i = 5731 (of 10192), d = 10, its = 51
i = 5732 (of 10192), d = 10, its = 51
i = 5733 (of 10192), d = 10, its = 58
i = 5734 (of 10192), d = 1.78025, its = 5
i = 5735 (of 10192), d = 10, its = 37
i = 5736 (of 10192), d = 3.9093, its = 6
i = 5737 (of 10192), d = 10, its = 37
i = 5738 (of 10192), d = 10, its = 50
i = 5739 (of 10192), d = 1.00301, its = 17
i = 5740 (of 10192), d = 10, its = 53
i = 5741 (of 10192), d = 1.64801, its = 5
i = 5742 (of 10192), d = 10, its = 54
i = 5743 (of 10192), d = 10, its = 39
i = 5744 (of 10192), d = 10, its = 53
i = 5745 (of 10192), d = 10, its = 37
i = 5746 (of 10192), d = 10, its = 50
i = 5747 (of 10192), d = 10, its = 54
i = 5748 (of 10192), d = 10, its = 54
i = 5749 (of 10192), d = 10, its = 52
i = 5750 (of 10192), d = 10, its = 48
i = 5751 (of 10192), d = 10, its = 41
i = 5752 (of 10192), d = 10, its = 52
i = 5753 (of 10192), d = 10, its = 54
i = 5754 (of 10192), d = 1.68136, its = 5
i = 5755 (of 10192), d = 10, its = 40
i = 5756 (of 10192), d = 10, its = 41
i = 5757 (of 10192), d = 0.852139, its = 20
i = 5758 (of 10192), d = 1.10681, its = 18
i = 5759 (of 10192), d = 10, its = 40
i = 5760 (of 10192), d = 10, its = 37
i = 5761 (of 10192), d = 0.699904, its = 17
i = 5762 (of 10192), d = 10, its = 40
i = 5763 (of 10192), d = 10, its = 37
i = 5764 (of 10192), d = 10, its = 39
i = 5765 (of 10192), d = 10, its = 53
i = 5766 (of 10192), d = 10, its = 54
i = 5767 (of 10192), d = 0.633827, its = 20
i = 5768 (of 10192), d = 10, its = 37
i = 5769 (of 10192), d = 1.37139, its = 6
i = 5770 (of 10192), d = 10, its = 50
i = 5771 (of 10192), d = 10, its = 37
i = 5772 (of 10192), d = 1.77903, its = 4
i = 5773 (of 10192), d = 3.13234, its = 6
i = 5774 (of 10192), d = 10, its = 53
i = 5775 (of 10192), d = 10, its = 38
i = 5776 (of 10192), d = 1.6816, its = 5
i = 5777 (of 10192), d = 1.65304, its = 5
i = 5778 (of 10192), d = 10, its = 54
i = 5779 (of 10192), d = 10, its = 37
i = 5780 (of 10192), d = 0.8365, its = 21
i = 5781 (of 10192), d = 10, its = 57
i = 5782 (of 10192), d = 10, its = 37
i = 5783 (of 10192), d = 10, its = 54
i = 5784 (of 10192), d = 10, its = 53
i = 5785 (of 10192), d = 10, its = 49
i = 5786 (of 10192), d = 10, its = 53
i = 5787 (of 10192), d = 10, its = 54
i = 5788 (of 10192), d = 10, its = 40
i = 5789 (of 10192), d = 6.50582, its = 23
i = 5790 (of 10192), d = 10, its = 48
i = 5791 (of 10192), d = 1.45088, its = 8
i = 5792 (of 10192), d = 10, its = 58
i = 5793 (of 10192), d = 1.1604, its = 23
i = 5794 (of 10192), d = 2.91512, its = 6
i = 5795 (of 10192), d = 10, its = 56
i = 5796 (of 10192), d = 7.13159, its = 9
i = 5797 (of 10192), d = 10, its = 51
i = 5798 (of 10192), d = 10, its = 56
i = 5799 (of 10192), d = 1.67867, its = 5
i = 5800 (of 10192), d = 10, its = 37
i = 5801 (of 10192), d = 10, its = 49
i = 5802 (of 10192), d = 10, its = 37
i = 5803 (of 10192), d = 10, its = 56
i = 5804 (of 10192), d = 10, its = 37
i = 5805 (of 10192), d = 10, its = 58
i = 5806 (of 10192), d = 10, its = 48
i = 5807 (of 10192), d = 10, its = 50
i = 5808 (of 10192), d = 1.31627, its = 8
i = 5809 (of 10192), d = 10, its = 52
i = 5810 (of 10192), d = 0.442599, its = 19
i = 5811 (of 10192), d = 10, its = 55
i = 5812 (of 10192), d = 10, its = 49
i = 5813 (of 10192), d = 10, its = 52
i = 5814 (of 10192), d = 7.615, its = 8
i = 5815 (of 10192), d = 1.95892, its = 3
i = 5816 (of 10192), d = 10, its = 37
i = 5817 (of 10192), d = 10, its = 58
i = 5818 (of 10192), d = 1.4614, its = 6
i = 5819 (of 10192), d = 10, its = 42
i = 5820 (of 10192), d = 10, its = 37
i = 5821 (of 10192), d = 10, its = 40
i = 5822 (of 10192), d = 4.9423, its = 8
i = 5823 (of 10192), d = 10, its = 50
i = 5824 (of 10192), d = 10, its = 53
i = 5825 (of 10192), d = 10, its = 56
i = 5826 (of 10192), d = 10, its = 37
i = 5827 (of 10192), d = 4.2271, its = 7
i = 5828 (of 10192), d = 10, its = 52
i = 5829 (of 10192), d = 10, its = 51
i = 5830 (of 10192), d = 10, its = 52
i = 5831 (of 10192), d = 1.73531, its = 5
i = 5832 (of 10192), d = 10, its = 52
i = 5833 (of 10192), d = 10, its = 37
i = 5834 (of 10192), d = 10, its = 42
i = 5835 (of 10192), d = 10, its = 55
i = 5836 (of 10192), d = 10, its = 51
i = 5837 (of 10192), d = 10, its = 55
i = 5838 (of 10192), d = 10, its = 52
i = 5839 (of 10192), d = 10, its = 37
i = 5840 (of 10192), d = 10, its = 48
i = 5841 (of 10192), d = 1.47811, its = 6
i = 5842 (of 10192), d = 10, its = 42
i = 5843 (of 10192), d = 2.37586, its = 5
i = 5844 (of 10192), d = 10, its = 39
i = 5845 (of 10192), d = 3.68702, its = 7
i = 5846 (of 10192), d = 2.72408, its = 6
i = 5847 (of 10192), d = 0.857787, its = 18
i = 5848 (of 10192), d = 0.567443, its = 17
i = 5849 (of 10192), d = 10, its = 37
i = 5850 (of 10192), d = 10, its = 52
i = 5851 (of 10192), d = 10, its = 40
i = 5852 (of 10192), d = 10, its = 51
i = 5853 (of 10192), d = 10, its = 37
i = 5854 (of 10192), d = 10, its = 41
i = 5855 (of 10192), d = 1.44356, its = 7
i = 5856 (of 10192), d = 10, its = 21
i = 5857 (of 10192), d = 1.99079, its = 3
i = 5858 (of 10192), d = 10, its = 54
i = 5859 (of 10192), d = 5.17377, its = 8
i = 5860 (of 10192), d = 10, its = 57
i = 5861 (of 10192), d = 10, its = 51
i = 5862 (of 10192), d = 2.40904, its = 5
i = 5863 (of 10192), d = 10, its = 56
i = 5864 (of 10192), d = 10, its = 41
i = 5865 (of 10192), d = 10, its = 52
i = 5866 (of 10192), d = 3.91648, its = 7
i = 5867 (of 10192), d = 10, its = 52
i = 5868 (of 10192), d = 10, its = 39
i = 5869 (of 10192), d = 10, its = 50
i = 5870 (of 10192), d = 2.79244, its = 6
i = 5871 (of 10192), d = 10, its = 54
i = 5872 (of 10192), d = 10, its = 49
i = 5873 (of 10192), d = 10, its = 50
i = 5874 (of 10192), d = 2.46439, its = 5
i = 5875 (of 10192), d = 10, its = 40
i = 5876 (of 10192), d = 10, its = 37
i = 5877 (of 10192), d = 10, its = 37
i = 5878 (of 10192), d = 10, its = 40
i = 5879 (of 10192), d = 10, its = 39
i = 5880 (of 10192), d = 10, its = 37
i = 5881 (of 10192), d = 10, its = 37
i = 5882 (of 10192), d = 10, its = 50
i = 5883 (of 10192), d = 10, its = 48
i = 5884 (of 10192), d = 10, its = 37
i = 5885 (of 10192), d = 2.59414, its = 5
i = 5886 (of 10192), d = 3.88853, its = 6
i = 5887 (of 10192), d = 10, its = 53
i = 5888 (of 10192), d = 10, its = 52
i = 5889 (of 10192), d = 10, its = 51
i = 5890 (of 10192), d = 10, its = 52
i = 5891 (of 10192), d = 10, its = 49
i = 5892 (of 10192), d = 10, its = 52
i = 5893 (of 10192), d = 10, its = 56
i = 5894 (of 10192), d = 10, its = 53
i = 5895 (of 10192), d = 2.12658, its = 4
i = 5896 (of 10192), d = 2.42748, its = 5
i = 5897 (of 10192), d = 10, its = 37
i = 5898 (of 10192), d = 3.77262, its = 7
i = 5899 (of 10192), d = 10, its = 53
i = 5900 (of 10192), d = 10, its = 55
i = 5901 (of 10192), d = 2.60841, its = 6
i = 5902 (of 10192), d = 4.92196, its = 7
i = 5903 (of 10192), d = 10, its = 53
i = 5904 (of 10192), d = 10, its = 54
i = 5905 (of 10192), d = 10, its = 54
i = 5906 (of 10192), d = 2.09814, its = 4
i = 5907 (of 10192), d = 10, its = 53
i = 5908 (of 10192), d = 10, its = 50
i = 5909 (of 10192), d = 2.10641, its = 4
i = 5910 (of 10192), d = 1.41046, its = 6
i = 5911 (of 10192), d = 10, its = 42
i = 5912 (of 10192), d = 10, its = 50
i = 5913 (of 10192), d = 10, its = 54
i = 5914 (of 10192), d = 10, its = 37
i = 5915 (of 10192), d = 10, its = 53
i = 5916 (of 10192), d = 2.46991, its = 6
i = 5917 (of 10192), d = 10, its = 52
i = 5918 (of 10192), d = 10, its = 53
i = 5919 (of 10192), d = 10, its = 53
i = 5920 (of 10192), d = 10, its = 44
i = 5921 (of 10192), d = 10, its = 56
i = 5922 (of 10192), d = 10, its = 40
i = 5923 (of 10192), d = 10, its = 39
i = 5924 (of 10192), d = 1.13486, its = 8
i = 5925 (of 10192), d = 0.535998, its = 17
i = 5926 (of 10192), d = 10, its = 50
i = 5927 (of 10192), d = 10, its = 55
i = 5928 (of 10192), d = 10, its = 51
i = 5929 (of 10192), d = 10, its = 60
i = 5930 (of 10192), d = 1.45442, its = 6
i = 5931 (of 10192), d = 10, its = 40
i = 5932 (of 10192), d = 1.79591, its = 4
i = 5933 (of 10192), d = 10, its = 41
i = 5934 (of 10192), d = 10, its = 37
i = 5935 (of 10192), d = 2.65657, its = 5
i = 5936 (of 10192), d = 10, its = 56
i = 5937 (of 10192), d = 10, its = 52
i = 5938 (of 10192), d = 10, its = 51
i = 5939 (of 10192), d = 10, its = 39
i = 5940 (of 10192), d = 10, its = 55
i = 5941 (of 10192), d = 10, its = 43
i = 5942 (of 10192), d = 10, its = 50
i = 5943 (of 10192), d = 10, its = 51
i = 5944 (of 10192), d = 10, its = 39
i = 5945 (of 10192), d = 10, its = 55
i = 5946 (of 10192), d = 0.88691, its = 19
i = 5947 (of 10192), d = 2.41095, its = 5
i = 5948 (of 10192), d = 10, its = 37
i = 5949 (of 10192), d = 2.54718, its = 5
i = 5950 (of 10192), d = 10, its = 38
i = 5951 (of 10192), d = 10, its = 40
i = 5952 (of 10192), d = 2.39832, its = 5
i = 5953 (of 10192), d = 10, its = 52
i = 5954 (of 10192), d = 10, its = 38
i = 5955 (of 10192), d = 10, its = 52
i = 5956 (of 10192), d = 1.38486, its = 6
i = 5957 (of 10192), d = 10, its = 52
i = 5958 (of 10192), d = 10, its = 51
i = 5959 (of 10192), d = 1.82314, its = 4
i = 5960 (of 10192), d = 10, its = 49
i = 5961 (of 10192), d = 10, its = 37
i = 5962 (of 10192), d = 10, its = 54
i = 5963 (of 10192), d = 10, its = 52
i = 5964 (of 10192), d = 10, its = 51
i = 5965 (of 10192), d = 10, its = 39
i = 5966 (of 10192), d = 10, its = 37
i = 5967 (of 10192), d = 10, its = 37
i = 5968 (of 10192), d = 10, its = 41
i = 5969 (of 10192), d = 10, its = 52
i = 5970 (of 10192), d = 10, its = 40
i = 5971 (of 10192), d = 10, its = 49
i = 5972 (of 10192), d = 10, its = 40
i = 5973 (of 10192), d = 2.91394, its = 6
i = 5974 (of 10192), d = 10, its = 37
i = 5975 (of 10192), d = 10, its = 49
i = 5976 (of 10192), d = 10, its = 37
i = 5977 (of 10192), d = 10, its = 54
i = 5978 (of 10192), d = 10, its = 37
i = 5979 (of 10192), d = 2.36629, its = 5
i = 5980 (of 10192), d = 10, its = 50
i = 5981 (of 10192), d = 10, its = 50
i = 5982 (of 10192), d = 10, its = 49
i = 5983 (of 10192), d = 10, its = 50
i = 5984 (of 10192), d = 10, its = 52
i = 5985 (of 10192), d = 4.58561, its = 8
i = 5986 (of 10192), d = 10, its = 56
i = 5987 (of 10192), d = 4.10424, its = 7
i = 5988 (of 10192), d = 10, its = 37
i = 5989 (of 10192), d = 10, its = 55
i = 5990 (of 10192), d = 10, its = 49
i = 5991 (of 10192), d = 1.71254, its = 5
i = 5992 (of 10192), d = 10, its = 51
i = 5993 (of 10192), d = 10, its = 49
i = 5994 (of 10192), d = 10, its = 56
i = 5995 (of 10192), d = 10, its = 37
i = 5996 (of 10192), d = 10, its = 55
i = 5997 (of 10192), d = 3.0005, its = 5
i = 5998 (of 10192), d = 3.25923, its = 6
i = 5999 (of 10192), d = 2.10692, its = 4
i = 6000 (of 10192), d = 3.16005, its = 6
i = 6001 (of 10192), d = 10, its = 51
i = 6002 (of 10192), d = 10, its = 37
i = 6003 (of 10192), d = 10, its = 40
i = 6004 (of 10192), d = 10, its = 53
i = 6005 (of 10192), d = 10, its = 52
i = 6006 (of 10192), d = 10, its = 40
i = 6007 (of 10192), d = 10, its = 53
i = 6008 (of 10192), d = 10, its = 54
i = 6009 (of 10192), d = 2.15355, its = 4
i = 6010 (of 10192), d = 1.77928, its = 5
i = 6011 (of 10192), d = 10, its = 54
i = 6012 (of 10192), d = 10, its = 53
i = 6013 (of 10192), d = 2.8773, its = 6
i = 6014 (of 10192), d = 10, its = 40
i = 6015 (of 10192), d = 10, its = 37
i = 6016 (of 10192), d = 10, its = 53
i = 6017 (of 10192), d = 10, its = 37
i = 6018 (of 10192), d = 10, its = 37
i = 6019 (of 10192), d = 1.55218, its = 6
i = 6020 (of 10192), d = 10, its = 48
i = 6021 (of 10192), d = 1.62614, its = 6
i = 6022 (of 10192), d = 10, its = 37
i = 6023 (of 10192), d = 10, its = 53
i = 6024 (of 10192), d = 10, its = 53
i = 6025 (of 10192), d = 10, its = 51
i = 6026 (of 10192), d = 10, its = 55
i = 6027 (of 10192), d = 2.52556, its = 5
i = 6028 (of 10192), d = 1.56409, its = 6
i = 6029 (of 10192), d = 1.74164, its = 6
i = 6030 (of 10192), d = 10, its = 52
i = 6031 (of 10192), d = 10, its = 54
i = 6032 (of 10192), d = 10, its = 55
i = 6033 (of 10192), d = 10, its = 55
i = 6034 (of 10192), d = 3.26892, its = 6
i = 6035 (of 10192), d = 10, its = 54
i = 6036 (of 10192), d = 10, its = 54
i = 6037 (of 10192), d = 10, its = 55
i = 6038 (of 10192), d = 10, its = 55
i = 6039 (of 10192), d = 10, its = 37
i = 6040 (of 10192), d = 10, its = 55
i = 6041 (of 10192), d = 10, its = 58
i = 6042 (of 10192), d = 10, its = 55
i = 6043 (of 10192), d = 10, its = 37
i = 6044 (of 10192), d = 0.699904, its = 17
i = 6045 (of 10192), d = 10, its = 53
i = 6046 (of 10192), d = 10, its = 51
i = 6047 (of 10192), d = 10, its = 54
i = 6048 (of 10192), d = 10, its = 38
i = 6049 (of 10192), d = 0.640663, its = 18
i = 6050 (of 10192), d = 10, its = 50
i = 6051 (of 10192), d = 10, its = 40
i = 6052 (of 10192), d = 10, its = 53
i = 6053 (of 10192), d = 0.880286, its = 18
i = 6054 (of 10192), d = 10, its = 46
i = 6055 (of 10192), d = 0.999963, its = 18
i = 6056 (of 10192), d = 10, its = 49
i = 6057 (of 10192), d = 10, its = 59
i = 6058 (of 10192), d = 10, its = 54
i = 6059 (of 10192), d = 10, its = 54
i = 6060 (of 10192), d = 2.76592, its = 6
i = 6061 (of 10192), d = 10, its = 42
i = 6062 (of 10192), d = 10, its = 51
i = 6063 (of 10192), d = 10, its = 52
i = 6064 (of 10192), d = 10, its = 53
i = 6065 (of 10192), d = 10, its = 38
i = 6066 (of 10192), d = 10, its = 54
i = 6067 (of 10192), d = 10, its = 56
i = 6068 (of 10192), d = 10, its = 56
i = 6069 (of 10192), d = 1.77903, its = 4
i = 6070 (of 10192), d = 10, its = 53
i = 6071 (of 10192), d = 10, its = 53
i = 6072 (of 10192), d = 10, its = 54
i = 6073 (of 10192), d = 10, its = 37
i = 6074 (of 10192), d = 2.85113, its = 6
i = 6075 (of 10192), d = 10, its = 55
i = 6076 (of 10192), d = 10, its = 37
i = 6077 (of 10192), d = 0.580918, its = 17
i = 6078 (of 10192), d = 1.76915, its = 4
i = 6079 (of 10192), d = 10, its = 52
i = 6080 (of 10192), d = 10, its = 55
i = 6081 (of 10192), d = 2.10657, its = 4
i = 6082 (of 10192), d = 1.54093, its = 6
i = 6083 (of 10192), d = 0.915479, its = 19
i = 6084 (of 10192), d = 10, its = 54
i = 6085 (of 10192), d = 10, its = 41
i = 6086 (of 10192), d = 0.726494, its = 19
i = 6087 (of 10192), d = 0.892563, its = 18
i = 6088 (of 10192), d = 10, its = 37
i = 6089 (of 10192), d = 2.80242, its = 6
i = 6090 (of 10192), d = 10, its = 55
i = 6091 (of 10192), d = 10, its = 52
i = 6092 (of 10192), d = 10, its = 41
i = 6093 (of 10192), d = 1.55855, its = 6
i = 6094 (of 10192), d = 10, its = 57
i = 6095 (of 10192), d = 10, its = 55
i = 6096 (of 10192), d = 10, its = 52
i = 6097 (of 10192), d = 10, its = 50
i = 6098 (of 10192), d = 1.77367, its = 5
i = 6099 (of 10192), d = 10, its = 52
i = 6100 (of 10192), d = 10, its = 49
i = 6101 (of 10192), d = 2.55073, its = 5
i = 6102 (of 10192), d = 2.37547, its = 5
i = 6103 (of 10192), d = 10, its = 44
i = 6104 (of 10192), d = 0.65293, its = 19
i = 6105 (of 10192), d = 10, its = 37
i = 6106 (of 10192), d = 10, its = 52
i = 6107 (of 10192), d = 10, its = 52
i = 6108 (of 10192), d = 2.12397, its = 4
i = 6109 (of 10192), d = 0.978859, its = 19
i = 6110 (of 10192), d = 10, its = 42
i = 6111 (of 10192), d = 10, its = 54
i = 6112 (of 10192), d = 10, its = 52
i = 6113 (of 10192), d = 10, its = 51
i = 6114 (of 10192), d = 10, its = 37
i = 6115 (of 10192), d = 1.81085, its = 4
i = 6116 (of 10192), d = 10, its = 49
i = 6117 (of 10192), d = 0.595907, its = 21
i = 6118 (of 10192), d = 10, its = 51
i = 6119 (of 10192), d = 10, its = 51
i = 6120 (of 10192), d = 10, its = 52
i = 6121 (of 10192), d = 10, its = 41
i = 6122 (of 10192), d = 10, its = 39
i = 6123 (of 10192), d = 10, its = 52
i = 6124 (of 10192), d = 10, its = 53
i = 6125 (of 10192), d = 10, its = 37
i = 6126 (of 10192), d = 10, its = 51
i = 6127 (of 10192), d = 10, its = 51
i = 6128 (of 10192), d = 0.708993, its = 17
i = 6129 (of 10192), d = 2.53769, its = 5
i = 6130 (of 10192), d = 10, its = 37
i = 6131 (of 10192), d = 10, its = 37
i = 6132 (of 10192), d = 10, its = 40
i = 6133 (of 10192), d = 10, its = 52
i = 6134 (of 10192), d = 1.42281, its = 6
i = 6135 (of 10192), d = 1.80938, its = 4
i = 6136 (of 10192), d = 3.22094, its = 7
i = 6137 (of 10192), d = 10, its = 51
i = 6138 (of 10192), d = 2.06501, its = 4
i = 6139 (of 10192), d = 10, its = 54
i = 6140 (of 10192), d = 2.88776, its = 5
i = 6141 (of 10192), d = 10, its = 52
i = 6142 (of 10192), d = 10, its = 54
i = 6143 (of 10192), d = 2.02854, its = 3
i = 6144 (of 10192), d = 10, its = 54
i = 6145 (of 10192), d = 10, its = 50
i = 6146 (of 10192), d = 10, its = 55
i = 6147 (of 10192), d = 10, its = 40
i = 6148 (of 10192), d = 10, its = 38
i = 6149 (of 10192), d = 10, its = 48
i = 6150 (of 10192), d = 10, its = 56
i = 6151 (of 10192), d = 10, its = 53
i = 6152 (of 10192), d = 0.668321, its = 17
i = 6153 (of 10192), d = 0.838994, its = 24
i = 6154 (of 10192), d = 10, its = 52
i = 6155 (of 10192), d = 4.61038, its = 7
i = 6156 (of 10192), d = 10, its = 57
i = 6157 (of 10192), d = 2.2891, its = 4
i = 6158 (of 10192), d = 10, its = 48
i = 6159 (of 10192), d = 10, its = 41
i = 6160 (of 10192), d = 10, its = 51
i = 6161 (of 10192), d = 10, its = 38
i = 6162 (of 10192), d = 10, its = 54
i = 6163 (of 10192), d = 10, its = 52
i = 6164 (of 10192), d = 1.53241, its = 6
i = 6165 (of 10192), d = 2.14077, its = 4
i = 6166 (of 10192), d = 10, its = 39
i = 6167 (of 10192), d = 10, its = 51
i = 6168 (of 10192), d = 10, its = 37
i = 6169 (of 10192), d = 10, its = 39
i = 6170 (of 10192), d = 10, its = 37
i = 6171 (of 10192), d = 0.411992, its = 20
i = 6172 (of 10192), d = 2.66042, its = 5
i = 6173 (of 10192), d = 1.87041, its = 4
i = 6174 (of 10192), d = 10, its = 54
i = 6175 (of 10192), d = 10, its = 39
i = 6176 (of 10192), d = 0.746262, its = 21
i = 6177 (of 10192), d = 10, its = 56
i = 6178 (of 10192), d = 10, its = 40
i = 6179 (of 10192), d = 3.23547, its = 6
i = 6180 (of 10192), d = 10, its = 49
i = 6181 (of 10192), d = 4.47161, its = 7
i = 6182 (of 10192), d = 10, its = 37
i = 6183 (of 10192), d = 1.47119, its = 6
i = 6184 (of 10192), d = 10, its = 37
i = 6185 (of 10192), d = 10, its = 39
i = 6186 (of 10192), d = 10, its = 40
i = 6187 (of 10192), d = 10, its = 48
i = 6188 (of 10192), d = 10, its = 50
i = 6189 (of 10192), d = 10, its = 52
i = 6190 (of 10192), d = 10, its = 53
i = 6191 (of 10192), d = 10, its = 37
i = 6192 (of 10192), d = 10, its = 52
i = 6193 (of 10192), d = 10, its = 37
i = 6194 (of 10192), d = 10, its = 41
i = 6195 (of 10192), d = 10, its = 55
i = 6196 (of 10192), d = 10, its = 51
i = 6197 (of 10192), d = 10, its = 42
i = 6198 (of 10192), d = 10, its = 38
i = 6199 (of 10192), d = 10, its = 53
i = 6200 (of 10192), d = 10, its = 37
i = 6201 (of 10192), d = 2.10803, its = 4
i = 6202 (of 10192), d = 10, its = 50
i = 6203 (of 10192), d = 10, its = 53
i = 6204 (of 10192), d = 10, its = 41
i = 6205 (of 10192), d = 1.15955, its = 19
i = 6206 (of 10192), d = 10, its = 37
i = 6207 (of 10192), d = 1.06917, its = 19
i = 6208 (of 10192), d = 10, its = 54
i = 6209 (of 10192), d = 1.87592, its = 5
i = 6210 (of 10192), d = 10, its = 53
i = 6211 (of 10192), d = 10, its = 55
i = 6212 (of 10192), d = 10, its = 37
i = 6213 (of 10192), d = 2.91137, its = 6
i = 6214 (of 10192), d = 10, its = 40
i = 6215 (of 10192), d = 10, its = 54
i = 6216 (of 10192), d = 10, its = 51
i = 6217 (of 10192), d = 2.36558, its = 5
i = 6218 (of 10192), d = 2.37863, its = 5
i = 6219 (of 10192), d = 10, its = 37
i = 6220 (of 10192), d = 1.49562, its = 7
i = 6221 (of 10192), d = 10, its = 37
i = 6222 (of 10192), d = 0.851266, its = 18
i = 6223 (of 10192), d = 10, its = 57
i = 6224 (of 10192), d = 2.14682, its = 5
i = 6225 (of 10192), d = 10, its = 50
i = 6226 (of 10192), d = 10, its = 39
i = 6227 (of 10192), d = 10, its = 54
i = 6228 (of 10192), d = 10, its = 50
i = 6229 (of 10192), d = 2.05096, its = 3
i = 6230 (of 10192), d = 10, its = 53
i = 6231 (of 10192), d = 10, its = 41
i = 6232 (of 10192), d = 10, its = 50
i = 6233 (of 10192), d = 10, its = 40
i = 6234 (of 10192), d = 10, its = 37
i = 6235 (of 10192), d = 10, its = 54
i = 6236 (of 10192), d = 10, its = 52
i = 6237 (of 10192), d = 0.718865, its = 16
i = 6238 (of 10192), d = 10, its = 53
i = 6239 (of 10192), d = 10, its = 51
i = 6240 (of 10192), d = 10, its = 55
i = 6241 (of 10192), d = 10, its = 50
i = 6242 (of 10192), d = 10, its = 51
i = 6243 (of 10192), d = 10, its = 54
i = 6244 (of 10192), d = 10, its = 52
i = 6245 (of 10192), d = 1.78113, its = 5
i = 6246 (of 10192), d = 3.05969, its = 6
i = 6247 (of 10192), d = 10, its = 42
i = 6248 (of 10192), d = 10, its = 37
i = 6249 (of 10192), d = 1.33665, its = 8
i = 6250 (of 10192), d = 10, its = 37
i = 6251 (of 10192), d = 10, its = 55
i = 6252 (of 10192), d = 10, its = 49
i = 6253 (of 10192), d = 10, its = 52
i = 6254 (of 10192), d = 0.326785, its = 20
i = 6255 (of 10192), d = 10, its = 40
i = 6256 (of 10192), d = 2.69568, its = 6
i = 6257 (of 10192), d = 10, its = 40
i = 6258 (of 10192), d = 10, its = 56
i = 6259 (of 10192), d = 10, its = 52
i = 6260 (of 10192), d = 10, its = 54
i = 6261 (of 10192), d = 10, its = 53
i = 6262 (of 10192), d = 0.683815, its = 22
i = 6263 (of 10192), d = 10, its = 50
i = 6264 (of 10192), d = 10, its = 52
i = 6265 (of 10192), d = 10, its = 56
i = 6266 (of 10192), d = 10, its = 41
i = 6267 (of 10192), d = 10, its = 50
i = 6268 (of 10192), d = 10, its = 52
i = 6269 (of 10192), d = 10, its = 53
i = 6270 (of 10192), d = 1.4708, its = 6
i = 6271 (of 10192), d = 10, its = 39
i = 6272 (of 10192), d = 10, its = 51
i = 6273 (of 10192), d = 10, its = 51
i = 6274 (of 10192), d = 10, its = 52
i = 6275 (of 10192), d = 10, its = 50
i = 6276 (of 10192), d = 10, its = 52
i = 6277 (of 10192), d = 10, its = 54
i = 6278 (of 10192), d = 10, its = 50
i = 6279 (of 10192), d = 10, its = 52
i = 6280 (of 10192), d = 10, its = 54
i = 6281 (of 10192), d = 10, its = 53
i = 6282 (of 10192), d = 10, its = 52
i = 6283 (of 10192), d = 10, its = 37
i = 6284 (of 10192), d = 10, its = 37
i = 6285 (of 10192), d = 10, its = 51
i = 6286 (of 10192), d = 10, its = 52
i = 6287 (of 10192), d = 10, its = 55
i = 6288 (of 10192), d = 10, its = 57
i = 6289 (of 10192), d = 10, its = 50
i = 6290 (of 10192), d = 1.16214, its = 24
i = 6291 (of 10192), d = 10, its = 52
i = 6292 (of 10192), d = 1.78909, its = 5
i = 6293 (of 10192), d = 2.6805, its = 5
i = 6294 (of 10192), d = 5.44429, its = 11
i = 6295 (of 10192), d = 10, its = 55
i = 6296 (of 10192), d = 1.60892, its = 6
i = 6297 (of 10192), d = 1.64003, its = 5
i = 6298 (of 10192), d = 10, its = 53
i = 6299 (of 10192), d = 10, its = 51
i = 6300 (of 10192), d = 10, its = 52
i = 6301 (of 10192), d = 10, its = 48
i = 6302 (of 10192), d = 10, its = 52
i = 6303 (of 10192), d = 10, its = 40
i = 6304 (of 10192), d = 10, its = 54
i = 6305 (of 10192), d = 10, its = 37
i = 6306 (of 10192), d = 10, its = 52
i = 6307 (of 10192), d = 10, its = 55
i = 6308 (of 10192), d = 10, its = 52
i = 6309 (of 10192), d = 10, its = 53
i = 6310 (of 10192), d = 10, its = 57
i = 6311 (of 10192), d = 10, its = 51
i = 6312 (of 10192), d = 3.86192, its = 7
i = 6313 (of 10192), d = 10, its = 53
i = 6314 (of 10192), d = 10, its = 51
i = 6315 (of 10192), d = 10, its = 40
i = 6316 (of 10192), d = 10, its = 52
i = 6317 (of 10192), d = 10, its = 37
i = 6318 (of 10192), d = 10, its = 37
i = 6319 (of 10192), d = 10, its = 51
i = 6320 (of 10192), d = 1.3225, its = 7
i = 6321 (of 10192), d = 10, its = 51
i = 6322 (of 10192), d = 1.07502, its = 21
i = 6323 (of 10192), d = 10, its = 53
i = 6324 (of 10192), d = 10, its = 48
i = 6325 (of 10192), d = 2.17905, its = 4
i = 6326 (of 10192), d = 1.66717, its = 6
i = 6327 (of 10192), d = 10, its = 39
i = 6328 (of 10192), d = 10, its = 52
i = 6329 (of 10192), d = 10, its = 51
i = 6330 (of 10192), d = 10, its = 52
i = 6331 (of 10192), d = 2.91405, its = 5
i = 6332 (of 10192), d = 1.76558, its = 6
i = 6333 (of 10192), d = 2.56623, its = 5
i = 6334 (of 10192), d = 10, its = 37
i = 6335 (of 10192), d = 10, its = 55
i = 6336 (of 10192), d = 10, its = 56
i = 6337 (of 10192), d = 10, its = 57
i = 6338 (of 10192), d = 10, its = 46
i = 6339 (of 10192), d = 10, its = 37
i = 6340 (of 10192), d = 10, its = 37
i = 6341 (of 10192), d = 10, its = 56
i = 6342 (of 10192), d = 10, its = 37
i = 6343 (of 10192), d = 10, its = 41
i = 6344 (of 10192), d = 10, its = 38
i = 6345 (of 10192), d = 10, its = 50
i = 6346 (of 10192), d = 4.00793, its = 9
i = 6347 (of 10192), d = 0.322342, its = 19
i = 6348 (of 10192), d = 10, its = 41
i = 6349 (of 10192), d = 10, its = 37
i = 6350 (of 10192), d = 10, its = 56
i = 6351 (of 10192), d = 10, its = 55
i = 6352 (of 10192), d = 10, its = 54
i = 6353 (of 10192), d = 10, its = 51
i = 6354 (of 10192), d = 0.771599, its = 20
i = 6355 (of 10192), d = 10, its = 50
i = 6356 (of 10192), d = 10, its = 55
i = 6357 (of 10192), d = 2.53817, its = 5
i = 6358 (of 10192), d = 10, its = 37
i = 6359 (of 10192), d = 10, its = 41
i = 6360 (of 10192), d = 10, its = 54
i = 6361 (of 10192), d = 1.98136, its = 3
i = 6362 (of 10192), d = 10, its = 51
i = 6363 (of 10192), d = 10, its = 37
i = 6364 (of 10192), d = 10, its = 52
i = 6365 (of 10192), d = 10, its = 53
i = 6366 (of 10192), d = 10, its = 46
i = 6367 (of 10192), d = 0.853344, its = 25
i = 6368 (of 10192), d = 10, its = 37
i = 6369 (of 10192), d = 10, its = 40
i = 6370 (of 10192), d = 10, its = 54
i = 6371 (of 10192), d = 10, its = 57
i = 6372 (of 10192), d = 10, its = 53
i = 6373 (of 10192), d = 10, its = 53
i = 6374 (of 10192), d = 3.41963, its = 6
i = 6375 (of 10192), d = 10, its = 37
i = 6376 (of 10192), d = 10, its = 54
i = 6377 (of 10192), d = 10, its = 37
i = 6378 (of 10192), d = 10, its = 42
i = 6379 (of 10192), d = 1.90885, its = 4
i = 6380 (of 10192), d = 0.674069, its = 16
i = 6381 (of 10192), d = 10, its = 56
i = 6382 (of 10192), d = 10, its = 55
i = 6383 (of 10192), d = 10, its = 54
i = 6384 (of 10192), d = 10, its = 54
i = 6385 (of 10192), d = 10, its = 46
i = 6386 (of 10192), d = 0.410957, its = 18
i = 6387 (of 10192), d = 10, its = 39
i = 6388 (of 10192), d = 10, its = 50
i = 6389 (of 10192), d = 2.14785, its = 4
i = 6390 (of 10192), d = 10, its = 55
i = 6391 (of 10192), d = 10, its = 37
i = 6392 (of 10192), d = 10, its = 54
i = 6393 (of 10192), d = 0.675883, its = 17
i = 6394 (of 10192), d = 10, its = 54
i = 6395 (of 10192), d = 10, its = 53
i = 6396 (of 10192), d = 0.587217, its = 17
i = 6397 (of 10192), d = 10, its = 52
i = 6398 (of 10192), d = 10, its = 52
i = 6399 (of 10192), d = 10, its = 51
i = 6400 (of 10192), d = 10, its = 51
i = 6401 (of 10192), d = 10, its = 42
i = 6402 (of 10192), d = 10, its = 52
i = 6403 (of 10192), d = 10, its = 50
i = 6404 (of 10192), d = 10, its = 55
i = 6405 (of 10192), d = 10, its = 48
i = 6406 (of 10192), d = 10, its = 50
i = 6407 (of 10192), d = 10, its = 53
i = 6408 (of 10192), d = 10, its = 57
i = 6409 (of 10192), d = 10, its = 50
i = 6410 (of 10192), d = 1.90811, its = 4
i = 6411 (of 10192), d = 10, its = 54
i = 6412 (of 10192), d = 10, its = 52
i = 6413 (of 10192), d = 10, its = 41
i = 6414 (of 10192), d = 10, its = 41
i = 6415 (of 10192), d = 10, its = 51
i = 6416 (of 10192), d = 10, its = 37
i = 6417 (of 10192), d = 2.16541, its = 4
i = 6418 (of 10192), d = 10, its = 50
i = 6419 (of 10192), d = 10, its = 53
i = 6420 (of 10192), d = 10, its = 37
i = 6421 (of 10192), d = 0.43599, its = 21
i = 6422 (of 10192), d = 10, its = 54
i = 6423 (of 10192), d = 10, its = 53
i = 6424 (of 10192), d = 0.672312, its = 22
i = 6425 (of 10192), d = 10, its = 52
i = 6426 (of 10192), d = 10, its = 55
i = 6427 (of 10192), d = 10, its = 53
i = 6428 (of 10192), d = 10, its = 40
i = 6429 (of 10192), d = 1.69021, its = 6
i = 6430 (of 10192), d = 10, its = 56
i = 6431 (of 10192), d = 10, its = 41
i = 6432 (of 10192), d = 0.40659, its = 18
i = 6433 (of 10192), d = 10, its = 37
i = 6434 (of 10192), d = 10, its = 54
i = 6435 (of 10192), d = 10, its = 39
i = 6436 (of 10192), d = 10, its = 54
i = 6437 (of 10192), d = 10, its = 37
i = 6438 (of 10192), d = 10, its = 40
i = 6439 (of 10192), d = 10, its = 55
i = 6440 (of 10192), d = 10, its = 46
i = 6441 (of 10192), d = 0.896997, its = 22
i = 6442 (of 10192), d = 10, its = 52
i = 6443 (of 10192), d = 1.81964, its = 4
i = 6444 (of 10192), d = 10, its = 37
i = 6445 (of 10192), d = 1.69342, its = 5
i = 6446 (of 10192), d = 10, its = 49
i = 6447 (of 10192), d = 10, its = 51
i = 6448 (of 10192), d = 1.80219, its = 4
i = 6449 (of 10192), d = 10, its = 37
i = 6450 (of 10192), d = 10, its = 37
i = 6451 (of 10192), d = 1.84166, its = 4
i = 6452 (of 10192), d = 1.70116, its = 5
i = 6453 (of 10192), d = 10, its = 40
i = 6454 (of 10192), d = 2.0979, its = 4
i = 6455 (of 10192), d = 0.677168, its = 16
i = 6456 (of 10192), d = 10, its = 56
i = 6457 (of 10192), d = 10, its = 52
i = 6458 (of 10192), d = 3.0861, its = 6
i = 6459 (of 10192), d = 0.619997, its = 18
i = 6460 (of 10192), d = 10, its = 55
i = 6461 (of 10192), d = 10, its = 50
i = 6462 (of 10192), d = 10, its = 54
i = 6463 (of 10192), d = 10, its = 37
i = 6464 (of 10192), d = 2.89822, its = 6
i = 6465 (of 10192), d = 10, its = 37
i = 6466 (of 10192), d = 9.37519, its = 8
i = 6467 (of 10192), d = 10, its = 49
i = 6468 (of 10192), d = 1.35968, its = 7
i = 6469 (of 10192), d = 10, its = 52
i = 6470 (of 10192), d = 10, its = 56
i = 6471 (of 10192), d = 10, its = 53
i = 6472 (of 10192), d = 10, its = 57
i = 6473 (of 10192), d = 0.802861, its = 17
i = 6474 (of 10192), d = 10, its = 57
i = 6475 (of 10192), d = 0.795029, its = 15
i = 6476 (of 10192), d = 10, its = 40
i = 6477 (of 10192), d = 10, its = 49
i = 6478 (of 10192), d = 10, its = 51
i = 6479 (of 10192), d = 10, its = 40
i = 6480 (of 10192), d = 10, its = 52
i = 6481 (of 10192), d = 10, its = 51
i = 6482 (of 10192), d = 7.09582, its = 8
i = 6483 (of 10192), d = 10, its = 51
i = 6484 (of 10192), d = 10, its = 54
i = 6485 (of 10192), d = 10, its = 52
i = 6486 (of 10192), d = 10, its = 57
i = 6487 (of 10192), d = 10, its = 56
i = 6488 (of 10192), d = 10, its = 53
i = 6489 (of 10192), d = 10, its = 57
i = 6490 (of 10192), d = 10, its = 40
i = 6491 (of 10192), d = 2.24688, its = 5
i = 6492 (of 10192), d = 10, its = 52
i = 6493 (of 10192), d = 4.96656, its = 8
i = 6494 (of 10192), d = 10, its = 40
i = 6495 (of 10192), d = 10, its = 54
i = 6496 (of 10192), d = 10, its = 49
i = 6497 (of 10192), d = 10, its = 50
i = 6498 (of 10192), d = 10, its = 52
i = 6499 (of 10192), d = 10, its = 42
i = 6500 (of 10192), d = 10, its = 55
i = 6501 (of 10192), d = 10, its = 52
i = 6502 (of 10192), d = 10, its = 57
i = 6503 (of 10192), d = 10, its = 54
i = 6504 (of 10192), d = 10, its = 53
i = 6505 (of 10192), d = 10, its = 54
i = 6506 (of 10192), d = 1.98564, its = 3
i = 6507 (of 10192), d = 0.414551, its = 18
i = 6508 (of 10192), d = 10, its = 57
i = 6509 (of 10192), d = 10, its = 54
i = 6510 (of 10192), d = 10, its = 54
i = 6511 (of 10192), d = 5.01989, its = 7
i = 6512 (of 10192), d = 10, its = 55
i = 6513 (of 10192), d = 2.24682, its = 4
i = 6514 (of 10192), d = 10, its = 53
i = 6515 (of 10192), d = 10, its = 56
i = 6516 (of 10192), d = 10, its = 53
i = 6517 (of 10192), d = 10, its = 57
i = 6518 (of 10192), d = 1.90878, its = 4
i = 6519 (of 10192), d = 10, its = 51
i = 6520 (of 10192), d = 10, its = 51
i = 6521 (of 10192), d = 10, its = 54
i = 6522 (of 10192), d = 10, its = 53
i = 6523 (of 10192), d = 10, its = 37
i = 6524 (of 10192), d = 10, its = 40
i = 6525 (of 10192), d = 10, its = 51
i = 6526 (of 10192), d = 10, its = 37
i = 6527 (of 10192), d = 2.43991, its = 5
i = 6528 (of 10192), d = 1.86179, its = 4
i = 6529 (of 10192), d = 1.50074, its = 6
i = 6530 (of 10192), d = 1.21019, its = 28
i = 6531 (of 10192), d = 0.879688, its = 28
i = 6532 (of 10192), d = 2.80039, its = 5
i = 6533 (of 10192), d = 10, its = 44
i = 6534 (of 10192), d = 10, its = 37
i = 6535 (of 10192), d = 10, its = 58
i = 6536 (of 10192), d = 10, its = 50
i = 6537 (of 10192), d = 1.63454, its = 5
i = 6538 (of 10192), d = 10, its = 48
i = 6539 (of 10192), d = 10, its = 51
i = 6540 (of 10192), d = 2.80527, its = 6
i = 6541 (of 10192), d = 10, its = 52
i = 6542 (of 10192), d = 1.0124, its = 17
i = 6543 (of 10192), d = 10, its = 54
i = 6544 (of 10192), d = 2.75098, its = 6
i = 6545 (of 10192), d = 10, its = 54
i = 6546 (of 10192), d = 10, its = 40
i = 6547 (of 10192), d = 1.2856, its = 6
i = 6548 (of 10192), d = 10, its = 55
i = 6549 (of 10192), d = 10, its = 37
i = 6550 (of 10192), d = 10, its = 54
i = 6551 (of 10192), d = 10, its = 53
i = 6552 (of 10192), d = 10, its = 50
i = 6553 (of 10192), d = 10, its = 40
i = 6554 (of 10192), d = 10, its = 55
i = 6555 (of 10192), d = 10, its = 37
i = 6556 (of 10192), d = 2.17202, its = 4
i = 6557 (of 10192), d = 10, its = 41
i = 6558 (of 10192), d = 10, its = 37
i = 6559 (of 10192), d = 10, its = 44
i = 6560 (of 10192), d = 10, its = 37
i = 6561 (of 10192), d = 10, its = 40
i = 6562 (of 10192), d = 10, its = 52
i = 6563 (of 10192), d = 10, its = 55
i = 6564 (of 10192), d = 1.52528, its = 6
i = 6565 (of 10192), d = 10, its = 51
i = 6566 (of 10192), d = 10, its = 54
i = 6567 (of 10192), d = 10, its = 43
i = 6568 (of 10192), d = 10, its = 37
i = 6569 (of 10192), d = 1.90742, its = 4
i = 6570 (of 10192), d = 10, its = 51
i = 6571 (of 10192), d = 10, its = 40
i = 6572 (of 10192), d = 10, its = 53
i = 6573 (of 10192), d = 10, its = 37
i = 6574 (of 10192), d = 10, its = 44
i = 6575 (of 10192), d = 10, its = 58
i = 6576 (of 10192), d = 10, its = 55
i = 6577 (of 10192), d = 10, its = 49
i = 6578 (of 10192), d = 10, its = 37
i = 6579 (of 10192), d = 10, its = 39
i = 6580 (of 10192), d = 10, its = 52
i = 6581 (of 10192), d = 10, its = 51
i = 6582 (of 10192), d = 10, its = 37
i = 6583 (of 10192), d = 10, its = 46
i = 6584 (of 10192), d = 10, its = 37
i = 6585 (of 10192), d = 10, its = 43
i = 6586 (of 10192), d = 10, its = 38
i = 6587 (of 10192), d = 10, its = 53
i = 6588 (of 10192), d = 10, its = 54
i = 6589 (of 10192), d = 10, its = 56
i = 6590 (of 10192), d = 10, its = 39
i = 6591 (of 10192), d = 1.02487, its = 16
i = 6592 (of 10192), d = 10, its = 51
i = 6593 (of 10192), d = 5.41095, its = 7
i = 6594 (of 10192), d = 10, its = 50
i = 6595 (of 10192), d = 10, its = 53
i = 6596 (of 10192), d = 10, its = 55
i = 6597 (of 10192), d = 2.40757, its = 5
i = 6598 (of 10192), d = 10, its = 51
i = 6599 (of 10192), d = 10, its = 57
i = 6600 (of 10192), d = 0.5931, its = 17
i = 6601 (of 10192), d = 2.67217, its = 5
i = 6602 (of 10192), d = 10, its = 55
i = 6603 (of 10192), d = 10, its = 56
i = 6604 (of 10192), d = 1.13257, its = 18
i = 6605 (of 10192), d = 10, its = 50
i = 6606 (of 10192), d = 1.39647, its = 8
i = 6607 (of 10192), d = 10, its = 52
i = 6608 (of 10192), d = 10, its = 49
i = 6609 (of 10192), d = 3.82122, its = 7
i = 6610 (of 10192), d = 10, its = 52
i = 6611 (of 10192), d = 10, its = 41
i = 6612 (of 10192), d = 10, its = 54
i = 6613 (of 10192), d = 2.20762, its = 4
i = 6614 (of 10192), d = 2.36813, its = 4
i = 6615 (of 10192), d = 10, its = 54
i = 6616 (of 10192), d = 10, its = 45
i = 6617 (of 10192), d = 10, its = 50
i = 6618 (of 10192), d = 2.13574, its = 4
i = 6619 (of 10192), d = 1.59107, its = 6
i = 6620 (of 10192), d = 10, its = 49
i = 6621 (of 10192), d = 10, its = 54
i = 6622 (of 10192), d = 10, its = 56
i = 6623 (of 10192), d = 10, its = 53
i = 6624 (of 10192), d = 10, its = 49
i = 6625 (of 10192), d = 1.98101, its = 3
i = 6626 (of 10192), d = 1.96192, its = 3
i = 6627 (of 10192), d = 10, its = 55
i = 6628 (of 10192), d = 10, its = 42
i = 6629 (of 10192), d = 10, its = 53
i = 6630 (of 10192), d = 10, its = 56
i = 6631 (of 10192), d = 10, its = 51
i = 6632 (of 10192), d = 10, its = 52
i = 6633 (of 10192), d = 10, its = 49
i = 6634 (of 10192), d = 1.09042, its = 26
i = 6635 (of 10192), d = 10, its = 51
i = 6636 (of 10192), d = 10, its = 52
i = 6637 (of 10192), d = 0.942124, its = 20
i = 6638 (of 10192), d = 10, its = 53
i = 6639 (of 10192), d = 10, its = 50
i = 6640 (of 10192), d = 10, its = 41
i = 6641 (of 10192), d = 10, its = 50
i = 6642 (of 10192), d = 10, its = 38
i = 6643 (of 10192), d = 10, its = 56
i = 6644 (of 10192), d = 10, its = 51
i = 6645 (of 10192), d = 10, its = 50
i = 6646 (of 10192), d = 2.00798, its = 3
i = 6647 (of 10192), d = 2.41284, its = 5
i = 6648 (of 10192), d = 10, its = 48
i = 6649 (of 10192), d = 10, its = 49
i = 6650 (of 10192), d = 10, its = 51
i = 6651 (of 10192), d = 1.86179, its = 4
i = 6652 (of 10192), d = 10, its = 16
i = 6653 (of 10192), d = 10, its = 53
i = 6654 (of 10192), d = 10, its = 50
i = 6655 (of 10192), d = 10, its = 50
i = 6656 (of 10192), d = 10, its = 51
i = 6657 (of 10192), d = 1.54578, its = 6
i = 6658 (of 10192), d = 10, its = 49
i = 6659 (of 10192), d = 0.933283, its = 20
i = 6660 (of 10192), d = 10, its = 52
i = 6661 (of 10192), d = 10, its = 54
i = 6662 (of 10192), d = 10, its = 52
i = 6663 (of 10192), d = 10, its = 53
i = 6664 (of 10192), d = 4.00793, its = 9
i = 6665 (of 10192), d = 10, its = 37
i = 6666 (of 10192), d = 2.09577, its = 4
i = 6667 (of 10192), d = 0.779151, its = 18
i = 6668 (of 10192), d = 1.72377, its = 5
i = 6669 (of 10192), d = 10, its = 52
i = 6670 (of 10192), d = 10, its = 39
i = 6671 (of 10192), d = 10, its = 40
i = 6672 (of 10192), d = 10, its = 41
i = 6673 (of 10192), d = 10, its = 39
i = 6674 (of 10192), d = 0.902309, its = 19
i = 6675 (of 10192), d = 2.17725, its = 5
i = 6676 (of 10192), d = 10, its = 39
i = 6677 (of 10192), d = 10, its = 37
i = 6678 (of 10192), d = 10, its = 51
i = 6679 (of 10192), d = 10, its = 39
i = 6680 (of 10192), d = 10, its = 55
i = 6681 (of 10192), d = 2.01136, its = 3
i = 6682 (of 10192), d = 0.692327, its = 18
i = 6683 (of 10192), d = 10, its = 54
i = 6684 (of 10192), d = 1.89011, its = 4
i = 6685 (of 10192), d = 1.03347, its = 17
i = 6686 (of 10192), d = 10, its = 50
i = 6687 (of 10192), d = 10, its = 54
i = 6688 (of 10192), d = 10, its = 54
i = 6689 (of 10192), d = 10, its = 37
i = 6690 (of 10192), d = 10, its = 53
i = 6691 (of 10192), d = 3.7248, its = 7
i = 6692 (of 10192), d = 10, its = 55
i = 6693 (of 10192), d = 10, its = 39
i = 6694 (of 10192), d = 10, its = 37
i = 6695 (of 10192), d = 10, its = 55
i = 6696 (of 10192), d = 10, its = 52
i = 6697 (of 10192), d = 1.3395, its = 6
i = 6698 (of 10192), d = 10, its = 46
i = 6699 (of 10192), d = 10, its = 52
i = 6700 (of 10192), d = 10, its = 51
i = 6701 (of 10192), d = 10, its = 42
i = 6702 (of 10192), d = 10, its = 52
i = 6703 (of 10192), d = 10, its = 53
i = 6704 (of 10192), d = 3.04015, its = 6
i = 6705 (of 10192), d = 10, its = 37
i = 6706 (of 10192), d = 10, its = 52
i = 6707 (of 10192), d = 9.46381, its = 9
i = 6708 (of 10192), d = 10, its = 39
i = 6709 (of 10192), d = 10, its = 52
i = 6710 (of 10192), d = 10, its = 37
i = 6711 (of 10192), d = 10, its = 37
i = 6712 (of 10192), d = 10, its = 52
i = 6713 (of 10192), d = 10, its = 56
i = 6714 (of 10192), d = 4.20503, its = 7
i = 6715 (of 10192), d = 10, its = 51
i = 6716 (of 10192), d = 10, its = 53
i = 6717 (of 10192), d = 10, its = 55
i = 6718 (of 10192), d = 3.10388, its = 6
i = 6719 (of 10192), d = 10, its = 49
i = 6720 (of 10192), d = 10, its = 53
i = 6721 (of 10192), d = 10, its = 57
i = 6722 (of 10192), d = 10, its = 52
i = 6723 (of 10192), d = 10, its = 55
i = 6724 (of 10192), d = 1.68711, its = 5
i = 6725 (of 10192), d = 10, its = 47
i = 6726 (of 10192), d = 10, its = 37
i = 6727 (of 10192), d = 10, its = 56
i = 6728 (of 10192), d = 10, its = 55
i = 6729 (of 10192), d = 10, its = 51
i = 6730 (of 10192), d = 10, its = 54
i = 6731 (of 10192), d = 10, its = 49
i = 6732 (of 10192), d = 10, its = 52
i = 6733 (of 10192), d = 10, its = 37
i = 6734 (of 10192), d = 10, its = 55
i = 6735 (of 10192), d = 0.559619, its = 18
i = 6736 (of 10192), d = 10, its = 37
i = 6737 (of 10192), d = 10, its = 60
i = 6738 (of 10192), d = 10, its = 54
i = 6739 (of 10192), d = 1.69937, its = 5
i = 6740 (of 10192), d = 10, its = 54
i = 6741 (of 10192), d = 10, its = 54
i = 6742 (of 10192), d = 0.539257, its = 18
i = 6743 (of 10192), d = 10, its = 53
i = 6744 (of 10192), d = 10, its = 52
i = 6745 (of 10192), d = 10, its = 53
i = 6746 (of 10192), d = 2.54718, its = 5
i = 6747 (of 10192), d = 10, its = 39
i = 6748 (of 10192), d = 10, its = 37
i = 6749 (of 10192), d = 10, its = 58
i = 6750 (of 10192), d = 10, its = 54
i = 6751 (of 10192), d = 10, its = 41
i = 6752 (of 10192), d = 4.19758, its = 7
i = 6753 (of 10192), d = 10, its = 56
i = 6754 (of 10192), d = 10, its = 44
i = 6755 (of 10192), d = 1.75325, its = 5
i = 6756 (of 10192), d = 0.640663, its = 18
i = 6757 (of 10192), d = 10, its = 53
i = 6758 (of 10192), d = 3.46747, its = 6
i = 6759 (of 10192), d = 10, its = 40
i = 6760 (of 10192), d = 10, its = 55
i = 6761 (of 10192), d = 10, its = 37
i = 6762 (of 10192), d = 4.58552, its = 7
i = 6763 (of 10192), d = 10, its = 51
i = 6764 (of 10192), d = 10, its = 44
i = 6765 (of 10192), d = 1.43758, its = 8
i = 6766 (of 10192), d = 10, its = 54
i = 6767 (of 10192), d = 10, its = 55
i = 6768 (of 10192), d = 10, its = 52
i = 6769 (of 10192), d = 10, its = 51
i = 6770 (of 10192), d = 10, its = 43
i = 6771 (of 10192), d = 10, its = 53
i = 6772 (of 10192), d = 10, its = 41
i = 6773 (of 10192), d = 1.28463, its = 26
i = 6774 (of 10192), d = 0.835721, its = 17
i = 6775 (of 10192), d = 10, its = 44
i = 6776 (of 10192), d = 10, its = 55
i = 6777 (of 10192), d = 10, its = 55
i = 6778 (of 10192), d = 10, its = 37
i = 6779 (of 10192), d = 10, its = 54
i = 6780 (of 10192), d = 10, its = 52
i = 6781 (of 10192), d = 10, its = 51
i = 6782 (of 10192), d = 0.830959, its = 20
i = 6783 (of 10192), d = 10, its = 52
i = 6784 (of 10192), d = 10, its = 52
i = 6785 (of 10192), d = 10, its = 52
i = 6786 (of 10192), d = 10, its = 53
i = 6787 (of 10192), d = 10, its = 51
i = 6788 (of 10192), d = 10, its = 55
i = 6789 (of 10192), d = 10, its = 55
i = 6790 (of 10192), d = 10, its = 50
i = 6791 (of 10192), d = 10, its = 41
i = 6792 (of 10192), d = 1.37569, its = 6
i = 6793 (of 10192), d = 10, its = 37
i = 6794 (of 10192), d = 10, its = 53
i = 6795 (of 10192), d = 1.60881, its = 6
i = 6796 (of 10192), d = 10, its = 47
i = 6797 (of 10192), d = 2.51002, its = 5
i = 6798 (of 10192), d = 1.85093, its = 4
i = 6799 (of 10192), d = 10, its = 55
i = 6800 (of 10192), d = 10, its = 41
i = 6801 (of 10192), d = 10, its = 39
i = 6802 (of 10192), d = 1.759, its = 5
i = 6803 (of 10192), d = 10, its = 40
i = 6804 (of 10192), d = 10, its = 56
i = 6805 (of 10192), d = 1.46252, its = 8
i = 6806 (of 10192), d = 10, its = 43
i = 6807 (of 10192), d = 10, its = 41
i = 6808 (of 10192), d = 10, its = 53
i = 6809 (of 10192), d = 10, its = 58
i = 6810 (of 10192), d = 10, its = 56
i = 6811 (of 10192), d = 10, its = 51
i = 6812 (of 10192), d = 10, its = 40
i = 6813 (of 10192), d = 10, its = 52
i = 6814 (of 10192), d = 10, its = 53
i = 6815 (of 10192), d = 10, its = 53
i = 6816 (of 10192), d = 1.98575, its = 3
i = 6817 (of 10192), d = 2.08889, its = 4
i = 6818 (of 10192), d = 2.53726, its = 5
i = 6819 (of 10192), d = 10, its = 53
i = 6820 (of 10192), d = 10, its = 47
i = 6821 (of 10192), d = 10, its = 56
i = 6822 (of 10192), d = 10, its = 55
i = 6823 (of 10192), d = 1.33947, its = 6
i = 6824 (of 10192), d = 10, its = 51
i = 6825 (of 10192), d = 2.58636, its = 5
i = 6826 (of 10192), d = 2.41312, its = 5
i = 6827 (of 10192), d = 10, its = 52
i = 6828 (of 10192), d = 10, its = 52
i = 6829 (of 10192), d = 10, its = 51
i = 6830 (of 10192), d = 10, its = 58
i = 6831 (of 10192), d = 10, its = 51
i = 6832 (of 10192), d = 10, its = 52
i = 6833 (of 10192), d = 10, its = 51
i = 6834 (of 10192), d = 10, its = 49
i = 6835 (of 10192), d = 10, its = 50
i = 6836 (of 10192), d = 2.74846, its = 6
i = 6837 (of 10192), d = 10, its = 56
i = 6838 (of 10192), d = 10, its = 40
i = 6839 (of 10192), d = 10, its = 53
i = 6840 (of 10192), d = 10, its = 53
i = 6841 (of 10192), d = 10, its = 55
i = 6842 (of 10192), d = 10, its = 50
i = 6843 (of 10192), d = 10, its = 51
i = 6844 (of 10192), d = 10, its = 51
i = 6845 (of 10192), d = 10, its = 53
i = 6846 (of 10192), d = 10, its = 37
i = 6847 (of 10192), d = 10, its = 51
i = 6848 (of 10192), d = 10, its = 43
i = 6849 (of 10192), d = 10, its = 40
i = 6850 (of 10192), d = 10, its = 40
i = 6851 (of 10192), d = 3.68573, its = 7
i = 6852 (of 10192), d = 10, its = 41
i = 6853 (of 10192), d = 0.88862, its = 20
i = 6854 (of 10192), d = 0.551077, its = 16
i = 6855 (of 10192), d = 10, its = 49
i = 6856 (of 10192), d = 10, its = 53
i = 6857 (of 10192), d = 10, its = 37
i = 6858 (of 10192), d = 1.14679, its = 7
i = 6859 (of 10192), d = 10, its = 58
i = 6860 (of 10192), d = 10, its = 37
i = 6861 (of 10192), d = 10, its = 53
i = 6862 (of 10192), d = 2.17641, its = 5
i = 6863 (of 10192), d = 10, its = 52
i = 6864 (of 10192), d = 10, its = 58
i = 6865 (of 10192), d = 0.710213, its = 24
i = 6866 (of 10192), d = 10, its = 51
i = 6867 (of 10192), d = 10, its = 51
i = 6868 (of 10192), d = 1.84385, its = 4
i = 6869 (of 10192), d = 10, its = 44
i = 6870 (of 10192), d = 10, its = 53
i = 6871 (of 10192), d = 10, its = 56
i = 6872 (of 10192), d = 8.46289, its = 23
i = 6873 (of 10192), d = 10, its = 54
i = 6874 (of 10192), d = 10, its = 59
i = 6875 (of 10192), d = 10, its = 53
i = 6876 (of 10192), d = 10, its = 55
i = 6877 (of 10192), d = 10, its = 41
i = 6878 (of 10192), d = 1.40932, its = 6
i = 6879 (of 10192), d = 10, its = 51
i = 6880 (of 10192), d = 1.55863, its = 6
i = 6881 (of 10192), d = 4.79855, its = 7
i = 6882 (of 10192), d = 3.253, its = 7
i = 6883 (of 10192), d = 10, its = 52
i = 6884 (of 10192), d = 10, its = 53
i = 6885 (of 10192), d = 10, its = 56
i = 6886 (of 10192), d = 10, its = 54
i = 6887 (of 10192), d = 10, its = 40
i = 6888 (of 10192), d = 10, its = 49
i = 6889 (of 10192), d = 10, its = 53
i = 6890 (of 10192), d = 10, its = 37
i = 6891 (of 10192), d = 1.88593, its = 4
i = 6892 (of 10192), d = 10, its = 49
i = 6893 (of 10192), d = 10, its = 53
i = 6894 (of 10192), d = 10, its = 49
i = 6895 (of 10192), d = 1.57214, its = 6
i = 6896 (of 10192), d = 10, its = 49
i = 6897 (of 10192), d = 0.411331, its = 20
i = 6898 (of 10192), d = 10, its = 41
i = 6899 (of 10192), d = 0.890247, its = 19
i = 6900 (of 10192), d = 10, its = 51
i = 6901 (of 10192), d = 10, its = 52
i = 6902 (of 10192), d = 10, its = 51
i = 6903 (of 10192), d = 10, its = 55
i = 6904 (of 10192), d = 10, its = 54
i = 6905 (of 10192), d = 10, its = 51
i = 6906 (of 10192), d = 10, its = 53
i = 6907 (of 10192), d = 10, its = 51
i = 6908 (of 10192), d = 1.09832, its = 18
i = 6909 (of 10192), d = 1.79373, its = 5
i = 6910 (of 10192), d = 10, its = 40
i = 6911 (of 10192), d = 10, its = 37
i = 6912 (of 10192), d = 10, its = 37
i = 6913 (of 10192), d = 10, its = 51
i = 6914 (of 10192), d = 10, its = 54
i = 6915 (of 10192), d = 10, its = 53
i = 6916 (of 10192), d = 2.40379, its = 5
i = 6917 (of 10192), d = 10, its = 56
i = 6918 (of 10192), d = 10, its = 53
i = 6919 (of 10192), d = 10, its = 56
i = 6920 (of 10192), d = 0.79412, its = 21
i = 6921 (of 10192), d = 1.50627, its = 7
i = 6922 (of 10192), d = 10, its = 54
i = 6923 (of 10192), d = 0.922179, its = 19
i = 6924 (of 10192), d = 10, its = 50
i = 6925 (of 10192), d = 10, its = 37
i = 6926 (of 10192), d = 3.78323, its = 7
i = 6927 (of 10192), d = 10, its = 53
i = 6928 (of 10192), d = 10, its = 40
i = 6929 (of 10192), d = 10, its = 37
i = 6930 (of 10192), d = 10, its = 55
i = 6931 (of 10192), d = 8.03702, its = 18
i = 6932 (of 10192), d = 10, its = 41
i = 6933 (of 10192), d = 10, its = 54
i = 6934 (of 10192), d = 1.00929, its = 20
i = 6935 (of 10192), d = 10, its = 53
i = 6936 (of 10192), d = 10, its = 50
i = 6937 (of 10192), d = 10, its = 37
i = 6938 (of 10192), d = 10, its = 56
i = 6939 (of 10192), d = 10, its = 37
i = 6940 (of 10192), d = 10, its = 39
i = 6941 (of 10192), d = 10, its = 48
i = 6942 (of 10192), d = 2.62667, its = 5
i = 6943 (of 10192), d = 10, its = 56
i = 6944 (of 10192), d = 10, its = 51
i = 6945 (of 10192), d = 10, its = 53
i = 6946 (of 10192), d = 10, its = 55
i = 6947 (of 10192), d = 10, its = 40
i = 6948 (of 10192), d = 10, its = 52
i = 6949 (of 10192), d = 1.75996, its = 8
i = 6950 (of 10192), d = 10, its = 52
i = 6951 (of 10192), d = 10, its = 52
i = 6952 (of 10192), d = 10, its = 55
i = 6953 (of 10192), d = 10, its = 50
i = 6954 (of 10192), d = 10, its = 52
i = 6955 (of 10192), d = 10, its = 52
i = 6956 (of 10192), d = 10, its = 48
i = 6957 (of 10192), d = 1.48751, its = 7
i = 6958 (of 10192), d = 10, its = 37
i = 6959 (of 10192), d = 2.29658, its = 4
i = 6960 (of 10192), d = 10, its = 54
i = 6961 (of 10192), d = 10, its = 43
i = 6962 (of 10192), d = 10, its = 56
i = 6963 (of 10192), d = 10, its = 52
i = 6964 (of 10192), d = 10, its = 37
i = 6965 (of 10192), d = 10, its = 39
i = 6966 (of 10192), d = 10, its = 43
i = 6967 (of 10192), d = 10, its = 51
i = 6968 (of 10192), d = 10, its = 55
i = 6969 (of 10192), d = 3.08387, its = 8
i = 6970 (of 10192), d = 10, its = 48
i = 6971 (of 10192), d = 10, its = 37
i = 6972 (of 10192), d = 10, its = 54
i = 6973 (of 10192), d = 10, its = 37
i = 6974 (of 10192), d = 10, its = 49
i = 6975 (of 10192), d = 10, its = 43
i = 6976 (of 10192), d = 10, its = 51
i = 6977 (of 10192), d = 10, its = 49
i = 6978 (of 10192), d = 10, its = 51
i = 6979 (of 10192), d = 10, its = 53
i = 6980 (of 10192), d = 10, its = 37
i = 6981 (of 10192), d = 10, its = 54
i = 6982 (of 10192), d = 1.99538, its = 2
i = 6983 (of 10192), d = 10, its = 54
i = 6984 (of 10192), d = 10, its = 58
i = 6985 (of 10192), d = 10, its = 50
i = 6986 (of 10192), d = 10, its = 54
i = 6987 (of 10192), d = 2.29213, its = 4
i = 6988 (of 10192), d = 10, its = 49
i = 6989 (of 10192), d = 10, its = 50
i = 6990 (of 10192), d = 1.90942, its = 4
i = 6991 (of 10192), d = 10, its = 52
i = 6992 (of 10192), d = 10, its = 37
i = 6993 (of 10192), d = 10, its = 52
i = 6994 (of 10192), d = 10, its = 51
i = 6995 (of 10192), d = 10, its = 48
i = 6996 (of 10192), d = 10, its = 53
i = 6997 (of 10192), d = 1.5438, its = 6
i = 6998 (of 10192), d = 10, its = 51
i = 6999 (of 10192), d = 10, its = 43
i = 7000 (of 10192), d = 1.2715, its = 21
i = 7001 (of 10192), d = 10, its = 40
i = 7002 (of 10192), d = 10, its = 52
i = 7003 (of 10192), d = 10, its = 52
i = 7004 (of 10192), d = 0.887154, its = 18
i = 7005 (of 10192), d = 10, its = 52
i = 7006 (of 10192), d = 10, its = 46
i = 7007 (of 10192), d = 3.59692, its = 6
i = 7008 (of 10192), d = 10, its = 54
i = 7009 (of 10192), d = 10, its = 37
i = 7010 (of 10192), d = 10, its = 49
i = 7011 (of 10192), d = 10, its = 55
i = 7012 (of 10192), d = 0.514213, its = 18
i = 7013 (of 10192), d = 10, its = 50
i = 7014 (of 10192), d = 10, its = 37
i = 7015 (of 10192), d = 10, its = 39
i = 7016 (of 10192), d = 10, its = 37
i = 7017 (of 10192), d = 10, its = 53
i = 7018 (of 10192), d = 10, its = 44
i = 7019 (of 10192), d = 10, its = 54
i = 7020 (of 10192), d = 10, its = 54
i = 7021 (of 10192), d = 10, its = 41
i = 7022 (of 10192), d = 10, its = 40
i = 7023 (of 10192), d = 10, its = 56
i = 7024 (of 10192), d = 10, its = 57
i = 7025 (of 10192), d = 10, its = 50
i = 7026 (of 10192), d = 0.879498, its = 18
i = 7027 (of 10192), d = 10, its = 56
i = 7028 (of 10192), d = 0.693109, its = 18
i = 7029 (of 10192), d = 10, its = 53
i = 7030 (of 10192), d = 10, its = 37
i = 7031 (of 10192), d = 10, its = 52
i = 7032 (of 10192), d = 10, its = 38
i = 7033 (of 10192), d = 10, its = 51
i = 7034 (of 10192), d = 10, its = 38
i = 7035 (of 10192), d = 2.53235, its = 5
i = 7036 (of 10192), d = 10, its = 54
i = 7037 (of 10192), d = 2.5236, its = 6
i = 7038 (of 10192), d = 2.78547, its = 6
i = 7039 (of 10192), d = 10, its = 51
i = 7040 (of 10192), d = 10, its = 52
i = 7041 (of 10192), d = 10, its = 37
i = 7042 (of 10192), d = 10, its = 39
i = 7043 (of 10192), d = 10, its = 37
i = 7044 (of 10192), d = 10, its = 52
i = 7045 (of 10192), d = 10, its = 56
i = 7046 (of 10192), d = 10, its = 37
i = 7047 (of 10192), d = 10, its = 52
i = 7048 (of 10192), d = 10, its = 41
i = 7049 (of 10192), d = 3.47986, its = 7
i = 7050 (of 10192), d = 10, its = 39
i = 7051 (of 10192), d = 1.96738, its = 3
i = 7052 (of 10192), d = 2.76815, its = 9
i = 7053 (of 10192), d = 10, its = 51
i = 7054 (of 10192), d = 10, its = 37
i = 7055 (of 10192), d = 10, its = 52
i = 7056 (of 10192), d = 10, its = 37
i = 7057 (of 10192), d = 10, its = 41
i = 7058 (of 10192), d = 0.874317, its = 28
i = 7059 (of 10192), d = 1.85078, its = 4
i = 7060 (of 10192), d = 10, its = 51
i = 7061 (of 10192), d = 10, its = 50
i = 7062 (of 10192), d = 10, its = 52
i = 7063 (of 10192), d = 10, its = 56
i = 7064 (of 10192), d = 10, its = 51
i = 7065 (of 10192), d = 1.3085, its = 6
i = 7066 (of 10192), d = 10, its = 54
i = 7067 (of 10192), d = 10, its = 40
i = 7068 (of 10192), d = 10, its = 37
i = 7069 (of 10192), d = 1.93983, its = 4
i = 7070 (of 10192), d = 10, its = 56
i = 7071 (of 10192), d = 10, its = 47
i = 7072 (of 10192), d = 1.39806, its = 7
i = 7073 (of 10192), d = 3.24395, its = 6
i = 7074 (of 10192), d = 10, its = 40
i = 7075 (of 10192), d = 10, its = 54
i = 7076 (of 10192), d = 1.71145, its = 5
i = 7077 (of 10192), d = 10, its = 54
i = 7078 (of 10192), d = 10, its = 40
i = 7079 (of 10192), d = 10, its = 51
i = 7080 (of 10192), d = 2.14953, its = 4
i = 7081 (of 10192), d = 10, its = 39
i = 7082 (of 10192), d = 10, its = 54
i = 7083 (of 10192), d = 10, its = 48
i = 7084 (of 10192), d = 10, its = 53
i = 7085 (of 10192), d = 10, its = 50
i = 7086 (of 10192), d = 10, its = 53
i = 7087 (of 10192), d = 2.99133, its = 6
i = 7088 (of 10192), d = 2.15417, its = 4
i = 7089 (of 10192), d = 10, its = 49
i = 7090 (of 10192), d = 10, its = 48
i = 7091 (of 10192), d = 10, its = 55
i = 7092 (of 10192), d = 10, its = 51
i = 7093 (of 10192), d = 10, its = 52
i = 7094 (of 10192), d = 10, its = 57
i = 7095 (of 10192), d = 10, its = 37
i = 7096 (of 10192), d = 10, its = 51
i = 7097 (of 10192), d = 10, its = 54
i = 7098 (of 10192), d = 10, its = 55
i = 7099 (of 10192), d = 10, its = 57
i = 7100 (of 10192), d = 1.12301, its = 18
i = 7101 (of 10192), d = 10, its = 55
i = 7102 (of 10192), d = 10, its = 56
i = 7103 (of 10192), d = 10, its = 41
i = 7104 (of 10192), d = 10, its = 37
i = 7105 (of 10192), d = 10, its = 39
i = 7106 (of 10192), d = 2.2888, its = 4
i = 7107 (of 10192), d = 10, its = 37
i = 7108 (of 10192), d = 3.7248, its = 7
i = 7109 (of 10192), d = 10, its = 51
i = 7110 (of 10192), d = 2.93604, its = 7
i = 7111 (of 10192), d = 10, its = 55
i = 7112 (of 10192), d = 2.34836, its = 4
i = 7113 (of 10192), d = 10, its = 37
i = 7114 (of 10192), d = 10, its = 51
i = 7115 (of 10192), d = 10, its = 51
i = 7116 (of 10192), d = 10, its = 55
i = 7117 (of 10192), d = 10, its = 37
i = 7118 (of 10192), d = 10, its = 57
i = 7119 (of 10192), d = 1.63229, its = 5
i = 7120 (of 10192), d = 10, its = 40
i = 7121 (of 10192), d = 2.33402, its = 4
i = 7122 (of 10192), d = 2.05556, its = 4
i = 7123 (of 10192), d = 1.73039, its = 5
i = 7124 (of 10192), d = 0.530037, its = 17
i = 7125 (of 10192), d = 10, its = 37
i = 7126 (of 10192), d = 10, its = 37
i = 7127 (of 10192), d = 10, its = 56
i = 7128 (of 10192), d = 10, its = 53
i = 7129 (of 10192), d = 3.23171, its = 7
i = 7130 (of 10192), d = 1.96135, its = 3
i = 7131 (of 10192), d = 2.40907, its = 5
i = 7132 (of 10192), d = 10, its = 51
i = 7133 (of 10192), d = 1.13786, its = 20
i = 7134 (of 10192), d = 1.84224, its = 4
i = 7135 (of 10192), d = 10, its = 51
i = 7136 (of 10192), d = 0.8826, its = 29
i = 7137 (of 10192), d = 2.33489, its = 6
i = 7138 (of 10192), d = 10, its = 40
i = 7139 (of 10192), d = 10, its = 40
i = 7140 (of 10192), d = 10, its = 42
i = 7141 (of 10192), d = 10, its = 51
i = 7142 (of 10192), d = 2.13619, its = 4
i = 7143 (of 10192), d = 10, its = 37
i = 7144 (of 10192), d = 10, its = 56
i = 7145 (of 10192), d = 10, its = 55
i = 7146 (of 10192), d = 10, its = 37
i = 7147 (of 10192), d = 10, its = 52
i = 7148 (of 10192), d = 10, its = 54
i = 7149 (of 10192), d = 10, its = 37
i = 7150 (of 10192), d = 10, its = 54
i = 7151 (of 10192), d = 10, its = 51
i = 7152 (of 10192), d = 10, its = 48
i = 7153 (of 10192), d = 10, its = 40
i = 7154 (of 10192), d = 2.50688, its = 6
i = 7155 (of 10192), d = 10, its = 39
i = 7156 (of 10192), d = 10, its = 37
i = 7157 (of 10192), d = 10, its = 37
i = 7158 (of 10192), d = 3.06319, its = 6
i = 7159 (of 10192), d = 10, its = 37
i = 7160 (of 10192), d = 10, its = 37
i = 7161 (of 10192), d = 10, its = 54
i = 7162 (of 10192), d = 10, its = 56
i = 7163 (of 10192), d = 10, its = 40
i = 7164 (of 10192), d = 10, its = 37
i = 7165 (of 10192), d = 10, its = 37
i = 7166 (of 10192), d = 10, its = 52
i = 7167 (of 10192), d = 1.67381, its = 6
i = 7168 (of 10192), d = 10, its = 51
i = 7169 (of 10192), d = 10, its = 50
i = 7170 (of 10192), d = 1.53813, its = 6
i = 7171 (of 10192), d = 10, its = 54
i = 7172 (of 10192), d = 2.26131, its = 5
i = 7173 (of 10192), d = 10, its = 45
i = 7174 (of 10192), d = 10, its = 51
i = 7175 (of 10192), d = 10, its = 50
i = 7176 (of 10192), d = 10, its = 58
i = 7177 (of 10192), d = 2.3547, its = 5
i = 7178 (of 10192), d = 10, its = 51
i = 7179 (of 10192), d = 10, its = 48
i = 7180 (of 10192), d = 10, its = 55
i = 7181 (of 10192), d = 10, its = 50
i = 7182 (of 10192), d = 10, its = 37
i = 7183 (of 10192), d = 10, its = 49
i = 7184 (of 10192), d = 10, its = 52
i = 7185 (of 10192), d = 10, its = 55
i = 7186 (of 10192), d = 10, its = 51
i = 7187 (of 10192), d = 0.984328, its = 19
i = 7188 (of 10192), d = 2.75883, its = 5
i = 7189 (of 10192), d = 0.656208, its = 17
i = 7190 (of 10192), d = 2.57475, its = 6
i = 7191 (of 10192), d = 10, its = 41
i = 7192 (of 10192), d = 10, its = 50
i = 7193 (of 10192), d = 0.495692, its = 17
i = 7194 (of 10192), d = 4.55979, its = 7
i = 7195 (of 10192), d = 10, its = 39
i = 7196 (of 10192), d = 3.73958, its = 6
i = 7197 (of 10192), d = 10, its = 55
i = 7198 (of 10192), d = 10, its = 51
i = 7199 (of 10192), d = 0.583921, its = 20
i = 7200 (of 10192), d = 10, its = 37
i = 7201 (of 10192), d = 0.434293, its = 18
i = 7202 (of 10192), d = 10, its = 57
i = 7203 (of 10192), d = 10, its = 52
i = 7204 (of 10192), d = 10, its = 50
i = 7205 (of 10192), d = 10, its = 56
i = 7206 (of 10192), d = 10, its = 37
i = 7207 (of 10192), d = 5.64739, its = 8
i = 7208 (of 10192), d = 10, its = 43
i = 7209 (of 10192), d = 2.01823, its = 3
i = 7210 (of 10192), d = 10, its = 37
i = 7211 (of 10192), d = 10, its = 53
i = 7212 (of 10192), d = 10, its = 52
i = 7213 (of 10192), d = 10, its = 55
i = 7214 (of 10192), d = 4.76503, its = 7
i = 7215 (of 10192), d = 10, its = 58
i = 7216 (of 10192), d = 10, its = 58
i = 7217 (of 10192), d = 10, its = 53
i = 7218 (of 10192), d = 4.88741, its = 7
i = 7219 (of 10192), d = 10, its = 46
i = 7220 (of 10192), d = 10, its = 56
i = 7221 (of 10192), d = 10, its = 51
i = 7222 (of 10192), d = 10, its = 52
i = 7223 (of 10192), d = 10, its = 54
i = 7224 (of 10192), d = 10, its = 53
i = 7225 (of 10192), d = 1.56843, its = 6
i = 7226 (of 10192), d = 3.33814, its = 6
i = 7227 (of 10192), d = 8.23535, its = 8
i = 7228 (of 10192), d = 10, its = 57
i = 7229 (of 10192), d = 10, its = 53
i = 7230 (of 10192), d = 10, its = 52
i = 7231 (of 10192), d = 10, its = 54
i = 7232 (of 10192), d = 10, its = 40
i = 7233 (of 10192), d = 10, its = 37
i = 7234 (of 10192), d = 2.83945, its = 6
i = 7235 (of 10192), d = 10, its = 37
i = 7236 (of 10192), d = 10, its = 37
i = 7237 (of 10192), d = 10, its = 52
i = 7238 (of 10192), d = 10, its = 53
i = 7239 (of 10192), d = 10, its = 52
i = 7240 (of 10192), d = 2.68063, its = 5
i = 7241 (of 10192), d = 10, its = 52
i = 7242 (of 10192), d = 0.792222, its = 19
i = 7243 (of 10192), d = 10, its = 39
i = 7244 (of 10192), d = 10, its = 51
i = 7245 (of 10192), d = 10, its = 41
i = 7246 (of 10192), d = 1.54738, its = 6
i = 7247 (of 10192), d = 0.6422, its = 21
i = 7248 (of 10192), d = 10, its = 49
i = 7249 (of 10192), d = 0.866582, its = 19
i = 7250 (of 10192), d = 10, its = 55
i = 7251 (of 10192), d = 10, its = 54
i = 7252 (of 10192), d = 0.481533, its = 19
i = 7253 (of 10192), d = 10, its = 54
i = 7254 (of 10192), d = 0.88836, its = 27
i = 7255 (of 10192), d = 10, its = 37
i = 7256 (of 10192), d = 10, its = 39
i = 7257 (of 10192), d = 10, its = 37
i = 7258 (of 10192), d = 1.08349, its = 20
i = 7259 (of 10192), d = 2.46098, its = 5
i = 7260 (of 10192), d = 10, its = 37
i = 7261 (of 10192), d = 10, its = 37
i = 7262 (of 10192), d = 10, its = 51
i = 7263 (of 10192), d = 10, its = 51
i = 7264 (of 10192), d = 1.71565, its = 5
i = 7265 (of 10192), d = 10, its = 55
i = 7266 (of 10192), d = 2.20509, its = 4
i = 7267 (of 10192), d = 10, its = 56
i = 7268 (of 10192), d = 10, its = 48
i = 7269 (of 10192), d = 1.59122, its = 6
i = 7270 (of 10192), d = 10, its = 56
i = 7271 (of 10192), d = 10, its = 57
i = 7272 (of 10192), d = 10, its = 52
i = 7273 (of 10192), d = 10, its = 52
i = 7274 (of 10192), d = 10, its = 56
i = 7275 (of 10192), d = 10, its = 41
i = 7276 (of 10192), d = 10, its = 55
i = 7277 (of 10192), d = 2.29797, its = 5
i = 7278 (of 10192), d = 10, its = 57
i = 7279 (of 10192), d = 10, its = 53
i = 7280 (of 10192), d = 10, its = 53
i = 7281 (of 10192), d = 10, its = 54
i = 7282 (of 10192), d = 10, its = 52
i = 7283 (of 10192), d = 0.96099, its = 19
i = 7284 (of 10192), d = 10, its = 37
i = 7285 (of 10192), d = 0.861595, its = 20
i = 7286 (of 10192), d = 10, its = 53
i = 7287 (of 10192), d = 10, its = 39
i = 7288 (of 10192), d = 10, its = 55
i = 7289 (of 10192), d = 1.82424, its = 5
i = 7290 (of 10192), d = 10, its = 41
i = 7291 (of 10192), d = 10, its = 49
i = 7292 (of 10192), d = 10, its = 51
i = 7293 (of 10192), d = 10, its = 37
i = 7294 (of 10192), d = 2.16472, its = 4
i = 7295 (of 10192), d = 1.60774, its = 6
i = 7296 (of 10192), d = 10, its = 52
i = 7297 (of 10192), d = 0.603626, its = 18
i = 7298 (of 10192), d = 10, its = 53
i = 7299 (of 10192), d = 10, its = 52
i = 7300 (of 10192), d = 10, its = 55
i = 7301 (of 10192), d = 10, its = 37
i = 7302 (of 10192), d = 10, its = 52
i = 7303 (of 10192), d = 10, its = 48
i = 7304 (of 10192), d = 10, its = 51
i = 7305 (of 10192), d = 10, its = 54
i = 7306 (of 10192), d = 0.764767, its = 29
i = 7307 (of 10192), d = 10, its = 42
i = 7308 (of 10192), d = 10, its = 54
i = 7309 (of 10192), d = 10, its = 46
i = 7310 (of 10192), d = 10, its = 54
i = 7311 (of 10192), d = 10, its = 37
i = 7312 (of 10192), d = 0.865521, its = 17
i = 7313 (of 10192), d = 10, its = 37
i = 7314 (of 10192), d = 10, its = 54
i = 7315 (of 10192), d = 10, its = 55
i = 7316 (of 10192), d = 10, its = 54
i = 7317 (of 10192), d = 10, its = 37
i = 7318 (of 10192), d = 10, its = 52
i = 7319 (of 10192), d = 10, its = 51
i = 7320 (of 10192), d = 10, its = 57
i = 7321 (of 10192), d = 10, its = 54
i = 7322 (of 10192), d = 10, its = 40
i = 7323 (of 10192), d = 10, its = 37
i = 7324 (of 10192), d = 10, its = 55
i = 7325 (of 10192), d = 10, its = 37
i = 7326 (of 10192), d = 10, its = 55
i = 7327 (of 10192), d = 10, its = 53
i = 7328 (of 10192), d = 8.33418, its = 9
i = 7329 (of 10192), d = 10, its = 52
i = 7330 (of 10192), d = 10, its = 51
i = 7331 (of 10192), d = 10, its = 52
i = 7332 (of 10192), d = 10, its = 38
i = 7333 (of 10192), d = 10, its = 44
i = 7334 (of 10192), d = 10, its = 37
i = 7335 (of 10192), d = 10, its = 50
i = 7336 (of 10192), d = 10, its = 52
i = 7337 (of 10192), d = 10, its = 53
i = 7338 (of 10192), d = 1.46048, its = 6
i = 7339 (of 10192), d = 10, its = 37
i = 7340 (of 10192), d = 1.64849, its = 5
i = 7341 (of 10192), d = 10, its = 40
i = 7342 (of 10192), d = 10, its = 53
i = 7343 (of 10192), d = 10, its = 53
i = 7344 (of 10192), d = 8.12934, its = 9
i = 7345 (of 10192), d = 10, its = 37
i = 7346 (of 10192), d = 10, its = 52
i = 7347 (of 10192), d = 10, its = 53
i = 7348 (of 10192), d = 10, its = 37
i = 7349 (of 10192), d = 10, its = 51
i = 7350 (of 10192), d = 10, its = 42
i = 7351 (of 10192), d = 10, its = 48
i = 7352 (of 10192), d = 2.27033, its = 5
i = 7353 (of 10192), d = 3.13138, its = 6
i = 7354 (of 10192), d = 10, its = 39
i = 7355 (of 10192), d = 10, its = 37
i = 7356 (of 10192), d = 1.31627, its = 8
i = 7357 (of 10192), d = 0.792751, its = 16
i = 7358 (of 10192), d = 10, its = 37
i = 7359 (of 10192), d = 10, its = 37
i = 7360 (of 10192), d = 10, its = 42
i = 7361 (of 10192), d = 0.690912, its = 20
i = 7362 (of 10192), d = 10, its = 51
i = 7363 (of 10192), d = 10, its = 38
i = 7364 (of 10192), d = 10, its = 39
i = 7365 (of 10192), d = 1.85091, its = 4
i = 7366 (of 10192), d = 1.72388, its = 5
i = 7367 (of 10192), d = 1.3881, its = 7
i = 7368 (of 10192), d = 10, its = 53
i = 7369 (of 10192), d = 1.25304, its = 6
i = 7370 (of 10192), d = 10, its = 54
i = 7371 (of 10192), d = 1.56235, its = 7
i = 7372 (of 10192), d = 10, its = 53
i = 7373 (of 10192), d = 0.754306, its = 18
i = 7374 (of 10192), d = 10, its = 37
i = 7375 (of 10192), d = 10, its = 37
i = 7376 (of 10192), d = 10, its = 58
i = 7377 (of 10192), d = 2.37192, its = 5
i = 7378 (of 10192), d = 10, its = 54
i = 7379 (of 10192), d = 10, its = 56
i = 7380 (of 10192), d = 10, its = 37
i = 7381 (of 10192), d = 10, its = 37
i = 7382 (of 10192), d = 10, its = 57
i = 7383 (of 10192), d = 1.98511, its = 3
i = 7384 (of 10192), d = 10, its = 40
i = 7385 (of 10192), d = 10, its = 53
i = 7386 (of 10192), d = 2.42555, its = 5
i = 7387 (of 10192), d = 10, its = 54
i = 7388 (of 10192), d = 1.33861, its = 6
i = 7389 (of 10192), d = 10, its = 37
i = 7390 (of 10192), d = 2.05498, its = 4
i = 7391 (of 10192), d = 10, its = 41
i = 7392 (of 10192), d = 10, its = 53
i = 7393 (of 10192), d = 10, its = 37
i = 7394 (of 10192), d = 10, its = 53
i = 7395 (of 10192), d = 10, its = 39
i = 7396 (of 10192), d = 10, its = 54
i = 7397 (of 10192), d = 0.802302, its = 20
i = 7398 (of 10192), d = 2.17042, its = 4
i = 7399 (of 10192), d = 10, its = 53
i = 7400 (of 10192), d = 10, its = 37
i = 7401 (of 10192), d = 10, its = 55
i = 7402 (of 10192), d = 1.98555, its = 3
i = 7403 (of 10192), d = 10, its = 37
i = 7404 (of 10192), d = 10, its = 53
i = 7405 (of 10192), d = 2.80452, its = 5
i = 7406 (of 10192), d = 0.690634, its = 20
i = 7407 (of 10192), d = 10, its = 47
i = 7408 (of 10192), d = 10, its = 55
i = 7409 (of 10192), d = 10, its = 41
i = 7410 (of 10192), d = 10, its = 55
i = 7411 (of 10192), d = 2.47941, its = 5
i = 7412 (of 10192), d = 10, its = 37
i = 7413 (of 10192), d = 1.46158, its = 6
i = 7414 (of 10192), d = 0.751355, its = 20
i = 7415 (of 10192), d = 10, its = 53
i = 7416 (of 10192), d = 10, its = 57
i = 7417 (of 10192), d = 0.4667, its = 17
i = 7418 (of 10192), d = 10, its = 53
i = 7419 (of 10192), d = 10, its = 55
i = 7420 (of 10192), d = 10, its = 53
i = 7421 (of 10192), d = 10, its = 55
i = 7422 (of 10192), d = 10, its = 37
i = 7423 (of 10192), d = 10, its = 37
i = 7424 (of 10192), d = 10, its = 53
i = 7425 (of 10192), d = 10, its = 40
i = 7426 (of 10192), d = 10, its = 54
i = 7427 (of 10192), d = 5.45045, its = 8
i = 7428 (of 10192), d = 4.13346, its = 6
i = 7429 (of 10192), d = 2.41205, its = 5
i = 7430 (of 10192), d = 1.72984, its = 7
i = 7431 (of 10192), d = 10, its = 52
i = 7432 (of 10192), d = 10, its = 51
i = 7433 (of 10192), d = 10, its = 50
i = 7434 (of 10192), d = 1.87081, its = 4
i = 7435 (of 10192), d = 10, its = 37
i = 7436 (of 10192), d = 10, its = 54
i = 7437 (of 10192), d = 10, its = 47
i = 7438 (of 10192), d = 10, its = 55
i = 7439 (of 10192), d = 10, its = 48
i = 7440 (of 10192), d = 0.385868, its = 19
i = 7441 (of 10192), d = 6.27548, its = 7
i = 7442 (of 10192), d = 10, its = 49
i = 7443 (of 10192), d = 10, its = 37
i = 7444 (of 10192), d = 10, its = 57
i = 7445 (of 10192), d = 10, its = 55
i = 7446 (of 10192), d = 10, its = 56
i = 7447 (of 10192), d = 10, its = 37
i = 7448 (of 10192), d = 0.8617, its = 19
i = 7449 (of 10192), d = 10, its = 51
i = 7450 (of 10192), d = 10, its = 51
i = 7451 (of 10192), d = 3.30545, its = 6
i = 7452 (of 10192), d = 2.49213, its = 5
i = 7453 (of 10192), d = 10, its = 43
i = 7454 (of 10192), d = 10, its = 58
i = 7455 (of 10192), d = 10, its = 37
i = 7456 (of 10192), d = 10, its = 57
i = 7457 (of 10192), d = 10, its = 39
i = 7458 (of 10192), d = 10, its = 54
i = 7459 (of 10192), d = 10, its = 55
i = 7460 (of 10192), d = 0.496256, its = 17
i = 7461 (of 10192), d = 0.970632, its = 20
i = 7462 (of 10192), d = 10, its = 40
i = 7463 (of 10192), d = 2.00371, its = 3
i = 7464 (of 10192), d = 10, its = 52
i = 7465 (of 10192), d = 10, its = 37
i = 7466 (of 10192), d = 10, its = 50
i = 7467 (of 10192), d = 0.8617, its = 19
i = 7468 (of 10192), d = 10, its = 53
i = 7469 (of 10192), d = 10, its = 41
i = 7470 (of 10192), d = 9.28927, its = 11
i = 7471 (of 10192), d = 1.92682, its = 4
i = 7472 (of 10192), d = 10, its = 49
i = 7473 (of 10192), d = 0.693593, its = 19
i = 7474 (of 10192), d = 10, its = 57
i = 7475 (of 10192), d = 10, its = 53
i = 7476 (of 10192), d = 3.0863, its = 5
i = 7477 (of 10192), d = 1.59485, its = 6
i = 7478 (of 10192), d = 10, its = 51
i = 7479 (of 10192), d = 10, its = 55
i = 7480 (of 10192), d = 10, its = 52
i = 7481 (of 10192), d = 10, its = 53
i = 7482 (of 10192), d = 10, its = 40
i = 7483 (of 10192), d = 10, its = 43
i = 7484 (of 10192), d = 10, its = 49
i = 7485 (of 10192), d = 0.925052, its = 18
i = 7486 (of 10192), d = 10, its = 53
i = 7487 (of 10192), d = 10, its = 39
i = 7488 (of 10192), d = 0.861038, its = 18
i = 7489 (of 10192), d = 10, its = 54
i = 7490 (of 10192), d = 1.59755, its = 6
i = 7491 (of 10192), d = 10, its = 54
i = 7492 (of 10192), d = 10, its = 56
i = 7493 (of 10192), d = 10, its = 52
i = 7494 (of 10192), d = 0.940264, its = 20
i = 7495 (of 10192), d = 0.471104, its = 17
i = 7496 (of 10192), d = 10, its = 37
i = 7497 (of 10192), d = 10, its = 48
i = 7498 (of 10192), d = 0.600746, its = 17
i = 7499 (of 10192), d = 1.25617, its = 6
i = 7500 (of 10192), d = 10, its = 37
i = 7501 (of 10192), d = 1.88971, its = 4
i = 7502 (of 10192), d = 1.62859, its = 5
i = 7503 (of 10192), d = 10, its = 47
i = 7504 (of 10192), d = 10, its = 52
i = 7505 (of 10192), d = 10, its = 55
i = 7506 (of 10192), d = 2.38261, its = 5
i = 7507 (of 10192), d = 10, its = 50
i = 7508 (of 10192), d = 10, its = 40
i = 7509 (of 10192), d = 1.81673, its = 4
i = 7510 (of 10192), d = 10, its = 37
i = 7511 (of 10192), d = 10, its = 42
i = 7512 (of 10192), d = 0.987669, its = 20
i = 7513 (of 10192), d = 10, its = 46
i = 7514 (of 10192), d = 2.30025, its = 4
i = 7515 (of 10192), d = 10, its = 37
i = 7516 (of 10192), d = 0.714306, its = 19
i = 7517 (of 10192), d = 10, its = 46
i = 7518 (of 10192), d = 10, its = 52
i = 7519 (of 10192), d = 10, its = 53
i = 7520 (of 10192), d = 10, its = 53
i = 7521 (of 10192), d = 1.65771, its = 5
i = 7522 (of 10192), d = 10, its = 41
i = 7523 (of 10192), d = 10, its = 53
i = 7524 (of 10192), d = 1.59454, its = 6
i = 7525 (of 10192), d = 10, its = 39
i = 7526 (of 10192), d = 10, its = 51
i = 7527 (of 10192), d = 10, its = 55
i = 7528 (of 10192), d = 10, its = 54
i = 7529 (of 10192), d = 1.2863, its = 27
i = 7530 (of 10192), d = 10, its = 41
i = 7531 (of 10192), d = 1.46683, its = 25
i = 7532 (of 10192), d = 10, its = 56
i = 7533 (of 10192), d = 10, its = 37
i = 7534 (of 10192), d = 1.4419, its = 8
i = 7535 (of 10192), d = 10, its = 52
i = 7536 (of 10192), d = 2.61619, its = 5
i = 7537 (of 10192), d = 10, its = 37
i = 7538 (of 10192), d = 10, its = 54
i = 7539 (of 10192), d = 10, its = 46
i = 7540 (of 10192), d = 10, its = 51
i = 7541 (of 10192), d = 10, its = 50
i = 7542 (of 10192), d = 10, its = 51
i = 7543 (of 10192), d = 2.43246, its = 5
i = 7544 (of 10192), d = 2.58636, its = 5
i = 7545 (of 10192), d = 1.21094, its = 6
i = 7546 (of 10192), d = 0.341445, its = 18
i = 7547 (of 10192), d = 10, its = 37
i = 7548 (of 10192), d = 2.25718, its = 4
i = 7549 (of 10192), d = 10, its = 52
i = 7550 (of 10192), d = 10, its = 55
i = 7551 (of 10192), d = 10, its = 40
i = 7552 (of 10192), d = 10, its = 51
i = 7553 (of 10192), d = 10, its = 42
i = 7554 (of 10192), d = 10, its = 51
i = 7555 (of 10192), d = 10, its = 52
i = 7556 (of 10192), d = 10, its = 40
i = 7557 (of 10192), d = 1.6813, its = 5
i = 7558 (of 10192), d = 0.811943, its = 17
i = 7559 (of 10192), d = 10, its = 56
i = 7560 (of 10192), d = 10, its = 54
i = 7561 (of 10192), d = 10, its = 56
i = 7562 (of 10192), d = 10, its = 53
i = 7563 (of 10192), d = 10, its = 54
i = 7564 (of 10192), d = 10, its = 49
i = 7565 (of 10192), d = 10, its = 56
i = 7566 (of 10192), d = 10, its = 53
i = 7567 (of 10192), d = 1.83648, its = 4
i = 7568 (of 10192), d = 10, its = 51
i = 7569 (of 10192), d = 10, its = 40
i = 7570 (of 10192), d = 10, its = 51
i = 7571 (of 10192), d = 10, its = 37
i = 7572 (of 10192), d = 10, its = 51
i = 7573 (of 10192), d = 10, its = 57
i = 7574 (of 10192), d = 2.48017, its = 5
i = 7575 (of 10192), d = 0.531787, its = 17
i = 7576 (of 10192), d = 10, its = 37
i = 7577 (of 10192), d = 10, its = 50
i = 7578 (of 10192), d = 2.27761, its = 5
i = 7579 (of 10192), d = 10, its = 37
i = 7580 (of 10192), d = 10, its = 56
i = 7581 (of 10192), d = 10, its = 52
i = 7582 (of 10192), d = 10, its = 37
i = 7583 (of 10192), d = 10, its = 49
i = 7584 (of 10192), d = 10, its = 51
i = 7585 (of 10192), d = 3.71531, its = 7
i = 7586 (of 10192), d = 10, its = 54
i = 7587 (of 10192), d = 10, its = 52
i = 7588 (of 10192), d = 0.669266, its = 20
i = 7589 (of 10192), d = 10, its = 37
i = 7590 (of 10192), d = 10, its = 53
i = 7591 (of 10192), d = 10, its = 43
i = 7592 (of 10192), d = 1.46109, its = 7
i = 7593 (of 10192), d = 10, its = 37
i = 7594 (of 10192), d = 10, its = 50
i = 7595 (of 10192), d = 10, its = 52
i = 7596 (of 10192), d = 10, its = 37
i = 7597 (of 10192), d = 0.222748, its = 22
i = 7598 (of 10192), d = 10, its = 49
i = 7599 (of 10192), d = 10, its = 53
i = 7600 (of 10192), d = 10, its = 39
i = 7601 (of 10192), d = 1.08268, its = 20
i = 7602 (of 10192), d = 2.91189, its = 6
i = 7603 (of 10192), d = 10, its = 38
i = 7604 (of 10192), d = 10, its = 52
i = 7605 (of 10192), d = 2.4216, its = 5
i = 7606 (of 10192), d = 1.88613, its = 4
i = 7607 (of 10192), d = 10, its = 41
i = 7608 (of 10192), d = 2.34836, its = 4
i = 7609 (of 10192), d = 0.987669, its = 20
i = 7610 (of 10192), d = 10, its = 39
i = 7611 (of 10192), d = 10, its = 56
i = 7612 (of 10192), d = 10, its = 50
i = 7613 (of 10192), d = 10, its = 53
i = 7614 (of 10192), d = 10, its = 52
i = 7615 (of 10192), d = 10, its = 41
i = 7616 (of 10192), d = 10, its = 53
i = 7617 (of 10192), d = 10, its = 40
i = 7618 (of 10192), d = 10, its = 40
i = 7619 (of 10192), d = 10, its = 37
i = 7620 (of 10192), d = 10, its = 37
i = 7621 (of 10192), d = 10, its = 42
i = 7622 (of 10192), d = 10, its = 55
i = 7623 (of 10192), d = 0.475427, its = 17
i = 7624 (of 10192), d = 10, its = 53
i = 7625 (of 10192), d = 10, its = 55
i = 7626 (of 10192), d = 2.04496, its = 3
i = 7627 (of 10192), d = 10, its = 54
i = 7628 (of 10192), d = 10, its = 57
i = 7629 (of 10192), d = 3.85214, its = 7
i = 7630 (of 10192), d = 10, its = 54
i = 7631 (of 10192), d = 10, its = 50
i = 7632 (of 10192), d = 1.54623, its = 6
i = 7633 (of 10192), d = 10, its = 54
i = 7634 (of 10192), d = 3.14501, its = 6
i = 7635 (of 10192), d = 10, its = 37
i = 7636 (of 10192), d = 10, its = 53
i = 7637 (of 10192), d = 10, its = 50
i = 7638 (of 10192), d = 10, its = 50
i = 7639 (of 10192), d = 10, its = 56
i = 7640 (of 10192), d = 10, its = 41
i = 7641 (of 10192), d = 10, its = 39
i = 7642 (of 10192), d = 10, its = 50
i = 7643 (of 10192), d = 10, its = 56
i = 7644 (of 10192), d = 10, its = 52
i = 7645 (of 10192), d = 10, its = 51
i = 7646 (of 10192), d = 10, its = 53
i = 7647 (of 10192), d = 10, its = 48
i = 7648 (of 10192), d = 10, its = 54
i = 7649 (of 10192), d = 10, its = 37
i = 7650 (of 10192), d = 10, its = 56
i = 7651 (of 10192), d = 10, its = 37
i = 7652 (of 10192), d = 10, its = 42
i = 7653 (of 10192), d = 10, its = 53
i = 7654 (of 10192), d = 1.91612, its = 4
i = 7655 (of 10192), d = 10, its = 52
i = 7656 (of 10192), d = 3.0901, its = 7
i = 7657 (of 10192), d = 2.531, its = 6
i = 7658 (of 10192), d = 10, its = 56
i = 7659 (of 10192), d = 10, its = 52
i = 7660 (of 10192), d = 10, its = 37
i = 7661 (of 10192), d = 10, its = 57
i = 7662 (of 10192), d = 10, its = 55
i = 7663 (of 10192), d = 10, its = 50
i = 7664 (of 10192), d = 10, its = 42
i = 7665 (of 10192), d = 10, its = 57
i = 7666 (of 10192), d = 10, its = 51
i = 7667 (of 10192), d = 1.81875, its = 4
i = 7668 (of 10192), d = 2.09419, its = 4
i = 7669 (of 10192), d = 10, its = 53
i = 7670 (of 10192), d = 10, its = 55
i = 7671 (of 10192), d = 10, its = 39
i = 7672 (of 10192), d = 10, its = 37
i = 7673 (of 10192), d = 10, its = 55
i = 7674 (of 10192), d = 10, its = 37
i = 7675 (of 10192), d = 2.2891, its = 4
i = 7676 (of 10192), d = 10, its = 52
i = 7677 (of 10192), d = 1.18656, its = 18
i = 7678 (of 10192), d = 10, its = 55
i = 7679 (of 10192), d = 0.828085, its = 18
i = 7680 (of 10192), d = 10, its = 59
i = 7681 (of 10192), d = 10, its = 40
i = 7682 (of 10192), d = 10, its = 55
i = 7683 (of 10192), d = 2.39562, its = 6
i = 7684 (of 10192), d = 10, its = 48
i = 7685 (of 10192), d = 10, its = 39
i = 7686 (of 10192), d = 10, its = 51
i = 7687 (of 10192), d = 10, its = 52
i = 7688 (of 10192), d = 10, its = 56
i = 7689 (of 10192), d = 10, its = 54
i = 7690 (of 10192), d = 10, its = 55
i = 7691 (of 10192), d = 10, its = 53
i = 7692 (of 10192), d = 0.940206, its = 19
i = 7693 (of 10192), d = 10, its = 45
i = 7694 (of 10192), d = 10, its = 56
i = 7695 (of 10192), d = 1.40608, its = 7
i = 7696 (of 10192), d = 10, its = 57
i = 7697 (of 10192), d = 3.27304, its = 6
i = 7698 (of 10192), d = 10, its = 50
i = 7699 (of 10192), d = 10, its = 53
i = 7700 (of 10192), d = 10, its = 57
i = 7701 (of 10192), d = 10, its = 55
i = 7702 (of 10192), d = 10, its = 37
i = 7703 (of 10192), d = 10, its = 48
i = 7704 (of 10192), d = 10, its = 41
i = 7705 (of 10192), d = 10, its = 37
i = 7706 (of 10192), d = 10, its = 41
i = 7707 (of 10192), d = 10, its = 39
i = 7708 (of 10192), d = 10, its = 55
i = 7709 (of 10192), d = 1.83722, its = 5
i = 7710 (of 10192), d = 1.56265, its = 6
i = 7711 (of 10192), d = 10, its = 47
i = 7712 (of 10192), d = 10, its = 54
i = 7713 (of 10192), d = 2.84548, its = 5
i = 7714 (of 10192), d = 3.83418, its = 7
i = 7715 (of 10192), d = 2.32376, its = 5
i = 7716 (of 10192), d = 10, its = 55
i = 7717 (of 10192), d = 0.980436, its = 18
i = 7718 (of 10192), d = 1.74185, its = 5
i = 7719 (of 10192), d = 10, its = 47
i = 7720 (of 10192), d = 10, its = 50
i = 7721 (of 10192), d = 10, its = 37
i = 7722 (of 10192), d = 10, its = 53
i = 7723 (of 10192), d = 2.32391, its = 4
i = 7724 (of 10192), d = 10, its = 37
i = 7725 (of 10192), d = 10, its = 55
i = 7726 (of 10192), d = 3.67792, its = 7
i = 7727 (of 10192), d = 10, its = 41
i = 7728 (of 10192), d = 10, its = 50
i = 7729 (of 10192), d = 10, its = 54
i = 7730 (of 10192), d = 10, its = 40
i = 7731 (of 10192), d = 10, its = 51
i = 7732 (of 10192), d = 10, its = 53
i = 7733 (of 10192), d = 10, its = 55
i = 7734 (of 10192), d = 10, its = 56
i = 7735 (of 10192), d = 10, its = 51
i = 7736 (of 10192), d = 0.583498, its = 17
i = 7737 (of 10192), d = 10, its = 54
i = 7738 (of 10192), d = 10, its = 52
i = 7739 (of 10192), d = 10, its = 52
i = 7740 (of 10192), d = 10, its = 37
i = 7741 (of 10192), d = 10, its = 58
i = 7742 (of 10192), d = 0.692327, its = 18
i = 7743 (of 10192), d = 10, its = 40
i = 7744 (of 10192), d = 0.419526, its = 21
i = 7745 (of 10192), d = 1.70146, its = 5
i = 7746 (of 10192), d = 2.18151, its = 4
i = 7747 (of 10192), d = 10, its = 54
i = 7748 (of 10192), d = 10, its = 39
i = 7749 (of 10192), d = 10, its = 39
i = 7750 (of 10192), d = 10, its = 52
i = 7751 (of 10192), d = 10, its = 37
i = 7752 (of 10192), d = 1.9932, its = 2
i = 7753 (of 10192), d = 10, its = 51
i = 7754 (of 10192), d = 10, its = 49
i = 7755 (of 10192), d = 10, its = 37
i = 7756 (of 10192), d = 1.59454, its = 6
i = 7757 (of 10192), d = 1.37658, its = 7
i = 7758 (of 10192), d = 1.99947, its = 2
i = 7759 (of 10192), d = 10, its = 48
i = 7760 (of 10192), d = 2.6321, its = 5
i = 7761 (of 10192), d = 2.17105, its = 4
i = 7762 (of 10192), d = 0.951461, its = 25
i = 7763 (of 10192), d = 10, its = 50
i = 7764 (of 10192), d = 10, its = 55
i = 7765 (of 10192), d = 0.878047, its = 18
i = 7766 (of 10192), d = 10, its = 37
i = 7767 (of 10192), d = 0.636424, its = 18
i = 7768 (of 10192), d = 0.60329, its = 24
i = 7769 (of 10192), d = 10, its = 51
i = 7770 (of 10192), d = 10, its = 41
i = 7771 (of 10192), d = 2.14247, its = 4
i = 7772 (of 10192), d = 1.79314, its = 4
i = 7773 (of 10192), d = 10, its = 51
i = 7774 (of 10192), d = 10, its = 51
i = 7775 (of 10192), d = 1.3941, its = 6
i = 7776 (of 10192), d = 0.404517, its = 24
i = 7777 (of 10192), d = 10, its = 37
i = 7778 (of 10192), d = 1.01136, its = 17
i = 7779 (of 10192), d = 10, its = 51
i = 7780 (of 10192), d = 10, its = 37
i = 7781 (of 10192), d = 10, its = 50
i = 7782 (of 10192), d = 10, its = 39
i = 7783 (of 10192), d = 10, its = 40
i = 7784 (of 10192), d = 1.48357, its = 7
i = 7785 (of 10192), d = 10, its = 53
i = 7786 (of 10192), d = 10, its = 49
i = 7787 (of 10192), d = 10, its = 49
i = 7788 (of 10192), d = 10, its = 55
i = 7789 (of 10192), d = 10, its = 39
i = 7790 (of 10192), d = 10, its = 56
i = 7791 (of 10192), d = 10, its = 57
i = 7792 (of 10192), d = 10, its = 37
i = 7793 (of 10192), d = 10, its = 49
i = 7794 (of 10192), d = 10, its = 51
i = 7795 (of 10192), d = 10, its = 52
i = 7796 (of 10192), d = 1.83058, its = 5
i = 7797 (of 10192), d = 10, its = 37
i = 7798 (of 10192), d = 10, its = 39
i = 7799 (of 10192), d = 10, its = 53
i = 7800 (of 10192), d = 10, its = 54
i = 7801 (of 10192), d = 2.51036, its = 5
i = 7802 (of 10192), d = 10, its = 49
i = 7803 (of 10192), d = 10, its = 52
i = 7804 (of 10192), d = 10, its = 55
i = 7805 (of 10192), d = 10, its = 40
i = 7806 (of 10192), d = 10, its = 56
i = 7807 (of 10192), d = 2.59364, its = 5
i = 7808 (of 10192), d = 10, its = 55
i = 7809 (of 10192), d = 10, its = 48
i = 7810 (of 10192), d = 10, its = 54
i = 7811 (of 10192), d = 10, its = 57
i = 7812 (of 10192), d = 2.95963, its = 5
i = 7813 (of 10192), d = 0.504376, its = 17
i = 7814 (of 10192), d = 3.405, its = 6
i = 7815 (of 10192), d = 10, its = 52
i = 7816 (of 10192), d = 10, its = 56
i = 7817 (of 10192), d = 10, its = 54
i = 7818 (of 10192), d = 10, its = 54
i = 7819 (of 10192), d = 10, its = 54
i = 7820 (of 10192), d = 10, its = 50
i = 7821 (of 10192), d = 1.41789, its = 5
i = 7822 (of 10192), d = 10, its = 52
i = 7823 (of 10192), d = 10, its = 51
i = 7824 (of 10192), d = 10, its = 37
i = 7825 (of 10192), d = 10, its = 40
i = 7826 (of 10192), d = 0.514648, its = 22
i = 7827 (of 10192), d = 10, its = 54
i = 7828 (of 10192), d = 10, its = 41
i = 7829 (of 10192), d = 0.793589, its = 16
i = 7830 (of 10192), d = 10, its = 53
i = 7831 (of 10192), d = 8.39759, its = 8
i = 7832 (of 10192), d = 1.89428, its = 4
i = 7833 (of 10192), d = 10, its = 49
i = 7834 (of 10192), d = 10, its = 53
i = 7835 (of 10192), d = 10, its = 54
i = 7836 (of 10192), d = 10, its = 52
i = 7837 (of 10192), d = 1.79564, its = 5
i = 7838 (of 10192), d = 10, its = 56
i = 7839 (of 10192), d = 10, its = 54
i = 7840 (of 10192), d = 10, its = 53
i = 7841 (of 10192), d = 10, its = 55
i = 7842 (of 10192), d = 10, its = 50
i = 7843 (of 10192), d = 1.45938, its = 6
i = 7844 (of 10192), d = 10, its = 54
i = 7845 (of 10192), d = 10, its = 37
i = 7846 (of 10192), d = 10, its = 48
i = 7847 (of 10192), d = 10, its = 53
i = 7848 (of 10192), d = 1.44141, its = 6
i = 7849 (of 10192), d = 1.00294, its = 19
i = 7850 (of 10192), d = 0.711028, its = 20
i = 7851 (of 10192), d = 0.407778, its = 18
i = 7852 (of 10192), d = 10, its = 53
i = 7853 (of 10192), d = 10, its = 54
i = 7854 (of 10192), d = 10, its = 48
i = 7855 (of 10192), d = 10, its = 37
i = 7856 (of 10192), d = 10, its = 50
i = 7857 (of 10192), d = 2.36108, its = 5
i = 7858 (of 10192), d = 1.38226, its = 6
i = 7859 (of 10192), d = 10, its = 50
i = 7860 (of 10192), d = 10, its = 52
i = 7861 (of 10192), d = 10, its = 37
i = 7862 (of 10192), d = 0.715409, its = 18
i = 7863 (of 10192), d = 10, its = 39
i = 7864 (of 10192), d = 10, its = 37
i = 7865 (of 10192), d = 1.38708, its = 7
i = 7866 (of 10192), d = 10, its = 52
i = 7867 (of 10192), d = 2.06675, its = 4
i = 7868 (of 10192), d = 10, its = 39
i = 7869 (of 10192), d = 2.54265, its = 5
i = 7870 (of 10192), d = 3.26421, its = 8
i = 7871 (of 10192), d = 1.2706, its = 18
i = 7872 (of 10192), d = 1.90878, its = 4
i = 7873 (of 10192), d = 10, its = 37
i = 7874 (of 10192), d = 10, its = 56
i = 7875 (of 10192), d = 10, its = 52
i = 7876 (of 10192), d = 10, its = 37
i = 7877 (of 10192), d = 1.76308, its = 5
i = 7878 (of 10192), d = 10, its = 50
i = 7879 (of 10192), d = 10, its = 53
i = 7880 (of 10192), d = 10, its = 51
i = 7881 (of 10192), d = 0.932523, its = 23
i = 7882 (of 10192), d = 10, its = 40
i = 7883 (of 10192), d = 10, its = 53
i = 7884 (of 10192), d = 10, its = 37
i = 7885 (of 10192), d = 10, its = 41
i = 7886 (of 10192), d = 10, its = 53
i = 7887 (of 10192), d = 10, its = 53
i = 7888 (of 10192), d = 10, its = 52
i = 7889 (of 10192), d = 10, its = 56
i = 7890 (of 10192), d = 10, its = 55
i = 7891 (of 10192), d = 0.563536, its = 26
i = 7892 (of 10192), d = 1.34407, its = 8
i = 7893 (of 10192), d = 10, its = 43
i = 7894 (of 10192), d = 10, its = 39
i = 7895 (of 10192), d = 10, its = 47
i = 7896 (of 10192), d = 10, its = 55
i = 7897 (of 10192), d = 10, its = 52
i = 7898 (of 10192), d = 0.472459, its = 25
i = 7899 (of 10192), d = 10, its = 51
i = 7900 (of 10192), d = 10, its = 39
i = 7901 (of 10192), d = 10, its = 37
i = 7902 (of 10192), d = 10, its = 39
i = 7903 (of 10192), d = 1.98268, its = 3
i = 7904 (of 10192), d = 10, its = 55
i = 7905 (of 10192), d = 1.49673, its = 6
i = 7906 (of 10192), d = 10, its = 51
i = 7907 (of 10192), d = 10, its = 56
i = 7908 (of 10192), d = 10, its = 54
i = 7909 (of 10192), d = 10, its = 53
i = 7910 (of 10192), d = 10, its = 49
i = 7911 (of 10192), d = 10, its = 37
i = 7912 (of 10192), d = 10, its = 52
i = 7913 (of 10192), d = 10, its = 37
i = 7914 (of 10192), d = 10, its = 52
i = 7915 (of 10192), d = 1.15984, its = 19
i = 7916 (of 10192), d = 10, its = 57
i = 7917 (of 10192), d = 10, its = 54
i = 7918 (of 10192), d = 10, its = 54
i = 7919 (of 10192), d = 10, its = 52
i = 7920 (of 10192), d = 10, its = 54
i = 7921 (of 10192), d = 2.38261, its = 5
i = 7922 (of 10192), d = 10, its = 55
i = 7923 (of 10192), d = 0.535998, its = 17
i = 7924 (of 10192), d = 2.69181, its = 6
i = 7925 (of 10192), d = 10, its = 52
i = 7926 (of 10192), d = 10, its = 53
i = 7927 (of 10192), d = 0.320752, its = 19
i = 7928 (of 10192), d = 10, its = 53
i = 7929 (of 10192), d = 10, its = 52
i = 7930 (of 10192), d = 10, its = 51
i = 7931 (of 10192), d = 2.70402, its = 6
i = 7932 (of 10192), d = 0.805315, its = 21
i = 7933 (of 10192), d = 10, its = 50
i = 7934 (of 10192), d = 1.44406, its = 6
i = 7935 (of 10192), d = 10, its = 43
i = 7936 (of 10192), d = 10, its = 37
i = 7937 (of 10192), d = 10, its = 56
i = 7938 (of 10192), d = 10, its = 51
i = 7939 (of 10192), d = 1.77632, its = 6
i = 7940 (of 10192), d = 10, its = 52
i = 7941 (of 10192), d = 10, its = 53
i = 7942 (of 10192), d = 2.34554, its = 5
i = 7943 (of 10192), d = 2.70126, its = 5
i = 7944 (of 10192), d = 10, its = 39
i = 7945 (of 10192), d = 0.727886, its = 19
i = 7946 (of 10192), d = 10, its = 51
i = 7947 (of 10192), d = 10, its = 52
i = 7948 (of 10192), d = 10, its = 55
i = 7949 (of 10192), d = 10, its = 49
i = 7950 (of 10192), d = 10, its = 52
i = 7951 (of 10192), d = 10, its = 37
i = 7952 (of 10192), d = 1.99952, its = 2
i = 7953 (of 10192), d = 10, its = 51
i = 7954 (of 10192), d = 10, its = 55
i = 7955 (of 10192), d = 10, its = 53
i = 7956 (of 10192), d = 10, its = 52
i = 7957 (of 10192), d = 1.92901, its = 3
i = 7958 (of 10192), d = 10, its = 38
i = 7959 (of 10192), d = 10, its = 51
i = 7960 (of 10192), d = 10, its = 37
i = 7961 (of 10192), d = 2.74119, its = 6
i = 7962 (of 10192), d = 10, its = 55
i = 7963 (of 10192), d = 10, its = 51
i = 7964 (of 10192), d = 10, its = 37
i = 7965 (of 10192), d = 10, its = 52
i = 7966 (of 10192), d = 10, its = 37
i = 7967 (of 10192), d = 10, its = 37
i = 7968 (of 10192), d = 10, its = 37
i = 7969 (of 10192), d = 10, its = 52
i = 7970 (of 10192), d = 10, its = 37
i = 7971 (of 10192), d = 10, its = 47
i = 7972 (of 10192), d = 10, its = 37
i = 7973 (of 10192), d = 10, its = 54
i = 7974 (of 10192), d = 10, its = 54
i = 7975 (of 10192), d = 10, its = 50
i = 7976 (of 10192), d = 10, its = 51
i = 7977 (of 10192), d = 10, its = 51
i = 7978 (of 10192), d = 10, its = 55
i = 7979 (of 10192), d = 10, its = 41
i = 7980 (of 10192), d = 10, its = 53
i = 7981 (of 10192), d = 0.517615, its = 21
i = 7982 (of 10192), d = 10, its = 53
i = 7983 (of 10192), d = 10, its = 55
i = 7984 (of 10192), d = 10, its = 55
i = 7985 (of 10192), d = 1.91525, its = 4
i = 7986 (of 10192), d = 10, its = 49
i = 7987 (of 10192), d = 2.36544, its = 5
i = 7988 (of 10192), d = 10, its = 50
i = 7989 (of 10192), d = 10, its = 50
i = 7990 (of 10192), d = 10, its = 41
i = 7991 (of 10192), d = 10, its = 53
i = 7992 (of 10192), d = 10, its = 54
i = 7993 (of 10192), d = 0.697736, its = 18
i = 7994 (of 10192), d = 1.50439, its = 6
i = 7995 (of 10192), d = 10, its = 51
i = 7996 (of 10192), d = 10, its = 37
i = 7997 (of 10192), d = 1.40955, its = 6
i = 7998 (of 10192), d = 10, its = 52
i = 7999 (of 10192), d = 10, its = 55
i = 8000 (of 10192), d = 10, its = 55
i = 8001 (of 10192), d = 9.37519, its = 8
i = 8002 (of 10192), d = 10, its = 53
i = 8003 (of 10192), d = 10, its = 52
i = 8004 (of 10192), d = 10, its = 40
i = 8005 (of 10192), d = 10, its = 53
i = 8006 (of 10192), d = 10, its = 55
i = 8007 (of 10192), d = 10, its = 44
i = 8008 (of 10192), d = 10, its = 54
i = 8009 (of 10192), d = 10, its = 55
i = 8010 (of 10192), d = 10, its = 55
i = 8011 (of 10192), d = 1.80785, its = 4
i = 8012 (of 10192), d = 3.58089, its = 8
i = 8013 (of 10192), d = 3.69219, its = 7
i = 8014 (of 10192), d = 2.56289, its = 5
i = 8015 (of 10192), d = 10, its = 55
i = 8016 (of 10192), d = 10, its = 52
i = 8017 (of 10192), d = 10, its = 56
i = 8018 (of 10192), d = 0.543725, its = 18
i = 8019 (of 10192), d = 10, its = 51
i = 8020 (of 10192), d = 10, its = 37
i = 8021 (of 10192), d = 10, its = 39
i = 8022 (of 10192), d = 10, its = 53
i = 8023 (of 10192), d = 10, its = 55
i = 8024 (of 10192), d = 0.476584, its = 18
i = 8025 (of 10192), d = 10, its = 37
i = 8026 (of 10192), d = 10, its = 53
i = 8027 (of 10192), d = 2.7842, its = 6
i = 8028 (of 10192), d = 10, its = 51
i = 8029 (of 10192), d = 10, its = 40
i = 8030 (of 10192), d = 10, its = 53
i = 8031 (of 10192), d = 10, its = 54
i = 8032 (of 10192), d = 10, its = 53
i = 8033 (of 10192), d = 10, its = 51
i = 8034 (of 10192), d = 10, its = 53
i = 8035 (of 10192), d = 10, its = 53
i = 8036 (of 10192), d = 0.713396, its = 24
i = 8037 (of 10192), d = 10, its = 47
i = 8038 (of 10192), d = 1.17305, its = 5
i = 8039 (of 10192), d = 10, its = 37
i = 8040 (of 10192), d = 1.91126, its = 4
i = 8041 (of 10192), d = 0.64131, its = 23
i = 8042 (of 10192), d = 0.832812, its = 19
i = 8043 (of 10192), d = 10, its = 52
i = 8044 (of 10192), d = 10, its = 51
i = 8045 (of 10192), d = 2.07374, its = 3
i = 8046 (of 10192), d = 10, its = 51
i = 8047 (of 10192), d = 10, its = 41
i = 8048 (of 10192), d = 10, its = 40
i = 8049 (of 10192), d = 1.94535, its = 4
i = 8050 (of 10192), d = 10, its = 50
i = 8051 (of 10192), d = 10, its = 50
i = 8052 (of 10192), d = 2.79603, its = 6
i = 8053 (of 10192), d = 10, its = 42
i = 8054 (of 10192), d = 10, its = 49
i = 8055 (of 10192), d = 1.72223, its = 5
i = 8056 (of 10192), d = 10, its = 51
i = 8057 (of 10192), d = 10, its = 46
i = 8058 (of 10192), d = 10, its = 54
i = 8059 (of 10192), d = 2.25236, its = 4
i = 8060 (of 10192), d = 0.854333, its = 25
i = 8061 (of 10192), d = 10, its = 53
i = 8062 (of 10192), d = 1.62037, its = 6
i = 8063 (of 10192), d = 0.373146, its = 18
i = 8064 (of 10192), d = 10, its = 39
i = 8065 (of 10192), d = 10, its = 53
i = 8066 (of 10192), d = 10, its = 40
i = 8067 (of 10192), d = 10, its = 16
i = 8068 (of 10192), d = 10, its = 37
i = 8069 (of 10192), d = 10, its = 55
i = 8070 (of 10192), d = 10, its = 53
i = 8071 (of 10192), d = 10, its = 56
i = 8072 (of 10192), d = 10, its = 52
i = 8073 (of 10192), d = 10, its = 37
i = 8074 (of 10192), d = 10, its = 54
i = 8075 (of 10192), d = 5.97536, its = 19
i = 8076 (of 10192), d = 10, its = 53
i = 8077 (of 10192), d = 10, its = 56
i = 8078 (of 10192), d = 10, its = 53
i = 8079 (of 10192), d = 1.19618, its = 28
i = 8080 (of 10192), d = 10, its = 49
i = 8081 (of 10192), d = 3.12257, its = 8
i = 8082 (of 10192), d = 10, its = 37
i = 8083 (of 10192), d = 10, its = 37
i = 8084 (of 10192), d = 10, its = 40
i = 8085 (of 10192), d = 10, its = 40
i = 8086 (of 10192), d = 0.831921, its = 18
i = 8087 (of 10192), d = 0.425142, its = 19
i = 8088 (of 10192), d = 10, its = 51
i = 8089 (of 10192), d = 10, its = 56
i = 8090 (of 10192), d = 10, its = 51
i = 8091 (of 10192), d = 10, its = 52
i = 8092 (of 10192), d = 10, its = 42
i = 8093 (of 10192), d = 1.23663, its = 6
i = 8094 (of 10192), d = 10, its = 57
i = 8095 (of 10192), d = 10, its = 57
i = 8096 (of 10192), d = 4.09201, its = 7
i = 8097 (of 10192), d = 10, its = 53
i = 8098 (of 10192), d = 2.10882, its = 4
i = 8099 (of 10192), d = 10, its = 54
i = 8100 (of 10192), d = 2.63852, its = 5
i = 8101 (of 10192), d = 10, its = 37
i = 8102 (of 10192), d = 0.60213, its = 17
i = 8103 (of 10192), d = 10, its = 55
i = 8104 (of 10192), d = 10, its = 37
i = 8105 (of 10192), d = 2.14064, its = 4
i = 8106 (of 10192), d = 10, its = 50
i = 8107 (of 10192), d = 10, its = 41
i = 8108 (of 10192), d = 10, its = 53
i = 8109 (of 10192), d = 10, its = 39
i = 8110 (of 10192), d = 10, its = 56
i = 8111 (of 10192), d = 10, its = 49
i = 8112 (of 10192), d = 10, its = 52
i = 8113 (of 10192), d = 10, its = 50
i = 8114 (of 10192), d = 10, its = 51
i = 8115 (of 10192), d = 10, its = 52
i = 8116 (of 10192), d = 10, its = 39
i = 8117 (of 10192), d = 10, its = 38
i = 8118 (of 10192), d = 1.88611, its = 4
i = 8119 (of 10192), d = 10, its = 56
i = 8120 (of 10192), d = 10, its = 53
i = 8121 (of 10192), d = 10, its = 53
i = 8122 (of 10192), d = 10, its = 37
i = 8123 (of 10192), d = 1.93869, its = 3
i = 8124 (of 10192), d = 2.37563, its = 6
i = 8125 (of 10192), d = 10, its = 54
i = 8126 (of 10192), d = 10, its = 51
i = 8127 (of 10192), d = 10, its = 53
i = 8128 (of 10192), d = 10, its = 37
i = 8129 (of 10192), d = 10, its = 51
i = 8130 (of 10192), d = 10, its = 18
i = 8131 (of 10192), d = 10, its = 50
i = 8132 (of 10192), d = 2.87576, its = 7
i = 8133 (of 10192), d = 1.4961, its = 6
i = 8134 (of 10192), d = 10, its = 49
i = 8135 (of 10192), d = 10, its = 37
i = 8136 (of 10192), d = 10, its = 37
i = 8137 (of 10192), d = 10, its = 54
i = 8138 (of 10192), d = 10, its = 37
i = 8139 (of 10192), d = 10, its = 50
i = 8140 (of 10192), d = 10, its = 53
i = 8141 (of 10192), d = 10, its = 37
i = 8142 (of 10192), d = 10, its = 55
i = 8143 (of 10192), d = 2.39502, its = 5
i = 8144 (of 10192), d = 10, its = 37
i = 8145 (of 10192), d = 10, its = 52
i = 8146 (of 10192), d = 0.612235, its = 18
i = 8147 (of 10192), d = 1.65803, its = 6
i = 8148 (of 10192), d = 10, its = 50
i = 8149 (of 10192), d = 10, its = 37
i = 8150 (of 10192), d = 10, its = 53
i = 8151 (of 10192), d = 10, its = 52
i = 8152 (of 10192), d = 10, its = 54
i = 8153 (of 10192), d = 10, its = 40
i = 8154 (of 10192), d = 10, its = 41
i = 8155 (of 10192), d = 10, its = 51
i = 8156 (of 10192), d = 10, its = 37
i = 8157 (of 10192), d = 10, its = 51
i = 8158 (of 10192), d = 10, its = 48
i = 8159 (of 10192), d = 10, its = 51
i = 8160 (of 10192), d = 5.37339, its = 8
i = 8161 (of 10192), d = 10, its = 55
i = 8162 (of 10192), d = 2.32254, its = 5
i = 8163 (of 10192), d = 10, its = 54
i = 8164 (of 10192), d = 1.71572, its = 5
i = 8165 (of 10192), d = 10, its = 53
i = 8166 (of 10192), d = 10, its = 37
i = 8167 (of 10192), d = 10, its = 50
i = 8168 (of 10192), d = 1.69325, its = 5
i = 8169 (of 10192), d = 10, its = 41
i = 8170 (of 10192), d = 10, its = 52
i = 8171 (of 10192), d = 10, its = 50
i = 8172 (of 10192), d = 10, its = 54
i = 8173 (of 10192), d = 10, its = 58
i = 8174 (of 10192), d = 10, its = 37
i = 8175 (of 10192), d = 10, its = 39
i = 8176 (of 10192), d = 10, its = 40
i = 8177 (of 10192), d = 10, its = 41
i = 8178 (of 10192), d = 4.25201, its = 6
i = 8179 (of 10192), d = 10, its = 55
i = 8180 (of 10192), d = 10, its = 55
i = 8181 (of 10192), d = 2.55079, its = 6
i = 8182 (of 10192), d = 10, its = 52
i = 8183 (of 10192), d = 2.52556, its = 5
i = 8184 (of 10192), d = 1.38032, its = 6
i = 8185 (of 10192), d = 10, its = 41
i = 8186 (of 10192), d = 0.498283, its = 17
i = 8187 (of 10192), d = 10, its = 53
i = 8188 (of 10192), d = 10, its = 52
i = 8189 (of 10192), d = 10, its = 52
i = 8190 (of 10192), d = 10, its = 54
i = 8191 (of 10192), d = 10, its = 53
i = 8192 (of 10192), d = 10, its = 55
i = 8193 (of 10192), d = 10, its = 53
i = 8194 (of 10192), d = 10, its = 51
i = 8195 (of 10192), d = 10, its = 50
i = 8196 (of 10192), d = 10, its = 37
i = 8197 (of 10192), d = 10, its = 52
i = 8198 (of 10192), d = 10, its = 51
i = 8199 (of 10192), d = 10, its = 37
i = 8200 (of 10192), d = 10, its = 53
i = 8201 (of 10192), d = 10, its = 50
i = 8202 (of 10192), d = 10, its = 55
i = 8203 (of 10192), d = 1.85916, its = 4
i = 8204 (of 10192), d = 10, its = 54
i = 8205 (of 10192), d = 10, its = 53
i = 8206 (of 10192), d = 10, its = 51
i = 8207 (of 10192), d = 10, its = 52
i = 8208 (of 10192), d = 10, its = 51
i = 8209 (of 10192), d = 10, its = 40
i = 8210 (of 10192), d = 0.727862, its = 19
i = 8211 (of 10192), d = 10, its = 40
i = 8212 (of 10192), d = 10, its = 50
i = 8213 (of 10192), d = 10, its = 45
i = 8214 (of 10192), d = 0.795029, its = 15
i = 8215 (of 10192), d = 10, its = 49
i = 8216 (of 10192), d = 0.478319, its = 17
i = 8217 (of 10192), d = 1.94912, its = 3
i = 8218 (of 10192), d = 10, its = 46
i = 8219 (of 10192), d = 10, its = 51
i = 8220 (of 10192), d = 10, its = 55
i = 8221 (of 10192), d = 10, its = 51
i = 8222 (of 10192), d = 10, its = 51
i = 8223 (of 10192), d = 10, its = 39
i = 8224 (of 10192), d = 1.48158, its = 6
i = 8225 (of 10192), d = 2.076, its = 4
i = 8226 (of 10192), d = 10, its = 51
i = 8227 (of 10192), d = 10, its = 55
i = 8228 (of 10192), d = 10, its = 53
i = 8229 (of 10192), d = 10, its = 37
i = 8230 (of 10192), d = 10, its = 51
i = 8231 (of 10192), d = 10, its = 37
i = 8232 (of 10192), d = 2.02645, its = 3
i = 8233 (of 10192), d = 10, its = 50
i = 8234 (of 10192), d = 1.60942, its = 6
i = 8235 (of 10192), d = 10, its = 53
i = 8236 (of 10192), d = 2.80757, its = 6
i = 8237 (of 10192), d = 10, its = 47
i = 8238 (of 10192), d = 2.21929, its = 5
i = 8239 (of 10192), d = 10, its = 51
i = 8240 (of 10192), d = 10, its = 56
i = 8241 (of 10192), d = 10, its = 56
i = 8242 (of 10192), d = 10, its = 53
i = 8243 (of 10192), d = 10, its = 51
i = 8244 (of 10192), d = 10, its = 37
i = 8245 (of 10192), d = 10, its = 51
i = 8246 (of 10192), d = 10, its = 40
i = 8247 (of 10192), d = 10, its = 54
i = 8248 (of 10192), d = 10, its = 51
i = 8249 (of 10192), d = 10, its = 57
i = 8250 (of 10192), d = 10, its = 48
i = 8251 (of 10192), d = 10, its = 55
i = 8252 (of 10192), d = 10, its = 50
i = 8253 (of 10192), d = 10, its = 51
i = 8254 (of 10192), d = 10, its = 56
i = 8255 (of 10192), d = 0.753083, its = 19
i = 8256 (of 10192), d = 10, its = 54
i = 8257 (of 10192), d = 2.01845, its = 3
i = 8258 (of 10192), d = 10, its = 37
i = 8259 (of 10192), d = 10, its = 53
i = 8260 (of 10192), d = 10, its = 39
i = 8261 (of 10192), d = 10, its = 55
i = 8262 (of 10192), d = 10, its = 37
i = 8263 (of 10192), d = 2.13845, its = 4
i = 8264 (of 10192), d = 1.17998, its = 5
i = 8265 (of 10192), d = 10, its = 49
i = 8266 (of 10192), d = 10, its = 51
i = 8267 (of 10192), d = 10, its = 51
i = 8268 (of 10192), d = 10, its = 52
i = 8269 (of 10192), d = 10, its = 37
i = 8270 (of 10192), d = 10, its = 37
i = 8271 (of 10192), d = 10, its = 45
i = 8272 (of 10192), d = 1.91462, its = 4
i = 8273 (of 10192), d = 1.47846, its = 6
i = 8274 (of 10192), d = 0.677697, its = 16
i = 8275 (of 10192), d = 10, its = 52
i = 8276 (of 10192), d = 0.858121, its = 20
i = 8277 (of 10192), d = 0.536136, its = 17
i = 8278 (of 10192), d = 10, its = 55
i = 8279 (of 10192), d = 10, its = 37
i = 8280 (of 10192), d = 1.85068, its = 4
i = 8281 (of 10192), d = 10, its = 39
i = 8282 (of 10192), d = 1.36475, its = 6
i = 8283 (of 10192), d = 1.05648, its = 21
i = 8284 (of 10192), d = 10, its = 51
i = 8285 (of 10192), d = 1.51217, its = 6
i = 8286 (of 10192), d = 10, its = 55
i = 8287 (of 10192), d = 10, its = 54
i = 8288 (of 10192), d = 10, its = 48
i = 8289 (of 10192), d = 10, its = 54
i = 8290 (of 10192), d = 10, its = 37
i = 8291 (of 10192), d = 1.63467, its = 5
i = 8292 (of 10192), d = 10, its = 52
i = 8293 (of 10192), d = 10, its = 47
i = 8294 (of 10192), d = 10, its = 53
i = 8295 (of 10192), d = 3.26001, its = 6
i = 8296 (of 10192), d = 10, its = 38
i = 8297 (of 10192), d = 10, its = 39
i = 8298 (of 10192), d = 10, its = 37
i = 8299 (of 10192), d = 10, its = 37
i = 8300 (of 10192), d = 10, its = 37
i = 8301 (of 10192), d = 10, its = 55
i = 8302 (of 10192), d = 10, its = 54
i = 8303 (of 10192), d = 10, its = 49
i = 8304 (of 10192), d = 10, its = 49
i = 8305 (of 10192), d = 10, its = 50
i = 8306 (of 10192), d = 10, its = 52
i = 8307 (of 10192), d = 10, its = 37
i = 8308 (of 10192), d = 10, its = 52
i = 8309 (of 10192), d = 10, its = 54
i = 8310 (of 10192), d = 10, its = 51
i = 8311 (of 10192), d = 10, its = 49
i = 8312 (of 10192), d = 10, its = 53
i = 8313 (of 10192), d = 10, its = 53
i = 8314 (of 10192), d = 10, its = 50
i = 8315 (of 10192), d = 10, its = 37
i = 8316 (of 10192), d = 2.08858, its = 4
i = 8317 (of 10192), d = 2.58404, its = 6
i = 8318 (of 10192), d = 10, its = 40
i = 8319 (of 10192), d = 10, its = 53
i = 8320 (of 10192), d = 10, its = 37
i = 8321 (of 10192), d = 1.59642, its = 6
i = 8322 (of 10192), d = 2.06913, its = 3
i = 8323 (of 10192), d = 10, its = 57
i = 8324 (of 10192), d = 3.02193, its = 6
i = 8325 (of 10192), d = 1.00148, its = 21
i = 8326 (of 10192), d = 10, its = 55
i = 8327 (of 10192), d = 10, its = 56
i = 8328 (of 10192), d = 2.14128, its = 4
i = 8329 (of 10192), d = 2.28844, its = 5
i = 8330 (of 10192), d = 0.966069, its = 17
i = 8331 (of 10192), d = 10, its = 41
i = 8332 (of 10192), d = 10, its = 48
i = 8333 (of 10192), d = 2.30069, its = 5
i = 8334 (of 10192), d = 10, its = 48
i = 8335 (of 10192), d = 10, its = 41
i = 8336 (of 10192), d = 1.94573, its = 4
i = 8337 (of 10192), d = 10, its = 53
i = 8338 (of 10192), d = 10, its = 53
i = 8339 (of 10192), d = 1.40255, its = 7
i = 8340 (of 10192), d = 10, its = 57
i = 8341 (of 10192), d = 10, its = 37
i = 8342 (of 10192), d = 0.644198, its = 22
i = 8343 (of 10192), d = 10, its = 51
i = 8344 (of 10192), d = 10, its = 54
i = 8345 (of 10192), d = 10, its = 57
i = 8346 (of 10192), d = 10, its = 51
i = 8347 (of 10192), d = 10, its = 52
i = 8348 (of 10192), d = 10, its = 52
i = 8349 (of 10192), d = 1.57393, its = 6
i = 8350 (of 10192), d = 10, its = 53
i = 8351 (of 10192), d = 10, its = 41
i = 8352 (of 10192), d = 0.393489, its = 22
i = 8353 (of 10192), d = 10, its = 42
i = 8354 (of 10192), d = 10, its = 54
i = 8355 (of 10192), d = 1.86395, its = 4
i = 8356 (of 10192), d = 10, its = 44
i = 8357 (of 10192), d = 10, its = 54
i = 8358 (of 10192), d = 10, its = 52
i = 8359 (of 10192), d = 2.25571, its = 4
i = 8360 (of 10192), d = 10, its = 37
i = 8361 (of 10192), d = 10, its = 37
i = 8362 (of 10192), d = 1.9647, its = 3
i = 8363 (of 10192), d = 10, its = 37
i = 8364 (of 10192), d = 0.541012, its = 16
i = 8365 (of 10192), d = 10, its = 57
i = 8366 (of 10192), d = 10, its = 40
i = 8367 (of 10192), d = 1.47875, its = 7
i = 8368 (of 10192), d = 10, its = 51
i = 8369 (of 10192), d = 2.20613, its = 4
i = 8370 (of 10192), d = 10, its = 40
i = 8371 (of 10192), d = 1.98719, its = 3
i = 8372 (of 10192), d = 10, its = 49
i = 8373 (of 10192), d = 2.2157, its = 4
i = 8374 (of 10192), d = 10, its = 51
i = 8375 (of 10192), d = 2.09541, its = 4
i = 8376 (of 10192), d = 10, its = 51
i = 8377 (of 10192), d = 10, its = 48
i = 8378 (of 10192), d = 1.84224, its = 4
i = 8379 (of 10192), d = 1.80501, its = 5
i = 8380 (of 10192), d = 10, its = 55
i = 8381 (of 10192), d = 10, its = 37
i = 8382 (of 10192), d = 5.76957, its = 8
i = 8383 (of 10192), d = 10, its = 52
i = 8384 (of 10192), d = 10, its = 57
i = 8385 (of 10192), d = 0.674041, its = 16
i = 8386 (of 10192), d = 10, its = 41
i = 8387 (of 10192), d = 1.41851, its = 6
i = 8388 (of 10192), d = 1.67367, its = 5
i = 8389 (of 10192), d = 10, its = 40
i = 8390 (of 10192), d = 10, its = 54
i = 8391 (of 10192), d = 10, its = 50
i = 8392 (of 10192), d = 10, its = 50
i = 8393 (of 10192), d = 10, its = 55
i = 8394 (of 10192), d = 10, its = 37
i = 8395 (of 10192), d = 10, its = 55
i = 8396 (of 10192), d = 10, its = 53
i = 8397 (of 10192), d = 0.39654, its = 19
i = 8398 (of 10192), d = 10, its = 49
i = 8399 (of 10192), d = 1.10562, its = 22
i = 8400 (of 10192), d = 10, its = 48
i = 8401 (of 10192), d = 10, its = 37
i = 8402 (of 10192), d = 7.68478, its = 9
i = 8403 (of 10192), d = 10, its = 58
i = 8404 (of 10192), d = 10, its = 41
i = 8405 (of 10192), d = 1.47846, its = 6
i = 8406 (of 10192), d = 2.28757, its = 5
i = 8407 (of 10192), d = 10, its = 37
i = 8408 (of 10192), d = 10, its = 53
i = 8409 (of 10192), d = 10, its = 55
i = 8410 (of 10192), d = 10, its = 56
i = 8411 (of 10192), d = 10, its = 49
i = 8412 (of 10192), d = 10, its = 41
i = 8413 (of 10192), d = 0.764482, its = 19
i = 8414 (of 10192), d = 10, its = 55
i = 8415 (of 10192), d = 10, its = 52
i = 8416 (of 10192), d = 10, its = 55
i = 8417 (of 10192), d = 10, its = 50
i = 8418 (of 10192), d = 10, its = 51
i = 8419 (of 10192), d = 10, its = 40
i = 8420 (of 10192), d = 1.29369, its = 6
i = 8421 (of 10192), d = 10, its = 51
i = 8422 (of 10192), d = 10, its = 55
i = 8423 (of 10192), d = 10, its = 55
i = 8424 (of 10192), d = 10, its = 52
i = 8425 (of 10192), d = 10, its = 57
i = 8426 (of 10192), d = 10, its = 53
i = 8427 (of 10192), d = 10, its = 50
i = 8428 (of 10192), d = 10, its = 49
i = 8429 (of 10192), d = 10, its = 40
i = 8430 (of 10192), d = 0.973688, its = 18
i = 8431 (of 10192), d = 2.38261, its = 5
i = 8432 (of 10192), d = 10, its = 37
i = 8433 (of 10192), d = 10, its = 55
i = 8434 (of 10192), d = 10, its = 41
i = 8435 (of 10192), d = 10, its = 51
i = 8436 (of 10192), d = 10, its = 37
i = 8437 (of 10192), d = 10, its = 39
i = 8438 (of 10192), d = 10, its = 37
i = 8439 (of 10192), d = 10, its = 40
i = 8440 (of 10192), d = 5.50493, its = 7
i = 8441 (of 10192), d = 4.6813, its = 8
i = 8442 (of 10192), d = 10, its = 55
i = 8443 (of 10192), d = 2.6592, its = 6
i = 8444 (of 10192), d = 10, its = 37
i = 8445 (of 10192), d = 10, its = 54
i = 8446 (of 10192), d = 2.72899, its = 6
i = 8447 (of 10192), d = 10, its = 41
i = 8448 (of 10192), d = 10, its = 53
i = 8449 (of 10192), d = 0.89624, its = 22
i = 8450 (of 10192), d = 2.78137, its = 5
i = 8451 (of 10192), d = 3.02596, its = 6
i = 8452 (of 10192), d = 10, its = 53
i = 8453 (of 10192), d = 10, its = 52
i = 8454 (of 10192), d = 10, its = 37
i = 8455 (of 10192), d = 2.44107, its = 5
i = 8456 (of 10192), d = 10, its = 39
i = 8457 (of 10192), d = 10, its = 53
i = 8458 (of 10192), d = 10, its = 54
i = 8459 (of 10192), d = 10, its = 50
i = 8460 (of 10192), d = 10, its = 50
i = 8461 (of 10192), d = 10, its = 55
i = 8462 (of 10192), d = 10, its = 51
i = 8463 (of 10192), d = 2.52528, its = 5
i = 8464 (of 10192), d = 1.79376, its = 5
i = 8465 (of 10192), d = 10, its = 48
i = 8466 (of 10192), d = 2.0252, its = 3
i = 8467 (of 10192), d = 10, its = 37
i = 8468 (of 10192), d = 10, its = 40
i = 8469 (of 10192), d = 10, its = 51
i = 8470 (of 10192), d = 10, its = 44
i = 8471 (of 10192), d = 10, its = 52
i = 8472 (of 10192), d = 1.06372, its = 20
i = 8473 (of 10192), d = 10, its = 37
i = 8474 (of 10192), d = 1.97055, its = 3
i = 8475 (of 10192), d = 1.76915, its = 4
i = 8476 (of 10192), d = 2.74332, its = 5
i = 8477 (of 10192), d = 10, its = 56
i = 8478 (of 10192), d = 0.905532, its = 22
i = 8479 (of 10192), d = 10, its = 37
i = 8480 (of 10192), d = 10, its = 58
i = 8481 (of 10192), d = 0.981816, its = 18
i = 8482 (of 10192), d = 10, its = 52
i = 8483 (of 10192), d = 10, its = 57
i = 8484 (of 10192), d = 10, its = 37
i = 8485 (of 10192), d = 10, its = 38
i = 8486 (of 10192), d = 10, its = 52
i = 8487 (of 10192), d = 1.10473, its = 16
i = 8488 (of 10192), d = 10, its = 49
i = 8489 (of 10192), d = 2.61126, its = 6
i = 8490 (of 10192), d = 10, its = 39
i = 8491 (of 10192), d = 10, its = 53
i = 8492 (of 10192), d = 10, its = 54
i = 8493 (of 10192), d = 10, its = 51
i = 8494 (of 10192), d = 10, its = 42
i = 8495 (of 10192), d = 10, its = 50
i = 8496 (of 10192), d = 10, its = 41
i = 8497 (of 10192), d = 10, its = 55
i = 8498 (of 10192), d = 2.87508, its = 5
i = 8499 (of 10192), d = 10, its = 55
i = 8500 (of 10192), d = 10, its = 37
i = 8501 (of 10192), d = 2.01116, its = 3
i = 8502 (of 10192), d = 1.3858, its = 6
i = 8503 (of 10192), d = 10, its = 56
i = 8504 (of 10192), d = 10, its = 37
i = 8505 (of 10192), d = 10, its = 51
i = 8506 (of 10192), d = 10, its = 37
i = 8507 (of 10192), d = 10, its = 53
i = 8508 (of 10192), d = 10, its = 52
i = 8509 (of 10192), d = 1.38886, its = 6
i = 8510 (of 10192), d = 1.72988, its = 5
i = 8511 (of 10192), d = 10, its = 53
i = 8512 (of 10192), d = 10, its = 39
i = 8513 (of 10192), d = 10, its = 56
i = 8514 (of 10192), d = 10, its = 51
i = 8515 (of 10192), d = 10, its = 56
i = 8516 (of 10192), d = 10, its = 55
i = 8517 (of 10192), d = 2.90389, its = 7
i = 8518 (of 10192), d = 10, its = 50
i = 8519 (of 10192), d = 10, its = 55
i = 8520 (of 10192), d = 2.69886, its = 5
i = 8521 (of 10192), d = 10, its = 51
i = 8522 (of 10192), d = 10, its = 53
i = 8523 (of 10192), d = 1.93489, its = 4
i = 8524 (of 10192), d = 10, its = 40
i = 8525 (of 10192), d = 10, its = 52
i = 8526 (of 10192), d = 1.88894, its = 4
i = 8527 (of 10192), d = 10, its = 50
i = 8528 (of 10192), d = 2.59739, its = 6
i = 8529 (of 10192), d = 10, its = 53
i = 8530 (of 10192), d = 1.48311, its = 6
i = 8531 (of 10192), d = 10, its = 42
i = 8532 (of 10192), d = 10, its = 39
i = 8533 (of 10192), d = 10, its = 51
i = 8534 (of 10192), d = 2.19205, its = 4
i = 8535 (of 10192), d = 10, its = 50
i = 8536 (of 10192), d = 2.5879, its = 5
i = 8537 (of 10192), d = 10, its = 44
i = 8538 (of 10192), d = 10, its = 54
i = 8539 (of 10192), d = 10, its = 52
i = 8540 (of 10192), d = 10, its = 37
i = 8541 (of 10192), d = 10, its = 54
i = 8542 (of 10192), d = 10, its = 51
i = 8543 (of 10192), d = 10, its = 55
i = 8544 (of 10192), d = 10, its = 54
i = 8545 (of 10192), d = 10, its = 54
i = 8546 (of 10192), d = 10, its = 37
i = 8547 (of 10192), d = 2.03442, its = 3
i = 8548 (of 10192), d = 10, its = 39
i = 8549 (of 10192), d = 10, its = 50
i = 8550 (of 10192), d = 1.7102, its = 7
i = 8551 (of 10192), d = 10, its = 37
i = 8552 (of 10192), d = 10, its = 38
i = 8553 (of 10192), d = 10, its = 40
i = 8554 (of 10192), d = 10, its = 53
i = 8555 (of 10192), d = 10, its = 39
i = 8556 (of 10192), d = 10, its = 53
i = 8557 (of 10192), d = 10, its = 37
i = 8558 (of 10192), d = 1.60382, its = 6
i = 8559 (of 10192), d = 2.31905, its = 4
i = 8560 (of 10192), d = 10, its = 57
i = 8561 (of 10192), d = 10, its = 51
i = 8562 (of 10192), d = 10, its = 52
i = 8563 (of 10192), d = 6.19117, its = 8
i = 8564 (of 10192), d = 2.33454, its = 5
i = 8565 (of 10192), d = 10, its = 39
i = 8566 (of 10192), d = 2.37963, its = 5
i = 8567 (of 10192), d = 10, its = 41
i = 8568 (of 10192), d = 1.00928, its = 17
i = 8569 (of 10192), d = 10, its = 41
i = 8570 (of 10192), d = 10, its = 53
i = 8571 (of 10192), d = 1.43758, its = 8
i = 8572 (of 10192), d = 10, its = 37
i = 8573 (of 10192), d = 1.24423, its = 6
i = 8574 (of 10192), d = 10, its = 51
i = 8575 (of 10192), d = 3.14726, its = 6
i = 8576 (of 10192), d = 10, its = 50
i = 8577 (of 10192), d = 10, its = 39
i = 8578 (of 10192), d = 10, its = 53
i = 8579 (of 10192), d = 10, its = 37
i = 8580 (of 10192), d = 10, its = 37
i = 8581 (of 10192), d = 10, its = 37
i = 8582 (of 10192), d = 10, its = 53
i = 8583 (of 10192), d = 2.35244, its = 5
i = 8584 (of 10192), d = 1.98795, its = 3
i = 8585 (of 10192), d = 10, its = 50
i = 8586 (of 10192), d = 0.787771, its = 16
i = 8587 (of 10192), d = 1.25359, its = 7
i = 8588 (of 10192), d = 10, its = 47
i = 8589 (of 10192), d = 10, its = 54
i = 8590 (of 10192), d = 10, its = 53
i = 8591 (of 10192), d = 1.10594, its = 22
i = 8592 (of 10192), d = 10, its = 50
i = 8593 (of 10192), d = 10, its = 37
i = 8594 (of 10192), d = 10, its = 37
i = 8595 (of 10192), d = 10, its = 37
i = 8596 (of 10192), d = 10, its = 53
i = 8597 (of 10192), d = 10, its = 39
i = 8598 (of 10192), d = 10, its = 55
i = 8599 (of 10192), d = 10, its = 40
i = 8600 (of 10192), d = 10, its = 41
i = 8601 (of 10192), d = 10, its = 37
i = 8602 (of 10192), d = 10, its = 52
i = 8603 (of 10192), d = 1.9587, its = 3
i = 8604 (of 10192), d = 10, its = 51
i = 8605 (of 10192), d = 10, its = 56
i = 8606 (of 10192), d = 3.22354, its = 7
i = 8607 (of 10192), d = 3.0861, its = 6
i = 8608 (of 10192), d = 2.29133, its = 4
i = 8609 (of 10192), d = 10, its = 50
i = 8610 (of 10192), d = 10, its = 52
i = 8611 (of 10192), d = 10, its = 51
i = 8612 (of 10192), d = 10, its = 37
i = 8613 (of 10192), d = 10, its = 53
i = 8614 (of 10192), d = 10, its = 52
i = 8615 (of 10192), d = 1.70501, its = 5
i = 8616 (of 10192), d = 3.70846, its = 6
i = 8617 (of 10192), d = 10, its = 40
i = 8618 (of 10192), d = 10, its = 55
i = 8619 (of 10192), d = 10, its = 55
i = 8620 (of 10192), d = 10, its = 53
i = 8621 (of 10192), d = 10, its = 50
i = 8622 (of 10192), d = 10, its = 38
i = 8623 (of 10192), d = 4.76503, its = 7
i = 8624 (of 10192), d = 10, its = 52
i = 8625 (of 10192), d = 10, its = 56
i = 8626 (of 10192), d = 2.1602, its = 4
i = 8627 (of 10192), d = 10, its = 53
i = 8628 (of 10192), d = 10, its = 38
i = 8629 (of 10192), d = 10, its = 60
i = 8630 (of 10192), d = 2.46961, its = 6
i = 8631 (of 10192), d = 10, its = 55
i = 8632 (of 10192), d = 10, its = 39
i = 8633 (of 10192), d = 10, its = 51
i = 8634 (of 10192), d = 10, its = 40
i = 8635 (of 10192), d = 10, its = 51
i = 8636 (of 10192), d = 10, its = 51
i = 8637 (of 10192), d = 0.716971, its = 16
i = 8638 (of 10192), d = 10, its = 59
i = 8639 (of 10192), d = 10, its = 50
i = 8640 (of 10192), d = 10, its = 51
i = 8641 (of 10192), d = 10, its = 52
i = 8642 (of 10192), d = 3.31709, its = 7
i = 8643 (of 10192), d = 1.09151, its = 20
i = 8644 (of 10192), d = 1.54837, its = 6
i = 8645 (of 10192), d = 10, its = 51
i = 8646 (of 10192), d = 10, its = 52
i = 8647 (of 10192), d = 10, its = 55
i = 8648 (of 10192), d = 1.24793, its = 6
i = 8649 (of 10192), d = 10, its = 38
i = 8650 (of 10192), d = 10, its = 56
i = 8651 (of 10192), d = 10, its = 59
i = 8652 (of 10192), d = 10, its = 49
i = 8653 (of 10192), d = 10, its = 49
i = 8654 (of 10192), d = 1.312, its = 23
i = 8655 (of 10192), d = 10, its = 41
i = 8656 (of 10192), d = 10, its = 51
i = 8657 (of 10192), d = 0.900055, its = 18
i = 8658 (of 10192), d = 10, its = 50
i = 8659 (of 10192), d = 10, its = 53
i = 8660 (of 10192), d = 10, its = 52
i = 8661 (of 10192), d = 10, its = 49
i = 8662 (of 10192), d = 10, its = 56
i = 8663 (of 10192), d = 10, its = 54
i = 8664 (of 10192), d = 0.612953, its = 18
i = 8665 (of 10192), d = 2.62641, its = 5
i = 8666 (of 10192), d = 10, its = 57
i = 8667 (of 10192), d = 10, its = 14
i = 8668 (of 10192), d = 10, its = 57
i = 8669 (of 10192), d = 10, its = 37
i = 8670 (of 10192), d = 5.17377, its = 8
i = 8671 (of 10192), d = 10, its = 52
i = 8672 (of 10192), d = 1.14176, its = 20
i = 8673 (of 10192), d = 10, its = 42
i = 8674 (of 10192), d = 10, its = 37
i = 8675 (of 10192), d = 10, its = 57
i = 8676 (of 10192), d = 10, its = 51
i = 8677 (of 10192), d = 10, its = 37
i = 8678 (of 10192), d = 10, its = 37
i = 8679 (of 10192), d = 10, its = 39
i = 8680 (of 10192), d = 10, its = 37
i = 8681 (of 10192), d = 3.26441, its = 6
i = 8682 (of 10192), d = 10, its = 39
i = 8683 (of 10192), d = 10, its = 55
i = 8684 (of 10192), d = 10, its = 52
i = 8685 (of 10192), d = 10, its = 57
i = 8686 (of 10192), d = 10, its = 55
i = 8687 (of 10192), d = 10, its = 51
i = 8688 (of 10192), d = 0.561765, its = 19
i = 8689 (of 10192), d = 10, its = 49
i = 8690 (of 10192), d = 0.89008, its = 25
i = 8691 (of 10192), d = 10, its = 56
i = 8692 (of 10192), d = 1.82096, its = 4
i = 8693 (of 10192), d = 10, its = 51
i = 8694 (of 10192), d = 2.52147, its = 5
i = 8695 (of 10192), d = 10, its = 54
i = 8696 (of 10192), d = 10, its = 39
i = 8697 (of 10192), d = 10, its = 37
i = 8698 (of 10192), d = 10, its = 37
i = 8699 (of 10192), d = 10, its = 56
i = 8700 (of 10192), d = 2.46348, its = 5
i = 8701 (of 10192), d = 10, its = 55
i = 8702 (of 10192), d = 2.2517, its = 4
i = 8703 (of 10192), d = 1.45434, its = 7
i = 8704 (of 10192), d = 10, its = 49
i = 8705 (of 10192), d = 10, its = 57
i = 8706 (of 10192), d = 1.18868, its = 5
i = 8707 (of 10192), d = 10, its = 41
i = 8708 (of 10192), d = 10, its = 37
i = 8709 (of 10192), d = 10, its = 41
i = 8710 (of 10192), d = 10, its = 37
i = 8711 (of 10192), d = 10, its = 57
i = 8712 (of 10192), d = 10, its = 52
i = 8713 (of 10192), d = 10, its = 51
i = 8714 (of 10192), d = 10, its = 40
i = 8715 (of 10192), d = 10, its = 37
i = 8716 (of 10192), d = 2.21587, its = 4
i = 8717 (of 10192), d = 10, its = 51
i = 8718 (of 10192), d = 10, its = 37
i = 8719 (of 10192), d = 2.34257, its = 5
i = 8720 (of 10192), d = 10, its = 53
i = 8721 (of 10192), d = 10, its = 51
i = 8722 (of 10192), d = 10, its = 55
i = 8723 (of 10192), d = 1.75165, its = 5
i = 8724 (of 10192), d = 10, its = 51
i = 8725 (of 10192), d = 10, its = 54
i = 8726 (of 10192), d = 2.81737, its = 6
i = 8727 (of 10192), d = 1.70936, its = 5
i = 8728 (of 10192), d = 1.1728, its = 7
i = 8729 (of 10192), d = 1.86584, its = 4
i = 8730 (of 10192), d = 3.39125, its = 6
i = 8731 (of 10192), d = 10, its = 51
i = 8732 (of 10192), d = 10, its = 54
i = 8733 (of 10192), d = 10, its = 54
i = 8734 (of 10192), d = 3.37727, its = 8
i = 8735 (of 10192), d = 10, its = 49
i = 8736 (of 10192), d = 10, its = 49
i = 8737 (of 10192), d = 3.09293, its = 6
i = 8738 (of 10192), d = 0.560824, its = 20
i = 8739 (of 10192), d = 1.49055, its = 7
i = 8740 (of 10192), d = 0.881562, its = 25
i = 8741 (of 10192), d = 2.01984, its = 3
i = 8742 (of 10192), d = 10, its = 57
i = 8743 (of 10192), d = 2.01217, its = 3
i = 8744 (of 10192), d = 2.07981, its = 4
i = 8745 (of 10192), d = 0.785377, its = 16
i = 8746 (of 10192), d = 0.691811, its = 19
i = 8747 (of 10192), d = 10, its = 52
i = 8748 (of 10192), d = 10, its = 40
i = 8749 (of 10192), d = 10, its = 53
i = 8750 (of 10192), d = 10, its = 39
i = 8751 (of 10192), d = 1.60088, its = 6
i = 8752 (of 10192), d = 10, its = 52
i = 8753 (of 10192), d = 3.11822, its = 7
i = 8754 (of 10192), d = 10, its = 54
i = 8755 (of 10192), d = 5.07026, its = 8
i = 8756 (of 10192), d = 10, its = 50
i = 8757 (of 10192), d = 0.313731, its = 18
i = 8758 (of 10192), d = 2.28918, its = 5
i = 8759 (of 10192), d = 2.23379, its = 5
i = 8760 (of 10192), d = 10, its = 52
i = 8761 (of 10192), d = 1.21995, its = 7
i = 8762 (of 10192), d = 10, its = 53
i = 8763 (of 10192), d = 1.20023, its = 8
i = 8764 (of 10192), d = 10, its = 37
i = 8765 (of 10192), d = 10, its = 46
i = 8766 (of 10192), d = 10, its = 57
i = 8767 (of 10192), d = 10, its = 46
i = 8768 (of 10192), d = 10, its = 49
i = 8769 (of 10192), d = 10, its = 51
i = 8770 (of 10192), d = 10, its = 43
i = 8771 (of 10192), d = 1.06729, its = 21
i = 8772 (of 10192), d = 10, its = 52
i = 8773 (of 10192), d = 10, its = 48
i = 8774 (of 10192), d = 10, its = 55
i = 8775 (of 10192), d = 0.843094, its = 25
i = 8776 (of 10192), d = 10, its = 56
i = 8777 (of 10192), d = 10, its = 37
i = 8778 (of 10192), d = 10, its = 53
i = 8779 (of 10192), d = 10, its = 56
i = 8780 (of 10192), d = 10, its = 54
i = 8781 (of 10192), d = 1.91126, its = 4
i = 8782 (of 10192), d = 10, its = 50
i = 8783 (of 10192), d = 10, its = 42
i = 8784 (of 10192), d = 10, its = 55
i = 8785 (of 10192), d = 10, its = 41
i = 8786 (of 10192), d = 10, its = 51
i = 8787 (of 10192), d = 10, its = 51
i = 8788 (of 10192), d = 1.77278, its = 5
i = 8789 (of 10192), d = 10, its = 39
i = 8790 (of 10192), d = 10, its = 51
i = 8791 (of 10192), d = 10, its = 50
i = 8792 (of 10192), d = 10, its = 53
i = 8793 (of 10192), d = 10, its = 56
i = 8794 (of 10192), d = 10, its = 53
i = 8795 (of 10192), d = 10, its = 37
i = 8796 (of 10192), d = 1.1286, its = 23
i = 8797 (of 10192), d = 10, its = 53
i = 8798 (of 10192), d = 10, its = 55
i = 8799 (of 10192), d = 10, its = 51
i = 8800 (of 10192), d = 10, its = 56
i = 8801 (of 10192), d = 10, its = 37
i = 8802 (of 10192), d = 10, its = 52
i = 8803 (of 10192), d = 10, its = 52
i = 8804 (of 10192), d = 1.28816, its = 7
i = 8805 (of 10192), d = 10, its = 54
i = 8806 (of 10192), d = 1.49114, its = 7
i = 8807 (of 10192), d = 10, its = 48
i = 8808 (of 10192), d = 10, its = 40
i = 8809 (of 10192), d = 10, its = 56
i = 8810 (of 10192), d = 10, its = 37
i = 8811 (of 10192), d = 10, its = 53
i = 8812 (of 10192), d = 10, its = 50
i = 8813 (of 10192), d = 10, its = 52
i = 8814 (of 10192), d = 2.06153, its = 4
i = 8815 (of 10192), d = 0.822232, its = 20
i = 8816 (of 10192), d = 1.15001, its = 21
i = 8817 (of 10192), d = 10, its = 51
i = 8818 (of 10192), d = 1.68082, its = 6
i = 8819 (of 10192), d = 10, its = 55
i = 8820 (of 10192), d = 10, its = 37
i = 8821 (of 10192), d = 10, its = 37
i = 8822 (of 10192), d = 10, its = 55
i = 8823 (of 10192), d = 10, its = 37
i = 8824 (of 10192), d = 2.89055, its = 5
i = 8825 (of 10192), d = 10, its = 55
i = 8826 (of 10192), d = 2.42344, its = 5
i = 8827 (of 10192), d = 10, its = 51
i = 8828 (of 10192), d = 10, its = 48
i = 8829 (of 10192), d = 10, its = 46
i = 8830 (of 10192), d = 10, its = 55
i = 8831 (of 10192), d = 10, its = 39
i = 8832 (of 10192), d = 10, its = 52
i = 8833 (of 10192), d = 10, its = 58
i = 8834 (of 10192), d = 10, its = 37
i = 8835 (of 10192), d = 3.79602, its = 6
i = 8836 (of 10192), d = 10, its = 37
i = 8837 (of 10192), d = 10, its = 40
i = 8838 (of 10192), d = 10, its = 52
i = 8839 (of 10192), d = 3.12179, its = 6
i = 8840 (of 10192), d = 2.37552, its = 5
i = 8841 (of 10192), d = 10, its = 55
i = 8842 (of 10192), d = 2.28768, its = 5
i = 8843 (of 10192), d = 0.713269, its = 17
i = 8844 (of 10192), d = 10, its = 50
i = 8845 (of 10192), d = 0.530037, its = 17
i = 8846 (of 10192), d = 2.85134, its = 6
i = 8847 (of 10192), d = 10, its = 37
i = 8848 (of 10192), d = 10, its = 53
i = 8849 (of 10192), d = 10, its = 53
i = 8850 (of 10192), d = 10, its = 55
i = 8851 (of 10192), d = 2.29213, its = 4
i = 8852 (of 10192), d = 10, its = 51
i = 8853 (of 10192), d = 10, its = 53
i = 8854 (of 10192), d = 10, its = 38
i = 8855 (of 10192), d = 10, its = 42
i = 8856 (of 10192), d = 10, its = 55
i = 8857 (of 10192), d = 1.50992, its = 6
i = 8858 (of 10192), d = 10, its = 51
i = 8859 (of 10192), d = 10, its = 37
i = 8860 (of 10192), d = 10, its = 52
i = 8861 (of 10192), d = 10, its = 40
i = 8862 (of 10192), d = 10, its = 40
i = 8863 (of 10192), d = 2.12305, its = 4
i = 8864 (of 10192), d = 10, its = 54
i = 8865 (of 10192), d = 10, its = 54
i = 8866 (of 10192), d = 6.43308, its = 8
i = 8867 (of 10192), d = 0.591373, its = 19
i = 8868 (of 10192), d = 2.91394, its = 6
i = 8869 (of 10192), d = 10, its = 54
i = 8870 (of 10192), d = 10, its = 54
i = 8871 (of 10192), d = 10, its = 37
i = 8872 (of 10192), d = 10, its = 37
i = 8873 (of 10192), d = 0.473384, its = 17
i = 8874 (of 10192), d = 0.666357, its = 17
i = 8875 (of 10192), d = 10, its = 45
i = 8876 (of 10192), d = 2.36949, its = 5
i = 8877 (of 10192), d = 10, its = 41
i = 8878 (of 10192), d = 10, its = 54
i = 8879 (of 10192), d = 10, its = 50
i = 8880 (of 10192), d = 1.8818, its = 4
i = 8881 (of 10192), d = 1.13298, its = 23
i = 8882 (of 10192), d = 10, its = 40
i = 8883 (of 10192), d = 10, its = 53
i = 8884 (of 10192), d = 10, its = 50
i = 8885 (of 10192), d = 10, its = 50
i = 8886 (of 10192), d = 10, its = 56
i = 8887 (of 10192), d = 1.87242, its = 4
i = 8888 (of 10192), d = 10, its = 54
i = 8889 (of 10192), d = 10, its = 56
i = 8890 (of 10192), d = 10, its = 53
i = 8891 (of 10192), d = 10, its = 56
i = 8892 (of 10192), d = 10, its = 37
i = 8893 (of 10192), d = 10, its = 55
i = 8894 (of 10192), d = 10, its = 53
i = 8895 (of 10192), d = 10, its = 49
i = 8896 (of 10192), d = 3.83496, its = 7
i = 8897 (of 10192), d = 10, its = 58
i = 8898 (of 10192), d = 10, its = 52
i = 8899 (of 10192), d = 8.04597, its = 8
i = 8900 (of 10192), d = 10, its = 40
i = 8901 (of 10192), d = 10, its = 54
i = 8902 (of 10192), d = 0.761189, its = 24
i = 8903 (of 10192), d = 10, its = 51
i = 8904 (of 10192), d = 10, its = 41
i = 8905 (of 10192), d = 10, its = 51
i = 8906 (of 10192), d = 10, its = 51
i = 8907 (of 10192), d = 10, its = 39
i = 8908 (of 10192), d = 10, its = 49
i = 8909 (of 10192), d = 10, its = 37
i = 8910 (of 10192), d = 10, its = 52
i = 8911 (of 10192), d = 10, its = 52
i = 8912 (of 10192), d = 5.1902, its = 7
i = 8913 (of 10192), d = 10, its = 52
i = 8914 (of 10192), d = 10, its = 55
i = 8915 (of 10192), d = 10, its = 50
i = 8916 (of 10192), d = 10, its = 51
i = 8917 (of 10192), d = 0.9804, its = 19
i = 8918 (of 10192), d = 10, its = 52
i = 8919 (of 10192), d = 10, its = 51
i = 8920 (of 10192), d = 10, its = 40
i = 8921 (of 10192), d = 10, its = 55
i = 8922 (of 10192), d = 0.763706, its = 18
i = 8923 (of 10192), d = 10, its = 55
i = 8924 (of 10192), d = 3.04369, its = 6
i = 8925 (of 10192), d = 2.17633, its = 4
i = 8926 (of 10192), d = 10, its = 40
i = 8927 (of 10192), d = 10, its = 60
i = 8928 (of 10192), d = 1.88409, its = 4
i = 8929 (of 10192), d = 0.999962, its = 21
i = 8930 (of 10192), d = 10, its = 54
i = 8931 (of 10192), d = 0.518211, its = 20
i = 8932 (of 10192), d = 10, its = 53
i = 8933 (of 10192), d = 10, its = 55
i = 8934 (of 10192), d = 10, its = 48
i = 8935 (of 10192), d = 10, its = 54
i = 8936 (of 10192), d = 10, its = 49
i = 8937 (of 10192), d = 10, its = 52
i = 8938 (of 10192), d = 10, its = 56
i = 8939 (of 10192), d = 10, its = 55
i = 8940 (of 10192), d = 10, its = 55
i = 8941 (of 10192), d = 10, its = 40
i = 8942 (of 10192), d = 0.633827, its = 20
i = 8943 (of 10192), d = 0.989935, its = 19
i = 8944 (of 10192), d = 10, its = 59
i = 8945 (of 10192), d = 10, its = 37
i = 8946 (of 10192), d = 10, its = 37
i = 8947 (of 10192), d = 10, its = 50
i = 8948 (of 10192), d = 10, its = 51
i = 8949 (of 10192), d = 10, its = 43
i = 8950 (of 10192), d = 10, its = 56
i = 8951 (of 10192), d = 10, its = 52
i = 8952 (of 10192), d = 10, its = 56
i = 8953 (of 10192), d = 10, its = 37
i = 8954 (of 10192), d = 1.55949, its = 6
i = 8955 (of 10192), d = 10, its = 52
i = 8956 (of 10192), d = 10, its = 51
i = 8957 (of 10192), d = 10, its = 55
i = 8958 (of 10192), d = 10, its = 51
i = 8959 (of 10192), d = 1.35968, its = 7
i = 8960 (of 10192), d = 10, its = 43
i = 8961 (of 10192), d = 10, its = 54
i = 8962 (of 10192), d = 10, its = 40
i = 8963 (of 10192), d = 1.39383, its = 6
i = 8964 (of 10192), d = 10, its = 55
i = 8965 (of 10192), d = 10, its = 52
i = 8966 (of 10192), d = 10, its = 57
i = 8967 (of 10192), d = 0.916458, its = 18
i = 8968 (of 10192), d = 0.841548, its = 19
i = 8969 (of 10192), d = 10, its = 37
i = 8970 (of 10192), d = 1.99308, its = 3
i = 8971 (of 10192), d = 10, its = 50
i = 8972 (of 10192), d = 1.84282, its = 4
i = 8973 (of 10192), d = 2.22709, its = 4
i = 8974 (of 10192), d = 1.93869, its = 3
i = 8975 (of 10192), d = 0.512164, its = 19
i = 8976 (of 10192), d = 10, its = 52
i = 8977 (of 10192), d = 10, its = 51
i = 8978 (of 10192), d = 10, its = 40
i = 8979 (of 10192), d = 2.62012, its = 5
i = 8980 (of 10192), d = 10, its = 42
i = 8981 (of 10192), d = 10, its = 56
i = 8982 (of 10192), d = 1.92913, its = 4
i = 8983 (of 10192), d = 10, its = 54
i = 8984 (of 10192), d = 2.70736, its = 5
i = 8985 (of 10192), d = 10, its = 54
i = 8986 (of 10192), d = 3.20652, its = 7
i = 8987 (of 10192), d = 10, its = 51
i = 8988 (of 10192), d = 1.99377, its = 2
i = 8989 (of 10192), d = 10, its = 49
i = 8990 (of 10192), d = 2.41447, its = 5
i = 8991 (of 10192), d = 10, its = 37
i = 8992 (of 10192), d = 10, its = 53
i = 8993 (of 10192), d = 10, its = 54
i = 8994 (of 10192), d = 10, its = 52
i = 8995 (of 10192), d = 10, its = 37
i = 8996 (of 10192), d = 10, its = 55
i = 8997 (of 10192), d = 10, its = 54
i = 8998 (of 10192), d = 0.98843, its = 16
i = 8999 (of 10192), d = 10, its = 37
i = 9000 (of 10192), d = 10, its = 56
i = 9001 (of 10192), d = 3.00359, its = 6
i = 9002 (of 10192), d = 10, its = 52
i = 9003 (of 10192), d = 10, its = 52
i = 9004 (of 10192), d = 10, its = 41
i = 9005 (of 10192), d = 10, its = 50
i = 9006 (of 10192), d = 10, its = 37
i = 9007 (of 10192), d = 10, its = 55
i = 9008 (of 10192), d = 2.06182, its = 3
i = 9009 (of 10192), d = 10, its = 37
i = 9010 (of 10192), d = 2.55077, its = 6
i = 9011 (of 10192), d = 1.07518, its = 19
i = 9012 (of 10192), d = 10, its = 52
i = 9013 (of 10192), d = 10, its = 49
i = 9014 (of 10192), d = 10, its = 52
i = 9015 (of 10192), d = 10, its = 54
i = 9016 (of 10192), d = 1.05345, its = 18
i = 9017 (of 10192), d = 2.09587, its = 4
i = 9018 (of 10192), d = 0.861774, its = 17
i = 9019 (of 10192), d = 2.32005, its = 5
i = 9020 (of 10192), d = 10, its = 52
i = 9021 (of 10192), d = 10, its = 49
i = 9022 (of 10192), d = 10, its = 50
i = 9023 (of 10192), d = 1.86448, its = 4
i = 9024 (of 10192), d = 10, its = 53
i = 9025 (of 10192), d = 10, its = 53
i = 9026 (of 10192), d = 10, its = 55
i = 9027 (of 10192), d = 1.65166, its = 5
i = 9028 (of 10192), d = 10, its = 40
i = 9029 (of 10192), d = 10, its = 54
i = 9030 (of 10192), d = 10, its = 49
i = 9031 (of 10192), d = 0.770519, its = 19
i = 9032 (of 10192), d = 10, its = 54
i = 9033 (of 10192), d = 1.54198, its = 7
i = 9034 (of 10192), d = 10, its = 37
i = 9035 (of 10192), d = 10, its = 37
i = 9036 (of 10192), d = 3.05133, its = 7
i = 9037 (of 10192), d = 10, its = 50
i = 9038 (of 10192), d = 10, its = 58
i = 9039 (of 10192), d = 10, its = 47
i = 9040 (of 10192), d = 10, its = 57
i = 9041 (of 10192), d = 10, its = 55
i = 9042 (of 10192), d = 1.92267, its = 4
i = 9043 (of 10192), d = 10, its = 40
i = 9044 (of 10192), d = 2.25042, its = 4
i = 9045 (of 10192), d = 10, its = 53
i = 9046 (of 10192), d = 10, its = 39
i = 9047 (of 10192), d = 10, its = 52
i = 9048 (of 10192), d = 6.91084, its = 8
i = 9049 (of 10192), d = 10, its = 52
i = 9050 (of 10192), d = 10, its = 37
i = 9051 (of 10192), d = 10, its = 53
i = 9052 (of 10192), d = 10, its = 53
i = 9053 (of 10192), d = 10, its = 39
i = 9054 (of 10192), d = 10, its = 58
i = 9055 (of 10192), d = 10, its = 52
i = 9056 (of 10192), d = 10, its = 50
i = 9057 (of 10192), d = 10, its = 53
i = 9058 (of 10192), d = 10, its = 37
i = 9059 (of 10192), d = 1.58766, its = 6
i = 9060 (of 10192), d = 10, its = 40
i = 9061 (of 10192), d = 2.62605, its = 6
i = 9062 (of 10192), d = 0.747657, its = 28
i = 9063 (of 10192), d = 1.23841, its = 15
i = 9064 (of 10192), d = 10, its = 37
i = 9065 (of 10192), d = 10, its = 51
i = 9066 (of 10192), d = 10, its = 37
i = 9067 (of 10192), d = 10, its = 49
i = 9068 (of 10192), d = 10, its = 50
i = 9069 (of 10192), d = 10, its = 54
i = 9070 (of 10192), d = 10, its = 51
i = 9071 (of 10192), d = 10, its = 39
i = 9072 (of 10192), d = 10, its = 37
i = 9073 (of 10192), d = 10, its = 52
i = 9074 (of 10192), d = 0.812205, its = 21
i = 9075 (of 10192), d = 10, its = 51
i = 9076 (of 10192), d = 10, its = 53
i = 9077 (of 10192), d = 1.04911, its = 18
i = 9078 (of 10192), d = 10, its = 49
i = 9079 (of 10192), d = 0.900055, its = 18
i = 9080 (of 10192), d = 10, its = 53
i = 9081 (of 10192), d = 0.772473, its = 20
i = 9082 (of 10192), d = 2.16481, its = 4
i = 9083 (of 10192), d = 10, its = 57
i = 9084 (of 10192), d = 2.42344, its = 5
i = 9085 (of 10192), d = 10, its = 53
i = 9086 (of 10192), d = 10, its = 49
i = 9087 (of 10192), d = 10, its = 53
i = 9088 (of 10192), d = 10, its = 37
i = 9089 (of 10192), d = 10, its = 55
i = 9090 (of 10192), d = 2.93904, its = 6
i = 9091 (of 10192), d = 10, its = 54
i = 9092 (of 10192), d = 1.92396, its = 4
i = 9093 (of 10192), d = 10, its = 53
i = 9094 (of 10192), d = 10, its = 37
i = 9095 (of 10192), d = 10, its = 53
i = 9096 (of 10192), d = 1.67733, its = 6
i = 9097 (of 10192), d = 10, its = 37
i = 9098 (of 10192), d = 10, its = 38
i = 9099 (of 10192), d = 1.42536, its = 7
i = 9100 (of 10192), d = 10, its = 39
i = 9101 (of 10192), d = 1.91329, its = 4
i = 9102 (of 10192), d = 10, its = 53
i = 9103 (of 10192), d = 10, its = 51
i = 9104 (of 10192), d = 10, its = 50
i = 9105 (of 10192), d = 10, its = 37
i = 9106 (of 10192), d = 10, its = 58
i = 9107 (of 10192), d = 10, its = 39
i = 9108 (of 10192), d = 2.06818, its = 4
i = 9109 (of 10192), d = 10, its = 54
i = 9110 (of 10192), d = 10, its = 53
i = 9111 (of 10192), d = 10, its = 40
i = 9112 (of 10192), d = 3.10388, its = 6
i = 9113 (of 10192), d = 3.61659, its = 6
i = 9114 (of 10192), d = 10, its = 39
i = 9115 (of 10192), d = 10, its = 53
i = 9116 (of 10192), d = 2.74003, its = 5
i = 9117 (of 10192), d = 10, its = 56
i = 9118 (of 10192), d = 10, its = 57
i = 9119 (of 10192), d = 2.17805, its = 4
i = 9120 (of 10192), d = 10, its = 57
i = 9121 (of 10192), d = 0.636141, its = 18
i = 9122 (of 10192), d = 1.89307, its = 4
i = 9123 (of 10192), d = 10, its = 39
i = 9124 (of 10192), d = 10, its = 56
i = 9125 (of 10192), d = 10, its = 37
i = 9126 (of 10192), d = 10, its = 52
i = 9127 (of 10192), d = 10, its = 54
i = 9128 (of 10192), d = 10, its = 51
i = 9129 (of 10192), d = 10, its = 55
i = 9130 (of 10192), d = 10, its = 51
i = 9131 (of 10192), d = 1.56299, its = 6
i = 9132 (of 10192), d = 10, its = 37
i = 9133 (of 10192), d = 10, its = 51
i = 9134 (of 10192), d = 10, its = 54
i = 9135 (of 10192), d = 10, its = 54
i = 9136 (of 10192), d = 10, its = 50
i = 9137 (of 10192), d = 1.59966, its = 7
i = 9138 (of 10192), d = 10, its = 54
i = 9139 (of 10192), d = 10, its = 52
i = 9140 (of 10192), d = 10, its = 37
i = 9141 (of 10192), d = 10, its = 53
i = 9142 (of 10192), d = 10, its = 41
i = 9143 (of 10192), d = 10, its = 40
i = 9144 (of 10192), d = 10, its = 51
i = 9145 (of 10192), d = 10, its = 51
i = 9146 (of 10192), d = 10, its = 53
i = 9147 (of 10192), d = 10, its = 40
i = 9148 (of 10192), d = 10, its = 38
i = 9149 (of 10192), d = 2.17404, its = 4
i = 9150 (of 10192), d = 10, its = 57
i = 9151 (of 10192), d = 10, its = 59
i = 9152 (of 10192), d = 10, its = 54
i = 9153 (of 10192), d = 10, its = 52
i = 9154 (of 10192), d = 10, its = 37
i = 9155 (of 10192), d = 10, its = 53
i = 9156 (of 10192), d = 10, its = 52
i = 9157 (of 10192), d = 10, its = 53
i = 9158 (of 10192), d = 10, its = 53
i = 9159 (of 10192), d = 10, its = 54
i = 9160 (of 10192), d = 10, its = 37
i = 9161 (of 10192), d = 2.12793, its = 4
i = 9162 (of 10192), d = 10, its = 50
i = 9163 (of 10192), d = 10, its = 54
i = 9164 (of 10192), d = 10, its = 56
i = 9165 (of 10192), d = 4.29187, its = 8
i = 9166 (of 10192), d = 0.6361, its = 18
i = 9167 (of 10192), d = 10, its = 37
i = 9168 (of 10192), d = 2.50558, its = 5
i = 9169 (of 10192), d = 10, its = 43
i = 9170 (of 10192), d = 8.51151, its = 9
i = 9171 (of 10192), d = 10, its = 50
i = 9172 (of 10192), d = 10, its = 53
i = 9173 (of 10192), d = 10, its = 52
i = 9174 (of 10192), d = 3.9921, its = 7
i = 9175 (of 10192), d = 1.65674, its = 5
i = 9176 (of 10192), d = 10, its = 55
i = 9177 (of 10192), d = 10, its = 56
i = 9178 (of 10192), d = 10, its = 54
i = 9179 (of 10192), d = 10, its = 51
i = 9180 (of 10192), d = 10, its = 53
i = 9181 (of 10192), d = 10, its = 51
i = 9182 (of 10192), d = 10, its = 50
i = 9183 (of 10192), d = 10, its = 54
i = 9184 (of 10192), d = 10, its = 55
i = 9185 (of 10192), d = 1.05341, its = 19
i = 9186 (of 10192), d = 10, its = 53
i = 9187 (of 10192), d = 10, its = 54
i = 9188 (of 10192), d = 10, its = 37
i = 9189 (of 10192), d = 10, its = 37
i = 9190 (of 10192), d = 10, its = 49
i = 9191 (of 10192), d = 10, its = 50
i = 9192 (of 10192), d = 10, its = 37
i = 9193 (of 10192), d = 10, its = 51
i = 9194 (of 10192), d = 10, its = 52
i = 9195 (of 10192), d = 10, its = 51
i = 9196 (of 10192), d = 10, its = 55
i = 9197 (of 10192), d = 10, its = 55
i = 9198 (of 10192), d = 10, its = 52
i = 9199 (of 10192), d = 10, its = 54
i = 9200 (of 10192), d = 10, its = 37
i = 9201 (of 10192), d = 10, its = 37
i = 9202 (of 10192), d = 2.44219, its = 6
i = 9203 (of 10192), d = 10, its = 55
i = 9204 (of 10192), d = 10, its = 43
i = 9205 (of 10192), d = 10, its = 43
i = 9206 (of 10192), d = 2.33654, its = 5
i = 9207 (of 10192), d = 10, its = 55
i = 9208 (of 10192), d = 10, its = 43
i = 9209 (of 10192), d = 10, its = 50
i = 9210 (of 10192), d = 10, its = 51
i = 9211 (of 10192), d = 10, its = 52
i = 9212 (of 10192), d = 2.6681, its = 6
i = 9213 (of 10192), d = 10, its = 41
i = 9214 (of 10192), d = 10, its = 53
i = 9215 (of 10192), d = 10, its = 50
i = 9216 (of 10192), d = 10, its = 54
i = 9217 (of 10192), d = 10, its = 53
i = 9218 (of 10192), d = 10, its = 46
i = 9219 (of 10192), d = 10, its = 51
i = 9220 (of 10192), d = 10, its = 57
i = 9221 (of 10192), d = 10, its = 54
i = 9222 (of 10192), d = 10, its = 49
i = 9223 (of 10192), d = 10, its = 52
i = 9224 (of 10192), d = 10, its = 48
i = 9225 (of 10192), d = 10, its = 48
i = 9226 (of 10192), d = 0.75728, its = 19
i = 9227 (of 10192), d = 10, its = 54
i = 9228 (of 10192), d = 10, its = 54
i = 9229 (of 10192), d = 10, its = 55
i = 9230 (of 10192), d = 2.19293, its = 4
i = 9231 (of 10192), d = 10, its = 42
i = 9232 (of 10192), d = 10, its = 40
i = 9233 (of 10192), d = 2.79149, its = 6
i = 9234 (of 10192), d = 10, its = 52
i = 9235 (of 10192), d = 10, its = 50
i = 9236 (of 10192), d = 0.854333, its = 25
i = 9237 (of 10192), d = 10, its = 51
i = 9238 (of 10192), d = 4.5186, its = 8
i = 9239 (of 10192), d = 10, its = 55
i = 9240 (of 10192), d = 10, its = 52
i = 9241 (of 10192), d = 2.80659, its = 6
i = 9242 (of 10192), d = 10, its = 59
i = 9243 (of 10192), d = 10, its = 53
i = 9244 (of 10192), d = 10, its = 51
i = 9245 (of 10192), d = 10, its = 39
i = 9246 (of 10192), d = 10, its = 55
i = 9247 (of 10192), d = 10, its = 52
i = 9248 (of 10192), d = 10, its = 53
i = 9249 (of 10192), d = 10, its = 54
i = 9250 (of 10192), d = 10, its = 37
i = 9251 (of 10192), d = 10, its = 46
i = 9252 (of 10192), d = 1.62919, its = 6
i = 9253 (of 10192), d = 10, its = 53
i = 9254 (of 10192), d = 10, its = 51
i = 9255 (of 10192), d = 10, its = 40
i = 9256 (of 10192), d = 10, its = 51
i = 9257 (of 10192), d = 10, its = 54
i = 9258 (of 10192), d = 1.8798, its = 4
i = 9259 (of 10192), d = 0.959055, its = 18
i = 9260 (of 10192), d = 10, its = 52
i = 9261 (of 10192), d = 10, its = 51
i = 9262 (of 10192), d = 10, its = 48
i = 9263 (of 10192), d = 1.29253, its = 6
i = 9264 (of 10192), d = 1.63795, its = 7
i = 9265 (of 10192), d = 1.49953, its = 6
i = 9266 (of 10192), d = 2.78396, its = 6
i = 9267 (of 10192), d = 10, its = 56
i = 9268 (of 10192), d = 10, its = 47
i = 9269 (of 10192), d = 4.27647, its = 7
i = 9270 (of 10192), d = 2.76644, its = 6
i = 9271 (of 10192), d = 10, its = 39
i = 9272 (of 10192), d = 0.866582, its = 19
i = 9273 (of 10192), d = 10, its = 37
i = 9274 (of 10192), d = 10, its = 49
i = 9275 (of 10192), d = 10, its = 39
i = 9276 (of 10192), d = 10, its = 40
i = 9277 (of 10192), d = 10, its = 37
i = 9278 (of 10192), d = 10, its = 55
i = 9279 (of 10192), d = 10, its = 43
i = 9280 (of 10192), d = 10, its = 49
i = 9281 (of 10192), d = 1.80075, its = 6
i = 9282 (of 10192), d = 10, its = 54
i = 9283 (of 10192), d = 2.48076, its = 5
i = 9284 (of 10192), d = 10, its = 43
i = 9285 (of 10192), d = 10, its = 37
i = 9286 (of 10192), d = 10, its = 47
i = 9287 (of 10192), d = 1.60037, its = 7
i = 9288 (of 10192), d = 1.3857, its = 6
i = 9289 (of 10192), d = 2.51813, its = 5
i = 9290 (of 10192), d = 10, its = 49
i = 9291 (of 10192), d = 0.998498, its = 17
i = 9292 (of 10192), d = 10, its = 53
i = 9293 (of 10192), d = 0.715417, its = 19
i = 9294 (of 10192), d = 10, its = 55
i = 9295 (of 10192), d = 10, its = 49
i = 9296 (of 10192), d = 10, its = 49
i = 9297 (of 10192), d = 1.60321, its = 6
i = 9298 (of 10192), d = 2.25783, its = 4
i = 9299 (of 10192), d = 10, its = 37
i = 9300 (of 10192), d = 10, its = 53
i = 9301 (of 10192), d = 10, its = 37
i = 9302 (of 10192), d = 2.65534, its = 6
i = 9303 (of 10192), d = 10, its = 51
i = 9304 (of 10192), d = 10, its = 37
i = 9305 (of 10192), d = 1.75683, its = 5
i = 9306 (of 10192), d = 10, its = 53
i = 9307 (of 10192), d = 0.835903, its = 19
i = 9308 (of 10192), d = 2.6739, its = 6
i = 9309 (of 10192), d = 10, its = 54
i = 9310 (of 10192), d = 10, its = 51
i = 9311 (of 10192), d = 3.13488, its = 6
i = 9312 (of 10192), d = 10, its = 58
i = 9313 (of 10192), d = 10, its = 37
i = 9314 (of 10192), d = 10, its = 56
i = 9315 (of 10192), d = 1.34596, its = 8
i = 9316 (of 10192), d = 10, its = 41
i = 9317 (of 10192), d = 10, its = 50
i = 9318 (of 10192), d = 3.21872, its = 6
i = 9319 (of 10192), d = 10, its = 53
i = 9320 (of 10192), d = 10, its = 52
i = 9321 (of 10192), d = 10, its = 54
i = 9322 (of 10192), d = 1.93328, its = 4
i = 9323 (of 10192), d = 10, its = 51
i = 9324 (of 10192), d = 2.18915, its = 5
i = 9325 (of 10192), d = 10, its = 55
i = 9326 (of 10192), d = 10, its = 51
i = 9327 (of 10192), d = 10, its = 37
i = 9328 (of 10192), d = 10, its = 52
i = 9329 (of 10192), d = 0.542592, its = 16
i = 9330 (of 10192), d = 10, its = 55
i = 9331 (of 10192), d = 10, its = 49
i = 9332 (of 10192), d = 2.28782, its = 5
i = 9333 (of 10192), d = 10, its = 39
i = 9334 (of 10192), d = 10, its = 39
i = 9335 (of 10192), d = 10, its = 37
i = 9336 (of 10192), d = 10, its = 37
i = 9337 (of 10192), d = 10, its = 52
i = 9338 (of 10192), d = 10, its = 41
i = 9339 (of 10192), d = 10, its = 50
i = 9340 (of 10192), d = 10, its = 53
i = 9341 (of 10192), d = 10, its = 53
i = 9342 (of 10192), d = 10, its = 41
i = 9343 (of 10192), d = 10, its = 54
i = 9344 (of 10192), d = 10, its = 51
i = 9345 (of 10192), d = 10, its = 39
i = 9346 (of 10192), d = 10, its = 39
i = 9347 (of 10192), d = 10, its = 52
i = 9348 (of 10192), d = 10, its = 37
i = 9349 (of 10192), d = 10, its = 50
i = 9350 (of 10192), d = 10, its = 54
i = 9351 (of 10192), d = 0.699394, its = 17
i = 9352 (of 10192), d = 10, its = 54
i = 9353 (of 10192), d = 10, its = 39
i = 9354 (of 10192), d = 10, its = 52
i = 9355 (of 10192), d = 10, its = 51
i = 9356 (of 10192), d = 2.35375, its = 5
i = 9357 (of 10192), d = 10, its = 37
i = 9358 (of 10192), d = 2.46576, its = 5
i = 9359 (of 10192), d = 10, its = 53
i = 9360 (of 10192), d = 2.1108, its = 4
i = 9361 (of 10192), d = 10, its = 54
i = 9362 (of 10192), d = 2.03317, its = 3
i = 9363 (of 10192), d = 0.696624, its = 17
i = 9364 (of 10192), d = 10, its = 52
i = 9365 (of 10192), d = 10, its = 50
i = 9366 (of 10192), d = 1.51704, its = 6
i = 9367 (of 10192), d = 10, its = 53
i = 9368 (of 10192), d = 1.82766, its = 4
i = 9369 (of 10192), d = 10, its = 52
i = 9370 (of 10192), d = 10, its = 37
i = 9371 (of 10192), d = 10, its = 54
i = 9372 (of 10192), d = 10, its = 55
i = 9373 (of 10192), d = 1.6013, its = 6
i = 9374 (of 10192), d = 10, its = 57
i = 9375 (of 10192), d = 1.94535, its = 4
i = 9376 (of 10192), d = 10, its = 49
i = 9377 (of 10192), d = 10, its = 54
i = 9378 (of 10192), d = 2.33712, its = 5
i = 9379 (of 10192), d = 1.84528, its = 4
i = 9380 (of 10192), d = 1.40865, its = 6
i = 9381 (of 10192), d = 10, its = 55
i = 9382 (of 10192), d = 10, its = 37
i = 9383 (of 10192), d = 10, its = 53
i = 9384 (of 10192), d = 10, its = 54
i = 9385 (of 10192), d = 10, its = 52
i = 9386 (of 10192), d = 0.810375, its = 15
i = 9387 (of 10192), d = 10, its = 53
i = 9388 (of 10192), d = 10, its = 52
i = 9389 (of 10192), d = 10, its = 20
i = 9390 (of 10192), d = 1.93328, its = 4
i = 9391 (of 10192), d = 3.04363, its = 6
i = 9392 (of 10192), d = 10, its = 53
i = 9393 (of 10192), d = 2.00904, its = 3
i = 9394 (of 10192), d = 3.90916, its = 6
i = 9395 (of 10192), d = 10, its = 51
i = 9396 (of 10192), d = 10, its = 40
i = 9397 (of 10192), d = 0.756622, its = 28
i = 9398 (of 10192), d = 10, its = 56
i = 9399 (of 10192), d = 10, its = 53
i = 9400 (of 10192), d = 10, its = 53
i = 9401 (of 10192), d = 10, its = 37
i = 9402 (of 10192), d = 10, its = 48
i = 9403 (of 10192), d = 10, its = 40
i = 9404 (of 10192), d = 10, its = 56
i = 9405 (of 10192), d = 10, its = 53
i = 9406 (of 10192), d = 10, its = 52
i = 9407 (of 10192), d = 10, its = 37
i = 9408 (of 10192), d = 10, its = 54
i = 9409 (of 10192), d = 1.87962, its = 4
i = 9410 (of 10192), d = 10, its = 56
i = 9411 (of 10192), d = 10, its = 51
i = 9412 (of 10192), d = 10, its = 37
i = 9413 (of 10192), d = 10, its = 50
i = 9414 (of 10192), d = 2.1668, its = 4
i = 9415 (of 10192), d = 1.26169, its = 7
i = 9416 (of 10192), d = 1.98147, its = 3
i = 9417 (of 10192), d = 10, its = 54
i = 9418 (of 10192), d = 10, its = 37
i = 9419 (of 10192), d = 10, its = 39
i = 9420 (of 10192), d = 0.563966, its = 18
i = 9421 (of 10192), d = 10, its = 51
i = 9422 (of 10192), d = 1.58099, its = 6
i = 9423 (of 10192), d = 1.97561, its = 3
i = 9424 (of 10192), d = 10, its = 54
i = 9425 (of 10192), d = 10, its = 37
i = 9426 (of 10192), d = 10, its = 54
i = 9427 (of 10192), d = 10, its = 37
i = 9428 (of 10192), d = 2.4123, its = 5
i = 9429 (of 10192), d = 10, its = 37
i = 9430 (of 10192), d = 0.878137, its = 18
i = 9431 (of 10192), d = 10, its = 52
i = 9432 (of 10192), d = 10, its = 54
i = 9433 (of 10192), d = 10, its = 37
i = 9434 (of 10192), d = 1.78855, its = 4
i = 9435 (of 10192), d = 10, its = 52
i = 9436 (of 10192), d = 10, its = 37
i = 9437 (of 10192), d = 10, its = 50
i = 9438 (of 10192), d = 10, its = 52
i = 9439 (of 10192), d = 10, its = 51
i = 9440 (of 10192), d = 10, its = 41
i = 9441 (of 10192), d = 10, its = 54
i = 9442 (of 10192), d = 10, its = 51
i = 9443 (of 10192), d = 10, its = 56
i = 9444 (of 10192), d = 10, its = 52
i = 9445 (of 10192), d = 10, its = 37
i = 9446 (of 10192), d = 10, its = 51
i = 9447 (of 10192), d = 3.46506, its = 7
i = 9448 (of 10192), d = 10, its = 52
i = 9449 (of 10192), d = 10, its = 51
i = 9450 (of 10192), d = 10, its = 37
i = 9451 (of 10192), d = 10, its = 52
i = 9452 (of 10192), d = 10, its = 54
i = 9453 (of 10192), d = 10, its = 52
i = 9454 (of 10192), d = 2.70679, its = 5
i = 9455 (of 10192), d = 10, its = 51
i = 9456 (of 10192), d = 10, its = 53
i = 9457 (of 10192), d = 10, its = 53
i = 9458 (of 10192), d = 10, its = 37
i = 9459 (of 10192), d = 10, its = 51
i = 9460 (of 10192), d = 1.12237, its = 27
i = 9461 (of 10192), d = 10, its = 37
i = 9462 (of 10192), d = 2.24471, its = 6
i = 9463 (of 10192), d = 0.787771, its = 16
i = 9464 (of 10192), d = 10, its = 52
i = 9465 (of 10192), d = 10, its = 58
i = 9466 (of 10192), d = 10, its = 55
i = 9467 (of 10192), d = 10, its = 37
i = 9468 (of 10192), d = 2.73216, its = 5
i = 9469 (of 10192), d = 10, its = 53
i = 9470 (of 10192), d = 10, its = 54
i = 9471 (of 10192), d = 10, its = 37
i = 9472 (of 10192), d = 3.0861, its = 6
i = 9473 (of 10192), d = 10, its = 54
i = 9474 (of 10192), d = 0.629528, its = 18
i = 9475 (of 10192), d = 1.49562, its = 7
i = 9476 (of 10192), d = 1.07272, its = 19
i = 9477 (of 10192), d = 10, its = 49
i = 9478 (of 10192), d = 10, its = 55
i = 9479 (of 10192), d = 10, its = 37
i = 9480 (of 10192), d = 10, its = 37
i = 9481 (of 10192), d = 10, its = 37
i = 9482 (of 10192), d = 1.40684, its = 6
i = 9483 (of 10192), d = 10, its = 54
i = 9484 (of 10192), d = 0.851732, its = 17
i = 9485 (of 10192), d = 2.10641, its = 4
i = 9486 (of 10192), d = 10, its = 53
i = 9487 (of 10192), d = 0.702818, its = 25
i = 9488 (of 10192), d = 10, its = 37
i = 9489 (of 10192), d = 1.65166, its = 5
i = 9490 (of 10192), d = 10, its = 37
i = 9491 (of 10192), d = 10, its = 52
i = 9492 (of 10192), d = 10, its = 56
i = 9493 (of 10192), d = 10, its = 42
i = 9494 (of 10192), d = 10, its = 41
i = 9495 (of 10192), d = 10, its = 41
i = 9496 (of 10192), d = 10, its = 42
i = 9497 (of 10192), d = 10, its = 22
i = 9498 (of 10192), d = 3.98746, its = 7
i = 9499 (of 10192), d = 7.16067, its = 8
i = 9500 (of 10192), d = 1.21332, its = 26
i = 9501 (of 10192), d = 10, its = 55
i = 9502 (of 10192), d = 0.675577, its = 25
i = 9503 (of 10192), d = 10, its = 40
i = 9504 (of 10192), d = 0.665499, its = 17
i = 9505 (of 10192), d = 1.66538, its = 5
i = 9506 (of 10192), d = 10, its = 56
i = 9507 (of 10192), d = 10, its = 51
i = 9508 (of 10192), d = 0.6481, its = 20
i = 9509 (of 10192), d = 1.82476, its = 4
i = 9510 (of 10192), d = 10, its = 50
i = 9511 (of 10192), d = 10, its = 58
i = 9512 (of 10192), d = 10, its = 56
i = 9513 (of 10192), d = 2.11808, its = 4
i = 9514 (of 10192), d = 2.53769, its = 5
i = 9515 (of 10192), d = 10, its = 51
i = 9516 (of 10192), d = 10, its = 53
i = 9517 (of 10192), d = 10, its = 54
i = 9518 (of 10192), d = 10, its = 42
i = 9519 (of 10192), d = 0.599881, its = 28
i = 9520 (of 10192), d = 10, its = 56
i = 9521 (of 10192), d = 10, its = 54
i = 9522 (of 10192), d = 10, its = 54
i = 9523 (of 10192), d = 10, its = 53
i = 9524 (of 10192), d = 10, its = 40
i = 9525 (of 10192), d = 10, its = 53
i = 9526 (of 10192), d = 10, its = 38
i = 9527 (of 10192), d = 10, its = 51
i = 9528 (of 10192), d = 1.83271, its = 5
i = 9529 (of 10192), d = 10, its = 56
i = 9530 (of 10192), d = 10, its = 37
i = 9531 (of 10192), d = 10, its = 53
i = 9532 (of 10192), d = 10, its = 37
i = 9533 (of 10192), d = 10, its = 58
i = 9534 (of 10192), d = 2.16555, its = 4
i = 9535 (of 10192), d = 10, its = 55
i = 9536 (of 10192), d = 10, its = 51
i = 9537 (of 10192), d = 10, its = 37
i = 9538 (of 10192), d = 10, its = 55
i = 9539 (of 10192), d = 10, its = 39
i = 9540 (of 10192), d = 10, its = 55
i = 9541 (of 10192), d = 10, its = 41
i = 9542 (of 10192), d = 10, its = 40
i = 9543 (of 10192), d = 10, its = 52
i = 9544 (of 10192), d = 10, its = 49
i = 9545 (of 10192), d = 10, its = 55
i = 9546 (of 10192), d = 10, its = 40
i = 9547 (of 10192), d = 2.60448, its = 5
i = 9548 (of 10192), d = 1.24672, its = 6
i = 9549 (of 10192), d = 10, its = 53
i = 9550 (of 10192), d = 1.59212, its = 7
i = 9551 (of 10192), d = 1.45434, its = 7
i = 9552 (of 10192), d = 10, its = 54
i = 9553 (of 10192), d = 10, its = 39
i = 9554 (of 10192), d = 0.422618, its = 29
i = 9555 (of 10192), d = 10, its = 53
i = 9556 (of 10192), d = 10, its = 50
i = 9557 (of 10192), d = 10, its = 55
i = 9558 (of 10192), d = 10, its = 50
i = 9559 (of 10192), d = 3.11879, its = 6
i = 9560 (of 10192), d = 10, its = 50
i = 9561 (of 10192), d = 10, its = 55
i = 9562 (of 10192), d = 0.613505, its = 19
i = 9563 (of 10192), d = 1.80513, its = 5
i = 9564 (of 10192), d = 10, its = 56
i = 9565 (of 10192), d = 2.79628, its = 5
i = 9566 (of 10192), d = 10, its = 54
i = 9567 (of 10192), d = 10, its = 51
i = 9568 (of 10192), d = 10, its = 42
i = 9569 (of 10192), d = 10, its = 55
i = 9570 (of 10192), d = 3.72314, its = 8
i = 9571 (of 10192), d = 10, its = 54
i = 9572 (of 10192), d = 10, its = 50
i = 9573 (of 10192), d = 10, its = 50
i = 9574 (of 10192), d = 1.99246, its = 3
i = 9575 (of 10192), d = 5.8773, its = 8
i = 9576 (of 10192), d = 10, its = 39
i = 9577 (of 10192), d = 2.91394, its = 6
i = 9578 (of 10192), d = 10, its = 50
i = 9579 (of 10192), d = 10, its = 41
i = 9580 (of 10192), d = 10, its = 55
i = 9581 (of 10192), d = 10, its = 54
i = 9582 (of 10192), d = 0.86076, its = 25
i = 9583 (of 10192), d = 10, its = 41
i = 9584 (of 10192), d = 10, its = 51
i = 9585 (of 10192), d = 10, its = 51
i = 9586 (of 10192), d = 10, its = 38
i = 9587 (of 10192), d = 10, its = 37
i = 9588 (of 10192), d = 10, its = 53
i = 9589 (of 10192), d = 10, its = 51
i = 9590 (of 10192), d = 10, its = 53
i = 9591 (of 10192), d = 10, its = 56
i = 9592 (of 10192), d = 10, its = 51
i = 9593 (of 10192), d = 0.954757, its = 19
i = 9594 (of 10192), d = 0.346056, its = 17
i = 9595 (of 10192), d = 10, its = 53
i = 9596 (of 10192), d = 10, its = 49
i = 9597 (of 10192), d = 10, its = 55
i = 9598 (of 10192), d = 10, its = 37
i = 9599 (of 10192), d = 10, its = 37
i = 9600 (of 10192), d = 2.48602, its = 6
i = 9601 (of 10192), d = 2.59414, its = 5
i = 9602 (of 10192), d = 4.76265, its = 7
i = 9603 (of 10192), d = 2.06648, its = 3
i = 9604 (of 10192), d = 1.79835, its = 4
i = 9605 (of 10192), d = 10, its = 39
i = 9606 (of 10192), d = 10, its = 49
i = 9607 (of 10192), d = 0.763685, its = 31
i = 9608 (of 10192), d = 10, its = 52
i = 9609 (of 10192), d = 2.34174, its = 6
i = 9610 (of 10192), d = 2.92544, its = 6
i = 9611 (of 10192), d = 10, its = 44
i = 9612 (of 10192), d = 1.47473, its = 6
i = 9613 (of 10192), d = 1.96715, its = 3
i = 9614 (of 10192), d = 10, its = 37
i = 9615 (of 10192), d = 10, its = 51
i = 9616 (of 10192), d = 10, its = 52
i = 9617 (of 10192), d = 10, its = 37
i = 9618 (of 10192), d = 10, its = 54
i = 9619 (of 10192), d = 10, its = 50
i = 9620 (of 10192), d = 10, its = 40
i = 9621 (of 10192), d = 9.12631, its = 8
i = 9622 (of 10192), d = 10, its = 37
i = 9623 (of 10192), d = 5.01989, its = 7
i = 9624 (of 10192), d = 0.85628, its = 19
i = 9625 (of 10192), d = 0.625838, its = 18
i = 9626 (of 10192), d = 10, its = 53
i = 9627 (of 10192), d = 10, its = 37
i = 9628 (of 10192), d = 0.297186, its = 18
i = 9629 (of 10192), d = 10, its = 51
i = 9630 (of 10192), d = 10, its = 54
i = 9631 (of 10192), d = 10, its = 37
i = 9632 (of 10192), d = 10, its = 56
i = 9633 (of 10192), d = 10, its = 51
i = 9634 (of 10192), d = 10, its = 37
i = 9635 (of 10192), d = 1.66538, its = 5
i = 9636 (of 10192), d = 10, its = 40
i = 9637 (of 10192), d = 1.44159, its = 6
i = 9638 (of 10192), d = 10, its = 51
i = 9639 (of 10192), d = 3.43598, its = 8
i = 9640 (of 10192), d = 10, its = 51
i = 9641 (of 10192), d = 10, its = 52
i = 9642 (of 10192), d = 0.841548, its = 19
i = 9643 (of 10192), d = 10, its = 55
i = 9644 (of 10192), d = 10, its = 44
i = 9645 (of 10192), d = 10, its = 54
i = 9646 (of 10192), d = 10, its = 54
i = 9647 (of 10192), d = 10, its = 52
i = 9648 (of 10192), d = 10, its = 49
i = 9649 (of 10192), d = 10, its = 55
i = 9650 (of 10192), d = 9.28159, its = 10
i = 9651 (of 10192), d = 10, its = 40
i = 9652 (of 10192), d = 10, its = 38
i = 9653 (of 10192), d = 10, its = 53
i = 9654 (of 10192), d = 10, its = 52
i = 9655 (of 10192), d = 10, its = 53
i = 9656 (of 10192), d = 10, its = 51
i = 9657 (of 10192), d = 9.04021, its = 9
i = 9658 (of 10192), d = 10, its = 38
i = 9659 (of 10192), d = 10, its = 40
i = 9660 (of 10192), d = 10, its = 52
i = 9661 (of 10192), d = 10, its = 50
i = 9662 (of 10192), d = 10, its = 52
i = 9663 (of 10192), d = 10, its = 53
i = 9664 (of 10192), d = 10, its = 41
i = 9665 (of 10192), d = 10, its = 54
i = 9666 (of 10192), d = 1.74888, its = 5
i = 9667 (of 10192), d = 10, its = 54
i = 9668 (of 10192), d = 10, its = 37
i = 9669 (of 10192), d = 10, its = 53
i = 9670 (of 10192), d = 1.80451, its = 4
i = 9671 (of 10192), d = 10, its = 53
i = 9672 (of 10192), d = 10, its = 55
i = 9673 (of 10192), d = 10, its = 51
i = 9674 (of 10192), d = 1.80036, its = 4
i = 9675 (of 10192), d = 10, its = 54
i = 9676 (of 10192), d = 10, its = 50
i = 9677 (of 10192), d = 10, its = 50
i = 9678 (of 10192), d = 10, its = 50
i = 9679 (of 10192), d = 10, its = 56
i = 9680 (of 10192), d = 10, its = 38
i = 9681 (of 10192), d = 10, its = 57
i = 9682 (of 10192), d = 10, its = 37
i = 9683 (of 10192), d = 0.712965, its = 17
i = 9684 (of 10192), d = 10, its = 57
i = 9685 (of 10192), d = 10, its = 53
i = 9686 (of 10192), d = 10, its = 50
i = 9687 (of 10192), d = 1.22606, its = 8
i = 9688 (of 10192), d = 0.840856, its = 24
i = 9689 (of 10192), d = 2.00355, its = 2
i = 9690 (of 10192), d = 10, its = 57
i = 9691 (of 10192), d = 10, its = 58
i = 9692 (of 10192), d = 2.2807, its = 5
i = 9693 (of 10192), d = 10, its = 53
i = 9694 (of 10192), d = 10, its = 52
i = 9695 (of 10192), d = 10, its = 53
i = 9696 (of 10192), d = 0.788497, its = 16
i = 9697 (of 10192), d = 10, its = 56
i = 9698 (of 10192), d = 10, its = 37
i = 9699 (of 10192), d = 10, its = 53
i = 9700 (of 10192), d = 10, its = 53
i = 9701 (of 10192), d = 10, its = 53
i = 9702 (of 10192), d = 10, its = 53
i = 9703 (of 10192), d = 10, its = 54
i = 9704 (of 10192), d = 10, its = 55
i = 9705 (of 10192), d = 3.05526, its = 7
i = 9706 (of 10192), d = 2.93352, its = 6
i = 9707 (of 10192), d = 2.42266, its = 5
i = 9708 (of 10192), d = 10, its = 55
i = 9709 (of 10192), d = 10, its = 41
i = 9710 (of 10192), d = 10, its = 54
i = 9711 (of 10192), d = 10, its = 37
i = 9712 (of 10192), d = 10, its = 37
i = 9713 (of 10192), d = 10, its = 50
i = 9714 (of 10192), d = 1.63552, its = 5
i = 9715 (of 10192), d = 10, its = 54
i = 9716 (of 10192), d = 3.54127, its = 6
i = 9717 (of 10192), d = 10, its = 53
i = 9718 (of 10192), d = 10, its = 39
i = 9719 (of 10192), d = 10, its = 52
i = 9720 (of 10192), d = 10, its = 51
i = 9721 (of 10192), d = 1.89708, its = 4
i = 9722 (of 10192), d = 10, its = 40
i = 9723 (of 10192), d = 10, its = 37
i = 9724 (of 10192), d = 10, its = 54
i = 9725 (of 10192), d = 10, its = 49
i = 9726 (of 10192), d = 10, its = 51
i = 9727 (of 10192), d = 10, its = 53
i = 9728 (of 10192), d = 10, its = 38
i = 9729 (of 10192), d = 10, its = 41
i = 9730 (of 10192), d = 0.929995, its = 17
i = 9731 (of 10192), d = 2.19645, its = 4
i = 9732 (of 10192), d = 1.88029, its = 4
i = 9733 (of 10192), d = 10, its = 39
i = 9734 (of 10192), d = 1.04196, its = 24
i = 9735 (of 10192), d = 10, its = 49
i = 9736 (of 10192), d = 10, its = 56
i = 9737 (of 10192), d = 3.48072, its = 6
i = 9738 (of 10192), d = 10, its = 37
i = 9739 (of 10192), d = 10, its = 52
i = 9740 (of 10192), d = 10, its = 37
i = 9741 (of 10192), d = 1.28813, its = 7
i = 9742 (of 10192), d = 0.580043, its = 17
i = 9743 (of 10192), d = 10, its = 53
i = 9744 (of 10192), d = 1.71534, its = 5
i = 9745 (of 10192), d = 10, its = 55
i = 9746 (of 10192), d = 10, its = 60
i = 9747 (of 10192), d = 10, its = 52
i = 9748 (of 10192), d = 10, its = 41
i = 9749 (of 10192), d = 1.29562, its = 8
i = 9750 (of 10192), d = 1.22619, its = 7
i = 9751 (of 10192), d = 10, its = 38
i = 9752 (of 10192), d = 10, its = 50
i = 9753 (of 10192), d = 10, its = 40
i = 9754 (of 10192), d = 10, its = 52
i = 9755 (of 10192), d = 1.28309, its = 8
i = 9756 (of 10192), d = 1.45239, its = 7
i = 9757 (of 10192), d = 0.576329, its = 18
i = 9758 (of 10192), d = 8.45366, its = 22
i = 9759 (of 10192), d = 3.08819, its = 6
i = 9760 (of 10192), d = 10, its = 37
i = 9761 (of 10192), d = 10, its = 51
i = 9762 (of 10192), d = 10, its = 50
i = 9763 (of 10192), d = 1.66495, its = 6
i = 9764 (of 10192), d = 10, its = 37
i = 9765 (of 10192), d = 10, its = 58
i = 9766 (of 10192), d = 10, its = 54
i = 9767 (of 10192), d = 10, its = 37
i = 9768 (of 10192), d = 10, its = 37
i = 9769 (of 10192), d = 10, its = 52
i = 9770 (of 10192), d = 0.863329, its = 20
i = 9771 (of 10192), d = 10, its = 47
i = 9772 (of 10192), d = 10, its = 43
i = 9773 (of 10192), d = 0.580657, its = 17
i = 9774 (of 10192), d = 10, its = 48
i = 9775 (of 10192), d = 10, its = 48
i = 9776 (of 10192), d = 10, its = 53
i = 9777 (of 10192), d = 10, its = 49
i = 9778 (of 10192), d = 10, its = 48
i = 9779 (of 10192), d = 10, its = 40
i = 9780 (of 10192), d = 1.25148, its = 24
i = 9781 (of 10192), d = 10, its = 40
i = 9782 (of 10192), d = 10, its = 56
i = 9783 (of 10192), d = 10, its = 38
i = 9784 (of 10192), d = 1.50516, its = 7
i = 9785 (of 10192), d = 10, its = 52
i = 9786 (of 10192), d = 2.57387, its = 5
i = 9787 (of 10192), d = 10, its = 52
i = 9788 (of 10192), d = 10, its = 17
i = 9789 (of 10192), d = 10, its = 53
i = 9790 (of 10192), d = 10, its = 56
i = 9791 (of 10192), d = 2.84548, its = 5
i = 9792 (of 10192), d = 10, its = 37
i = 9793 (of 10192), d = 10, its = 54
i = 9794 (of 10192), d = 10, its = 52
i = 9795 (of 10192), d = 10, its = 51
i = 9796 (of 10192), d = 10, its = 55
i = 9797 (of 10192), d = 10, its = 53
i = 9798 (of 10192), d = 10, its = 39
i = 9799 (of 10192), d = 10, its = 54
i = 9800 (of 10192), d = 0.978517, its = 21
i = 9801 (of 10192), d = 10, its = 51
i = 9802 (of 10192), d = 10, its = 52
i = 9803 (of 10192), d = 10, its = 54
i = 9804 (of 10192), d = 10, its = 37
i = 9805 (of 10192), d = 2.21379, its = 4
i = 9806 (of 10192), d = 10, its = 37
i = 9807 (of 10192), d = 10, its = 53
i = 9808 (of 10192), d = 10, its = 42
i = 9809 (of 10192), d = 0.656208, its = 17
i = 9810 (of 10192), d = 10, its = 51
i = 9811 (of 10192), d = 10, its = 54
i = 9812 (of 10192), d = 10, its = 50
i = 9813 (of 10192), d = 10, its = 57
i = 9814 (of 10192), d = 10, its = 37
i = 9815 (of 10192), d = 10, its = 37
i = 9816 (of 10192), d = 2.19397, its = 4
i = 9817 (of 10192), d = 10, its = 40
i = 9818 (of 10192), d = 10, its = 45
i = 9819 (of 10192), d = 1.43532, its = 8
i = 9820 (of 10192), d = 10, its = 41
i = 9821 (of 10192), d = 10, its = 54
i = 9822 (of 10192), d = 10, its = 53
i = 9823 (of 10192), d = 0.976533, its = 19
i = 9824 (of 10192), d = 10, its = 41
i = 9825 (of 10192), d = 10, its = 56
i = 9826 (of 10192), d = 1.28754, its = 6
i = 9827 (of 10192), d = 10, its = 52
i = 9828 (of 10192), d = 10, its = 49
i = 9829 (of 10192), d = 10, its = 54
i = 9830 (of 10192), d = 10, its = 49
i = 9831 (of 10192), d = 0.472048, its = 17
i = 9832 (of 10192), d = 10, its = 50
i = 9833 (of 10192), d = 10, its = 53
i = 9834 (of 10192), d = 1.02945, its = 19
i = 9835 (of 10192), d = 10, its = 40
i = 9836 (of 10192), d = 3.10388, its = 6
i = 9837 (of 10192), d = 2.46093, its = 5
i = 9838 (of 10192), d = 10, its = 54
i = 9839 (of 10192), d = 10, its = 57
i = 9840 (of 10192), d = 2.23387, its = 4
i = 9841 (of 10192), d = 2.69152, its = 6
i = 9842 (of 10192), d = 10, its = 53
i = 9843 (of 10192), d = 1.65852, its = 6
i = 9844 (of 10192), d = 10, its = 37
i = 9845 (of 10192), d = 10, its = 55
i = 9846 (of 10192), d = 10, its = 48
i = 9847 (of 10192), d = 10, its = 37
i = 9848 (of 10192), d = 10, its = 39
i = 9849 (of 10192), d = 10, its = 59
i = 9850 (of 10192), d = 10, its = 54
i = 9851 (of 10192), d = 10, its = 55
i = 9852 (of 10192), d = 10, its = 51
i = 9853 (of 10192), d = 10, its = 44
i = 9854 (of 10192), d = 2.50997, its = 5
i = 9855 (of 10192), d = 10, its = 51
i = 9856 (of 10192), d = 2.5784, its = 5
i = 9857 (of 10192), d = 10, its = 53
i = 9858 (of 10192), d = 0.689829, its = 18
i = 9859 (of 10192), d = 10, its = 40
i = 9860 (of 10192), d = 1.87983, its = 4
i = 9861 (of 10192), d = 10, its = 53
i = 9862 (of 10192), d = 2.08863, its = 4
i = 9863 (of 10192), d = 10, its = 59
i = 9864 (of 10192), d = 10, its = 37
i = 9865 (of 10192), d = 10, its = 52
i = 9866 (of 10192), d = 10, its = 37
i = 9867 (of 10192), d = 0.74035, its = 18
i = 9868 (of 10192), d = 1.83366, its = 8
i = 9869 (of 10192), d = 10, its = 37
i = 9870 (of 10192), d = 10, its = 53
i = 9871 (of 10192), d = 1.48158, its = 6
i = 9872 (of 10192), d = 6.66053, its = 9
i = 9873 (of 10192), d = 10, its = 38
i = 9874 (of 10192), d = 2.21011, its = 4
i = 9875 (of 10192), d = 10, its = 52
i = 9876 (of 10192), d = 10, its = 37
i = 9877 (of 10192), d = 0.431239, its = 19
i = 9878 (of 10192), d = 10, its = 37
i = 9879 (of 10192), d = 10, its = 37
i = 9880 (of 10192), d = 10, its = 52
i = 9881 (of 10192), d = 10, its = 40
i = 9882 (of 10192), d = 10, its = 49
i = 9883 (of 10192), d = 10, its = 57
i = 9884 (of 10192), d = 2.32552, its = 5
i = 9885 (of 10192), d = 0.856718, its = 19
i = 9886 (of 10192), d = 10, its = 51
i = 9887 (of 10192), d = 10, its = 37
i = 9888 (of 10192), d = 10, its = 52
i = 9889 (of 10192), d = 2.92818, its = 5
i = 9890 (of 10192), d = 10, its = 51
i = 9891 (of 10192), d = 10, its = 54
i = 9892 (of 10192), d = 10, its = 37
i = 9893 (of 10192), d = 10, its = 51
i = 9894 (of 10192), d = 2.27809, its = 4
i = 9895 (of 10192), d = 10, its = 50
i = 9896 (of 10192), d = 10, its = 41
i = 9897 (of 10192), d = 10, its = 53
i = 9898 (of 10192), d = 10, its = 52
i = 9899 (of 10192), d = 10, its = 37
i = 9900 (of 10192), d = 10, its = 50
i = 9901 (of 10192), d = 10, its = 52
i = 9902 (of 10192), d = 10, its = 43
i = 9903 (of 10192), d = 10, its = 51
i = 9904 (of 10192), d = 10, its = 37
i = 9905 (of 10192), d = 10, its = 55
i = 9906 (of 10192), d = 10, its = 52
i = 9907 (of 10192), d = 0.678407, its = 23
i = 9908 (of 10192), d = 10, its = 53
i = 9909 (of 10192), d = 10, its = 37
i = 9910 (of 10192), d = 10, its = 37
i = 9911 (of 10192), d = 2.62283, its = 5
i = 9912 (of 10192), d = 1.46125, its = 6
i = 9913 (of 10192), d = 0.461196, its = 19
i = 9914 (of 10192), d = 10, its = 48
i = 9915 (of 10192), d = 1.52451, its = 6
i = 9916 (of 10192), d = 0.550902, its = 22
i = 9917 (of 10192), d = 10, its = 37
i = 9918 (of 10192), d = 10, its = 51
i = 9919 (of 10192), d = 0.985656, its = 20
i = 9920 (of 10192), d = 10, its = 56
i = 9921 (of 10192), d = 10, its = 40
i = 9922 (of 10192), d = 10, its = 55
i = 9923 (of 10192), d = 1.85204, its = 4
i = 9924 (of 10192), d = 10, its = 53
i = 9925 (of 10192), d = 2.01472, its = 3
i = 9926 (of 10192), d = 4.15021, its = 7
i = 9927 (of 10192), d = 10, its = 37
i = 9928 (of 10192), d = 10, its = 51
i = 9929 (of 10192), d = 10, its = 57
i = 9930 (of 10192), d = 1.36353, its = 6
i = 9931 (of 10192), d = 10, its = 51
i = 9932 (of 10192), d = 10, its = 41
i = 9933 (of 10192), d = 10, its = 53
i = 9934 (of 10192), d = 10, its = 53
i = 9935 (of 10192), d = 0.727697, its = 23
i = 9936 (of 10192), d = 10, its = 37
i = 9937 (of 10192), d = 10, its = 37
i = 9938 (of 10192), d = 10, its = 39
i = 9939 (of 10192), d = 10, its = 54
i = 9940 (of 10192), d = 10, its = 37
i = 9941 (of 10192), d = 2.4216, its = 5
i = 9942 (of 10192), d = 2.64438, its = 6
i = 9943 (of 10192), d = 10, its = 53
i = 9944 (of 10192), d = 10, its = 41
i = 9945 (of 10192), d = 10, its = 40
i = 9946 (of 10192), d = 10, its = 37
i = 9947 (of 10192), d = 1.88792, its = 4
i = 9948 (of 10192), d = 10, its = 56
i = 9949 (of 10192), d = 10, its = 49
i = 9950 (of 10192), d = 1.70835, its = 8
i = 9951 (of 10192), d = 10, its = 37
i = 9952 (of 10192), d = 10, its = 52
i = 9953 (of 10192), d = 10, its = 56
i = 9954 (of 10192), d = 1.89954, its = 4
i = 9955 (of 10192), d = 1.3993, its = 7
i = 9956 (of 10192), d = 0.959055, its = 18
i = 9957 (of 10192), d = 10, its = 46
i = 9958 (of 10192), d = 10, its = 37
i = 9959 (of 10192), d = 10, its = 41
i = 9960 (of 10192), d = 10, its = 52
i = 9961 (of 10192), d = 10, its = 51
i = 9962 (of 10192), d = 10, its = 53
i = 9963 (of 10192), d = 0.740084, its = 19
i = 9964 (of 10192), d = 0.461159, its = 25
i = 9965 (of 10192), d = 10, its = 49
i = 9966 (of 10192), d = 10, its = 54
i = 9967 (of 10192), d = 10, its = 54
i = 9968 (of 10192), d = 10, its = 55
i = 9969 (of 10192), d = 3.29099, its = 6
i = 9970 (of 10192), d = 10, its = 52
i = 9971 (of 10192), d = 10, its = 56
i = 9972 (of 10192), d = 10, its = 41
i = 9973 (of 10192), d = 0.304251, its = 18
i = 9974 (of 10192), d = 10, its = 52
i = 9975 (of 10192), d = 3.83356, its = 7
i = 9976 (of 10192), d = 10, its = 49
i = 9977 (of 10192), d = 10, its = 37
i = 9978 (of 10192), d = 10, its = 37
i = 9979 (of 10192), d = 10, its = 54
i = 9980 (of 10192), d = 1.92025, its = 4
i = 9981 (of 10192), d = 10, its = 53
i = 9982 (of 10192), d = 10, its = 56
i = 9983 (of 10192), d = 10, its = 53
i = 9984 (of 10192), d = 10, its = 52
i = 9985 (of 10192), d = 10, its = 57
i = 9986 (of 10192), d = 10, its = 55
i = 9987 (of 10192), d = 10, its = 50
i = 9988 (of 10192), d = 10, its = 52
i = 9989 (of 10192), d = 10, its = 38
i = 9990 (of 10192), d = 10, its = 57
i = 9991 (of 10192), d = 10, its = 53
i = 9992 (of 10192), d = 10, its = 38
i = 9993 (of 10192), d = 10, its = 57
i = 9994 (of 10192), d = 10, its = 57
i = 9995 (of 10192), d = 10, its = 56
i = 9996 (of 10192), d = 10, its = 52
i = 9997 (of 10192), d = 10, its = 37
i = 9998 (of 10192), d = 10, its = 54
i = 9999 (of 10192), d = 10, its = 39
i = 10000 (of 10192), d = 10, its = 37
i = 10001 (of 10192), d = 1.26169, its = 7
i = 10002 (of 10192), d = 10, its = 51
i = 10003 (of 10192), d = 10, its = 50
i = 10004 (of 10192), d = 10, its = 49
i = 10005 (of 10192), d = 10, its = 40
i = 10006 (of 10192), d = 10, its = 56
i = 10007 (of 10192), d = 10, its = 55
i = 10008 (of 10192), d = 10, its = 50
i = 10009 (of 10192), d = 10, its = 55
i = 10010 (of 10192), d = 0.931421, its = 24
i = 10011 (of 10192), d = 10, its = 54
i = 10012 (of 10192), d = 10, its = 52
i = 10013 (of 10192), d = 10, its = 40
i = 10014 (of 10192), d = 10, its = 53
i = 10015 (of 10192), d = 10, its = 52
i = 10016 (of 10192), d = 10, its = 53
i = 10017 (of 10192), d = 2.93402, its = 6
i = 10018 (of 10192), d = 10, its = 37
i = 10019 (of 10192), d = 10, its = 57
i = 10020 (of 10192), d = 1.59755, its = 6
i = 10021 (of 10192), d = 10, its = 51
i = 10022 (of 10192), d = 10, its = 37
i = 10023 (of 10192), d = 10, its = 53
i = 10024 (of 10192), d = 1.85204, its = 4
i = 10025 (of 10192), d = 0.753718, its = 20
i = 10026 (of 10192), d = 1.89408, its = 4
i = 10027 (of 10192), d = 10, its = 60
i = 10028 (of 10192), d = 3.29089, its = 6
i = 10029 (of 10192), d = 3.33016, its = 6
i = 10030 (of 10192), d = 0.954757, its = 19
i = 10031 (of 10192), d = 10, its = 39
i = 10032 (of 10192), d = 10, its = 37
i = 10033 (of 10192), d = 10, its = 37
i = 10034 (of 10192), d = 10, its = 54
i = 10035 (of 10192), d = 1.06135, its = 18
i = 10036 (of 10192), d = 10, its = 40
i = 10037 (of 10192), d = 10, its = 37
i = 10038 (of 10192), d = 10, its = 54
i = 10039 (of 10192), d = 10, its = 52
i = 10040 (of 10192), d = 10, its = 52
i = 10041 (of 10192), d = 10, its = 47
i = 10042 (of 10192), d = 10, its = 53
i = 10043 (of 10192), d = 10, its = 40
i = 10044 (of 10192), d = 10, its = 48
i = 10045 (of 10192), d = 0.548551, its = 18
i = 10046 (of 10192), d = 10, its = 55
i = 10047 (of 10192), d = 10, its = 55
i = 10048 (of 10192), d = 2.00951, its = 3
i = 10049 (of 10192), d = 10, its = 53
i = 10050 (of 10192), d = 10, its = 40
i = 10051 (of 10192), d = 10, its = 48
i = 10052 (of 10192), d = 2.14167, its = 4
i = 10053 (of 10192), d = 10, its = 55
i = 10054 (of 10192), d = 1.33378, its = 8
i = 10055 (of 10192), d = 2.23306, its = 4
i = 10056 (of 10192), d = 10, its = 52
i = 10057 (of 10192), d = 1.99565, its = 2
i = 10058 (of 10192), d = 10, its = 51
i = 10059 (of 10192), d = 10, its = 58
i = 10060 (of 10192), d = 10, its = 55
i = 10061 (of 10192), d = 1.29336, its = 20
i = 10062 (of 10192), d = 2.75811, its = 5
i = 10063 (of 10192), d = 3.59904, its = 7
i = 10064 (of 10192), d = 10, its = 52
i = 10065 (of 10192), d = 10, its = 54
i = 10066 (of 10192), d = 10, its = 54
i = 10067 (of 10192), d = 2.12476, its = 4
i = 10068 (of 10192), d = 10, its = 55
i = 10069 (of 10192), d = 10, its = 53
i = 10070 (of 10192), d = 10, its = 57
i = 10071 (of 10192), d = 10, its = 37
i = 10072 (of 10192), d = 10, its = 52
i = 10073 (of 10192), d = 10, its = 49
i = 10074 (of 10192), d = 10, its = 50
i = 10075 (of 10192), d = 10, its = 50
i = 10076 (of 10192), d = 10, its = 53
i = 10077 (of 10192), d = 10, its = 40
i = 10078 (of 10192), d = 10, its = 51
i = 10079 (of 10192), d = 10, its = 37
i = 10080 (of 10192), d = 10, its = 38
i = 10081 (of 10192), d = 2.07038, its = 4
i = 10082 (of 10192), d = 10, its = 51
i = 10083 (of 10192), d = 4.98558, its = 7
i = 10084 (of 10192), d = 10, its = 51
i = 10085 (of 10192), d = 10, its = 54
i = 10086 (of 10192), d = 10, its = 54
i = 10087 (of 10192), d = 10, its = 56
i = 10088 (of 10192), d = 10, its = 54
i = 10089 (of 10192), d = 10, its = 51
i = 10090 (of 10192), d = 10, its = 40
i = 10091 (of 10192), d = 10, its = 51
i = 10092 (of 10192), d = 10, its = 53
i = 10093 (of 10192), d = 1.26894, its = 7
i = 10094 (of 10192), d = 10, its = 54
i = 10095 (of 10192), d = 10, its = 52
i = 10096 (of 10192), d = 10, its = 40
i = 10097 (of 10192), d = 10, its = 55
i = 10098 (of 10192), d = 10, its = 37
i = 10099 (of 10192), d = 3.22279, its = 6
i = 10100 (of 10192), d = 10, its = 55
i = 10101 (of 10192), d = 10, its = 37
i = 10102 (of 10192), d = 6.64095, its = 8
i = 10103 (of 10192), d = 10, its = 51
i = 10104 (of 10192), d = 10, its = 37
i = 10105 (of 10192), d = 10, its = 51
i = 10106 (of 10192), d = 0.724726, its = 17
i = 10107 (of 10192), d = 4.58016, its = 7
i = 10108 (of 10192), d = 10, its = 40
i = 10109 (of 10192), d = 10, its = 56
i = 10110 (of 10192), d = 5.15057, its = 10
i = 10111 (of 10192), d = 10, its = 52
i = 10112 (of 10192), d = 0.810135, its = 19
i = 10113 (of 10192), d = 10, its = 53
i = 10114 (of 10192), d = 10, its = 42
i = 10115 (of 10192), d = 10, its = 37
i = 10116 (of 10192), d = 10, its = 40
i = 10117 (of 10192), d = 3.47091, its = 6
i = 10118 (of 10192), d = 1.05119, its = 17
i = 10119 (of 10192), d = 10, its = 52
i = 10120 (of 10192), d = 10, its = 50
i = 10121 (of 10192), d = 10, its = 37
i = 10122 (of 10192), d = 10, its = 54
i = 10123 (of 10192), d = 2.04795, its = 3
i = 10124 (of 10192), d = 10, its = 54
i = 10125 (of 10192), d = 10, its = 50
i = 10126 (of 10192), d = 10, its = 37
i = 10127 (of 10192), d = 10, its = 37
i = 10128 (of 10192), d = 10, its = 55
i = 10129 (of 10192), d = 10, its = 52
i = 10130 (of 10192), d = 2.83177, its = 6
i = 10131 (of 10192), d = 1.96024, its = 3
i = 10132 (of 10192), d = 10, its = 40
i = 10133 (of 10192), d = 10, its = 49
i = 10134 (of 10192), d = 10, its = 51
i = 10135 (of 10192), d = 10, its = 53
i = 10136 (of 10192), d = 0.615751, its = 18
i = 10137 (of 10192), d = 10, its = 55
i = 10138 (of 10192), d = 10, its = 37
i = 10139 (of 10192), d = 10, its = 51
i = 10140 (of 10192), d = 10, its = 51
i = 10141 (of 10192), d = 2.33016, its = 4
i = 10142 (of 10192), d = 1.98574, its = 3
i = 10143 (of 10192), d = 1.22372, its = 8
i = 10144 (of 10192), d = 10, its = 49
i = 10145 (of 10192), d = 10, its = 54
i = 10146 (of 10192), d = 10, its = 52
i = 10147 (of 10192), d = 1.10585, its = 16
i = 10148 (of 10192), d = 2.32404, its = 5
i = 10149 (of 10192), d = 2.62697, its = 6
i = 10150 (of 10192), d = 10, its = 49
i = 10151 (of 10192), d = 10, its = 53
i = 10152 (of 10192), d = 10, its = 49
i = 10153 (of 10192), d = 10, its = 37
i = 10154 (of 10192), d = 10, its = 50
i = 10155 (of 10192), d = 10, its = 50
i = 10156 (of 10192), d = 10, its = 37
i = 10157 (of 10192), d = 10, its = 56
i = 10158 (of 10192), d = 10, its = 51
i = 10159 (of 10192), d = 10, its = 55
i = 10160 (of 10192), d = 2.30218, its = 5
i = 10161 (of 10192), d = 10, its = 52
i = 10162 (of 10192), d = 10, its = 50
i = 10163 (of 10192), d = 10, its = 55
i = 10164 (of 10192), d = 10, its = 54
i = 10165 (of 10192), d = 10, its = 50
i = 10166 (of 10192), d = 10, its = 55
i = 10167 (of 10192), d = 10, its = 56
i = 10168 (of 10192), d = 1.06268, its = 20
i = 10169 (of 10192), d = 10, its = 53
i = 10170 (of 10192), d = 10, its = 37
i = 10171 (of 10192), d = 10, its = 55
i = 10172 (of 10192), d = 10, its = 51
i = 10173 (of 10192), d = 10, its = 37
i = 10174 (of 10192), d = 10, its = 51
i = 10175 (of 10192), d = 0.468767, its = 21
i = 10176 (of 10192), d = 10, its = 37
i = 10177 (of 10192), d = 1.88973, its = 4
i = 10178 (of 10192), d = 10, its = 52
i = 10179 (of 10192), d = 10, its = 54
i = 10180 (of 10192), d = 10, its = 52
i = 10181 (of 10192), d = 1.78269, its = 5
i = 10182 (of 10192), d = 10, its = 47
i = 10183 (of 10192), d = 10, its = 54
i = 10184 (of 10192), d = 1.64452, its = 6
i = 10185 (of 10192), d = 1.21994, its = 8
i = 10186 (of 10192), d = 1.63663, its = 5
i = 10187 (of 10192), d = 10, its = 37
i = 10188 (of 10192), d = 10, its = 49
i = 10189 (of 10192), d = 10, its = 49
i = 10190 (of 10192), d = 2.56337, its = 5
i = 10191 (of 10192), d = 10, its = 39
i = 10192 (of 10192), d = 10, its = 53
[1] "Fri Feb 09 11:05:47 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 11:05:54 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 11:05:58 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 11:06:02 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 11:06:08 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
Warning in train(allmodel, regr.task) :
  Could not train learner regr.mob: Error in trainLearner.regr.mob(.learner = structure(list(id = "regr.mob",  : 
  Failed to fit party::mob. Some coefficients are estimated as NA

[1] "Fri Feb 09 11:06:15 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=6; decay=2.94e-05
# weights:  73
initial  value 403025.225597 
iter  10 value 52984.071286
iter  20 value 36866.929230
iter  30 value 32443.648834
iter  40 value 29373.543284
iter  50 value 28436.738528
iter  60 value 24055.693512
iter  70 value 22430.243496
iter  80 value 21131.984095
iter  90 value 20448.124379
iter 100 value 20253.389368
final  value 20253.389368 
stopped after 100 iterations
# weights:  73
initial  value 403436.850496 
iter  10 value 79332.423785
iter  20 value 33526.009813
iter  30 value 30159.012800
iter  40 value 28514.964913
iter  50 value 26753.096016
iter  60 value 25018.365733
iter  70 value 23665.108743
iter  80 value 21306.686271
iter  90 value 20595.562778
iter 100 value 20450.240182
final  value 20450.240182 
stopped after 100 iterations
# weights:  73
initial  value 390280.799074 
iter  10 value 91703.401216
iter  20 value 68909.415686
iter  30 value 66087.102214
iter  40 value 60861.342345
iter  50 value 47014.121207
iter  60 value 32867.618260
iter  70 value 29291.298867
iter  80 value 26693.433173
iter  90 value 23558.195753
iter 100 value 22295.294106
final  value 22295.294106 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=1.02; time: 0.5 min
[Tune-x] 2: size=16; decay=5.01e-05
# weights:  193
initial  value 405203.024865 
iter  10 value 110870.322787
iter  20 value 26609.388712
iter  30 value 24044.777560
iter  40 value 22775.424041
iter  50 value 22346.417940
iter  60 value 21834.494634
iter  70 value 21274.104967
iter  80 value 20737.461710
iter  90 value 20526.989515
iter 100 value 20444.471529
final  value 20444.471529 
stopped after 100 iterations
# weights:  193
initial  value 447990.151566 
iter  10 value 67768.887371
iter  20 value 26572.547183
iter  30 value 23543.963988
iter  40 value 22434.151596
iter  50 value 22050.897125
iter  60 value 21704.558186
iter  70 value 21329.013545
iter  80 value 21108.047798
iter  90 value 20855.039441
iter 100 value 20629.698787
final  value 20629.698787 
stopped after 100 iterations
# weights:  193
initial  value 388646.971854 
iter  10 value 53185.925617
iter  20 value 29241.632730
iter  30 value 23543.017067
iter  40 value 21547.121709
iter  50 value 20855.706717
iter  60 value 20560.561984
iter  70 value 20389.163199
iter  80 value 20297.614007
iter  90 value 20219.815326
iter 100 value 20182.476948
final  value 20182.476948 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=   1; time: 1.5 min
[Tune-x] 3: size=15; decay=0.00656
# weights:  181
initial  value 378565.754632 
iter  10 value 56479.059842
iter  20 value 25357.216618
iter  30 value 22144.345880
iter  40 value 20936.294129
iter  50 value 20610.659303
iter  60 value 20457.721739
iter  70 value 20290.255831
iter  80 value 20204.676059
iter  90 value 20159.636628
iter 100 value 20118.539351
final  value 20118.539351 
stopped after 100 iterations
# weights:  181
initial  value 422920.428359 
iter  10 value 55233.772977
iter  20 value 22858.200882
iter  30 value 21057.808885
iter  40 value 20772.937711
iter  50 value 20600.765398
iter  60 value 20513.303706
iter  70 value 20450.780134
iter  80 value 20389.737987
iter  90 value 20332.919147
iter 100 value 20268.311951
final  value 20268.311951 
stopped after 100 iterations
# weights:  181
initial  value 405055.870739 
iter  10 value 65535.786885
iter  20 value 25496.910342
iter  30 value 22132.719887
iter  40 value 21299.460513
iter  50 value 20771.650286
iter  60 value 20477.994057
iter  70 value 20343.857524
iter  80 value 20228.233122
iter  90 value 20179.828252
iter 100 value 20143.029273
final  value 20143.029273 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=   1; time: 1.3 min
[Tune-x] 4: size=16; decay=0.00749
# weights:  193
initial  value 580326.637118 
iter  10 value 44830.617412
iter  20 value 24122.748384
iter  30 value 22138.904502
iter  40 value 21515.842283
iter  50 value 21055.521781
iter  60 value 20658.890982
iter  70 value 20415.978687
iter  80 value 20318.794604
iter  90 value 20226.641432
iter 100 value 20144.608249
final  value 20144.608249 
stopped after 100 iterations
# weights:  193
initial  value 414601.521942 
iter  10 value 68340.131026
iter  20 value 30894.237514
iter  30 value 24192.828494
iter  40 value 22096.700202
iter  50 value 21525.673161
iter  60 value 21169.345342
iter  70 value 20780.647139
iter  80 value 20559.354430
iter  90 value 20494.412725
iter 100 value 20446.617113
final  value 20446.617113 
stopped after 100 iterations
# weights:  193
initial  value 394777.969154 
iter  10 value 78714.766374
iter  20 value 24198.349738
iter  30 value 21664.810279
iter  40 value 20965.796392
iter  50 value 20543.908366
iter  60 value 20374.377534
iter  70 value 20276.062781
iter  80 value 20222.586968
iter  90 value 20192.599647
iter 100 value 20130.593075
final  value 20130.593075 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=   1; time: 1.5 min
[Tune-x] 5: size=7; decay=0.501
# weights:  85
initial  value 396072.901952 
iter  10 value 109495.549483
iter  20 value 56752.005897
iter  30 value 40965.818371
iter  40 value 37005.280396
iter  50 value 34175.445355
iter  60 value 31011.523884
iter  70 value 28165.559330
iter  80 value 24736.635945
iter  90 value 23320.897937
iter 100 value 22568.562451
final  value 22568.562451 
stopped after 100 iterations
# weights:  85
initial  value 400838.712623 
iter  10 value 55735.698586
iter  20 value 25592.781989
iter  30 value 22929.014445
iter  40 value 22448.033623
iter  50 value 22092.199232
iter  60 value 21909.809749
iter  70 value 21622.531116
iter  80 value 21397.748395
iter  90 value 21272.091575
iter 100 value 21150.978121
final  value 21150.978121 
stopped after 100 iterations
# weights:  85
initial  value 406953.945387 
iter  10 value 150117.360676
iter  20 value 40109.062461
iter  30 value 33749.316105
iter  40 value 31500.846889
iter  50 value 29351.312223
iter  60 value 27227.839698
iter  70 value 26283.715539
iter  80 value 24483.957808
iter  90 value 22728.741018
iter 100 value 21556.165422
final  value 21556.165422 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 6: size=20; decay=0.0368
# weights:  241
initial  value 393146.076570 
iter  10 value 67265.158345
iter  20 value 25338.039644
iter  30 value 22856.104145
iter  40 value 21319.654879
iter  50 value 20850.895093
iter  60 value 20509.608507
iter  70 value 20367.893036
iter  80 value 20296.550429
iter  90 value 20247.721008
iter 100 value 20204.697467
final  value 20204.697467 
stopped after 100 iterations
# weights:  241
initial  value 391838.301823 
iter  10 value 36019.766485
iter  20 value 24113.033580
iter  30 value 22254.402200
iter  40 value 21024.217975
iter  50 value 20570.768326
iter  60 value 20459.540181
iter  70 value 20371.572646
iter  80 value 20318.521617
iter  90 value 20272.385963
iter 100 value 20252.086706
final  value 20252.086706 
stopped after 100 iterations
# weights:  241
initial  value 391108.831117 
iter  10 value 63742.699369
iter  20 value 26410.490788
iter  30 value 21841.142450
iter  40 value 20828.878686
iter  50 value 20487.492884
iter  60 value 20313.956491
iter  70 value 20200.644563
iter  80 value 20145.077114
iter  90 value 20114.665404
iter 100 value 20092.191855
final  value 20092.191855 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=   1; time: 1.9 min
[Tune-x] 7: size=9; decay=0.000637
# weights:  109
initial  value 394934.285114 
iter  10 value 48971.432512
iter  20 value 25062.557247
iter  30 value 22025.637778
iter  40 value 21209.933166
iter  50 value 21001.857660
iter  60 value 20831.648016
iter  70 value 20719.166844
iter  80 value 20525.631526
iter  90 value 20409.680683
iter 100 value 20288.852797
final  value 20288.852797 
stopped after 100 iterations
# weights:  109
initial  value 386210.377985 
iter  10 value 75758.953250
iter  20 value 31074.506490
iter  30 value 25349.508887
iter  40 value 24285.570285
iter  50 value 23150.817393
iter  60 value 22369.284206
iter  70 value 21284.393016
iter  80 value 20966.972540
iter  90 value 20699.728018
iter 100 value 20579.901245
final  value 20579.901245 
stopped after 100 iterations
# weights:  109
initial  value 394403.031515 
iter  10 value 64704.328479
iter  20 value 37011.121066
iter  30 value 30024.334519
iter  40 value 25557.755586
iter  50 value 24268.945467
iter  60 value 23695.065258
iter  70 value 22669.493023
iter  80 value 21527.049487
iter  90 value 20932.747096
iter 100 value 20528.784756
final  value 20528.784756 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=1.01; time: 0.8 min
[Tune-x] 8: size=16; decay=2.08e-05
# weights:  193
initial  value 404651.557837 
iter  10 value 43967.476109
iter  20 value 24092.736353
iter  30 value 21357.611325
iter  40 value 20595.206673
iter  50 value 20355.716323
iter  60 value 20222.542003
iter  70 value 20160.066945
iter  80 value 20116.705178
iter  90 value 20093.829240
iter 100 value 20061.724672
final  value 20061.724672 
stopped after 100 iterations
# weights:  193
initial  value 458220.459159 
iter  10 value 57120.434554
iter  20 value 31207.362039
iter  30 value 25389.318149
iter  40 value 23620.366611
iter  50 value 22943.891011
iter  60 value 22054.615671
iter  70 value 21582.298685
iter  80 value 21276.201354
iter  90 value 20947.822519
iter 100 value 20683.923139
final  value 20683.923139 
stopped after 100 iterations
# weights:  193
initial  value 448173.361524 
iter  10 value 62917.170092
iter  20 value 25444.026454
iter  30 value 21942.017947
iter  40 value 21084.069019
iter  50 value 20878.537712
iter  60 value 20697.203800
iter  70 value 20489.195741
iter  80 value 20394.391427
iter  90 value 20302.112100
iter 100 value 20204.873545
final  value 20204.873545 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=   1; time: 1.5 min
[Tune-x] 9: size=5; decay=5.18e-05
# weights:  61
initial  value 399386.175725 
iter  10 value 136641.286863
iter  20 value 89942.238433
iter  30 value 75764.171929
iter  40 value 61160.049111
iter  50 value 49322.193429
iter  60 value 45074.893709
iter  70 value 40434.740979
iter  80 value 32044.925914
iter  90 value 26054.163958
iter 100 value 21687.727996
final  value 21687.727996 
stopped after 100 iterations
# weights:  61
initial  value 396839.525520 
iter  10 value 117069.466721
iter  20 value 56601.710352
iter  30 value 48337.612500
iter  40 value 46251.449613
iter  50 value 42880.259998
iter  60 value 33435.812741
iter  70 value 23491.964284
iter  80 value 20978.881706
iter  90 value 20594.530676
iter 100 value 20445.997451
final  value 20445.997451 
stopped after 100 iterations
# weights:  61
initial  value 392053.830653 
iter  10 value 157925.487423
iter  20 value 126026.506303
iter  30 value 121596.848630
iter  40 value 115365.597757
iter  50 value 108284.011105
iter  60 value 105485.753028
iter  70 value 100123.253299
iter  80 value 71353.899775
iter  90 value 50967.630327
iter 100 value 43371.948920
final  value 43371.948920 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=1.19; time: 0.4 min
[Tune-x] 10: size=18; decay=0.0244
# weights:  217
initial  value 433810.630458 
iter  10 value 62690.294224
iter  20 value 27675.081846
iter  30 value 22951.473556
iter  40 value 21207.038656
iter  50 value 20809.529349
iter  60 value 20603.758849
iter  70 value 20460.189443
iter  80 value 20390.927547
iter  90 value 20305.252349
iter 100 value 20227.276274
final  value 20227.276274 
stopped after 100 iterations
# weights:  217
initial  value 429312.814038 
iter  10 value 68875.311122
iter  20 value 35313.530535
iter  30 value 25496.088306
iter  40 value 22155.260932
iter  50 value 21441.528439
iter  60 value 21044.362043
iter  70 value 20807.193652
iter  80 value 20693.567895
iter  90 value 20634.704955
iter 100 value 20524.753329
final  value 20524.753329 
stopped after 100 iterations
# weights:  217
initial  value 420139.970133 
iter  10 value 47688.032843
iter  20 value 23023.606170
iter  30 value 20641.385296
iter  40 value 20329.010378
iter  50 value 20200.057718
iter  60 value 20139.651565
iter  70 value 20092.758285
iter  80 value 20061.882874
iter  90 value 20040.542514
iter 100 value 20020.879904
final  value 20020.879904 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=   1; time: 1.6 min
[Tune-x] 11: size=5; decay=0.236
# weights:  61
initial  value 386531.750571 
iter  10 value 134472.316855
iter  20 value 99812.249186
iter  30 value 82751.416021
iter  40 value 69928.817116
iter  50 value 60441.350693
iter  60 value 51328.329311
iter  70 value 33752.804530
iter  80 value 25508.940551
iter  90 value 24106.333543
iter 100 value 23772.509602
final  value 23772.509602 
stopped after 100 iterations
# weights:  61
initial  value 399116.127718 
iter  10 value 121207.274241
iter  20 value 35180.316589
iter  30 value 31025.068795
iter  40 value 29377.722142
iter  50 value 28610.132706
iter  60 value 26174.320972
iter  70 value 23213.532395
iter  80 value 21324.582919
iter  90 value 20828.506084
iter 100 value 20708.183675
final  value 20708.183675 
stopped after 100 iterations
# weights:  61
initial  value 417613.654175 
iter  10 value 138426.499296
iter  20 value 45199.726407
iter  30 value 37865.099178
iter  40 value 34779.518905
iter  50 value 33646.549005
iter  60 value 28228.441927
iter  70 value 22891.507493
iter  80 value 21168.874317
iter  90 value 20960.559583
iter 100 value 20832.960979
final  value 20832.960979 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 12: size=11; decay=0.00243
# weights:  133
initial  value 395087.014058 
iter  10 value 54374.592503
iter  20 value 24339.492602
iter  30 value 21884.760383
iter  40 value 21156.913614
iter  50 value 20992.065495
iter  60 value 20889.956485
iter  70 value 20713.125030
iter  80 value 20514.143958
iter  90 value 20383.605171
iter 100 value 20287.930444
final  value 20287.930444 
stopped after 100 iterations
# weights:  133
initial  value 399291.323232 
iter  10 value 90636.359346
iter  20 value 32547.481260
iter  30 value 24863.119502
iter  40 value 23315.223202
iter  50 value 22479.609102
iter  60 value 21863.682777
iter  70 value 21525.634919
iter  80 value 21124.200689
iter  90 value 20915.833701
iter 100 value 20680.635002
final  value 20680.635002 
stopped after 100 iterations
# weights:  133
initial  value 399825.307909 
iter  10 value 68391.139599
iter  20 value 37002.503972
iter  30 value 32430.130014
iter  40 value 29405.464569
iter  50 value 27487.211665
iter  60 value 25640.040048
iter  70 value 24244.903703
iter  80 value 22969.920152
iter  90 value 22231.980925
iter 100 value 21402.328108
final  value 21402.328108 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=1.01; time: 1.0 min
[Tune-x] 13: size=13; decay=0.00135
# weights:  157
initial  value 395598.026065 
iter  10 value 50047.067909
iter  20 value 23783.308290
iter  30 value 21436.742581
iter  40 value 20794.641796
iter  50 value 20463.705303
iter  60 value 20336.196634
iter  70 value 20277.664383
iter  80 value 20218.471320
iter  90 value 20182.197209
iter 100 value 20136.716169
final  value 20136.716169 
stopped after 100 iterations
# weights:  157
initial  value 408153.910344 
iter  10 value 58714.419804
iter  20 value 23600.238367
iter  30 value 21968.924108
iter  40 value 21357.149595
iter  50 value 20928.813619
iter  60 value 20653.226361
iter  70 value 20546.870534
iter  80 value 20433.629312
iter  90 value 20382.887382
iter 100 value 20356.216975
final  value 20356.216975 
stopped after 100 iterations
# weights:  157
initial  value 413011.071263 
iter  10 value 106533.635979
iter  20 value 36733.429028
iter  30 value 23246.217935
iter  40 value 21525.680062
iter  50 value 21060.874979
iter  60 value 20796.172761
iter  70 value 20657.490155
iter  80 value 20460.813273
iter  90 value 20350.204014
iter 100 value 20216.216482
final  value 20216.216482 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=   1; time: 1.3 min
[Tune-x] 14: size=20; decay=0.00151
# weights:  241
initial  value 405353.962628 
iter  10 value 73820.740569
iter  20 value 31194.983519
iter  30 value 23301.788028
iter  40 value 21136.558357
iter  50 value 20467.768244
iter  60 value 20210.586390
iter  70 value 20087.844705
iter  80 value 20039.893457
iter  90 value 20023.911077
iter 100 value 20010.531420
final  value 20010.531420 
stopped after 100 iterations
# weights:  241
initial  value 396667.516658 
iter  10 value 75022.069161
iter  20 value 33347.923474
iter  30 value 23355.007553
iter  40 value 21049.602714
iter  50 value 20643.105686
iter  60 value 20476.444806
iter  70 value 20406.941547
iter  80 value 20345.559131
iter  90 value 20307.465994
iter 100 value 20256.189892
final  value 20256.189892 
stopped after 100 iterations
# weights:  241
initial  value 411482.198904 
iter  10 value 43724.677105
iter  20 value 22672.497488
iter  30 value 20650.206756
iter  40 value 20257.150018
iter  50 value 20158.975614
iter  60 value 20116.773953
iter  70 value 20080.547513
iter  80 value 20051.473513
iter  90 value 20023.124733
iter 100 value 20010.504550
final  value 20010.504550 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=   1; time: 1.9 min
[Tune-x] 15: size=13; decay=0.521
# weights:  157
initial  value 392088.966442 
iter  10 value 75334.230181
iter  20 value 36786.829708
iter  30 value 27546.991904
iter  40 value 24645.800436
iter  50 value 22841.906290
iter  60 value 22137.321383
iter  70 value 21795.241559
iter  80 value 21618.766574
iter  90 value 21517.358146
iter 100 value 21345.113970
final  value 21345.113970 
stopped after 100 iterations
# weights:  157
initial  value 432777.280210 
iter  10 value 54933.802232
iter  20 value 31776.410876
iter  30 value 27143.382511
iter  40 value 24674.524868
iter  50 value 24071.563451
iter  60 value 23666.872104
iter  70 value 23005.607700
iter  80 value 22499.343478
iter  90 value 21952.166360
iter 100 value 21720.692781
final  value 21720.692781 
stopped after 100 iterations
# weights:  157
initial  value 417499.116700 
iter  10 value 68192.373666
iter  20 value 29286.549179
iter  30 value 23380.418682
iter  40 value 22232.983010
iter  50 value 21818.990602
iter  60 value 21633.602746
iter  70 value 21497.673234
iter  80 value 21331.609022
iter  90 value 21201.382625
iter 100 value 21065.424115
final  value 21065.424115 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=   1; time: 1.2 min
[Tune-x] 16: size=4; decay=4.57
# weights:  49
initial  value 395002.948606 
iter  10 value 195989.405581
iter  20 value 147580.623154
iter  30 value 145033.725486
iter  40 value 141575.457370
iter  50 value 127008.288460
iter  60 value 108086.317830
iter  70 value 96372.484758
iter  80 value 56021.670959
iter  90 value 32608.959847
iter 100 value 28749.519377
final  value 28749.519377 
stopped after 100 iterations
# weights:  49
initial  value 401152.666676 
iter  10 value 127661.913572
iter  20 value 87640.406330
iter  30 value 77680.112393
iter  40 value 63432.161318
iter  50 value 47290.742843
iter  60 value 33416.255256
iter  70 value 28819.988033
iter  80 value 26359.116823
iter  90 value 25318.865663
iter 100 value 25016.163463
final  value 25016.163463 
stopped after 100 iterations
# weights:  49
initial  value 394042.362212 
iter  10 value 95946.941928
iter  20 value 48953.170863
iter  30 value 46852.709047
iter  40 value 43588.183054
iter  50 value 38280.276786
iter  60 value 31196.765970
iter  70 value 28244.046744
iter  80 value 26442.023576
iter  90 value 25512.319771
iter 100 value 25001.752708
final  value 25001.752708 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=1.05; time: 0.3 min
[Tune-x] 17: size=12; decay=0.163
# weights:  145
initial  value 406133.244694 
iter  10 value 121418.175178
iter  20 value 34697.596828
iter  30 value 27790.429963
iter  40 value 26378.728402
iter  50 value 24945.814298
iter  60 value 23914.032857
iter  70 value 22927.402276
iter  80 value 22004.221678
iter  90 value 21398.686836
iter 100 value 21026.502448
final  value 21026.502448 
stopped after 100 iterations
# weights:  145
initial  value 402540.098117 
iter  10 value 32741.509813
iter  20 value 24137.921157
iter  30 value 22480.135452
iter  40 value 21524.194865
iter  50 value 21128.159706
iter  60 value 20910.353531
iter  70 value 20741.593411
iter  80 value 20653.397048
iter  90 value 20588.776352
iter 100 value 20518.790852
final  value 20518.790852 
stopped after 100 iterations
# weights:  145
initial  value 388895.810552 
iter  10 value 71424.657193
iter  20 value 32124.675513
iter  30 value 26680.658949
iter  40 value 22996.289862
iter  50 value 22158.324312
iter  60 value 21812.485670
iter  70 value 21538.827264
iter  80 value 21363.049258
iter  90 value 21102.508682
iter 100 value 20887.593773
final  value 20887.593773 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=   1; time: 1.1 min
[Tune-x] 18: size=20; decay=0.184
# weights:  241
initial  value 395833.199137 
iter  10 value 57645.147764
iter  20 value 25769.348108
iter  30 value 21368.512652
iter  40 value 20587.909775
iter  50 value 20451.892485
iter  60 value 20365.416371
iter  70 value 20318.925661
iter  80 value 20276.292051
iter  90 value 20250.795764
iter 100 value 20231.659813
final  value 20231.659813 
stopped after 100 iterations
# weights:  241
initial  value 393604.579145 
iter  10 value 52157.340189
iter  20 value 27155.953616
iter  30 value 23039.945462
iter  40 value 21818.405926
iter  50 value 21064.735985
iter  60 value 20844.899096
iter  70 value 20729.321314
iter  80 value 20645.789796
iter  90 value 20583.864002
iter 100 value 20535.491848
final  value 20535.491848 
stopped after 100 iterations
# weights:  241
initial  value 425368.607911 
iter  10 value 45876.569172
iter  20 value 26770.319951
iter  30 value 22980.705627
iter  40 value 21256.622872
iter  50 value 20771.632314
iter  60 value 20564.662648
iter  70 value 20461.884455
iter  80 value 20423.621895
iter  90 value 20394.796601
iter 100 value 20362.916141
final  value 20362.916141 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=   1; time: 1.9 min
[Tune-x] 19: size=9; decay=0.000975
# weights:  109
initial  value 413444.945076 
iter  10 value 119841.794293
iter  20 value 48873.910610
iter  30 value 26516.540313
iter  40 value 24409.606863
iter  50 value 23733.157535
iter  60 value 23080.870027
iter  70 value 22660.883200
iter  80 value 22054.550416
iter  90 value 21366.733099
iter 100 value 20746.233804
final  value 20746.233804 
stopped after 100 iterations
# weights:  109
initial  value 407033.724950 
iter  10 value 108396.320534
iter  20 value 43044.593111
iter  30 value 35824.798327
iter  40 value 32228.903896
iter  50 value 28557.856222
iter  60 value 26032.484954
iter  70 value 24557.253525
iter  80 value 22386.970775
iter  90 value 21127.376505
iter 100 value 20512.360968
final  value 20512.360968 
stopped after 100 iterations
# weights:  109
initial  value 393831.714212 
iter  10 value 92981.659941
iter  20 value 37949.810584
iter  30 value 28762.920947
iter  40 value 25834.183079
iter  50 value 24192.168851
iter  60 value 22855.841551
iter  70 value 21727.887023
iter  80 value 20996.174313
iter  90 value 20635.204274
iter 100 value 20401.018090
final  value 20401.018090 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=1.01; time: 0.7 min
[Tune-x] 20: size=9; decay=1.37e-05
# weights:  109
initial  value 395899.647128 
iter  10 value 50505.292404
iter  20 value 27116.081993
iter  30 value 23644.405496
iter  40 value 22748.334045
iter  50 value 22244.885263
iter  60 value 21709.318303
iter  70 value 21317.822104
iter  80 value 20892.020590
iter  90 value 20636.207726
iter 100 value 20444.243373
final  value 20444.243373 
stopped after 100 iterations
# weights:  109
initial  value 393356.457906 
iter  10 value 72532.944281
iter  20 value 40021.402483
iter  30 value 32638.244737
iter  40 value 30045.365460
iter  50 value 28461.613519
iter  60 value 26657.572409
iter  70 value 24184.439110
iter  80 value 23097.577511
iter  90 value 21571.001635
iter 100 value 21104.370031
final  value 21104.370031 
stopped after 100 iterations
# weights:  109
initial  value 385373.322867 
iter  10 value 52666.517739
iter  20 value 24188.943123
iter  30 value 21909.172163
iter  40 value 21359.786568
iter  50 value 21045.169176
iter  60 value 20809.529668
iter  70 value 20574.722363
iter  80 value 20411.373716
iter  90 value 20294.391335
iter 100 value 20201.827664
final  value 20201.827664 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=1.01; time: 0.8 min
[Tune] Result: size=20; decay=0.0368 : rmse.test.rmse=   1
# weights:  241
initial  value 592432.351093 
iter  10 value 62758.232414
iter  20 value 34369.332607
iter  30 value 32047.514914
iter  40 value 31267.276943
iter  50 value 30764.452561
iter  60 value 30575.618604
iter  70 value 30480.525471
iter  80 value 30428.224241
iter  90 value 30378.608964
iter 100 value 30328.436152
final  value 30328.436152 
stopped after 100 iterations
[1] "Fri Feb 09 11:29:15 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081Warning in train(allmodel, regr.task) :
  Could not train learner regr.nodeHarvest: Error : cannot allocate vector of size 252.2 Mb

[1] "Fri Feb 09 11:33:06 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
[1] "Fri Feb 09 11:33:11 2018"
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/src/contrib:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/src/contrib/PACKAGES'
Warning: unable to access index for repository https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4:
  cannot open URL 'https://rweb.crmda.ku.edu/cran/bin/windows/contrib/3.4/PACKAGES'
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 11:34:03 2018"
Error in curl::curl_fetch_memory(url, handle = handle) : 
  Could not resolve host: mlrhyperopt.jakob-r.de
<simpleError: cannot allocate vector of size 233.2 Mb>
Warning in train(allmodel, regr.task) :
  Could not train learner regr.randomForestSRC: Error in randomForestSRC::rfsrc(f, data = getTaskData(.task, .subset),  : 
  An error has occurred in the grow algorithm.  Please turn trace on for further analysis.

[1] "Fri Feb 09 11:34:10 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3 1 to 10   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=3; min.node.size=1
[Tune-y] 1: rmse.test.rmse=1.06; time: 2.6 min
[Tune-x] 2: mtry=8; min.node.size=2
[Tune-y] 2: rmse.test.rmse=1.11; time: 5.5 min
[Tune-x] 3: mtry=8; min.node.size=5
[Tune-y] 3: rmse.test.rmse=1.08; time: 4.5 min
[Tune-x] 4: mtry=8; min.node.size=5
[Tune-y] 4: rmse.test.rmse=1.08; time: 4.5 min
[Tune-x] 5: mtry=4; min.node.size=8
[Tune-y] 5: rmse.test.rmse=1.03; time: 2.5 min
[Tune-x] 6: mtry=10; min.node.size=6
[Tune-y] 6: rmse.test.rmse=1.08; time: 5.1 min
[Tune-x] 7: mtry=5; min.node.size=4
[Tune-y] 7: rmse.test.rmse=1.06; time: 3.4 min
[Tune-x] 8: mtry=8; min.node.size=1
[Tune-y] 8: rmse.test.rmse=1.11; time: 5.4 min
[Tune-x] 9: mtry=3; min.node.size=2
[Tune-y] 9: rmse.test.rmse=1.06; time: 2.2 min
[Tune-x] 10: mtry=9; min.node.size=6
[Tune-y] 10: rmse.test.rmse=1.07; time: 4.7 min
[Tune-x] 11: mtry=3; min.node.size=8
[Tune-y] 11: rmse.test.rmse=1.05; time: 2.1 min
[Tune-x] 12: mtry=6; min.node.size=4
[Tune-y] 12: rmse.test.rmse=1.07; time: 3.9 min
[Tune-x] 13: mtry=7; min.node.size=4
[Tune-y] 13: rmse.test.rmse=1.08; time: 4.4 min
[Tune-x] 14: mtry=10; min.node.size=4
[Tune-y] 14: rmse.test.rmse= 1.1; time: 5.5 min
[Tune-x] 15: mtry=7; min.node.size=8
[Tune-y] 15: rmse.test.rmse=1.05; time: 3.7 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse= 1.4; time: 1.7 min
[Tune-x] 17: mtry=6; min.node.size=8
[Tune-y] 17: rmse.test.rmse=1.05; time: 3.5 min
[Tune-x] 18: mtry=10; min.node.size=8
[Tune-y] 18: rmse.test.rmse=1.06; time: 4.7 min
[Tune-x] 19: mtry=5; min.node.size=4
[Tune-y] 19: rmse.test.rmse=1.06; time: 3.4 min
[Tune-x] 20: mtry=5; min.node.size=1
[Tune-y] 20: rmse.test.rmse=1.08; time: 3.8 min
[Tune] Result: mtry=4; min.node.size=8 : rmse.test.rmse=1.03
[1] "Fri Feb 09 12:52:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
Warning in predict.WrappedModel(mod, newdata = (testing)) :
  Could not predict with learner regr.rknn: Error in knn.reg(train = data[, fset], test = newdata[, fset], y = y,  : 
  too many ties in knn

[1] "Fri Feb 09 12:53:07 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.00598; maxdepth=5; minbucket=41; minsplit=10
[Tune-y] 1: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 2: cp=0.141; maxdepth=16; minbucket=40; minsplit=27
[Tune-y] 2: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 3: cp=0.0104; maxdepth=24; minbucket=50; minsplit=32
[Tune-y] 3: rmse.test.rmse=1.53; time: 0.0 min
[Tune-x] 4: cp=0.0196; maxdepth=11; minbucket=40; minsplit=7
[Tune-y] 4: rmse.test.rmse= 2.1; time: 0.0 min
[Tune-x] 5: cp=0.0048; maxdepth=6; minbucket=46; minsplit=30
[Tune-y] 5: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 6: cp=0.00457; maxdepth=23; minbucket=28; minsplit=23
[Tune-y] 6: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 7: cp=0.0636; maxdepth=12; minbucket=49; minsplit=21
[Tune-y] 7: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 8: cp=0.0652; maxdepth=25; minbucket=13; minsplit=48
[Tune-y] 8: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 9: cp=0.0466; maxdepth=22; minbucket=49; minsplit=37
[Tune-y] 9: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 10: cp=0.0174; maxdepth=12; minbucket=25; minsplit=6
[Tune-y] 10: rmse.test.rmse=1.68; time: 0.0 min
[Tune-x] 11: cp=0.0733; maxdepth=11; minbucket=34; minsplit=27
[Tune-y] 11: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 12: cp=0.18; maxdepth=23; minbucket=35; minsplit=34
[Tune-y] 12: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 13: cp=0.136; maxdepth=19; minbucket=46; minsplit=45
[Tune-y] 13: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 14: cp=0.0327; maxdepth=4; minbucket=7; minsplit=42
[Tune-y] 14: rmse.test.rmse= 2.1; time: 0.0 min
[Tune-x] 15: cp=0.33; maxdepth=17; minbucket=20; minsplit=43
[Tune-y] 15: rmse.test.rmse=3.23; time: 0.0 min
[Tune-x] 16: cp=0.00513; maxdepth=3; minbucket=39; minsplit=32
[Tune-y] 16: rmse.test.rmse=1.93; time: 0.0 min
[Tune-x] 17: cp=0.00301; maxdepth=15; minbucket=9; minsplit=12
[Tune-y] 17: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 18: cp=0.00458; maxdepth=6; minbucket=44; minsplit=6
[Tune-y] 18: rmse.test.rmse= 1.3; time: 0.0 min
[Tune-x] 19: cp=0.0732; maxdepth=23; minbucket=39; minsplit=38
[Tune-y] 19: rmse.test.rmse=2.43; time: 0.0 min
[Tune-x] 20: cp=0.0164; maxdepth=24; minbucket=36; minsplit=39
[Tune-y] 20: rmse.test.rmse=1.53; time: 0.0 min
[Tune] Result: cp=0.0048; maxdepth=6; minbucket=46; minsplit=30 : rmse.test.rmse= 1.3
[1] "Fri Feb 09 12:53:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 12:53:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.rvm: Error : cannot allocate vector of size 7.0 Gb

[1] "Fri Feb 09 12:53:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.67600 0.22800 0.07660 0.02580 0.00868
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 7 
Runtime: 11.75072 mins 

 Values of predicted responses: 
   index             3 
   lambda      0.07661 
    Y 1         -1.271 
    Y 2         -4.254 
    Y 3         0.1279 
    Y 4          3.304 
    Y 5         -3.545 
[1] "Fri Feb 09 13:05:32 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=48; max_depth=1; eta=0.476; gamma=1.17; colsample_bytree=0.587; min_child_weight=9.39; subsample=0.831
[Tune-y] 1: rmse.test.rmse=2.38; time: 0.0 min
[Tune-x] 2: nrounds=176; max_depth=4; eta=0.47; gamma=9.96; colsample_bytree=0.538; min_child_weight=8.65; subsample=0.476
[Tune-y] 2: rmse.test.rmse=1.01; time: 0.1 min
[Tune-x] 3: nrounds=1.06e+03; max_depth=1; eta=0.139; gamma=1.19; colsample_bytree=0.66; min_child_weight=11.3; subsample=0.417
[Tune-y] 3: rmse.test.rmse=2.38; time: 0.3 min
[Tune-x] 4: nrounds=788; max_depth=6; eta=0.239; gamma=6.03; colsample_bytree=0.442; min_child_weight=19.5; subsample=0.522
[Tune-y] 4: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 5: nrounds=378; max_depth=8; eta=0.118; gamma=9.43; colsample_bytree=0.523; min_child_weight=14; subsample=0.969
[Tune-y] 5: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 6: nrounds=707; max_depth=5; eta=0.2; gamma=4.49; colsample_bytree=0.309; min_child_weight=12.5; subsample=0.475
[Tune-y] 6: rmse.test.rmse=   1; time: 0.5 min
[Tune-x] 7: nrounds=494; max_depth=5; eta=0.452; gamma=7.22; colsample_bytree=0.567; min_child_weight=12.8; subsample=0.784
[Tune-y] 7: rmse.test.rmse=1.01; time: 0.4 min
[Tune-x] 8: nrounds=376; max_depth=10; eta=0.533; gamma=5.06; colsample_bytree=0.316; min_child_weight=1.2; subsample=0.869
[Tune-y] 8: rmse.test.rmse=   1; time: 0.3 min
[Tune-x] 9: nrounds=1.53e+03; max_depth=6; eta=0.207; gamma=8.27; colsample_bytree=0.396; min_child_weight=0.147; subsample=0.819
[Tune-y] 9: rmse.test.rmse=   1; time: 1.0 min
[Tune-x] 10: nrounds=368; max_depth=2; eta=0.267; gamma=0.929; colsample_bytree=0.362; min_child_weight=4.46; subsample=0.35
[Tune-y] 10: rmse.test.rmse=1.02; time: 0.1 min
[Tune-x] 11: nrounds=1.69e+03; max_depth=1; eta=0.374; gamma=7.47; colsample_bytree=0.598; min_child_weight=14.5; subsample=0.556
[Tune-y] 11: rmse.test.rmse=2.38; time: 0.5 min
[Tune-x] 12: nrounds=1.03e+03; max_depth=7; eta=0.455; gamma=1.51; colsample_bytree=0.42; min_child_weight=8.4; subsample=0.552
[Tune-y] 12: rmse.test.rmse=1.05; time: 0.9 min
[Tune-x] 13: nrounds=1.21e+03; max_depth=4; eta=0.177; gamma=1.43; colsample_bytree=0.427; min_child_weight=12.5; subsample=0.823
[Tune-y] 13: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 14: nrounds=2.49e+03; max_depth=3; eta=0.355; gamma=5.52; colsample_bytree=0.635; min_child_weight=13.3; subsample=0.33
[Tune-y] 14: rmse.test.rmse=1.01; time: 1.1 min
[Tune-x] 15: nrounds=1.89e+03; max_depth=9; eta=0.00259; gamma=3.36; colsample_bytree=0.529; min_child_weight=14.1; subsample=0.323
[Tune-y] 15: rmse.test.rmse=1.29; time: 2.5 min
[Tune-x] 16: nrounds=1.28e+03; max_depth=3; eta=0.4; gamma=5.92; colsample_bytree=0.483; min_child_weight=8.04; subsample=0.587
[Tune-y] 16: rmse.test.rmse=   1; time: 0.5 min
[Tune-x] 17: nrounds=13; max_depth=7; eta=0.26; gamma=9.85; colsample_bytree=0.514; min_child_weight=6.63; subsample=0.759
[Tune-y] 17: rmse.test.rmse=2.18; time: 0.0 min
[Tune-x] 18: nrounds=1.17e+03; max_depth=3; eta=0.47; gamma=2.78; colsample_bytree=0.529; min_child_weight=10.4; subsample=0.533
[Tune-y] 18: rmse.test.rmse=1.01; time: 0.6 min
[Tune-x] 19: nrounds=1.42e+03; max_depth=4; eta=0.54; gamma=7.08; colsample_bytree=0.665; min_child_weight=18.6; subsample=0.895
[Tune-y] 19: rmse.test.rmse=   1; time: 0.8 min
[Tune-x] 20: nrounds=3.05e+03; max_depth=1; eta=0.326; gamma=4.07; colsample_bytree=0.527; min_child_weight=16.7; subsample=0.501
[Tune-y] 20: rmse.test.rmse=2.38; time: 0.9 min
[Tune] Result: nrounds=1.53e+03; max_depth=6; eta=0.207; gamma=8.27; colsample_bytree=0.396; min_child_weight=0.147; subsample=0.819 : rmse.test.rmse=   1
[1] "Fri Feb 09 13:18:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:18:08 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.487577], n=(308,444)
**GROW** @depth 1: [1,0.500388], n=(171,136)
**GROW** @depth 1: [2,0.561963], n=(148,297)
**GROW** @depth 2: [2,0.244325], n=(15,156)
**GROW** @depth 2: [1,0.724539], n=(108,12)
**GROW** @depth 2: [2,0.74954], n=(220,36)
**GROW** @depth 2: [2,0.263497], n=(15,190)
**GROW** @depth 3: [1,0.616804], n=(57,51)
**GROW** @depth 3: [2,0.410276], n=(57,133)
**GROW** @depth 3: [2,0.623773], n=(138,83)
**GROW** @depth 4: [2,0.544785], n=(114,19)
**GROW** @depth 3: [1,0.249264], n=(18,138)
**GROW** @depth 5: [1,0.447062], n=(59,24)
**GROW** @depth 4: [1,0.337002], n=(37,101)
**GROW** @depth 4: [1,0.254224], n=(21,116)
**GROW** @depth 5: [1,0.3632], n=(50,66)
**GROW** @depth 6: [2,0.372853], n=(33,69)
**GROW** @depth 5: [2,0.443252], n=(28,87)
**PRUNE** @depth 4: [2,0.444325]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,18,36,33,70,21,49,66,57,24,37,14,58,115,20,57,50,12)
**PRUNE** @depth 4: [1,0.723144]
**GROW** @depth 5: [1,0.431096], n=(30,35)
**PRUNE** @depth 5: [1,0.3632]
**GROW** @depth 4: [1,0.724539], n=(48,12)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 6: [1,0.417455], n=(25,43)
**GROW** @depth 5: [1,0.37126], n=(25,32)
**GROW** @depth 4: [2,0.545706], n=(17,20)
**GROW** @depth 5: [1,0.342273], n=(34,46)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(14,18,36,35,25,43,20,34,46,16,20,25,32,24,37,14,58,115,22,58,60)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [1,0.42319], n=(18,17)
**GROW** @depth 7: [2,0.436043], n=(22,21)
**PRUNE** @depth 6: [1,0.42319]
**GROW** @depth 6: [2,0.554448], n=(25,21)
**PRUNE** @depth 6: [2,0.554755]
**PRUNE** @depth 3: [2,0.258742]
**GROW** @depth 3: [1,0.68408], n=(96,18)
**GROW** @depth 4: [1,0.595102], n=(65,31)
**GROW** @depth 3: [1,0.57464], n=(36,36)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,17,37,34,25,22,21,19,35,45,17,20,25,32,23,38,36,36,65,31,18,23,58,60)
**GROW** @depth 6: [2,0.406902], n=(21,16)
**PRUNE** @depth 3: [1,0.57464]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,18,19,17,33,25,23,21,19,36,44,17,20,25,32,22,38,72,66,30,18,23,59,60)
**GROW** @depth 5: [2,0.551994], n=(17,19)
**GROW** @depth 3: [1,0.68935], n=(40,20)
**GROW** @depth 5: [2,0.474847], n=(32,34)
**GROW** @depth 3: [1,0.618199], n=(11,11)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,19,17,34,25,22,24,18,17,19,44,14,20,25,32,24,37,72,32,35,30,18,11,11,58,40,20)
**GROW** @depth 6: [2,0.551534], n=(24,20)
**PRUNE** @depth 6: [2,0.551534]
**GROW** @depth 5: [2,0.488037], n=(16,14)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,19,33,25,23,24,18,16,19,45,14,20,25,32,23,38,72,32,34,16,14,18,13,11,54,43,20)
**GROW** @depth 6: [2,0.31273], n=(15,19)
**PRUNE** @depth 6: [2,0.31273]
**GROW** @depth 4: [1,0.390327], n=(15,23)
**GROW** @depth 3: [2,0.356902], n=(45,27)
**GROW** @depth 7: [2,0.427454], n=(11,14)
**PRUNE** @depth 3: [2,0.356902]
**PRUNE** @depth 7: [2,0.42684]
**GROW** @depth 6: [2,0.308589], n=(14,19)
**GROW** @depth 6: [2,0.678834], n=(18,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,20,14,19,24,23,24,18,16,19,46,14,20,24,19,13,24,14,23,71,33,34,16,14,18,12,11,54,44,20)
Grow: 12.18%, Prune: 3.55%, Change: 83.52%, Swap: 34.03%

[1] "Fri Feb 09 13:19:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:19:37 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:19:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9604 	 alpha= 0.9029 	 beta= 134622.2 
[1] "Fri Feb 09 13:19:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:19:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:19:42 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:19:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:19:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:19:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:19:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:19:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:19:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:19:47 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:19:50 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.299; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.854; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.237; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.486; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.482; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.25; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.279; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.344; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.294; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.823; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.647; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.227; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.159
[1] "Fri Feb 09 13:19:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:19:57 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.038; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.126; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0147; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.162; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0882; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00191; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0507; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.414; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00355; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0203; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0145; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.185; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.00855; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00191
[1] "Fri Feb 09 13:20:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:20:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:20:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:20:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==========                                                            |  14%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:20:25 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 650 is not positive definite

[1] "Fri Feb 09 13:21:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 4.06062, its = 9
i = 2 (of 248), d = 5.59561, its = 9
i = 3 (of 248), d = 5.08335, its = 36
i = 4 (of 248), d = 4.31959, its = 10
i = 5 (of 248), d = 8.66797, its = 11
i = 6 (of 248), d = 9.22476, its = 11
i = 7 (of 248), d = 6.96056, its = 10
i = 8 (of 248), d = 5.78405, its = 10
i = 9 (of 248), d = 2.82828, its = 8
i = 10 (of 248), d = 3.58542, its = 9
i = 11 (of 248), d = 3.91119, its = 9
i = 12 (of 248), d = 6.67653, its = 11
i = 13 (of 248), d = 6.65577, its = 10
i = 14 (of 248), d = 18.0006, its = 12
i = 15 (of 248), d = 5.08072, its = 9
i = 16 (of 248), d = 5.6842, its = 9
i = 17 (of 248), d = 5.54425, its = 14
i = 18 (of 248), d = 4.97646, its = 10
i = 19 (of 248), d = 7.88293, its = 10
i = 20 (of 248), d = 9.49932, its = 11
i = 21 (of 248), d = 4.15586, its = 9
i = 22 (of 248), d = 7.22389, its = 15
i = 23 (of 248), d = 5.59233, its = 10
i = 24 (of 248), d = 4.34637, its = 9
i = 25 (of 248), d = 4.70336, its = 9
i = 26 (of 248), d = 13.2449, its = 12
i = 27 (of 248), d = 15.7467, its = 12
i = 28 (of 248), d = 3.39923, its = 8
i = 29 (of 248), d = 6.27373, its = 10
i = 30 (of 248), d = 2.78208, its = 8
i = 31 (of 248), d = 8.09619, its = 10
i = 32 (of 248), d = 14.9145, its = 10
i = 33 (of 248), d = 5.64515, its = 9
i = 34 (of 248), d = 2.86336, its = 9
i = 35 (of 248), d = 3.03631, its = 9
i = 36 (of 248), d = 17.6274, its = 12
i = 37 (of 248), d = 11.36, its = 11
i = 38 (of 248), d = 2.94845, its = 29
i = 39 (of 248), d = 10.6901, its = 10
i = 40 (of 248), d = 7.13968, its = 32
i = 41 (of 248), d = 5.95892, its = 11
i = 42 (of 248), d = 7.15049, its = 10
i = 43 (of 248), d = 3.96092, its = 8
i = 44 (of 248), d = 6.80626, its = 10
i = 45 (of 248), d = 3.61037, its = 27
i = 46 (of 248), d = 9.11059, its = 9
i = 47 (of 248), d = 7.51896, its = 11
i = 48 (of 248), d = 8.50283, its = 11
i = 49 (of 248), d = 7.93903, its = 10
i = 50 (of 248), d = 6.26179, its = 10
i = 51 (of 248), d = 5.02274, its = 9
i = 52 (of 248), d = 7.59173, its = 10
i = 53 (of 248), d = 5.87844, its = 10
i = 54 (of 248), d = 9.19949, its = 11
i = 55 (of 248), d = 8.75032, its = 11
i = 56 (of 248), d = 8.49651, its = 10
i = 57 (of 248), d = 6.74454, its = 10
i = 58 (of 248), d = 13.4282, its = 11
i = 59 (of 248), d = 3.94956, its = 9
i = 60 (of 248), d = 8.32363, its = 9
i = 61 (of 248), d = 4.15974, its = 9
i = 62 (of 248), d = 9.0354, its = 11
i = 63 (of 248), d = 5.31658, its = 9
i = 64 (of 248), d = 3.02685, its = 8
i = 65 (of 248), d = 4.50288, its = 9
i = 66 (of 248), d = 3.14815, its = 8
i = 67 (of 248), d = 2.72913, its = 7
i = 68 (of 248), d = 3.64896, its = 8
i = 69 (of 248), d = 4.86042, its = 9
i = 70 (of 248), d = 12.5643, its = 11
i = 71 (of 248), d = 3.07218, its = 8
i = 72 (of 248), d = 7.4185, its = 11
i = 73 (of 248), d = 9.14848, its = 11
i = 74 (of 248), d = 9.26015, its = 10
i = 75 (of 248), d = 8.81789, its = 10
i = 76 (of 248), d = 9.61306, its = 10
i = 77 (of 248), d = 11.4998, its = 10
i = 78 (of 248), d = 3.64832, its = 8
i = 79 (of 248), d = 6.82955, its = 10
i = 80 (of 248), d = 7.63584, its = 9
i = 81 (of 248), d = 5.06626, its = 9
i = 82 (of 248), d = 4.2032, its = 9
i = 83 (of 248), d = 7.51228, its = 10
i = 84 (of 248), d = 11.9114, its = 11
i = 85 (of 248), d = 5.99023, its = 8
i = 86 (of 248), d = 7.62554, its = 11
i = 87 (of 248), d = 5.00016, its = 10
i = 88 (of 248), d = 7.48296, its = 11
i = 89 (of 248), d = 2.98655, its = 8
i = 90 (of 248), d = 10.0606, its = 11
i = 91 (of 248), d = 5.23287, its = 9
i = 92 (of 248), d = 5.72188, its = 29
i = 93 (of 248), d = 4.14558, its = 9
i = 94 (of 248), d = 12.2238, its = 12
i = 95 (of 248), d = 8.44958, its = 11
i = 96 (of 248), d = 9.32383, its = 10
i = 97 (of 248), d = 7.21559, its = 10
i = 98 (of 248), d = 6.14066, its = 9
i = 99 (of 248), d = 7.09138, its = 9
i = 100 (of 248), d = 4.02193, its = 9
i = 101 (of 248), d = 7.68388, its = 10
i = 102 (of 248), d = 9.63203, its = 10
i = 103 (of 248), d = 3.99874, its = 9
i = 104 (of 248), d = 3.71878, its = 8
i = 105 (of 248), d = 8.75533, its = 10
i = 106 (of 248), d = 6.09809, its = 9
i = 107 (of 248), d = 5.84079, its = 28
i = 108 (of 248), d = 4.00687, its = 9
i = 109 (of 248), d = 7.88964, its = 11
i = 110 (of 248), d = 4.49193, its = 28
i = 111 (of 248), d = 8.59859, its = 10
i = 112 (of 248), d = 8.51451, its = 10
i = 113 (of 248), d = 10.6737, its = 11
i = 114 (of 248), d = 4.35651, its = 9
i = 115 (of 248), d = 9.68954, its = 10
i = 116 (of 248), d = 3.51529, its = 9
i = 117 (of 248), d = 5.28936, its = 9
i = 118 (of 248), d = 11.0726, its = 11
i = 119 (of 248), d = 4.28408, its = 9
i = 120 (of 248), d = 5.80389, its = 10
i = 121 (of 248), d = 10.0397, its = 13
i = 122 (of 248), d = 6.29393, its = 10
i = 123 (of 248), d = 4.5092, its = 9
i = 124 (of 248), d = 5.74653, its = 10
i = 125 (of 248), d = 12.5306, its = 11
i = 126 (of 248), d = 3.04591, its = 8
i = 127 (of 248), d = 3.95336, its = 9
i = 128 (of 248), d = 5.99044, its = 10
i = 129 (of 248), d = 4.3166, its = 10
i = 130 (of 248), d = 3.31603, its = 8
i = 131 (of 248), d = 3.98777, its = 7
i = 132 (of 248), d = 8.20159, its = 9
i = 133 (of 248), d = 5.7322, its = 8
i = 134 (of 248), d = 9.88552, its = 11
i = 135 (of 248), d = 5.58832, its = 10
i = 136 (of 248), d = 12.3091, its = 11
i = 137 (of 248), d = 18.0364, its = 13
i = 138 (of 248), d = 4.51819, its = 10
i = 139 (of 248), d = 5.70304, its = 10
i = 140 (of 248), d = 6.48362, its = 8
i = 141 (of 248), d = 8.39053, its = 10
i = 142 (of 248), d = 3.8459, its = 9
i = 143 (of 248), d = 3.46949, its = 7
i = 144 (of 248), d = 5.06264, its = 8
i = 145 (of 248), d = 13.4116, its = 10
i = 146 (of 248), d = 8.49637, its = 12
i = 147 (of 248), d = 5.50801, its = 9
i = 148 (of 248), d = 8.66247, its = 9
i = 149 (of 248), d = 3.75463, its = 8
i = 150 (of 248), d = 3.87732, its = 7
i = 151 (of 248), d = 2.92282, its = 8
i = 152 (of 248), d = 8.22595, its = 10
i = 153 (of 248), d = 6.86007, its = 10
i = 154 (of 248), d = 5.60159, its = 8
i = 155 (of 248), d = 2.98718, its = 8
i = 156 (of 248), d = 2.95659, its = 8
i = 157 (of 248), d = 7.92399, its = 10
i = 158 (of 248), d = 6.80291, its = 10
i = 159 (of 248), d = 5.53844, its = 10
i = 160 (of 248), d = 6.76254, its = 10
i = 161 (of 248), d = 4.6702, its = 9
i = 162 (of 248), d = 10.7582, its = 11
i = 163 (of 248), d = 4.90755, its = 10
i = 164 (of 248), d = 6.9048, its = 10
i = 165 (of 248), d = 7.21481, its = 10
i = 166 (of 248), d = 7.9281, its = 10
i = 167 (of 248), d = 5.36823, its = 10
i = 168 (of 248), d = 4.27691, its = 9
i = 169 (of 248), d = 3.12277, its = 27
i = 170 (of 248), d = 2.73406, its = 8
i = 171 (of 248), d = 6.31031, its = 9
i = 172 (of 248), d = 8.29583, its = 10
i = 173 (of 248), d = 7.43075, its = 10
i = 174 (of 248), d = 15.6516, its = 12
i = 175 (of 248), d = 5.59501, its = 10
i = 176 (of 248), d = 8.32317, its = 10
i = 177 (of 248), d = 4.46465, its = 9
i = 178 (of 248), d = 5.23207, its = 9
i = 179 (of 248), d = 4.10329, its = 9
i = 180 (of 248), d = 4.48251, its = 9
i = 181 (of 248), d = 4.45294, its = 10
i = 182 (of 248), d = 6.98059, its = 9
i = 183 (of 248), d = 9.64693, its = 10
i = 184 (of 248), d = 6.53938, its = 10
i = 185 (of 248), d = 7.88791, its = 9
i = 186 (of 248), d = 3.02055, its = 8
i = 187 (of 248), d = 6.82572, its = 10
i = 188 (of 248), d = 5.22565, its = 9
i = 189 (of 248), d = 3.8273, its = 8
i = 190 (of 248), d = 3.15354, its = 10
i = 191 (of 248), d = 4.15024, its = 9
i = 192 (of 248), d = 9.66064, its = 11
i = 193 (of 248), d = 8.78814, its = 10
i = 194 (of 248), d = 8.18343, its = 10
i = 195 (of 248), d = 5.53554, its = 9
i = 196 (of 248), d = 4.95235, its = 9
i = 197 (of 248), d = 13.3536, its = 11
i = 198 (of 248), d = 3.22114, its = 7
i = 199 (of 248), d = 13.474, its = 11
i = 200 (of 248), d = 4.05969, its = 8
i = 201 (of 248), d = 10.6388, its = 11
i = 202 (of 248), d = 3.54729, its = 8
i = 203 (of 248), d = 5.87373, its = 8
i = 204 (of 248), d = 2.94386, its = 8
i = 205 (of 248), d = 7.42242, its = 10
i = 206 (of 248), d = 9.48048, its = 9
i = 207 (of 248), d = 12.7507, its = 11
i = 208 (of 248), d = 7.77518, its = 10
i = 209 (of 248), d = 8.13536, its = 11
i = 210 (of 248), d = 6.62019, its = 11
i = 211 (of 248), d = 5.94097, its = 10
i = 212 (of 248), d = 4.16523, its = 9
i = 213 (of 248), d = 5.71183, its = 10
i = 214 (of 248), d = 6.60865, its = 11
i = 215 (of 248), d = 9.03542, its = 11
i = 216 (of 248), d = 11.0888, its = 11
i = 217 (of 248), d = 8.27823, its = 11
i = 218 (of 248), d = 3.75383, its = 8
i = 219 (of 248), d = 9.17587, its = 10
i = 220 (of 248), d = 6.51798, its = 10
i = 221 (of 248), d = 10.0255, its = 12
i = 222 (of 248), d = 7.40285, its = 10
i = 223 (of 248), d = 4.74101, its = 9
i = 224 (of 248), d = 11.69, its = 13
i = 225 (of 248), d = 11.1832, its = 9
i = 226 (of 248), d = 13.39, its = 11
i = 227 (of 248), d = 8.69434, its = 11
i = 228 (of 248), d = 2.71491, its = 8
i = 229 (of 248), d = 3.45702, its = 8
i = 230 (of 248), d = 6.16922, its = 10
i = 231 (of 248), d = 11.5007, its = 11
i = 232 (of 248), d = 8.78622, its = 11
i = 233 (of 248), d = 6.87143, its = 10
i = 234 (of 248), d = 3.19644, its = 9
i = 235 (of 248), d = 5.68409, its = 10
i = 236 (of 248), d = 8.56088, its = 10
i = 237 (of 248), d = 9.24348, its = 10
i = 238 (of 248), d = 4.10884, its = 9
i = 239 (of 248), d = 3.77698, its = 9
i = 240 (of 248), d = 5.28921, its = 9
i = 241 (of 248), d = 7.61001, its = 10
i = 242 (of 248), d = 6.18389, its = 9
i = 243 (of 248), d = 5.44042, its = 9
i = 244 (of 248), d = 8.10591, its = 10
i = 245 (of 248), d = 4.20907, its = 9
i = 246 (of 248), d = 7.1918, its = 10
i = 247 (of 248), d = 5.48545, its = 10
i = 248 (of 248), d = 5.3134, its = 8
[1] "Fri Feb 09 13:21:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:21:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:21:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:21:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:21:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:21:53 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 1059.299679 
iter  10 value 166.035282
iter  20 value 33.866874
iter  30 value 10.577263
iter  40 value 6.987356
final  value 6.942912 
converged
# weights:  5
initial  value 1049.961534 
iter  10 value 45.941276
iter  20 value 8.003829
iter  30 value 6.618324
iter  40 value 6.491324
final  value 6.490996 
converged
# weights:  5
initial  value 993.489860 
iter  10 value 47.879778
iter  20 value 10.724267
iter  30 value 7.674687
iter  40 value 7.573752
final  value 7.573732 
converged
[Tune-y] 1: rmse.test.rmse=0.0794; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 795.062946 
iter  10 value 9.369793
iter  20 value 0.790798
iter  30 value 0.170531
iter  40 value 0.120300
iter  50 value 0.107275
iter  60 value 0.103153
iter  70 value 0.099649
iter  80 value 0.097352
iter  90 value 0.095521
iter 100 value 0.094351
final  value 0.094351 
stopped after 100 iterations
# weights:  57
initial  value 1174.949250 
iter  10 value 4.071977
iter  20 value 0.410776
iter  30 value 0.226279
iter  40 value 0.189604
iter  50 value 0.170805
iter  60 value 0.164593
iter  70 value 0.157567
iter  80 value 0.152424
iter  90 value 0.147999
iter 100 value 0.144817
final  value 0.144817 
stopped after 100 iterations
# weights:  57
initial  value 3716.482392 
iter  10 value 16.152607
iter  20 value 1.931926
iter  30 value 0.386108
iter  40 value 0.287261
iter  50 value 0.263043
iter  60 value 0.251605
iter  70 value 0.243988
iter  80 value 0.236371
iter  90 value 0.230762
iter 100 value 0.224344
final  value 0.224344 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0113; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 1299.209726 
iter  10 value 22.037834
iter  20 value 2.379424
iter  30 value 1.545340
iter  40 value 1.380039
iter  50 value 1.261278
iter  60 value 1.202898
iter  70 value 1.139232
iter  80 value 1.079034
iter  90 value 1.042799
iter 100 value 0.872402
final  value 0.872402 
stopped after 100 iterations
# weights:  41
initial  value 2222.533024 
iter  10 value 2.622584
iter  20 value 0.776061
iter  30 value 0.702844
iter  40 value 0.679448
iter  50 value 0.665373
iter  60 value 0.643015
iter  70 value 0.633865
iter  80 value 0.625209
iter  90 value 0.610828
iter 100 value 0.585368
final  value 0.585368 
stopped after 100 iterations
# weights:  41
initial  value 935.920239 
iter  10 value 8.546391
iter  20 value 2.368333
iter  30 value 0.862699
iter  40 value 0.730774
iter  50 value 0.711723
iter  60 value 0.700867
iter  70 value 0.687203
iter  80 value 0.677236
iter  90 value 0.661764
iter 100 value 0.617696
final  value 0.617696 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0248; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 1316.117860 
iter  10 value 78.983110
iter  20 value 24.892326
iter  30 value 4.952670
iter  40 value 1.904938
iter  50 value 1.123698
iter  60 value 1.018164
iter  70 value 1.013433
final  value 1.013426 
converged
# weights:  5
initial  value 1110.400139 
iter  10 value 232.683563
iter  20 value 84.677444
iter  30 value 16.510366
iter  40 value 4.006123
iter  50 value 1.898151
iter  60 value 0.993999
iter  70 value 0.939028
final  value 0.938993 
converged
# weights:  5
initial  value 1385.044503 
iter  10 value 207.826676
iter  20 value 25.108306
iter  30 value 5.146213
iter  40 value 1.711663
iter  50 value 1.147555
iter  60 value 1.100007
iter  70 value 1.099203
iter  70 value 1.099203
iter  70 value 1.099203
final  value 1.099203 
converged
[Tune-y] 4: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 1020.048221 
iter  10 value 2.355711
iter  20 value 0.789603
iter  30 value 0.137414
iter  40 value 0.031280
iter  50 value 0.028385
iter  60 value 0.027569
iter  70 value 0.027108
iter  80 value 0.026518
iter  90 value 0.026042
iter 100 value 0.025710
final  value 0.025710 
stopped after 100 iterations
# weights:  61
initial  value 1052.873103 
iter  10 value 8.730371
iter  20 value 0.656174
iter  30 value 0.056394
iter  40 value 0.028387
iter  50 value 0.026259
iter  60 value 0.025479
iter  70 value 0.025074
iter  80 value 0.024565
iter  90 value 0.024408
iter 100 value 0.024179
final  value 0.024179 
stopped after 100 iterations
# weights:  61
initial  value 2587.819258 
iter  10 value 24.498339
iter  20 value 1.279699
iter  30 value 0.193446
iter  40 value 0.063728
iter  50 value 0.052295
iter  60 value 0.049081
iter  70 value 0.047382
iter  80 value 0.045355
iter  90 value 0.042982
iter 100 value 0.041332
final  value 0.041332 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00648; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 1335.730077 
iter  10 value 7.542435
iter  20 value 2.308015
iter  30 value 1.815130
iter  40 value 1.685106
iter  50 value 1.581875
iter  60 value 1.459760
iter  70 value 1.342972
iter  80 value 1.266394
iter  90 value 1.196870
iter 100 value 1.108447
final  value 1.108447 
stopped after 100 iterations
# weights:  57
initial  value 1277.081162 
iter  10 value 3.174011
iter  20 value 1.115915
iter  30 value 0.768605
iter  40 value 0.682055
iter  50 value 0.662407
iter  60 value 0.650360
iter  70 value 0.642732
iter  80 value 0.637903
iter  90 value 0.634352
iter 100 value 0.630405
final  value 0.630405 
stopped after 100 iterations
# weights:  57
initial  value 874.657135 
iter  10 value 3.649500
iter  20 value 1.522589
iter  30 value 0.953158
iter  40 value 0.834056
iter  50 value 0.775385
iter  60 value 0.750444
iter  70 value 0.739108
iter  80 value 0.728492
iter  90 value 0.715293
iter 100 value 0.707387
final  value 0.707387 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0192; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 1304.127944 
iter  10 value 238.850200
iter  20 value 18.425322
iter  30 value 8.184349
iter  40 value 6.811430
iter  50 value 6.190418
iter  60 value 6.007520
iter  70 value 6.003607
iter  80 value 5.992108
iter  90 value 5.988069
iter 100 value 5.987663
final  value 5.987663 
stopped after 100 iterations
# weights:  13
initial  value 1113.135716 
iter  10 value 53.570016
iter  20 value 14.304861
iter  30 value 10.426925
iter  40 value 8.529135
iter  50 value 7.284018
iter  60 value 7.009615
iter  70 value 5.991983
iter  80 value 5.580263
iter  90 value 5.575885
iter 100 value 5.567355
final  value 5.567355 
stopped after 100 iterations
# weights:  13
initial  value 1121.505293 
iter  10 value 63.681140
iter  20 value 18.071938
iter  30 value 12.428215
iter  40 value 10.011710
iter  50 value 8.339400
iter  60 value 7.089082
iter  70 value 6.444263
iter  80 value 6.285360
iter  90 value 6.264700
iter 100 value 6.234914
final  value 6.234914 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0586; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2300.069287 
iter  10 value 11.543952
iter  20 value 2.251229
iter  30 value 1.940041
iter  40 value 1.761021
iter  50 value 1.594632
iter  60 value 1.446408
iter  70 value 1.313268
iter  80 value 1.174100
iter  90 value 1.098835
iter 100 value 1.046376
final  value 1.046376 
stopped after 100 iterations
# weights:  73
initial  value 1322.270928 
iter  10 value 27.389055
iter  20 value 3.634736
iter  30 value 2.094054
iter  40 value 1.764892
iter  50 value 1.586455
iter  60 value 1.465668
iter  70 value 1.374564
iter  80 value 1.229616
iter  90 value 1.090356
iter 100 value 1.004134
final  value 1.004134 
stopped after 100 iterations
# weights:  73
initial  value 1143.225988 
iter  10 value 4.713373
iter  20 value 1.078122
iter  30 value 0.783790
iter  40 value 0.681878
iter  50 value 0.637268
iter  60 value 0.618878
iter  70 value 0.607357
iter  80 value 0.596624
iter  90 value 0.588236
iter 100 value 0.579940
final  value 0.579940 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0222; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 1195.438988 
iter  10 value 11.348608
iter  20 value 2.122117
iter  30 value 0.646214
iter  40 value 0.364069
iter  50 value 0.339067
iter  60 value 0.327534
iter  70 value 0.317782
iter  80 value 0.310097
iter  90 value 0.305637
iter 100 value 0.302006
final  value 0.302006 
stopped after 100 iterations
# weights:  57
initial  value 1892.055001 
iter  10 value 5.622247
iter  20 value 0.884850
iter  30 value 0.363349
iter  40 value 0.336919
iter  50 value 0.321460
iter  60 value 0.310808
iter  70 value 0.305669
iter  80 value 0.301821
iter  90 value 0.295862
iter 100 value 0.292548
final  value 0.292548 
stopped after 100 iterations
# weights:  57
initial  value 1846.816488 
iter  10 value 3.205859
iter  20 value 0.637868
iter  30 value 0.444574
iter  40 value 0.422861
iter  50 value 0.411218
iter  60 value 0.400602
iter  70 value 0.392906
iter  80 value 0.384074
iter  90 value 0.372391
iter 100 value 0.365694
final  value 0.365694 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0151; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 1312.372598 
iter  10 value 16.045385
iter  20 value 6.561075
iter  30 value 1.556571
iter  40 value 0.526128
iter  50 value 0.306483
iter  60 value 0.192842
iter  70 value 0.156905
iter  80 value 0.127967
iter  90 value 0.113457
iter 100 value 0.099616
final  value 0.099616 
stopped after 100 iterations
# weights:  5
initial  value 1345.331484 
iter  10 value 231.805090
iter  20 value 33.113431
iter  30 value 5.434750
iter  40 value 1.933399
iter  50 value 0.873705
iter  60 value 0.416175
iter  70 value 0.231380
iter  80 value 0.182473
iter  90 value 0.140714
iter 100 value 0.122295
final  value 0.122295 
stopped after 100 iterations
# weights:  5
initial  value 1110.254975 
iter  10 value 102.787496
iter  20 value 13.916057
iter  30 value 5.598325
iter  40 value 2.578710
iter  50 value 1.081945
iter  60 value 0.491927
iter  70 value 0.290481
iter  80 value 0.187913
iter  90 value 0.150540
iter 100 value 0.127075
final  value 0.127075 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0168; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 1452.011286 
iter  10 value 10.212630
iter  20 value 1.959693
iter  30 value 0.536483
iter  40 value 0.384990
iter  50 value 0.368272
iter  60 value 0.348120
iter  70 value 0.323537
iter  80 value 0.283043
iter  90 value 0.232649
iter 100 value 0.214712
final  value 0.214712 
stopped after 100 iterations
# weights:  33
initial  value 1337.934019 
iter  10 value 16.214636
iter  20 value 0.888267
iter  30 value 0.325440
iter  40 value 0.288924
iter  50 value 0.269126
iter  60 value 0.249216
iter  70 value 0.241791
iter  80 value 0.227899
iter  90 value 0.203358
iter 100 value 0.193330
final  value 0.193330 
stopped after 100 iterations
# weights:  33
initial  value 976.350753 
iter  10 value 34.641850
iter  20 value 0.982770
iter  30 value 0.317655
iter  40 value 0.283764
iter  50 value 0.258476
iter  60 value 0.247736
iter  70 value 0.243556
iter  80 value 0.231311
iter  90 value 0.210054
iter 100 value 0.200908
final  value 0.200908 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0152; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 1120.000951 
iter  10 value 25.633125
iter  20 value 14.093253
iter  30 value 12.958636
iter  40 value 12.645189
iter  50 value 11.655252
iter  60 value 10.964852
iter  70 value 9.836737
iter  80 value 8.361473
iter  90 value 7.409875
iter 100 value 7.075420
final  value 7.075420 
stopped after 100 iterations
# weights:  17
initial  value 1228.253836 
iter  10 value 41.380129
iter  20 value 19.087112
iter  30 value 11.244799
iter  40 value 9.384795
iter  50 value 6.749824
iter  60 value 6.012560
iter  70 value 5.833187
iter  80 value 5.718934
iter  90 value 5.707245
iter 100 value 5.706399
final  value 5.706399 
stopped after 100 iterations
# weights:  17
initial  value 1032.076214 
iter  10 value 45.493932
iter  20 value 15.851012
iter  30 value 8.579395
iter  40 value 7.895280
iter  50 value 6.756943
iter  60 value 6.574960
iter  70 value 6.545108
iter  80 value 6.543114
iter  90 value 6.542280
final  value 6.542280 
converged
[Tune-y] 12: rmse.test.rmse=0.0576; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 1097.845296 
iter  10 value 68.858338
iter  20 value 12.916233
iter  30 value 5.091353
iter  40 value 1.625206
iter  50 value 0.241197
iter  60 value 0.138772
iter  70 value 0.083309
iter  80 value 0.057792
iter  90 value 0.052160
iter 100 value 0.046816
final  value 0.046816 
stopped after 100 iterations
# weights:  13
initial  value 1055.069163 
iter  10 value 46.712859
iter  20 value 3.700017
iter  30 value 0.503396
iter  40 value 0.080564
iter  50 value 0.020798
iter  60 value 0.016904
iter  70 value 0.012142
iter  80 value 0.010418
iter  90 value 0.010046
iter 100 value 0.009488
final  value 0.009488 
stopped after 100 iterations
# weights:  13
initial  value 1059.318527 
iter  10 value 164.003470
iter  20 value 46.859156
iter  30 value 17.996639
iter  40 value 5.795364
iter  50 value 1.149867
iter  60 value 0.352912
iter  70 value 0.084741
iter  80 value 0.038738
iter  90 value 0.033544
iter 100 value 0.022324
final  value 0.022324 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00972; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1571.022701 
iter  10 value 19.126207
iter  20 value 3.538768
iter  30 value 1.174483
iter  40 value 0.101658
iter  50 value 0.053843
iter  60 value 0.050664
iter  70 value 0.047098
iter  80 value 0.044497
iter  90 value 0.039888
iter 100 value 0.035900
final  value 0.035900 
stopped after 100 iterations
# weights:  25
initial  value 1262.730054 
iter  10 value 29.177657
iter  20 value 2.394576
iter  30 value 0.211621
iter  40 value 0.095861
iter  50 value 0.047101
iter  60 value 0.043508
iter  70 value 0.037532
iter  80 value 0.035220
iter  90 value 0.033215
iter 100 value 0.031726
final  value 0.031726 
stopped after 100 iterations
# weights:  25
initial  value 1128.600761 
iter  10 value 75.780444
iter  20 value 7.945936
iter  30 value 1.065356
iter  40 value 0.255307
iter  50 value 0.207281
iter  60 value 0.186825
iter  70 value 0.108895
iter  80 value 0.075172
iter  90 value 0.061267
iter 100 value 0.053933
final  value 0.053933 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.011; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2219.409507 
iter  10 value 7.188421
iter  20 value 2.570428
iter  30 value 1.031284
iter  40 value 0.198387
iter  50 value 0.110280
iter  60 value 0.095951
iter  70 value 0.085718
iter  80 value 0.081564
iter  90 value 0.074303
iter 100 value 0.069798
final  value 0.069798 
stopped after 100 iterations
# weights:  25
initial  value 1605.964962 
iter  10 value 10.376920
iter  20 value 0.487462
iter  30 value 0.224382
iter  40 value 0.121274
iter  50 value 0.090012
iter  60 value 0.085943
iter  70 value 0.080913
iter  80 value 0.076899
iter  90 value 0.070542
iter 100 value 0.066590
final  value 0.066590 
stopped after 100 iterations
# weights:  25
initial  value 1340.448466 
iter  10 value 11.541293
iter  20 value 1.641391
iter  30 value 0.504012
iter  40 value 0.190931
iter  50 value 0.142952
iter  60 value 0.137991
iter  70 value 0.123218
iter  80 value 0.103750
iter  90 value 0.084363
iter 100 value 0.077020
final  value 0.077020 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 885.727697 
iter  10 value 152.959714
iter  20 value 121.360035
iter  30 value 113.367129
iter  40 value 111.587349
iter  50 value 110.976261
iter  60 value 110.748425
iter  70 value 110.556347
iter  80 value 110.515102
iter  90 value 110.512386
iter 100 value 110.512288
final  value 110.512288 
stopped after 100 iterations
# weights:  65
initial  value 1872.791730 
iter  10 value 770.746907
iter  20 value 391.448120
iter  30 value 211.244214
iter  40 value 142.623995
iter  50 value 121.395860
iter  60 value 117.687529
iter  70 value 113.964317
iter  80 value 111.958727
iter  90 value 110.507466
iter 100 value 110.004790
final  value 110.004790 
stopped after 100 iterations
# weights:  65
initial  value 3350.745474 
iter  10 value 582.008384
iter  20 value 229.387439
iter  30 value 138.034105
iter  40 value 119.632466
iter  50 value 116.067621
iter  60 value 113.411936
iter  70 value 112.742623
iter  80 value 112.043633
iter  90 value 111.696173
iter 100 value 111.551225
final  value 111.551225 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 980.186556 
iter  10 value 9.161239
iter  20 value 1.828905
iter  30 value 0.130051
iter  40 value 0.053560
iter  50 value 0.049443
iter  60 value 0.046252
iter  70 value 0.044961
iter  80 value 0.042915
iter  90 value 0.041600
iter 100 value 0.040264
final  value 0.040264 
stopped after 100 iterations
# weights:  29
initial  value 954.790840 
iter  10 value 15.166020
iter  20 value 2.348370
iter  30 value 0.132372
iter  40 value 0.066237
iter  50 value 0.054193
iter  60 value 0.052768
iter  70 value 0.051329
iter  80 value 0.048147
iter  90 value 0.044775
iter 100 value 0.041506
final  value 0.041506 
stopped after 100 iterations
# weights:  29
initial  value 1581.897680 
iter  10 value 62.526649
iter  20 value 14.657795
iter  30 value 4.566391
iter  40 value 0.930275
iter  50 value 0.226962
iter  60 value 0.166439
iter  70 value 0.153223
iter  80 value 0.134367
iter  90 value 0.093762
iter 100 value 0.067767
final  value 0.067767 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 1473.965847 
iter  10 value 9.253773
iter  20 value 4.934695
iter  30 value 3.391214
iter  40 value 3.218571
iter  50 value 3.055863
iter  60 value 2.880273
iter  70 value 2.854760
iter  80 value 2.830829
iter  90 value 2.821559
iter 100 value 2.809078
final  value 2.809078 
stopped after 100 iterations
# weights:  21
initial  value 1543.264991 
iter  10 value 66.279928
iter  20 value 5.739694
iter  30 value 2.966144
iter  40 value 2.702899
iter  50 value 2.625814
iter  60 value 2.545368
iter  70 value 2.525469
iter  80 value 2.508536
iter  90 value 2.500483
iter 100 value 2.491147
final  value 2.491147 
stopped after 100 iterations
# weights:  21
initial  value 1340.018927 
iter  10 value 77.350089
iter  20 value 10.427835
iter  30 value 4.658557
iter  40 value 3.991325
iter  50 value 3.643746
iter  60 value 3.011846
iter  70 value 2.913530
iter  80 value 2.809329
iter  90 value 2.668026
iter 100 value 2.556764
final  value 2.556764 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2226.507642 
iter  10 value 5.001791
iter  20 value 1.255271
iter  30 value 0.906367
iter  40 value 0.828434
iter  50 value 0.802606
iter  60 value 0.778837
iter  70 value 0.764471
iter  80 value 0.754568
iter  90 value 0.745368
iter 100 value 0.739529
final  value 0.739529 
stopped after 100 iterations
# weights:  65
initial  value 1007.644163 
iter  10 value 8.916714
iter  20 value 1.748869
iter  30 value 1.245679
iter  40 value 1.074772
iter  50 value 0.966563
iter  60 value 0.930666
iter  70 value 0.901788
iter  80 value 0.874673
iter  90 value 0.853258
iter 100 value 0.828639
final  value 0.828639 
stopped after 100 iterations
# weights:  65
initial  value 1490.629441 
iter  10 value 15.929648
iter  20 value 1.467284
iter  30 value 1.185676
iter  40 value 1.072035
iter  50 value 0.994227
iter  60 value 0.958870
iter  70 value 0.920931
iter  80 value 0.886604
iter  90 value 0.861335
iter 100 value 0.846263
final  value 0.846263 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0178; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 1171.248051 
iter  10 value 8.574906
iter  20 value 0.512210
iter  30 value 0.126268
iter  40 value 0.033881
iter  50 value 0.017827
iter  60 value 0.015225
iter  70 value 0.014711
iter  80 value 0.014500
iter  90 value 0.014114
iter 100 value 0.013866
final  value 0.013866 
stopped after 100 iterations
# weights:  69
initial  value 1406.448369 
iter  10 value 9.061825
iter  20 value 0.724619
iter  30 value 0.203684
iter  40 value 0.061498
iter  50 value 0.043801
iter  60 value 0.034507
iter  70 value 0.031231
iter  80 value 0.029284
iter  90 value 0.028550
iter 100 value 0.027798
final  value 0.027798 
stopped after 100 iterations
# weights:  69
initial  value 1671.142797 
iter  10 value 8.599604
iter  20 value 1.520770
iter  30 value 0.293586
iter  40 value 0.067036
iter  50 value 0.034308
iter  60 value 0.029797
iter  70 value 0.028586
iter  80 value 0.027425
iter  90 value 0.026622
iter 100 value 0.025896
final  value 0.025896 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00744; time: 0.0 min
[Tune] Result: size=15; decay=0.000446 : rmse.test.rmse=0.00648
# weights:  61
initial  value 3541.451880 
iter  10 value 92.304952
iter  20 value 4.652718
iter  30 value 0.959275
iter  40 value 0.235677
iter  50 value 0.131231
iter  60 value 0.112773
iter  70 value 0.106157
iter  80 value 0.103870
iter  90 value 0.099578
iter 100 value 0.096212
final  value 0.096212 
stopped after 100 iterations
[1] "Fri Feb 09 13:22:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 708 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 429
 number of selected nodes                               : 71 
[1] "Fri Feb 09 13:22:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:22:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:22:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:22:47 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.189; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.185; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.191; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.204; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.184; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.184
[1] "Fri Feb 09 13:23:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:23:43 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.72; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.989; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.673; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.907; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.614; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.503; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.643; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.599; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.58; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.591; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.801; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.531; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.712; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.943; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.503
[1] "Fri Feb 09 13:23:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:23:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:24:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 2 
Runtime: 0.01900101 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1           0.31 
    Y 2          0.112 
    Y 3        -0.4739 
    Y 4         -0.282 
    Y 5          1.372 
[1] "Fri Feb 09 13:24:26 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.32; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.28; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:31] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:31] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:31] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.676; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:24:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.309; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.265; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.329; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.386; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.375; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.257; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:25:40] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.257
[1] "Fri Feb 09 13:25:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:25:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.441327], n=(296,456)
**GROW** @depth 1: [2,0.488497], n=(200,256)
**GROW** @depth 1: [1,0.224306], n=(36,260)
**GROW** @depth 2: [1,0.726864], n=(231,22)
**GROW** @depth 2: [2,0.312883], n=(41,162)
**GROW** @depth 3: [1,0.673694], n=(206,25)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 4: [1,0.588281], n=(136,73)
**GROW** @depth 2: [1,0.302589], n=(76,184)
**GROW** @depth 3: [2,0.499847], n=(79,105)
**GROW** @depth 4: [2,0.75138], n=(90,15)
**GROW** @depth 3: [2,0.428834], n=(81,82)
**GROW** @depth 5: [2,0.620706], n=(72,64)
**GROW** @depth 4: [1,0.668889], n=(71,11)
**GROW** @depth 4: [1,0.554178], n=(50,21)
**GROW** @depth 3: [1,0.336692], n=(47,60)
**GROW** @depth 2: [2,0.517485], n=(18,20)
**PRUNE** @depth 3: [1,0.334832]
**GROW** @depth 5: [2,0.615491], n=(50,38)
**GROW** @depth 4: [1,0.495737], n=(25,25)
**GROW** @depth 4: [2,0.663037], n=(32,14)
**GROW** @depth 5: [1,0.615563], n=(77,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,107,32,14,51,38,15,41,81,25,25,21,12,72,33,77,27,43)
**GROW** @depth 6: [1,0.519299], n=(44,28)
**GROW** @depth 3: [2,0.257822], n=(15,92)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,15,92,33,14,51,35,15,41,80,25,24,23,12,48,25,34,79,27,41)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [1,0.541466], n=(49,30)
**PRUNE** @depth 5: [1,0.541466]
**GROW** @depth 5: [2,0.559969], n=(12,22)
**GROW** @depth 4: [1,0.329251], n=(42,50)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(18,19,15,42,50,13,22,16,51,33,13,41,80,26,25,23,13,47,24,35,79,27,40)
**GROW** @depth 5: [2,0.379294], n=(17,23)
**GROW** @depth 5: [2,0.555368], n=(25,22)
**PRUNE** @depth 5: [2,0.554908]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(16,19,16,18,17,59,13,22,16,50,33,13,41,77,27,25,24,14,47,24,34,80,29,38)
**GROW** @depth 6: [2,0.557975], n=(13,11)
**PRUNE** @depth 6: [2,0.557975]
**PRUNE** @depth 5: [1,0.324446]
**GROW** @depth 5: [1,0.329251], n=(17,16)
**GROW** @depth 5: [1,0.330491], n=(25,34)
**PRUNE** @depth 5: [1,0.330491]
**GROW** @depth 5: [2,0.809356], n=(66,13)
**GROW** @depth 6: [1,0.52736], n=(40,26)
**GROW** @depth 6: [1,0.387072], n=(33,17)
**PRUNE** @depth 6: [1,0.386142]
**GROW** @depth 5: [2,0.693558], n=(16,11)
**GROW** @depth 5: [1,0.630755], n=(13,22)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,19,18,34,25,34,13,22,16,50,33,14,41,47,57,25,24,14,47,24,13,22,40,27,14,16,11,37)
**GROW** @depth 7: [2,0.713497], n=(25,15)
**PRUNE** @depth 6: [1,0.587041]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(16,19,18,33,25,34,13,22,16,50,33,13,41,47,58,25,23,14,47,37,22,25,15,26,14,16,13,37)
**PRUNE** @depth 3: [2,0.27592]
**GROW** @depth 5: [1,0.568749], n=(28,30)
**GROW** @depth 3: [2,0.228528], n=(11,40)
**GROW** @depth 4: [1,0.324446], n=(19,21)
**PRUNE** @depth 4: [1,0.324446]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,13,39,25,34,12,23,16,50,33,13,41,47,27,30,25,24,14,47,37,22,26,22,19,13,18,13,35)
Grow: 9.949%, Prune: 3.198%, Change: 83.98%, Swap: 27.08%

[1] "Fri Feb 09 13:26:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:26:01 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:26:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 6.6899 	 alpha= 0.2787 	 beta= 43351.17 
[1] "Fri Feb 09 13:26:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:26:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:26:06 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:26:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:26:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:26:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:26:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:26:10 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:26:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:26:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:26:14 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.317; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.183; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.124; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.284; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.306; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.954; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.195; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.124
[1] "Fri Feb 09 13:26:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:26:23 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0125; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0114; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0375; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0223; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0411; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.00162; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0705; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.361; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0367; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00145; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0305; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.204; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00145
[1] "Fri Feb 09 13:26:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:26:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================                                                |  32%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:26:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:26:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:26:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 445 is not positive definite

[1] "Fri Feb 09 13:26:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 3.53171, its = 10
i = 2 (of 248), d = 3.00224, its = 31
i = 3 (of 248), d = 9.19459, its = 11
i = 4 (of 248), d = 4.37973, its = 9
i = 5 (of 248), d = 4.12246, its = 9
i = 6 (of 248), d = 3.64527, its = 9
i = 7 (of 248), d = 5.18733, its = 10
i = 8 (of 248), d = 12.6145, its = 11
i = 9 (of 248), d = 4.95697, its = 9
i = 10 (of 248), d = 4.29332, its = 9
i = 11 (of 248), d = 7.32146, its = 10
i = 12 (of 248), d = 3.51974, its = 8
i = 13 (of 248), d = 3.77599, its = 7
i = 14 (of 248), d = 3.34441, its = 8
i = 15 (of 248), d = 5.59117, its = 11
i = 16 (of 248), d = 3.99009, its = 10
i = 17 (of 248), d = 7.62671, its = 11
i = 18 (of 248), d = 8.09881, its = 10
i = 19 (of 248), d = 9.33148, its = 10
i = 20 (of 248), d = 7.97046, its = 11
i = 21 (of 248), d = 7.17332, its = 11
i = 22 (of 248), d = 6.3987, its = 10
i = 23 (of 248), d = 3.32918, its = 9
i = 24 (of 248), d = 5.76789, its = 10
i = 25 (of 248), d = 5.55196, its = 9
i = 26 (of 248), d = 6.6714, its = 10
i = 27 (of 248), d = 4.07782, its = 8
i = 28 (of 248), d = 3.36244, its = 9
i = 29 (of 248), d = 5.28849, its = 9
i = 30 (of 248), d = 9.06168, its = 10
i = 31 (of 248), d = 4.43923, its = 9
i = 32 (of 248), d = 8.01705, its = 11
i = 33 (of 248), d = 4.99985, its = 9
i = 34 (of 248), d = 2.40982, its = 7
i = 35 (of 248), d = 5.56594, its = 9
i = 36 (of 248), d = 11.1619, its = 11
i = 37 (of 248), d = 13.2571, its = 12
i = 38 (of 248), d = 6.23807, its = 9
i = 39 (of 248), d = 4.00012, its = 8
i = 40 (of 248), d = 15.4113, its = 12
i = 41 (of 248), d = 11.9892, its = 11
i = 42 (of 248), d = 9.25869, its = 11
i = 43 (of 248), d = 7.39515, its = 10
i = 44 (of 248), d = 6.71478, its = 8
i = 45 (of 248), d = 5.61733, its = 10
i = 46 (of 248), d = 4.22221, its = 9
i = 47 (of 248), d = 7.81738, its = 11
i = 48 (of 248), d = 9.43691, its = 10
i = 49 (of 248), d = 5.40437, its = 9
i = 50 (of 248), d = 10.1323, its = 11
i = 51 (of 248), d = 3.65389, its = 9
i = 52 (of 248), d = 4.08785, its = 9
i = 53 (of 248), d = 7.09235, its = 10
i = 54 (of 248), d = 8.46482, its = 10
i = 55 (of 248), d = 4.55701, its = 9
i = 56 (of 248), d = 6.02945, its = 10
i = 57 (of 248), d = 5.26643, its = 9
i = 58 (of 248), d = 10.4535, its = 11
i = 59 (of 248), d = 4.8314, its = 10
i = 60 (of 248), d = 4.60137, its = 9
i = 61 (of 248), d = 5.67158, its = 10
i = 62 (of 248), d = 9.77967, its = 10
i = 63 (of 248), d = 7.98745, its = 10
i = 64 (of 248), d = 6.68692, its = 11
i = 65 (of 248), d = 5.00534, its = 9
i = 66 (of 248), d = 10.7848, its = 11
i = 67 (of 248), d = 7.55771, its = 10
i = 68 (of 248), d = 9.32511, its = 10
i = 69 (of 248), d = 4.64813, its = 9
i = 70 (of 248), d = 8.52353, its = 11
i = 71 (of 248), d = 4.97637, its = 9
i = 72 (of 248), d = 8.66409, its = 10
i = 73 (of 248), d = 4.87215, its = 9
i = 74 (of 248), d = 4.55864, its = 9
i = 75 (of 248), d = 13.2597, its = 12
i = 76 (of 248), d = 4.08495, its = 8
i = 77 (of 248), d = 5.42816, its = 8
i = 78 (of 248), d = 7.57013, its = 10
i = 79 (of 248), d = 4.90526, its = 10
i = 80 (of 248), d = 5.21088, its = 9
i = 81 (of 248), d = 9.42853, its = 11
i = 82 (of 248), d = 3.20954, its = 8
i = 83 (of 248), d = 9.41602, its = 27
i = 84 (of 248), d = 8.874, its = 11
i = 85 (of 248), d = 4.42144, its = 9
i = 86 (of 248), d = 4.34838, its = 9
i = 87 (of 248), d = 13.3814, its = 13
i = 88 (of 248), d = 2.10156, its = 8
i = 89 (of 248), d = 2.83153, its = 8
i = 90 (of 248), d = 8.05637, its = 10
i = 91 (of 248), d = 7.05619, its = 10
i = 92 (of 248), d = 8.17023, its = 10
i = 93 (of 248), d = 9.68324, its = 11
i = 94 (of 248), d = 7.5178, its = 10
i = 95 (of 248), d = 9.02816, its = 10
i = 96 (of 248), d = 6.92981, its = 10
i = 97 (of 248), d = 9.41248, its = 11
i = 98 (of 248), d = 8.46984, its = 11
i = 99 (of 248), d = 10.8357, its = 11
i = 100 (of 248), d = 3.81652, its = 9
i = 101 (of 248), d = 6.10018, its = 8
i = 102 (of 248), d = 4.04309, its = 9
i = 103 (of 248), d = 7.30395, its = 10
i = 104 (of 248), d = 5.63689, its = 9
i = 105 (of 248), d = 2.85595, its = 8
i = 106 (of 248), d = 4.38776, its = 9
i = 107 (of 248), d = 5.32637, its = 17
i = 108 (of 248), d = 6.21347, its = 9
i = 109 (of 248), d = 4.00698, its = 8
i = 110 (of 248), d = 6.74939, its = 10
i = 111 (of 248), d = 8.83881, its = 10
i = 112 (of 248), d = 4.59965, its = 9
i = 113 (of 248), d = 4.48326, its = 9
i = 114 (of 248), d = 9.33325, its = 10
i = 115 (of 248), d = 4.39195, its = 9
i = 116 (of 248), d = 10.4096, its = 10
i = 117 (of 248), d = 11.0511, its = 11
i = 118 (of 248), d = 3.57961, its = 9
i = 119 (of 248), d = 3.32858, its = 8
i = 120 (of 248), d = 6.77988, its = 9
i = 121 (of 248), d = 12.552, its = 11
i = 122 (of 248), d = 11.3816, its = 11
i = 123 (of 248), d = 4.74713, its = 9
i = 124 (of 248), d = 4.02325, its = 9
i = 125 (of 248), d = 5.16225, its = 10
i = 126 (of 248), d = 6.62118, its = 10
i = 127 (of 248), d = 3.50425, its = 9
i = 128 (of 248), d = 4.32575, its = 25
i = 129 (of 248), d = 4.4608, its = 9
i = 130 (of 248), d = 7.53013, its = 10
i = 131 (of 248), d = 6.41184, its = 11
i = 132 (of 248), d = 9.55833, its = 11
i = 133 (of 248), d = 4.47244, its = 13
i = 134 (of 248), d = 9.09342, its = 11
i = 135 (of 248), d = 9.53129, its = 27
i = 136 (of 248), d = 8.77541, its = 11
i = 137 (of 248), d = 5.27154, its = 8
i = 138 (of 248), d = 2.49053, its = 8
i = 139 (of 248), d = 9.91723, its = 11
i = 140 (of 248), d = 3.6866, its = 8
i = 141 (of 248), d = 5.17199, its = 9
i = 142 (of 248), d = 4.196, its = 8
i = 143 (of 248), d = 5.14502, its = 9
i = 144 (of 248), d = 4.19433, its = 9
i = 145 (of 248), d = 7.50274, its = 10
i = 146 (of 248), d = 4.30814, its = 8
i = 147 (of 248), d = 3.30071, its = 8
i = 148 (of 248), d = 3.72592, its = 9
i = 149 (of 248), d = 2.65032, its = 8
i = 150 (of 248), d = 7.92885, its = 10
i = 151 (of 248), d = 4.72288, its = 8
i = 152 (of 248), d = 5.74421, its = 9
i = 153 (of 248), d = 8.23373, its = 10
i = 154 (of 248), d = 10.6527, its = 11
i = 155 (of 248), d = 3.59229, its = 9
i = 156 (of 248), d = 3.85031, its = 9
i = 157 (of 248), d = 8.07081, its = 10
i = 158 (of 248), d = 3.85214, its = 12
i = 159 (of 248), d = 5.56903, its = 10
i = 160 (of 248), d = 4.36576, its = 8
i = 161 (of 248), d = 7.2735, its = 10
i = 162 (of 248), d = 4.54022, its = 9
i = 163 (of 248), d = 8.47849, its = 11
i = 164 (of 248), d = 5.00865, its = 9
i = 165 (of 248), d = 3.32847, its = 9
i = 166 (of 248), d = 2.63387, its = 7
i = 167 (of 248), d = 4.78634, its = 10
i = 168 (of 248), d = 3.67081, its = 9
i = 169 (of 248), d = 9.71615, its = 11
i = 170 (of 248), d = 6.69332, its = 10
i = 171 (of 248), d = 6.27381, its = 10
i = 172 (of 248), d = 4.7482, its = 9
i = 173 (of 248), d = 10.2211, its = 10
i = 174 (of 248), d = 7.65818, its = 10
i = 175 (of 248), d = 8.36911, its = 10
i = 176 (of 248), d = 8.06642, its = 10
i = 177 (of 248), d = 6.34544, its = 9
i = 178 (of 248), d = 6.06995, its = 9
i = 179 (of 248), d = 4.53427, its = 9
i = 180 (of 248), d = 2.95099, its = 8
i = 181 (of 248), d = 4.69595, its = 9
i = 182 (of 248), d = 14.4244, its = 11
i = 183 (of 248), d = 5.9784, its = 9
i = 184 (of 248), d = 5.78899, its = 10
i = 185 (of 248), d = 6.46068, its = 10
i = 186 (of 248), d = 3.96113, its = 9
i = 187 (of 248), d = 5.97411, its = 9
i = 188 (of 248), d = 5.20505, its = 11
i = 189 (of 248), d = 7.05234, its = 10
i = 190 (of 248), d = 8.37715, its = 10
i = 191 (of 248), d = 5.3619, its = 9
i = 192 (of 248), d = 4.96912, its = 12
i = 193 (of 248), d = 8.55339, its = 10
i = 194 (of 248), d = 4.62176, its = 10
i = 195 (of 248), d = 4.64434, its = 8
i = 196 (of 248), d = 4.88149, its = 9
i = 197 (of 248), d = 9.87559, its = 12
i = 198 (of 248), d = 4.5368, its = 10
i = 199 (of 248), d = 6.39367, its = 10
i = 200 (of 248), d = 3.5389, its = 8
i = 201 (of 248), d = 5.83281, its = 9
i = 202 (of 248), d = 3.31709, its = 8
i = 203 (of 248), d = 6.83566, its = 10
i = 204 (of 248), d = 5.81828, its = 9
i = 205 (of 248), d = 5.57609, its = 9
i = 206 (of 248), d = 3.45165, its = 9
i = 207 (of 248), d = 10.4862, its = 11
i = 208 (of 248), d = 3.84356, its = 11
i = 209 (of 248), d = 6.64437, its = 10
i = 210 (of 248), d = 3.46185, its = 8
i = 211 (of 248), d = 5.17978, its = 9
i = 212 (of 248), d = 4.00029, its = 9
i = 213 (of 248), d = 7.14018, its = 10
i = 214 (of 248), d = 4.02335, its = 9
i = 215 (of 248), d = 5.67629, its = 9
i = 216 (of 248), d = 6.6287, its = 9
i = 217 (of 248), d = 7.47038, its = 10
i = 218 (of 248), d = 5.46661, its = 14
i = 219 (of 248), d = 4.49396, its = 9
i = 220 (of 248), d = 5.51102, its = 9
i = 221 (of 248), d = 7.61511, its = 10
i = 222 (of 248), d = 3.46118, its = 8
i = 223 (of 248), d = 12.9283, its = 12
i = 224 (of 248), d = 8.73905, its = 10
i = 225 (of 248), d = 12.0577, its = 11
i = 226 (of 248), d = 10.4114, its = 11
i = 227 (of 248), d = 22.8583, its = 12
i = 228 (of 248), d = 3.44337, its = 8
i = 229 (of 248), d = 5.41405, its = 9
i = 230 (of 248), d = 6.78153, its = 10
i = 231 (of 248), d = 6.82181, its = 10
i = 232 (of 248), d = 6.10584, its = 9
i = 233 (of 248), d = 7.41151, its = 9
i = 234 (of 248), d = 7.90548, its = 10
i = 235 (of 248), d = 4.04185, its = 9
i = 236 (of 248), d = 5.58331, its = 9
i = 237 (of 248), d = 8.68572, its = 10
i = 238 (of 248), d = 6.5993, its = 9
i = 239 (of 248), d = 5.04953, its = 10
i = 240 (of 248), d = 3.67121, its = 9
i = 241 (of 248), d = 5.70459, its = 9
i = 242 (of 248), d = 5.72122, its = 8
i = 243 (of 248), d = 8.92034, its = 10
i = 244 (of 248), d = 9.61382, its = 11
i = 245 (of 248), d = 5.41901, its = 9
i = 246 (of 248), d = 6.95801, its = 10
i = 247 (of 248), d = 5.80054, its = 9
i = 248 (of 248), d = 2.96218, its = 13
[1] "Fri Feb 09 13:27:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:27:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:27:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:27:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:27:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:27:48 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 2055.117833 
iter  10 value 45.582649
iter  20 value 7.484084
iter  30 value 2.122760
iter  40 value 1.753765
iter  50 value 1.563779
iter  60 value 1.343846
iter  70 value 1.133012
iter  80 value 1.012962
iter  90 value 0.959031
iter 100 value 0.881147
final  value 0.881147 
stopped after 100 iterations
# weights:  77
initial  value 694.867128 
iter  10 value 3.053565
iter  20 value 0.692519
iter  30 value 0.242785
iter  40 value 0.213535
iter  50 value 0.199447
iter  60 value 0.190491
iter  70 value 0.185030
iter  80 value 0.180932
iter  90 value 0.177882
iter 100 value 0.173365
final  value 0.173365 
stopped after 100 iterations
# weights:  77
initial  value 1149.135024 
iter  10 value 25.710571
iter  20 value 2.125039
iter  30 value 1.235372
iter  40 value 1.142042
iter  50 value 0.978734
iter  60 value 0.819562
iter  70 value 0.730770
iter  80 value 0.659082
iter  90 value 0.572455
iter 100 value 0.508655
final  value 0.508655 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 1087.684266 
iter  10 value 3.741417
iter  20 value 0.591168
iter  30 value 0.170674
iter  40 value 0.051542
iter  50 value 0.041478
iter  60 value 0.037920
iter  70 value 0.037028
iter  80 value 0.035639
iter  90 value 0.034314
iter 100 value 0.033572
final  value 0.033572 
stopped after 100 iterations
# weights:  53
initial  value 1745.830252 
iter  10 value 3.257662
iter  20 value 0.750997
iter  30 value 0.176659
iter  40 value 0.051887
iter  50 value 0.037085
iter  60 value 0.033374
iter  70 value 0.032450
iter  80 value 0.030645
iter  90 value 0.028947
iter 100 value 0.027753
final  value 0.027753 
stopped after 100 iterations
# weights:  53
initial  value 1064.838413 
iter  10 value 15.852578
iter  20 value 1.014224
iter  30 value 0.057078
iter  40 value 0.027380
iter  50 value 0.023017
iter  60 value 0.022091
iter  70 value 0.021366
iter  80 value 0.020741
iter  90 value 0.020223
iter 100 value 0.019755
final  value 0.019755 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00421; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 1027.417993 
iter  10 value 5.260361
iter  20 value 1.247861
iter  30 value 0.070619
iter  40 value 0.032506
iter  50 value 0.025096
iter  60 value 0.023047
iter  70 value 0.022690
iter  80 value 0.022211
iter  90 value 0.021808
iter 100 value 0.021467
final  value 0.021467 
stopped after 100 iterations
# weights:  49
initial  value 1089.691359 
iter  10 value 7.049628
iter  20 value 1.164526
iter  30 value 0.147588
iter  40 value 0.037932
iter  50 value 0.023853
iter  60 value 0.020437
iter  70 value 0.019590
iter  80 value 0.019206
iter  90 value 0.018704
iter 100 value 0.018380
final  value 0.018380 
stopped after 100 iterations
# weights:  49
initial  value 2181.524730 
iter  10 value 22.381245
iter  20 value 0.671290
iter  30 value 0.202037
iter  40 value 0.081409
iter  50 value 0.050435
iter  60 value 0.039237
iter  70 value 0.037364
iter  80 value 0.036465
iter  90 value 0.035540
iter 100 value 0.034698
final  value 0.034698 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00447; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 1108.159734 
iter  10 value 207.209103
iter  20 value 2.631132
iter  30 value 1.735204
iter  40 value 1.048318
iter  50 value 0.910326
iter  60 value 0.856599
iter  70 value 0.837449
iter  80 value 0.783607
iter  90 value 0.679772
iter 100 value 0.584993
final  value 0.584993 
stopped after 100 iterations
# weights:  9
initial  value 1007.258294 
iter  10 value 66.241235
iter  20 value 7.216869
iter  30 value 1.183378
iter  40 value 0.456165
iter  50 value 0.344727
iter  60 value 0.314897
iter  70 value 0.299062
iter  80 value 0.272306
iter  90 value 0.267614
iter 100 value 0.266806
final  value 0.266806 
stopped after 100 iterations
# weights:  9
initial  value 1157.894174 
iter  10 value 242.752962
iter  20 value 46.249926
iter  30 value 17.129975
iter  40 value 2.062842
iter  50 value 0.742082
iter  60 value 0.432655
iter  70 value 0.388645
iter  80 value 0.384483
iter  90 value 0.380312
iter 100 value 0.377676
final  value 0.377676 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0177; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 985.146109 
iter  10 value 3.063101
iter  20 value 0.858363
iter  30 value 0.606254
iter  40 value 0.544569
iter  50 value 0.516944
iter  60 value 0.508876
iter  70 value 0.504626
iter  80 value 0.502151
iter  90 value 0.499448
iter 100 value 0.497377
final  value 0.497377 
stopped after 100 iterations
# weights:  77
initial  value 800.050673 
iter  10 value 3.710801
iter  20 value 1.043901
iter  30 value 0.656029
iter  40 value 0.568553
iter  50 value 0.512543
iter  60 value 0.492483
iter  70 value 0.475765
iter  80 value 0.464203
iter  90 value 0.459831
iter 100 value 0.457518
final  value 0.457518 
stopped after 100 iterations
# weights:  77
initial  value 1251.721960 
iter  10 value 5.747690
iter  20 value 1.343555
iter  30 value 0.683029
iter  40 value 0.597212
iter  50 value 0.542268
iter  60 value 0.519709
iter  70 value 0.506747
iter  80 value 0.496567
iter  90 value 0.490531
iter 100 value 0.480590
final  value 0.480590 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.013; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 1483.486108 
iter  10 value 117.941465
iter  20 value 96.186172
iter  30 value 66.966159
iter  40 value 41.650182
iter  50 value 31.325553
iter  60 value 26.845716
iter  70 value 23.984089
iter  80 value 22.721743
iter  90 value 22.223328
iter 100 value 21.929878
final  value 21.929878 
stopped after 100 iterations
# weights:  73
initial  value 982.109057 
iter  10 value 31.712990
iter  20 value 26.644337
iter  30 value 22.469172
iter  40 value 21.295876
iter  50 value 20.674097
iter  60 value 20.555912
iter  70 value 20.502228
iter  80 value 20.449899
iter  90 value 20.425871
iter 100 value 20.417880
final  value 20.417880 
stopped after 100 iterations
# weights:  73
initial  value 1162.587316 
iter  10 value 122.151668
iter  20 value 97.829916
iter  30 value 71.322727
iter  40 value 43.878136
iter  50 value 31.048454
iter  60 value 26.770571
iter  70 value 24.102599
iter  80 value 22.966959
iter  90 value 22.414946
iter 100 value 21.933447
final  value 21.933447 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0695; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 1459.586792 
iter  10 value 3.563537
iter  20 value 0.456492
iter  30 value 0.137335
iter  40 value 0.076422
iter  50 value 0.069296
iter  60 value 0.067361
iter  70 value 0.066482
iter  80 value 0.065483
iter  90 value 0.064886
iter 100 value 0.064088
final  value 0.064088 
stopped after 100 iterations
# weights:  53
initial  value 1297.414885 
iter  10 value 7.013280
iter  20 value 1.067238
iter  30 value 0.312913
iter  40 value 0.072910
iter  50 value 0.064347
iter  60 value 0.060753
iter  70 value 0.058871
iter  80 value 0.056727
iter  90 value 0.054372
iter 100 value 0.051512
final  value 0.051512 
stopped after 100 iterations
# weights:  53
initial  value 1168.716758 
iter  10 value 28.447095
iter  20 value 5.777698
iter  30 value 1.011815
iter  40 value 0.268560
iter  50 value 0.181461
iter  60 value 0.172374
iter  70 value 0.164774
iter  80 value 0.160262
iter  90 value 0.155605
iter 100 value 0.150496
final  value 0.150496 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00504; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 1497.874987 
iter  10 value 8.136736
iter  20 value 0.825447
iter  30 value 0.272059
iter  40 value 0.154858
iter  50 value 0.135247
iter  60 value 0.132281
iter  70 value 0.131498
iter  80 value 0.125278
iter  90 value 0.119734
iter 100 value 0.115825
final  value 0.115825 
stopped after 100 iterations
# weights:  33
initial  value 1502.081017 
iter  10 value 12.384504
iter  20 value 2.756247
iter  30 value 0.763558
iter  40 value 0.311804
iter  50 value 0.235902
iter  60 value 0.212206
iter  70 value 0.200395
iter  80 value 0.178449
iter  90 value 0.139202
iter 100 value 0.111455
final  value 0.111455 
stopped after 100 iterations
# weights:  33
initial  value 1251.321756 
iter  10 value 8.733350
iter  20 value 0.871172
iter  30 value 0.233539
iter  40 value 0.144135
iter  50 value 0.134698
iter  60 value 0.129882
iter  70 value 0.126277
iter  80 value 0.116588
iter  90 value 0.107351
iter 100 value 0.099357
final  value 0.099357 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00886; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 1308.059039 
iter  10 value 2.679876
iter  20 value 0.766573
iter  30 value 0.195556
iter  40 value 0.110186
iter  50 value 0.096261
iter  60 value 0.090292
iter  70 value 0.088335
iter  80 value 0.087106
iter  90 value 0.085822
iter 100 value 0.084566
final  value 0.084566 
stopped after 100 iterations
# weights:  77
initial  value 1390.867385 
iter  10 value 11.538744
iter  20 value 0.522298
iter  30 value 0.238779
iter  40 value 0.192254
iter  50 value 0.182446
iter  60 value 0.178038
iter  70 value 0.172803
iter  80 value 0.168420
iter  90 value 0.163525
iter 100 value 0.160400
final  value 0.160400 
stopped after 100 iterations
# weights:  77
initial  value 2159.450423 
iter  10 value 6.428711
iter  20 value 0.345054
iter  30 value 0.195085
iter  40 value 0.157322
iter  50 value 0.146140
iter  60 value 0.141224
iter  70 value 0.138232
iter  80 value 0.135832
iter  90 value 0.134121
iter 100 value 0.132894
final  value 0.132894 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00577; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 1058.898822 
iter  10 value 24.487263
iter  20 value 5.694847
iter  30 value 0.692141
iter  40 value 0.128839
iter  50 value 0.087521
iter  60 value 0.032403
iter  70 value 0.016954
iter  80 value 0.010846
iter  90 value 0.009495
iter 100 value 0.006293
final  value 0.006293 
stopped after 100 iterations
# weights:  21
initial  value 1053.166648 
iter  10 value 16.446398
iter  20 value 1.649898
iter  30 value 0.167146
iter  40 value 0.040056
iter  50 value 0.023859
iter  60 value 0.005520
iter  70 value 0.003536
iter  80 value 0.003309
iter  90 value 0.003298
iter 100 value 0.003290
final  value 0.003290 
stopped after 100 iterations
# weights:  21
initial  value 863.785966 
iter  10 value 18.993060
iter  20 value 3.593740
iter  30 value 0.551804
iter  40 value 0.077649
iter  50 value 0.035833
iter  60 value 0.015735
iter  70 value 0.008145
iter  80 value 0.005011
iter  90 value 0.004569
iter 100 value 0.004148
final  value 0.004148 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0026; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 1592.224624 
iter  10 value 168.597815
iter  20 value 131.794813
iter  30 value 95.439944
iter  40 value 70.787161
iter  50 value 55.717023
iter  60 value 49.145682
iter  70 value 39.725189
iter  80 value 31.033063
iter  90 value 23.401405
iter 100 value 20.667270
final  value 20.667270 
stopped after 100 iterations
# weights:  73
initial  value 1424.855050 
iter  10 value 184.093616
iter  20 value 105.804213
iter  30 value 78.695706
iter  40 value 62.691765
iter  50 value 45.076236
iter  60 value 30.255166
iter  70 value 23.196525
iter  80 value 19.768347
iter  90 value 18.573445
iter 100 value 17.624850
final  value 17.624850 
stopped after 100 iterations
# weights:  73
initial  value 1583.313226 
iter  10 value 40.819421
iter  20 value 24.397336
iter  30 value 21.672446
iter  40 value 19.111372
iter  50 value 18.200161
iter  60 value 17.834346
iter  70 value 17.464461
iter  80 value 17.148206
iter  90 value 16.893631
iter 100 value 16.840386
final  value 16.840386 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0712; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 1103.466968 
iter  10 value 22.528096
iter  20 value 18.803347
iter  30 value 17.969268
iter  40 value 17.831834
iter  50 value 17.756932
iter  60 value 17.738295
iter  70 value 17.735817
iter  80 value 17.734048
iter  90 value 17.732789
final  value 17.732771 
converged
# weights:  45
initial  value 1335.424841 
iter  10 value 125.616883
iter  20 value 90.349265
iter  30 value 56.882050
iter  40 value 37.247318
iter  50 value 26.733574
iter  60 value 21.216301
iter  70 value 18.642717
iter  80 value 17.484806
iter  90 value 17.026743
iter 100 value 16.312544
final  value 16.312544 
stopped after 100 iterations
# weights:  45
initial  value 1999.357937 
iter  10 value 36.324072
iter  20 value 29.660105
iter  30 value 24.112144
iter  40 value 19.707785
iter  50 value 18.080992
iter  60 value 17.557404
iter  70 value 17.382438
iter  80 value 17.325098
iter  90 value 17.313161
iter 100 value 17.302749
final  value 17.302749 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 1542.182544 
iter  10 value 116.709797
iter  20 value 12.048866
iter  30 value 2.355811
iter  40 value 2.100144
iter  50 value 1.830046
iter  60 value 1.621842
iter  70 value 1.581923
iter  80 value 1.567460
iter  90 value 1.562841
iter 100 value 1.553988
final  value 1.553988 
stopped after 100 iterations
# weights:  21
initial  value 1294.608964 
iter  10 value 12.491392
iter  20 value 3.124823
iter  30 value 1.918952
iter  40 value 1.756784
iter  50 value 1.698981
iter  60 value 1.529463
iter  70 value 1.466632
iter  80 value 1.414071
iter  90 value 1.377011
iter 100 value 1.308725
final  value 1.308725 
stopped after 100 iterations
# weights:  21
initial  value 1295.552436 
iter  10 value 18.580008
iter  20 value 6.978893
iter  30 value 3.136244
iter  40 value 2.536492
iter  50 value 2.275615
iter  60 value 1.761433
iter  70 value 1.610466
iter  80 value 1.554700
iter  90 value 1.546827
iter 100 value 1.508925
final  value 1.508925 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0261; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1734.569539 
iter  10 value 8.460384
iter  20 value 1.626651
iter  30 value 0.676268
iter  40 value 0.553304
iter  50 value 0.522520
iter  60 value 0.508538
iter  70 value 0.478411
iter  80 value 0.460305
iter  90 value 0.436146
iter 100 value 0.419410
final  value 0.419410 
stopped after 100 iterations
# weights:  53
initial  value 1035.398660 
iter  10 value 2.828417
iter  20 value 0.772681
iter  30 value 0.356076
iter  40 value 0.258332
iter  50 value 0.236782
iter  60 value 0.226002
iter  70 value 0.218307
iter  80 value 0.210956
iter  90 value 0.205741
iter 100 value 0.201393
final  value 0.201393 
stopped after 100 iterations
# weights:  53
initial  value 1928.565219 
iter  10 value 13.131115
iter  20 value 1.756963
iter  30 value 0.548556
iter  40 value 0.420123
iter  50 value 0.388231
iter  60 value 0.362493
iter  70 value 0.344873
iter  80 value 0.329689
iter  90 value 0.306646
iter 100 value 0.288898
final  value 0.288898 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 2003.777874 
iter  10 value 4.220337
iter  20 value 2.977334
iter  30 value 2.823573
iter  40 value 2.731263
iter  50 value 2.654086
iter  60 value 2.593312
iter  70 value 2.529262
iter  80 value 2.489336
iter  90 value 2.473715
iter 100 value 2.417291
final  value 2.417291 
stopped after 100 iterations
# weights:  45
initial  value 1933.007083 
iter  10 value 7.153002
iter  20 value 2.998510
iter  30 value 2.678521
iter  40 value 2.526532
iter  50 value 2.429596
iter  60 value 2.350756
iter  70 value 2.314772
iter  80 value 2.291128
iter  90 value 2.246936
iter 100 value 2.156871
final  value 2.156871 
stopped after 100 iterations
# weights:  45
initial  value 1688.804070 
iter  10 value 6.604984
iter  20 value 3.616178
iter  30 value 3.022415
iter  40 value 2.754876
iter  50 value 2.598699
iter  60 value 2.465649
iter  70 value 2.358463
iter  80 value 2.293084
iter  90 value 2.261109
iter 100 value 2.193081
final  value 2.193081 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0289; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 1108.344491 
iter  10 value 4.341171
iter  20 value 1.049466
iter  30 value 0.195645
iter  40 value 0.136408
iter  50 value 0.126091
iter  60 value 0.122390
iter  70 value 0.118331
iter  80 value 0.117384
iter  90 value 0.113868
iter 100 value 0.110421
final  value 0.110421 
stopped after 100 iterations
# weights:  37
initial  value 1079.956895 
iter  10 value 10.890182
iter  20 value 1.174800
iter  30 value 0.356382
iter  40 value 0.202473
iter  50 value 0.177760
iter  60 value 0.163282
iter  70 value 0.151423
iter  80 value 0.146837
iter  90 value 0.144662
iter 100 value 0.137817
final  value 0.137817 
stopped after 100 iterations
# weights:  37
initial  value 1722.644604 
iter  10 value 8.406899
iter  20 value 1.121248
iter  30 value 0.373329
iter  40 value 0.225426
iter  50 value 0.171359
iter  60 value 0.159580
iter  70 value 0.154703
iter  80 value 0.150577
iter  90 value 0.139035
iter 100 value 0.124337
final  value 0.124337 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0094; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 1280.656198 
iter  10 value 60.537067
iter  20 value 34.084838
iter  30 value 27.996945
iter  40 value 25.315374
iter  50 value 25.159333
iter  60 value 25.126998
final  value 25.126901 
converged
# weights:  21
initial  value 1126.529876 
iter  10 value 66.779199
iter  20 value 45.101933
iter  30 value 28.500280
iter  40 value 24.602072
iter  50 value 22.698549
iter  60 value 22.272125
iter  70 value 22.259997
final  value 22.259957 
converged
# weights:  21
initial  value 1450.721515 
iter  10 value 28.313995
iter  20 value 25.960620
iter  30 value 25.080950
iter  40 value 24.445853
iter  50 value 24.332790
iter  60 value 24.326727
final  value 24.326726 
converged
[Tune-y] 17: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 1212.656975 
iter  10 value 16.301504
iter  20 value 1.080015
iter  30 value 0.180294
iter  40 value 0.025110
iter  50 value 0.009676
iter  60 value 0.003133
iter  70 value 0.002039
iter  80 value 0.001937
iter  90 value 0.001860
iter 100 value 0.001771
final  value 0.001771 
stopped after 100 iterations
# weights:  57
initial  value 1861.611180 
iter  10 value 4.857890
iter  20 value 0.368648
iter  30 value 0.097809
iter  40 value 0.029819
iter  50 value 0.011033
iter  60 value 0.004838
iter  70 value 0.003542
iter  80 value 0.002542
iter  90 value 0.002112
iter 100 value 0.001822
final  value 0.001822 
stopped after 100 iterations
# weights:  57
initial  value 2013.623634 
iter  10 value 7.588357
iter  20 value 1.139318
iter  30 value 0.184693
iter  40 value 0.046169
iter  50 value 0.012540
iter  60 value 0.004886
iter  70 value 0.002758
iter  80 value 0.002169
iter  90 value 0.001953
iter 100 value 0.001767
final  value 0.001767 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0028; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 1236.188511 
iter  10 value 16.575413
iter  20 value 2.427276
iter  30 value 1.189568
iter  40 value 0.539883
iter  50 value 0.260108
iter  60 value 0.214511
iter  70 value 0.199262
iter  80 value 0.191820
iter  90 value 0.173836
iter 100 value 0.144367
final  value 0.144367 
stopped after 100 iterations
# weights:  17
initial  value 762.704005 
iter  10 value 43.051083
iter  20 value 3.405132
iter  30 value 1.210700
iter  40 value 0.557954
iter  50 value 0.254053
iter  60 value 0.230854
iter  70 value 0.214190
iter  80 value 0.194577
iter  90 value 0.170473
iter 100 value 0.155981
final  value 0.155981 
stopped after 100 iterations
# weights:  17
initial  value 1222.961585 
iter  10 value 13.827939
iter  20 value 3.234374
iter  30 value 0.362021
iter  40 value 0.270045
iter  50 value 0.174899
iter  60 value 0.162686
iter  70 value 0.156535
iter  80 value 0.154792
iter  90 value 0.150133
iter 100 value 0.140648
final  value 0.140648 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0132; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 1126.497456 
iter  10 value 5.450586
iter  20 value 2.398704
iter  30 value 2.076104
iter  40 value 1.915709
iter  50 value 1.779192
iter  60 value 1.710542
iter  70 value 1.610359
iter  80 value 1.484707
iter  90 value 1.457499
iter 100 value 1.442048
final  value 1.442048 
stopped after 100 iterations
# weights:  29
initial  value 1380.795889 
iter  10 value 25.551873
iter  20 value 2.946474
iter  30 value 2.120734
iter  40 value 1.869483
iter  50 value 1.606711
iter  60 value 1.506208
iter  70 value 1.471098
iter  80 value 1.433971
iter  90 value 1.415659
iter 100 value 1.405517
final  value 1.405517 
stopped after 100 iterations
# weights:  29
initial  value 1010.721783 
iter  10 value 13.728250
iter  20 value 2.472497
iter  30 value 1.693907
iter  40 value 1.557610
iter  50 value 1.479626
iter  60 value 1.436626
iter  70 value 1.367942
iter  80 value 1.315472
iter  90 value 1.308494
iter 100 value 1.306465
final  value 1.306465 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0281; time: 0.0 min
[Tune] Result: size=5; decay=1.79e-05 : rmse.test.rmse=0.0026
# weights:  21
initial  value 2587.329306 
iter  10 value 32.381812
iter  20 value 8.144918
iter  30 value 2.437661
iter  40 value 0.927443
iter  50 value 0.474610
iter  60 value 0.202168
iter  70 value 0.071951
iter  80 value 0.021054
iter  90 value 0.009470
iter 100 value 0.009206
final  value 0.009206 
stopped after 100 iterations
[1] "Fri Feb 09 13:28:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 683 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 412
 number of selected nodes                               : 85 
[1] "Fri Feb 09 13:28:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:28:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:28:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:28:44 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.164; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.164
[1] "Fri Feb 09 13:29:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:29:40 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.861; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.897; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.61; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.625; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.677; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.799; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.603; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.588; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=1.05; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.511
[1] "Fri Feb 09 13:29:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:29:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:30:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7240 0.3270 0.1480 0.0671 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01800108 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1482 
    Y 1           0.31 
    Y 2       -0.04699 
    Y 3         -2.103 
    Y 4        -0.4739 
    Y 5         -0.282 
[1] "Fri Feb 09 13:30:21 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.276; time: 0.6 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.268; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:25] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:25] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:31:25] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.497; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.306; time: 0.6 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.357; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.361; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.402; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:07] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:07] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:07] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.324; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:09] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:09] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:09] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:09] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:32:09] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.268
[1] "Fri Feb 09 13:32:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:32:18 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.487577], n=(308,444)
**GROW** @depth 1: [1,0.500388], n=(171,136)
**GROW** @depth 1: [2,0.561963], n=(148,297)
**GROW** @depth 2: [2,0.244325], n=(15,156)
**GROW** @depth 2: [1,0.724539], n=(108,12)
**GROW** @depth 2: [2,0.74954], n=(220,36)
**GROW** @depth 2: [2,0.263497], n=(15,190)
**GROW** @depth 3: [1,0.616804], n=(57,51)
**GROW** @depth 3: [2,0.410276], n=(57,133)
**GROW** @depth 3: [2,0.623773], n=(138,83)
**GROW** @depth 4: [2,0.544785], n=(114,19)
**GROW** @depth 3: [1,0.249264], n=(18,138)
**GROW** @depth 5: [1,0.447062], n=(59,24)
**GROW** @depth 4: [1,0.337002], n=(37,101)
**GROW** @depth 4: [1,0.254224], n=(21,116)
**GROW** @depth 5: [1,0.3632], n=(50,66)
**GROW** @depth 6: [2,0.372853], n=(33,69)
**GROW** @depth 5: [2,0.443252], n=(28,87)
**PRUNE** @depth 4: [2,0.444325]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,18,36,33,70,21,49,66,57,24,37,14,58,115,20,57,50,12)
**PRUNE** @depth 4: [1,0.723144]
**GROW** @depth 5: [1,0.431096], n=(30,35)
**PRUNE** @depth 5: [1,0.3632]
**GROW** @depth 4: [1,0.724539], n=(48,12)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 6: [1,0.417455], n=(25,43)
**GROW** @depth 5: [1,0.37126], n=(25,32)
**GROW** @depth 4: [2,0.545706], n=(17,20)
**GROW** @depth 5: [1,0.342273], n=(34,46)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(14,18,36,35,25,43,20,34,46,16,20,25,32,24,37,14,58,115,22,58,60)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [1,0.42319], n=(18,17)
**GROW** @depth 7: [2,0.436043], n=(22,21)
**PRUNE** @depth 6: [1,0.42319]
**GROW** @depth 6: [2,0.554448], n=(25,21)
**PRUNE** @depth 6: [2,0.554755]
**PRUNE** @depth 3: [2,0.258742]
**GROW** @depth 3: [1,0.68408], n=(96,18)
**GROW** @depth 4: [1,0.595102], n=(65,31)
**GROW** @depth 3: [1,0.57464], n=(36,36)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,17,37,34,25,22,21,19,35,45,17,20,25,32,23,38,36,36,65,31,18,23,58,60)
**GROW** @depth 6: [2,0.406902], n=(21,16)
**PRUNE** @depth 3: [1,0.57464]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,18,19,17,33,25,23,21,19,36,44,17,20,25,32,22,38,72,66,30,18,23,59,60)
**GROW** @depth 5: [2,0.551994], n=(17,19)
**GROW** @depth 3: [1,0.68935], n=(40,20)
**GROW** @depth 5: [2,0.474847], n=(32,34)
**GROW** @depth 3: [1,0.618199], n=(11,11)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,19,17,34,25,22,24,18,17,19,44,14,20,25,32,24,37,72,32,35,30,18,11,11,58,40,20)
**GROW** @depth 6: [2,0.551534], n=(24,20)
**PRUNE** @depth 6: [2,0.551534]
**GROW** @depth 5: [2,0.488037], n=(16,14)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,19,33,25,23,24,18,16,19,45,14,20,25,32,23,38,72,32,34,16,14,18,13,11,54,43,20)
**GROW** @depth 6: [2,0.31273], n=(15,19)
**PRUNE** @depth 6: [2,0.31273]
**GROW** @depth 4: [1,0.390327], n=(15,23)
**GROW** @depth 3: [2,0.356902], n=(45,27)
**GROW** @depth 7: [2,0.427454], n=(11,14)
**PRUNE** @depth 3: [2,0.356902]
**PRUNE** @depth 7: [2,0.42684]
**GROW** @depth 6: [2,0.308589], n=(14,19)
**GROW** @depth 6: [2,0.678834], n=(18,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,20,14,19,24,23,24,18,16,19,46,14,20,24,19,13,24,14,23,71,33,34,16,14,18,12,11,54,44,20)
Grow: 12.18%, Prune: 3.55%, Change: 83.52%, Swap: 34.03%

[1] "Fri Feb 09 13:32:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:32:25 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:32:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9604 	 alpha= 0.9029 	 beta= 134622.2 
[1] "Fri Feb 09 13:32:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:32:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:32:30 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:32:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:32:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:32:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:32:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:32:34 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:32:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:32:36 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:32:38 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.299; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.854; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.237; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.486; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.482; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.25; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.279; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.344; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.294; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.823; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.647; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.227; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.159
[1] "Fri Feb 09 13:32:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:32:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.038; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.126; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0147; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.162; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0882; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00191; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0507; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.414; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00355; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0203; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0145; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.185; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.00855; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00191
[1] "Fri Feb 09 13:32:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:32:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:33:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:33:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:33:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 650 is not positive definite

[1] "Fri Feb 09 13:33:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 4.06062, its = 9
i = 2 (of 248), d = 5.59561, its = 9
i = 3 (of 248), d = 5.08335, its = 36
i = 4 (of 248), d = 4.31959, its = 10
i = 5 (of 248), d = 8.66797, its = 11
i = 6 (of 248), d = 9.22476, its = 11
i = 7 (of 248), d = 6.96056, its = 10
i = 8 (of 248), d = 5.78405, its = 10
i = 9 (of 248), d = 2.82828, its = 8
i = 10 (of 248), d = 3.58542, its = 9
i = 11 (of 248), d = 3.91119, its = 9
i = 12 (of 248), d = 6.67653, its = 11
i = 13 (of 248), d = 6.65577, its = 10
i = 14 (of 248), d = 18.0006, its = 12
i = 15 (of 248), d = 5.08072, its = 9
i = 16 (of 248), d = 5.6842, its = 9
i = 17 (of 248), d = 5.54425, its = 14
i = 18 (of 248), d = 4.97646, its = 10
i = 19 (of 248), d = 7.88293, its = 10
i = 20 (of 248), d = 9.49932, its = 11
i = 21 (of 248), d = 4.15586, its = 9
i = 22 (of 248), d = 7.22389, its = 15
i = 23 (of 248), d = 5.59233, its = 10
i = 24 (of 248), d = 4.34637, its = 9
i = 25 (of 248), d = 4.70336, its = 9
i = 26 (of 248), d = 13.2449, its = 12
i = 27 (of 248), d = 15.7467, its = 12
i = 28 (of 248), d = 3.39923, its = 8
i = 29 (of 248), d = 6.27373, its = 10
i = 30 (of 248), d = 2.78208, its = 8
i = 31 (of 248), d = 8.09619, its = 10
i = 32 (of 248), d = 14.9145, its = 10
i = 33 (of 248), d = 5.64515, its = 9
i = 34 (of 248), d = 2.86336, its = 9
i = 35 (of 248), d = 3.03631, its = 9
i = 36 (of 248), d = 17.6274, its = 12
i = 37 (of 248), d = 11.36, its = 11
i = 38 (of 248), d = 2.94845, its = 29
i = 39 (of 248), d = 10.6901, its = 10
i = 40 (of 248), d = 7.13968, its = 32
i = 41 (of 248), d = 5.95892, its = 11
i = 42 (of 248), d = 7.15049, its = 10
i = 43 (of 248), d = 3.96092, its = 8
i = 44 (of 248), d = 6.80626, its = 10
i = 45 (of 248), d = 3.61037, its = 27
i = 46 (of 248), d = 9.11059, its = 9
i = 47 (of 248), d = 7.51896, its = 11
i = 48 (of 248), d = 8.50283, its = 11
i = 49 (of 248), d = 7.93903, its = 10
i = 50 (of 248), d = 6.26179, its = 10
i = 51 (of 248), d = 5.02274, its = 9
i = 52 (of 248), d = 7.59173, its = 10
i = 53 (of 248), d = 5.87844, its = 10
i = 54 (of 248), d = 9.19949, its = 11
i = 55 (of 248), d = 8.75032, its = 11
i = 56 (of 248), d = 8.49651, its = 10
i = 57 (of 248), d = 6.74454, its = 10
i = 58 (of 248), d = 13.4282, its = 11
i = 59 (of 248), d = 3.94956, its = 9
i = 60 (of 248), d = 8.32363, its = 9
i = 61 (of 248), d = 4.15974, its = 9
i = 62 (of 248), d = 9.0354, its = 11
i = 63 (of 248), d = 5.31658, its = 9
i = 64 (of 248), d = 3.02685, its = 8
i = 65 (of 248), d = 4.50288, its = 9
i = 66 (of 248), d = 3.14815, its = 8
i = 67 (of 248), d = 2.72913, its = 7
i = 68 (of 248), d = 3.64896, its = 8
i = 69 (of 248), d = 4.86042, its = 9
i = 70 (of 248), d = 12.5643, its = 11
i = 71 (of 248), d = 3.07218, its = 8
i = 72 (of 248), d = 7.4185, its = 11
i = 73 (of 248), d = 9.14848, its = 11
i = 74 (of 248), d = 9.26015, its = 10
i = 75 (of 248), d = 8.81789, its = 10
i = 76 (of 248), d = 9.61306, its = 10
i = 77 (of 248), d = 11.4998, its = 10
i = 78 (of 248), d = 3.64832, its = 8
i = 79 (of 248), d = 6.82955, its = 10
i = 80 (of 248), d = 7.63584, its = 9
i = 81 (of 248), d = 5.06626, its = 9
i = 82 (of 248), d = 4.2032, its = 9
i = 83 (of 248), d = 7.51228, its = 10
i = 84 (of 248), d = 11.9114, its = 11
i = 85 (of 248), d = 5.99023, its = 8
i = 86 (of 248), d = 7.62554, its = 11
i = 87 (of 248), d = 5.00016, its = 10
i = 88 (of 248), d = 7.48296, its = 11
i = 89 (of 248), d = 2.98655, its = 8
i = 90 (of 248), d = 10.0606, its = 11
i = 91 (of 248), d = 5.23287, its = 9
i = 92 (of 248), d = 5.72188, its = 29
i = 93 (of 248), d = 4.14558, its = 9
i = 94 (of 248), d = 12.2238, its = 12
i = 95 (of 248), d = 8.44958, its = 11
i = 96 (of 248), d = 9.32383, its = 10
i = 97 (of 248), d = 7.21559, its = 10
i = 98 (of 248), d = 6.14066, its = 9
i = 99 (of 248), d = 7.09138, its = 9
i = 100 (of 248), d = 4.02193, its = 9
i = 101 (of 248), d = 7.68388, its = 10
i = 102 (of 248), d = 9.63203, its = 10
i = 103 (of 248), d = 3.99874, its = 9
i = 104 (of 248), d = 3.71878, its = 8
i = 105 (of 248), d = 8.75533, its = 10
i = 106 (of 248), d = 6.09809, its = 9
i = 107 (of 248), d = 5.84079, its = 28
i = 108 (of 248), d = 4.00687, its = 9
i = 109 (of 248), d = 7.88964, its = 11
i = 110 (of 248), d = 4.49193, its = 28
i = 111 (of 248), d = 8.59859, its = 10
i = 112 (of 248), d = 8.51451, its = 10
i = 113 (of 248), d = 10.6737, its = 11
i = 114 (of 248), d = 4.35651, its = 9
i = 115 (of 248), d = 9.68954, its = 10
i = 116 (of 248), d = 3.51529, its = 9
i = 117 (of 248), d = 5.28936, its = 9
i = 118 (of 248), d = 11.0726, its = 11
i = 119 (of 248), d = 4.28408, its = 9
i = 120 (of 248), d = 5.80389, its = 10
i = 121 (of 248), d = 10.0397, its = 13
i = 122 (of 248), d = 6.29393, its = 10
i = 123 (of 248), d = 4.5092, its = 9
i = 124 (of 248), d = 5.74653, its = 10
i = 125 (of 248), d = 12.5306, its = 11
i = 126 (of 248), d = 3.04591, its = 8
i = 127 (of 248), d = 3.95336, its = 9
i = 128 (of 248), d = 5.99044, its = 10
i = 129 (of 248), d = 4.3166, its = 10
i = 130 (of 248), d = 3.31603, its = 8
i = 131 (of 248), d = 3.98777, its = 7
i = 132 (of 248), d = 8.20159, its = 9
i = 133 (of 248), d = 5.7322, its = 8
i = 134 (of 248), d = 9.88552, its = 11
i = 135 (of 248), d = 5.58832, its = 10
i = 136 (of 248), d = 12.3091, its = 11
i = 137 (of 248), d = 18.0364, its = 13
i = 138 (of 248), d = 4.51819, its = 10
i = 139 (of 248), d = 5.70304, its = 10
i = 140 (of 248), d = 6.48362, its = 8
i = 141 (of 248), d = 8.39053, its = 10
i = 142 (of 248), d = 3.8459, its = 9
i = 143 (of 248), d = 3.46949, its = 7
i = 144 (of 248), d = 5.06264, its = 8
i = 145 (of 248), d = 13.4116, its = 10
i = 146 (of 248), d = 8.49637, its = 12
i = 147 (of 248), d = 5.50801, its = 9
i = 148 (of 248), d = 8.66247, its = 9
i = 149 (of 248), d = 3.75463, its = 8
i = 150 (of 248), d = 3.87732, its = 7
i = 151 (of 248), d = 2.92282, its = 8
i = 152 (of 248), d = 8.22595, its = 10
i = 153 (of 248), d = 6.86007, its = 10
i = 154 (of 248), d = 5.60159, its = 8
i = 155 (of 248), d = 2.98718, its = 8
i = 156 (of 248), d = 2.95659, its = 8
i = 157 (of 248), d = 7.92399, its = 10
i = 158 (of 248), d = 6.80291, its = 10
i = 159 (of 248), d = 5.53844, its = 10
i = 160 (of 248), d = 6.76254, its = 10
i = 161 (of 248), d = 4.6702, its = 9
i = 162 (of 248), d = 10.7582, its = 11
i = 163 (of 248), d = 4.90755, its = 10
i = 164 (of 248), d = 6.9048, its = 10
i = 165 (of 248), d = 7.21481, its = 10
i = 166 (of 248), d = 7.9281, its = 10
i = 167 (of 248), d = 5.36823, its = 10
i = 168 (of 248), d = 4.27691, its = 9
i = 169 (of 248), d = 3.12277, its = 27
i = 170 (of 248), d = 2.73406, its = 8
i = 171 (of 248), d = 6.31031, its = 9
i = 172 (of 248), d = 8.29583, its = 10
i = 173 (of 248), d = 7.43075, its = 10
i = 174 (of 248), d = 15.6516, its = 12
i = 175 (of 248), d = 5.59501, its = 10
i = 176 (of 248), d = 8.32317, its = 10
i = 177 (of 248), d = 4.46465, its = 9
i = 178 (of 248), d = 5.23207, its = 9
i = 179 (of 248), d = 4.10329, its = 9
i = 180 (of 248), d = 4.48251, its = 9
i = 181 (of 248), d = 4.45294, its = 10
i = 182 (of 248), d = 6.98059, its = 9
i = 183 (of 248), d = 9.64693, its = 10
i = 184 (of 248), d = 6.53938, its = 10
i = 185 (of 248), d = 7.88791, its = 9
i = 186 (of 248), d = 3.02055, its = 8
i = 187 (of 248), d = 6.82572, its = 10
i = 188 (of 248), d = 5.22565, its = 9
i = 189 (of 248), d = 3.8273, its = 8
i = 190 (of 248), d = 3.15354, its = 10
i = 191 (of 248), d = 4.15024, its = 9
i = 192 (of 248), d = 9.66064, its = 11
i = 193 (of 248), d = 8.78814, its = 10
i = 194 (of 248), d = 8.18343, its = 10
i = 195 (of 248), d = 5.53554, its = 9
i = 196 (of 248), d = 4.95235, its = 9
i = 197 (of 248), d = 13.3536, its = 11
i = 198 (of 248), d = 3.22114, its = 7
i = 199 (of 248), d = 13.474, its = 11
i = 200 (of 248), d = 4.05969, its = 8
i = 201 (of 248), d = 10.6388, its = 11
i = 202 (of 248), d = 3.54729, its = 8
i = 203 (of 248), d = 5.87373, its = 8
i = 204 (of 248), d = 2.94386, its = 8
i = 205 (of 248), d = 7.42242, its = 10
i = 206 (of 248), d = 9.48048, its = 9
i = 207 (of 248), d = 12.7507, its = 11
i = 208 (of 248), d = 7.77518, its = 10
i = 209 (of 248), d = 8.13536, its = 11
i = 210 (of 248), d = 6.62019, its = 11
i = 211 (of 248), d = 5.94097, its = 10
i = 212 (of 248), d = 4.16523, its = 9
i = 213 (of 248), d = 5.71183, its = 10
i = 214 (of 248), d = 6.60865, its = 11
i = 215 (of 248), d = 9.03542, its = 11
i = 216 (of 248), d = 11.0888, its = 11
i = 217 (of 248), d = 8.27823, its = 11
i = 218 (of 248), d = 3.75383, its = 8
i = 219 (of 248), d = 9.17587, its = 10
i = 220 (of 248), d = 6.51798, its = 10
i = 221 (of 248), d = 10.0255, its = 12
i = 222 (of 248), d = 7.40285, its = 10
i = 223 (of 248), d = 4.74101, its = 9
i = 224 (of 248), d = 11.69, its = 13
i = 225 (of 248), d = 11.1832, its = 9
i = 226 (of 248), d = 13.39, its = 11
i = 227 (of 248), d = 8.69434, its = 11
i = 228 (of 248), d = 2.71491, its = 8
i = 229 (of 248), d = 3.45702, its = 8
i = 230 (of 248), d = 6.16922, its = 10
i = 231 (of 248), d = 11.5007, its = 11
i = 232 (of 248), d = 8.78622, its = 11
i = 233 (of 248), d = 6.87143, its = 10
i = 234 (of 248), d = 3.19644, its = 9
i = 235 (of 248), d = 5.68409, its = 10
i = 236 (of 248), d = 8.56088, its = 10
i = 237 (of 248), d = 9.24348, its = 10
i = 238 (of 248), d = 4.10884, its = 9
i = 239 (of 248), d = 3.77698, its = 9
i = 240 (of 248), d = 5.28921, its = 9
i = 241 (of 248), d = 7.61001, its = 10
i = 242 (of 248), d = 6.18389, its = 9
i = 243 (of 248), d = 5.44042, its = 9
i = 244 (of 248), d = 8.10591, its = 10
i = 245 (of 248), d = 4.20907, its = 9
i = 246 (of 248), d = 7.1918, its = 10
i = 247 (of 248), d = 5.48545, its = 10
i = 248 (of 248), d = 5.3134, its = 8
[1] "Fri Feb 09 13:34:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:34:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:34:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:34:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:34:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:34:11 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 1059.299679 
iter  10 value 166.035282
iter  20 value 33.866874
iter  30 value 10.577263
iter  40 value 6.987356
final  value 6.942912 
converged
# weights:  5
initial  value 1049.961534 
iter  10 value 45.941276
iter  20 value 8.003829
iter  30 value 6.618324
iter  40 value 6.491324
final  value 6.490996 
converged
# weights:  5
initial  value 993.489860 
iter  10 value 47.879778
iter  20 value 10.724267
iter  30 value 7.674687
iter  40 value 7.573752
final  value 7.573732 
converged
[Tune-y] 1: rmse.test.rmse=0.0794; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 795.062946 
iter  10 value 9.369793
iter  20 value 0.790798
iter  30 value 0.170531
iter  40 value 0.120300
iter  50 value 0.107275
iter  60 value 0.103153
iter  70 value 0.099649
iter  80 value 0.097352
iter  90 value 0.095521
iter 100 value 0.094351
final  value 0.094351 
stopped after 100 iterations
# weights:  57
initial  value 1174.949250 
iter  10 value 4.071977
iter  20 value 0.410776
iter  30 value 0.226279
iter  40 value 0.189604
iter  50 value 0.170805
iter  60 value 0.164593
iter  70 value 0.157567
iter  80 value 0.152424
iter  90 value 0.147999
iter 100 value 0.144817
final  value 0.144817 
stopped after 100 iterations
# weights:  57
initial  value 3716.482392 
iter  10 value 16.152607
iter  20 value 1.931926
iter  30 value 0.386108
iter  40 value 0.287261
iter  50 value 0.263043
iter  60 value 0.251605
iter  70 value 0.243988
iter  80 value 0.236371
iter  90 value 0.230762
iter 100 value 0.224344
final  value 0.224344 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0113; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 1299.209726 
iter  10 value 22.037834
iter  20 value 2.379424
iter  30 value 1.545340
iter  40 value 1.380039
iter  50 value 1.261278
iter  60 value 1.202898
iter  70 value 1.139232
iter  80 value 1.079034
iter  90 value 1.042799
iter 100 value 0.872402
final  value 0.872402 
stopped after 100 iterations
# weights:  41
initial  value 2222.533024 
iter  10 value 2.622584
iter  20 value 0.776061
iter  30 value 0.702844
iter  40 value 0.679448
iter  50 value 0.665373
iter  60 value 0.643015
iter  70 value 0.633865
iter  80 value 0.625209
iter  90 value 0.610828
iter 100 value 0.585368
final  value 0.585368 
stopped after 100 iterations
# weights:  41
initial  value 935.920239 
iter  10 value 8.546391
iter  20 value 2.368333
iter  30 value 0.862699
iter  40 value 0.730774
iter  50 value 0.711723
iter  60 value 0.700867
iter  70 value 0.687203
iter  80 value 0.677236
iter  90 value 0.661764
iter 100 value 0.617696
final  value 0.617696 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0248; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 1316.117860 
iter  10 value 78.983110
iter  20 value 24.892326
iter  30 value 4.952670
iter  40 value 1.904938
iter  50 value 1.123698
iter  60 value 1.018164
iter  70 value 1.013433
final  value 1.013426 
converged
# weights:  5
initial  value 1110.400139 
iter  10 value 232.683563
iter  20 value 84.677444
iter  30 value 16.510366
iter  40 value 4.006123
iter  50 value 1.898151
iter  60 value 0.993999
iter  70 value 0.939028
final  value 0.938993 
converged
# weights:  5
initial  value 1385.044503 
iter  10 value 207.826676
iter  20 value 25.108306
iter  30 value 5.146213
iter  40 value 1.711663
iter  50 value 1.147555
iter  60 value 1.100007
iter  70 value 1.099203
iter  70 value 1.099203
iter  70 value 1.099203
final  value 1.099203 
converged
[Tune-y] 4: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 1020.048221 
iter  10 value 2.355711
iter  20 value 0.789603
iter  30 value 0.137414
iter  40 value 0.031280
iter  50 value 0.028385
iter  60 value 0.027569
iter  70 value 0.027108
iter  80 value 0.026518
iter  90 value 0.026042
iter 100 value 0.025710
final  value 0.025710 
stopped after 100 iterations
# weights:  61
initial  value 1052.873103 
iter  10 value 8.730371
iter  20 value 0.656174
iter  30 value 0.056394
iter  40 value 0.028387
iter  50 value 0.026259
iter  60 value 0.025479
iter  70 value 0.025074
iter  80 value 0.024565
iter  90 value 0.024408
iter 100 value 0.024179
final  value 0.024179 
stopped after 100 iterations
# weights:  61
initial  value 2587.819258 
iter  10 value 24.498339
iter  20 value 1.279699
iter  30 value 0.193446
iter  40 value 0.063728
iter  50 value 0.052295
iter  60 value 0.049081
iter  70 value 0.047382
iter  80 value 0.045355
iter  90 value 0.042982
iter 100 value 0.041332
final  value 0.041332 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00648; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 1335.730077 
iter  10 value 7.542435
iter  20 value 2.308015
iter  30 value 1.815130
iter  40 value 1.685106
iter  50 value 1.581875
iter  60 value 1.459760
iter  70 value 1.342972
iter  80 value 1.266394
iter  90 value 1.196870
iter 100 value 1.108447
final  value 1.108447 
stopped after 100 iterations
# weights:  57
initial  value 1277.081162 
iter  10 value 3.174011
iter  20 value 1.115915
iter  30 value 0.768605
iter  40 value 0.682055
iter  50 value 0.662407
iter  60 value 0.650360
iter  70 value 0.642732
iter  80 value 0.637903
iter  90 value 0.634352
iter 100 value 0.630405
final  value 0.630405 
stopped after 100 iterations
# weights:  57
initial  value 874.657135 
iter  10 value 3.649500
iter  20 value 1.522589
iter  30 value 0.953158
iter  40 value 0.834056
iter  50 value 0.775385
iter  60 value 0.750444
iter  70 value 0.739108
iter  80 value 0.728492
iter  90 value 0.715293
iter 100 value 0.707387
final  value 0.707387 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0192; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 1304.127944 
iter  10 value 238.850200
iter  20 value 18.425322
iter  30 value 8.184349
iter  40 value 6.811430
iter  50 value 6.190418
iter  60 value 6.007520
iter  70 value 6.003607
iter  80 value 5.992108
iter  90 value 5.988069
iter 100 value 5.987663
final  value 5.987663 
stopped after 100 iterations
# weights:  13
initial  value 1113.135716 
iter  10 value 53.570016
iter  20 value 14.304861
iter  30 value 10.426925
iter  40 value 8.529135
iter  50 value 7.284018
iter  60 value 7.009615
iter  70 value 5.991983
iter  80 value 5.580263
iter  90 value 5.575885
iter 100 value 5.567355
final  value 5.567355 
stopped after 100 iterations
# weights:  13
initial  value 1121.505293 
iter  10 value 63.681140
iter  20 value 18.071938
iter  30 value 12.428215
iter  40 value 10.011710
iter  50 value 8.339400
iter  60 value 7.089082
iter  70 value 6.444263
iter  80 value 6.285360
iter  90 value 6.264700
iter 100 value 6.234914
final  value 6.234914 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0586; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2300.069287 
iter  10 value 11.543952
iter  20 value 2.251229
iter  30 value 1.940041
iter  40 value 1.761021
iter  50 value 1.594632
iter  60 value 1.446408
iter  70 value 1.313268
iter  80 value 1.174100
iter  90 value 1.098835
iter 100 value 1.046376
final  value 1.046376 
stopped after 100 iterations
# weights:  73
initial  value 1322.270928 
iter  10 value 27.389055
iter  20 value 3.634736
iter  30 value 2.094054
iter  40 value 1.764892
iter  50 value 1.586455
iter  60 value 1.465668
iter  70 value 1.374564
iter  80 value 1.229616
iter  90 value 1.090356
iter 100 value 1.004134
final  value 1.004134 
stopped after 100 iterations
# weights:  73
initial  value 1143.225988 
iter  10 value 4.713373
iter  20 value 1.078122
iter  30 value 0.783790
iter  40 value 0.681878
iter  50 value 0.637268
iter  60 value 0.618878
iter  70 value 0.607357
iter  80 value 0.596624
iter  90 value 0.588236
iter 100 value 0.579940
final  value 0.579940 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0222; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 1195.438988 
iter  10 value 11.348608
iter  20 value 2.122117
iter  30 value 0.646214
iter  40 value 0.364069
iter  50 value 0.339067
iter  60 value 0.327534
iter  70 value 0.317782
iter  80 value 0.310097
iter  90 value 0.305637
iter 100 value 0.302006
final  value 0.302006 
stopped after 100 iterations
# weights:  57
initial  value 1892.055001 
iter  10 value 5.622247
iter  20 value 0.884850
iter  30 value 0.363349
iter  40 value 0.336919
iter  50 value 0.321460
iter  60 value 0.310808
iter  70 value 0.305669
iter  80 value 0.301821
iter  90 value 0.295862
iter 100 value 0.292548
final  value 0.292548 
stopped after 100 iterations
# weights:  57
initial  value 1846.816488 
iter  10 value 3.205859
iter  20 value 0.637868
iter  30 value 0.444574
iter  40 value 0.422861
iter  50 value 0.411218
iter  60 value 0.400602
iter  70 value 0.392906
iter  80 value 0.384074
iter  90 value 0.372391
iter 100 value 0.365694
final  value 0.365694 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0151; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 1312.372598 
iter  10 value 16.045385
iter  20 value 6.561075
iter  30 value 1.556571
iter  40 value 0.526128
iter  50 value 0.306483
iter  60 value 0.192842
iter  70 value 0.156905
iter  80 value 0.127967
iter  90 value 0.113457
iter 100 value 0.099616
final  value 0.099616 
stopped after 100 iterations
# weights:  5
initial  value 1345.331484 
iter  10 value 231.805090
iter  20 value 33.113431
iter  30 value 5.434750
iter  40 value 1.933399
iter  50 value 0.873705
iter  60 value 0.416175
iter  70 value 0.231380
iter  80 value 0.182473
iter  90 value 0.140714
iter 100 value 0.122295
final  value 0.122295 
stopped after 100 iterations
# weights:  5
initial  value 1110.254975 
iter  10 value 102.787496
iter  20 value 13.916057
iter  30 value 5.598325
iter  40 value 2.578710
iter  50 value 1.081945
iter  60 value 0.491927
iter  70 value 0.290481
iter  80 value 0.187913
iter  90 value 0.150540
iter 100 value 0.127075
final  value 0.127075 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0168; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 1452.011286 
iter  10 value 10.212630
iter  20 value 1.959693
iter  30 value 0.536483
iter  40 value 0.384990
iter  50 value 0.368272
iter  60 value 0.348120
iter  70 value 0.323537
iter  80 value 0.283043
iter  90 value 0.232649
iter 100 value 0.214712
final  value 0.214712 
stopped after 100 iterations
# weights:  33
initial  value 1337.934019 
iter  10 value 16.214636
iter  20 value 0.888267
iter  30 value 0.325440
iter  40 value 0.288924
iter  50 value 0.269126
iter  60 value 0.249216
iter  70 value 0.241791
iter  80 value 0.227899
iter  90 value 0.203358
iter 100 value 0.193330
final  value 0.193330 
stopped after 100 iterations
# weights:  33
initial  value 976.350753 
iter  10 value 34.641850
iter  20 value 0.982770
iter  30 value 0.317655
iter  40 value 0.283764
iter  50 value 0.258476
iter  60 value 0.247736
iter  70 value 0.243556
iter  80 value 0.231311
iter  90 value 0.210054
iter 100 value 0.200908
final  value 0.200908 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0152; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 1120.000951 
iter  10 value 25.633125
iter  20 value 14.093253
iter  30 value 12.958636
iter  40 value 12.645189
iter  50 value 11.655252
iter  60 value 10.964852
iter  70 value 9.836737
iter  80 value 8.361473
iter  90 value 7.409875
iter 100 value 7.075420
final  value 7.075420 
stopped after 100 iterations
# weights:  17
initial  value 1228.253836 
iter  10 value 41.380129
iter  20 value 19.087112
iter  30 value 11.244799
iter  40 value 9.384795
iter  50 value 6.749824
iter  60 value 6.012560
iter  70 value 5.833187
iter  80 value 5.718934
iter  90 value 5.707245
iter 100 value 5.706399
final  value 5.706399 
stopped after 100 iterations
# weights:  17
initial  value 1032.076214 
iter  10 value 45.493932
iter  20 value 15.851012
iter  30 value 8.579395
iter  40 value 7.895280
iter  50 value 6.756943
iter  60 value 6.574960
iter  70 value 6.545108
iter  80 value 6.543114
iter  90 value 6.542280
final  value 6.542280 
converged
[Tune-y] 12: rmse.test.rmse=0.0576; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 1097.845296 
iter  10 value 68.858338
iter  20 value 12.916233
iter  30 value 5.091353
iter  40 value 1.625206
iter  50 value 0.241197
iter  60 value 0.138772
iter  70 value 0.083309
iter  80 value 0.057792
iter  90 value 0.052160
iter 100 value 0.046816
final  value 0.046816 
stopped after 100 iterations
# weights:  13
initial  value 1055.069163 
iter  10 value 46.712859
iter  20 value 3.700017
iter  30 value 0.503396
iter  40 value 0.080564
iter  50 value 0.020798
iter  60 value 0.016904
iter  70 value 0.012142
iter  80 value 0.010418
iter  90 value 0.010046
iter 100 value 0.009488
final  value 0.009488 
stopped after 100 iterations
# weights:  13
initial  value 1059.318527 
iter  10 value 164.003470
iter  20 value 46.859156
iter  30 value 17.996639
iter  40 value 5.795364
iter  50 value 1.149867
iter  60 value 0.352912
iter  70 value 0.084741
iter  80 value 0.038738
iter  90 value 0.033544
iter 100 value 0.022324
final  value 0.022324 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00972; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1571.022701 
iter  10 value 19.126207
iter  20 value 3.538768
iter  30 value 1.174483
iter  40 value 0.101658
iter  50 value 0.053843
iter  60 value 0.050664
iter  70 value 0.047098
iter  80 value 0.044497
iter  90 value 0.039888
iter 100 value 0.035900
final  value 0.035900 
stopped after 100 iterations
# weights:  25
initial  value 1262.730054 
iter  10 value 29.177657
iter  20 value 2.394576
iter  30 value 0.211621
iter  40 value 0.095861
iter  50 value 0.047101
iter  60 value 0.043508
iter  70 value 0.037532
iter  80 value 0.035220
iter  90 value 0.033215
iter 100 value 0.031726
final  value 0.031726 
stopped after 100 iterations
# weights:  25
initial  value 1128.600761 
iter  10 value 75.780444
iter  20 value 7.945936
iter  30 value 1.065356
iter  40 value 0.255307
iter  50 value 0.207281
iter  60 value 0.186825
iter  70 value 0.108895
iter  80 value 0.075172
iter  90 value 0.061267
iter 100 value 0.053933
final  value 0.053933 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.011; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2219.409507 
iter  10 value 7.188421
iter  20 value 2.570428
iter  30 value 1.031284
iter  40 value 0.198387
iter  50 value 0.110280
iter  60 value 0.095951
iter  70 value 0.085718
iter  80 value 0.081564
iter  90 value 0.074303
iter 100 value 0.069798
final  value 0.069798 
stopped after 100 iterations
# weights:  25
initial  value 1605.964962 
iter  10 value 10.376920
iter  20 value 0.487462
iter  30 value 0.224382
iter  40 value 0.121274
iter  50 value 0.090012
iter  60 value 0.085943
iter  70 value 0.080913
iter  80 value 0.076899
iter  90 value 0.070542
iter 100 value 0.066590
final  value 0.066590 
stopped after 100 iterations
# weights:  25
initial  value 1340.448466 
iter  10 value 11.541293
iter  20 value 1.641391
iter  30 value 0.504012
iter  40 value 0.190931
iter  50 value 0.142952
iter  60 value 0.137991
iter  70 value 0.123218
iter  80 value 0.103750
iter  90 value 0.084363
iter 100 value 0.077020
final  value 0.077020 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 885.727697 
iter  10 value 152.959714
iter  20 value 121.360035
iter  30 value 113.367129
iter  40 value 111.587349
iter  50 value 110.976261
iter  60 value 110.748425
iter  70 value 110.556347
iter  80 value 110.515102
iter  90 value 110.512386
iter 100 value 110.512288
final  value 110.512288 
stopped after 100 iterations
# weights:  65
initial  value 1872.791730 
iter  10 value 770.746907
iter  20 value 391.448120
iter  30 value 211.244214
iter  40 value 142.623995
iter  50 value 121.395860
iter  60 value 117.687529
iter  70 value 113.964317
iter  80 value 111.958727
iter  90 value 110.507466
iter 100 value 110.004790
final  value 110.004790 
stopped after 100 iterations
# weights:  65
initial  value 3350.745474 
iter  10 value 582.008384
iter  20 value 229.387439
iter  30 value 138.034105
iter  40 value 119.632466
iter  50 value 116.067621
iter  60 value 113.411936
iter  70 value 112.742623
iter  80 value 112.043633
iter  90 value 111.696173
iter 100 value 111.551225
final  value 111.551225 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 980.186556 
iter  10 value 9.161239
iter  20 value 1.828905
iter  30 value 0.130051
iter  40 value 0.053560
iter  50 value 0.049443
iter  60 value 0.046252
iter  70 value 0.044961
iter  80 value 0.042915
iter  90 value 0.041600
iter 100 value 0.040264
final  value 0.040264 
stopped after 100 iterations
# weights:  29
initial  value 954.790840 
iter  10 value 15.166020
iter  20 value 2.348370
iter  30 value 0.132372
iter  40 value 0.066237
iter  50 value 0.054193
iter  60 value 0.052768
iter  70 value 0.051329
iter  80 value 0.048147
iter  90 value 0.044775
iter 100 value 0.041506
final  value 0.041506 
stopped after 100 iterations
# weights:  29
initial  value 1581.897680 
iter  10 value 62.526649
iter  20 value 14.657795
iter  30 value 4.566391
iter  40 value 0.930275
iter  50 value 0.226962
iter  60 value 0.166439
iter  70 value 0.153223
iter  80 value 0.134367
iter  90 value 0.093762
iter 100 value 0.067767
final  value 0.067767 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 1473.965847 
iter  10 value 9.253773
iter  20 value 4.934695
iter  30 value 3.391214
iter  40 value 3.218571
iter  50 value 3.055863
iter  60 value 2.880273
iter  70 value 2.854760
iter  80 value 2.830829
iter  90 value 2.821559
iter 100 value 2.809078
final  value 2.809078 
stopped after 100 iterations
# weights:  21
initial  value 1543.264991 
iter  10 value 66.279928
iter  20 value 5.739694
iter  30 value 2.966144
iter  40 value 2.702899
iter  50 value 2.625814
iter  60 value 2.545368
iter  70 value 2.525469
iter  80 value 2.508536
iter  90 value 2.500483
iter 100 value 2.491147
final  value 2.491147 
stopped after 100 iterations
# weights:  21
initial  value 1340.018927 
iter  10 value 77.350089
iter  20 value 10.427835
iter  30 value 4.658557
iter  40 value 3.991325
iter  50 value 3.643746
iter  60 value 3.011846
iter  70 value 2.913530
iter  80 value 2.809329
iter  90 value 2.668026
iter 100 value 2.556764
final  value 2.556764 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2226.507642 
iter  10 value 5.001791
iter  20 value 1.255271
iter  30 value 0.906367
iter  40 value 0.828434
iter  50 value 0.802606
iter  60 value 0.778837
iter  70 value 0.764471
iter  80 value 0.754568
iter  90 value 0.745368
iter 100 value 0.739529
final  value 0.739529 
stopped after 100 iterations
# weights:  65
initial  value 1007.644163 
iter  10 value 8.916714
iter  20 value 1.748869
iter  30 value 1.245679
iter  40 value 1.074772
iter  50 value 0.966563
iter  60 value 0.930666
iter  70 value 0.901788
iter  80 value 0.874673
iter  90 value 0.853258
iter 100 value 0.828639
final  value 0.828639 
stopped after 100 iterations
# weights:  65
initial  value 1490.629441 
iter  10 value 15.929648
iter  20 value 1.467284
iter  30 value 1.185676
iter  40 value 1.072035
iter  50 value 0.994227
iter  60 value 0.958870
iter  70 value 0.920931
iter  80 value 0.886604
iter  90 value 0.861335
iter 100 value 0.846263
final  value 0.846263 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0178; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 1171.248051 
iter  10 value 8.574906
iter  20 value 0.512210
iter  30 value 0.126268
iter  40 value 0.033881
iter  50 value 0.017827
iter  60 value 0.015225
iter  70 value 0.014711
iter  80 value 0.014500
iter  90 value 0.014114
iter 100 value 0.013866
final  value 0.013866 
stopped after 100 iterations
# weights:  69
initial  value 1406.448369 
iter  10 value 9.061825
iter  20 value 0.724619
iter  30 value 0.203684
iter  40 value 0.061498
iter  50 value 0.043801
iter  60 value 0.034507
iter  70 value 0.031231
iter  80 value 0.029284
iter  90 value 0.028550
iter 100 value 0.027798
final  value 0.027798 
stopped after 100 iterations
# weights:  69
initial  value 1671.142797 
iter  10 value 8.599604
iter  20 value 1.520770
iter  30 value 0.293586
iter  40 value 0.067036
iter  50 value 0.034308
iter  60 value 0.029797
iter  70 value 0.028586
iter  80 value 0.027425
iter  90 value 0.026622
iter 100 value 0.025896
final  value 0.025896 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00744; time: 0.0 min
[Tune] Result: size=15; decay=0.000446 : rmse.test.rmse=0.00648
# weights:  61
initial  value 3541.451880 
iter  10 value 92.304952
iter  20 value 4.652718
iter  30 value 0.959275
iter  40 value 0.235677
iter  50 value 0.131231
iter  60 value 0.112773
iter  70 value 0.106157
iter  80 value 0.103870
iter  90 value 0.099578
iter 100 value 0.096212
final  value 0.096212 
stopped after 100 iterations
[1] "Fri Feb 09 13:34:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 708 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 429
 number of selected nodes                               : 71 
[1] "Fri Feb 09 13:34:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:34:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:34:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:35:02 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.189; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.185; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.191; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.204; time: 0.1 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.184; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.184
[1] "Fri Feb 09 13:35:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:35:57 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.72; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.989; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.673; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.907; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.614; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.503; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.643; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.599; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.58; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.591; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.801; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.531; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.712; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.943; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.503
[1] "Fri Feb 09 13:35:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:36:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:36:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 2 
Runtime: 0.01800108 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1           0.31 
    Y 2          0.112 
    Y 3        -0.4739 
    Y 4         -0.282 
    Y 5          1.372 
[1] "Fri Feb 09 13:36:39 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:39] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.32; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.28; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:43] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:43] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:43] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.676; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:48] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:48] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:36:48] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.309; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.265; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.329; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.386; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.375; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.257; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:37:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.257
[1] "Fri Feb 09 13:38:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:38:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.441327], n=(296,456)
**GROW** @depth 1: [2,0.488497], n=(200,256)
**GROW** @depth 1: [1,0.224306], n=(36,260)
**GROW** @depth 2: [1,0.726864], n=(231,22)
**GROW** @depth 2: [2,0.312883], n=(41,162)
**GROW** @depth 3: [1,0.673694], n=(206,25)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 4: [1,0.588281], n=(136,73)
**GROW** @depth 2: [1,0.302589], n=(76,184)
**GROW** @depth 3: [2,0.499847], n=(79,105)
**GROW** @depth 4: [2,0.75138], n=(90,15)
**GROW** @depth 3: [2,0.428834], n=(81,82)
**GROW** @depth 5: [2,0.620706], n=(72,64)
**GROW** @depth 4: [1,0.668889], n=(71,11)
**GROW** @depth 4: [1,0.554178], n=(50,21)
**GROW** @depth 3: [1,0.336692], n=(47,60)
**GROW** @depth 2: [2,0.517485], n=(18,20)
**PRUNE** @depth 3: [1,0.334832]
**GROW** @depth 5: [2,0.615491], n=(50,38)
**GROW** @depth 4: [1,0.495737], n=(25,25)
**GROW** @depth 4: [2,0.663037], n=(32,14)
**GROW** @depth 5: [1,0.615563], n=(77,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,107,32,14,51,38,15,41,81,25,25,21,12,72,33,77,27,43)
**GROW** @depth 6: [1,0.519299], n=(44,28)
**GROW** @depth 3: [2,0.257822], n=(15,92)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,15,92,33,14,51,35,15,41,80,25,24,23,12,48,25,34,79,27,41)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [1,0.541466], n=(49,30)
**PRUNE** @depth 5: [1,0.541466]
**GROW** @depth 5: [2,0.559969], n=(12,22)
**GROW** @depth 4: [1,0.329251], n=(42,50)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(18,19,15,42,50,13,22,16,51,33,13,41,80,26,25,23,13,47,24,35,79,27,40)
**GROW** @depth 5: [2,0.379294], n=(17,23)
**GROW** @depth 5: [2,0.555368], n=(25,22)
**PRUNE** @depth 5: [2,0.554908]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(16,19,16,18,17,59,13,22,16,50,33,13,41,77,27,25,24,14,47,24,34,80,29,38)
**GROW** @depth 6: [2,0.557975], n=(13,11)
**PRUNE** @depth 6: [2,0.557975]
**PRUNE** @depth 5: [1,0.324446]
**GROW** @depth 5: [1,0.329251], n=(17,16)
**GROW** @depth 5: [1,0.330491], n=(25,34)
**PRUNE** @depth 5: [1,0.330491]
**GROW** @depth 5: [2,0.809356], n=(66,13)
**GROW** @depth 6: [1,0.52736], n=(40,26)
**GROW** @depth 6: [1,0.387072], n=(33,17)
**PRUNE** @depth 6: [1,0.386142]
**GROW** @depth 5: [2,0.693558], n=(16,11)
**GROW** @depth 5: [1,0.630755], n=(13,22)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,19,18,34,25,34,13,22,16,50,33,14,41,47,57,25,24,14,47,24,13,22,40,27,14,16,11,37)
**GROW** @depth 7: [2,0.713497], n=(25,15)
**PRUNE** @depth 6: [1,0.587041]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(16,19,18,33,25,34,13,22,16,50,33,13,41,47,58,25,23,14,47,37,22,25,15,26,14,16,13,37)
**PRUNE** @depth 3: [2,0.27592]
**GROW** @depth 5: [1,0.568749], n=(28,30)
**GROW** @depth 3: [2,0.228528], n=(11,40)
**GROW** @depth 4: [1,0.324446], n=(19,21)
**PRUNE** @depth 4: [1,0.324446]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,13,39,25,34,12,23,16,50,33,13,41,47,27,30,25,24,14,47,37,22,26,22,19,13,18,13,35)
Grow: 9.949%, Prune: 3.198%, Change: 83.98%, Swap: 27.08%

[1] "Fri Feb 09 13:38:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:38:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:38:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 6.6899 	 alpha= 0.2787 	 beta= 43351.17 
[1] "Fri Feb 09 13:38:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:38:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:38:21 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:38:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:38:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:38:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:38:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:38:26 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:38:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:38:27 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:38:29 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.317; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.183; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.124; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.284; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.306; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.954; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.195; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.124
[1] "Fri Feb 09 13:38:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:38:39 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0125; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0114; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0375; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0223; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0411; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.00162; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0705; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.361; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0367; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00145; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0305; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.204; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00145
[1] "Fri Feb 09 13:38:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |==========================================                            |  60%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:38:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:38:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:38:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:39:02 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 445 is not positive definite

[1] "Fri Feb 09 13:39:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 3.53171, its = 10
i = 2 (of 248), d = 3.00224, its = 31
i = 3 (of 248), d = 9.19459, its = 11
i = 4 (of 248), d = 4.37973, its = 9
i = 5 (of 248), d = 4.12246, its = 9
i = 6 (of 248), d = 3.64527, its = 9
i = 7 (of 248), d = 5.18733, its = 10
i = 8 (of 248), d = 12.6145, its = 11
i = 9 (of 248), d = 4.95697, its = 9
i = 10 (of 248), d = 4.29332, its = 9
i = 11 (of 248), d = 7.32146, its = 10
i = 12 (of 248), d = 3.51974, its = 8
i = 13 (of 248), d = 3.77599, its = 7
i = 14 (of 248), d = 3.34441, its = 8
i = 15 (of 248), d = 5.59117, its = 11
i = 16 (of 248), d = 3.99009, its = 10
i = 17 (of 248), d = 7.62671, its = 11
i = 18 (of 248), d = 8.09881, its = 10
i = 19 (of 248), d = 9.33148, its = 10
i = 20 (of 248), d = 7.97046, its = 11
i = 21 (of 248), d = 7.17332, its = 11
i = 22 (of 248), d = 6.3987, its = 10
i = 23 (of 248), d = 3.32918, its = 9
i = 24 (of 248), d = 5.76789, its = 10
i = 25 (of 248), d = 5.55196, its = 9
i = 26 (of 248), d = 6.6714, its = 10
i = 27 (of 248), d = 4.07782, its = 8
i = 28 (of 248), d = 3.36244, its = 9
i = 29 (of 248), d = 5.28849, its = 9
i = 30 (of 248), d = 9.06168, its = 10
i = 31 (of 248), d = 4.43923, its = 9
i = 32 (of 248), d = 8.01705, its = 11
i = 33 (of 248), d = 4.99985, its = 9
i = 34 (of 248), d = 2.40982, its = 7
i = 35 (of 248), d = 5.56594, its = 9
i = 36 (of 248), d = 11.1619, its = 11
i = 37 (of 248), d = 13.2571, its = 12
i = 38 (of 248), d = 6.23807, its = 9
i = 39 (of 248), d = 4.00012, its = 8
i = 40 (of 248), d = 15.4113, its = 12
i = 41 (of 248), d = 11.9892, its = 11
i = 42 (of 248), d = 9.25869, its = 11
i = 43 (of 248), d = 7.39515, its = 10
i = 44 (of 248), d = 6.71478, its = 8
i = 45 (of 248), d = 5.61733, its = 10
i = 46 (of 248), d = 4.22221, its = 9
i = 47 (of 248), d = 7.81738, its = 11
i = 48 (of 248), d = 9.43691, its = 10
i = 49 (of 248), d = 5.40437, its = 9
i = 50 (of 248), d = 10.1323, its = 11
i = 51 (of 248), d = 3.65389, its = 9
i = 52 (of 248), d = 4.08785, its = 9
i = 53 (of 248), d = 7.09235, its = 10
i = 54 (of 248), d = 8.46482, its = 10
i = 55 (of 248), d = 4.55701, its = 9
i = 56 (of 248), d = 6.02945, its = 10
i = 57 (of 248), d = 5.26643, its = 9
i = 58 (of 248), d = 10.4535, its = 11
i = 59 (of 248), d = 4.8314, its = 10
i = 60 (of 248), d = 4.60137, its = 9
i = 61 (of 248), d = 5.67158, its = 10
i = 62 (of 248), d = 9.77967, its = 10
i = 63 (of 248), d = 7.98745, its = 10
i = 64 (of 248), d = 6.68692, its = 11
i = 65 (of 248), d = 5.00534, its = 9
i = 66 (of 248), d = 10.7848, its = 11
i = 67 (of 248), d = 7.55771, its = 10
i = 68 (of 248), d = 9.32511, its = 10
i = 69 (of 248), d = 4.64813, its = 9
i = 70 (of 248), d = 8.52353, its = 11
i = 71 (of 248), d = 4.97637, its = 9
i = 72 (of 248), d = 8.66409, its = 10
i = 73 (of 248), d = 4.87215, its = 9
i = 74 (of 248), d = 4.55864, its = 9
i = 75 (of 248), d = 13.2597, its = 12
i = 76 (of 248), d = 4.08495, its = 8
i = 77 (of 248), d = 5.42816, its = 8
i = 78 (of 248), d = 7.57013, its = 10
i = 79 (of 248), d = 4.90526, its = 10
i = 80 (of 248), d = 5.21088, its = 9
i = 81 (of 248), d = 9.42853, its = 11
i = 82 (of 248), d = 3.20954, its = 8
i = 83 (of 248), d = 9.41602, its = 27
i = 84 (of 248), d = 8.874, its = 11
i = 85 (of 248), d = 4.42144, its = 9
i = 86 (of 248), d = 4.34838, its = 9
i = 87 (of 248), d = 13.3814, its = 13
i = 88 (of 248), d = 2.10156, its = 8
i = 89 (of 248), d = 2.83153, its = 8
i = 90 (of 248), d = 8.05637, its = 10
i = 91 (of 248), d = 7.05619, its = 10
i = 92 (of 248), d = 8.17023, its = 10
i = 93 (of 248), d = 9.68324, its = 11
i = 94 (of 248), d = 7.5178, its = 10
i = 95 (of 248), d = 9.02816, its = 10
i = 96 (of 248), d = 6.92981, its = 10
i = 97 (of 248), d = 9.41248, its = 11
i = 98 (of 248), d = 8.46984, its = 11
i = 99 (of 248), d = 10.8357, its = 11
i = 100 (of 248), d = 3.81652, its = 9
i = 101 (of 248), d = 6.10018, its = 8
i = 102 (of 248), d = 4.04309, its = 9
i = 103 (of 248), d = 7.30395, its = 10
i = 104 (of 248), d = 5.63689, its = 9
i = 105 (of 248), d = 2.85595, its = 8
i = 106 (of 248), d = 4.38776, its = 9
i = 107 (of 248), d = 5.32637, its = 17
i = 108 (of 248), d = 6.21347, its = 9
i = 109 (of 248), d = 4.00698, its = 8
i = 110 (of 248), d = 6.74939, its = 10
i = 111 (of 248), d = 8.83881, its = 10
i = 112 (of 248), d = 4.59965, its = 9
i = 113 (of 248), d = 4.48326, its = 9
i = 114 (of 248), d = 9.33325, its = 10
i = 115 (of 248), d = 4.39195, its = 9
i = 116 (of 248), d = 10.4096, its = 10
i = 117 (of 248), d = 11.0511, its = 11
i = 118 (of 248), d = 3.57961, its = 9
i = 119 (of 248), d = 3.32858, its = 8
i = 120 (of 248), d = 6.77988, its = 9
i = 121 (of 248), d = 12.552, its = 11
i = 122 (of 248), d = 11.3816, its = 11
i = 123 (of 248), d = 4.74713, its = 9
i = 124 (of 248), d = 4.02325, its = 9
i = 125 (of 248), d = 5.16225, its = 10
i = 126 (of 248), d = 6.62118, its = 10
i = 127 (of 248), d = 3.50425, its = 9
i = 128 (of 248), d = 4.32575, its = 25
i = 129 (of 248), d = 4.4608, its = 9
i = 130 (of 248), d = 7.53013, its = 10
i = 131 (of 248), d = 6.41184, its = 11
i = 132 (of 248), d = 9.55833, its = 11
i = 133 (of 248), d = 4.47244, its = 13
i = 134 (of 248), d = 9.09342, its = 11
i = 135 (of 248), d = 9.53129, its = 27
i = 136 (of 248), d = 8.77541, its = 11
i = 137 (of 248), d = 5.27154, its = 8
i = 138 (of 248), d = 2.49053, its = 8
i = 139 (of 248), d = 9.91723, its = 11
i = 140 (of 248), d = 3.6866, its = 8
i = 141 (of 248), d = 5.17199, its = 9
i = 142 (of 248), d = 4.196, its = 8
i = 143 (of 248), d = 5.14502, its = 9
i = 144 (of 248), d = 4.19433, its = 9
i = 145 (of 248), d = 7.50274, its = 10
i = 146 (of 248), d = 4.30814, its = 8
i = 147 (of 248), d = 3.30071, its = 8
i = 148 (of 248), d = 3.72592, its = 9
i = 149 (of 248), d = 2.65032, its = 8
i = 150 (of 248), d = 7.92885, its = 10
i = 151 (of 248), d = 4.72288, its = 8
i = 152 (of 248), d = 5.74421, its = 9
i = 153 (of 248), d = 8.23373, its = 10
i = 154 (of 248), d = 10.6527, its = 11
i = 155 (of 248), d = 3.59229, its = 9
i = 156 (of 248), d = 3.85031, its = 9
i = 157 (of 248), d = 8.07081, its = 10
i = 158 (of 248), d = 3.85214, its = 12
i = 159 (of 248), d = 5.56903, its = 10
i = 160 (of 248), d = 4.36576, its = 8
i = 161 (of 248), d = 7.2735, its = 10
i = 162 (of 248), d = 4.54022, its = 9
i = 163 (of 248), d = 8.47849, its = 11
i = 164 (of 248), d = 5.00865, its = 9
i = 165 (of 248), d = 3.32847, its = 9
i = 166 (of 248), d = 2.63387, its = 7
i = 167 (of 248), d = 4.78634, its = 10
i = 168 (of 248), d = 3.67081, its = 9
i = 169 (of 248), d = 9.71615, its = 11
i = 170 (of 248), d = 6.69332, its = 10
i = 171 (of 248), d = 6.27381, its = 10
i = 172 (of 248), d = 4.7482, its = 9
i = 173 (of 248), d = 10.2211, its = 10
i = 174 (of 248), d = 7.65818, its = 10
i = 175 (of 248), d = 8.36911, its = 10
i = 176 (of 248), d = 8.06642, its = 10
i = 177 (of 248), d = 6.34544, its = 9
i = 178 (of 248), d = 6.06995, its = 9
i = 179 (of 248), d = 4.53427, its = 9
i = 180 (of 248), d = 2.95099, its = 8
i = 181 (of 248), d = 4.69595, its = 9
i = 182 (of 248), d = 14.4244, its = 11
i = 183 (of 248), d = 5.9784, its = 9
i = 184 (of 248), d = 5.78899, its = 10
i = 185 (of 248), d = 6.46068, its = 10
i = 186 (of 248), d = 3.96113, its = 9
i = 187 (of 248), d = 5.97411, its = 9
i = 188 (of 248), d = 5.20505, its = 11
i = 189 (of 248), d = 7.05234, its = 10
i = 190 (of 248), d = 8.37715, its = 10
i = 191 (of 248), d = 5.3619, its = 9
i = 192 (of 248), d = 4.96912, its = 12
i = 193 (of 248), d = 8.55339, its = 10
i = 194 (of 248), d = 4.62176, its = 10
i = 195 (of 248), d = 4.64434, its = 8
i = 196 (of 248), d = 4.88149, its = 9
i = 197 (of 248), d = 9.87559, its = 12
i = 198 (of 248), d = 4.5368, its = 10
i = 199 (of 248), d = 6.39367, its = 10
i = 200 (of 248), d = 3.5389, its = 8
i = 201 (of 248), d = 5.83281, its = 9
i = 202 (of 248), d = 3.31709, its = 8
i = 203 (of 248), d = 6.83566, its = 10
i = 204 (of 248), d = 5.81828, its = 9
i = 205 (of 248), d = 5.57609, its = 9
i = 206 (of 248), d = 3.45165, its = 9
i = 207 (of 248), d = 10.4862, its = 11
i = 208 (of 248), d = 3.84356, its = 11
i = 209 (of 248), d = 6.64437, its = 10
i = 210 (of 248), d = 3.46185, its = 8
i = 211 (of 248), d = 5.17978, its = 9
i = 212 (of 248), d = 4.00029, its = 9
i = 213 (of 248), d = 7.14018, its = 10
i = 214 (of 248), d = 4.02335, its = 9
i = 215 (of 248), d = 5.67629, its = 9
i = 216 (of 248), d = 6.6287, its = 9
i = 217 (of 248), d = 7.47038, its = 10
i = 218 (of 248), d = 5.46661, its = 14
i = 219 (of 248), d = 4.49396, its = 9
i = 220 (of 248), d = 5.51102, its = 9
i = 221 (of 248), d = 7.61511, its = 10
i = 222 (of 248), d = 3.46118, its = 8
i = 223 (of 248), d = 12.9283, its = 12
i = 224 (of 248), d = 8.73905, its = 10
i = 225 (of 248), d = 12.0577, its = 11
i = 226 (of 248), d = 10.4114, its = 11
i = 227 (of 248), d = 22.8583, its = 12
i = 228 (of 248), d = 3.44337, its = 8
i = 229 (of 248), d = 5.41405, its = 9
i = 230 (of 248), d = 6.78153, its = 10
i = 231 (of 248), d = 6.82181, its = 10
i = 232 (of 248), d = 6.10584, its = 9
i = 233 (of 248), d = 7.41151, its = 9
i = 234 (of 248), d = 7.90548, its = 10
i = 235 (of 248), d = 4.04185, its = 9
i = 236 (of 248), d = 5.58331, its = 9
i = 237 (of 248), d = 8.68572, its = 10
i = 238 (of 248), d = 6.5993, its = 9
i = 239 (of 248), d = 5.04953, its = 10
i = 240 (of 248), d = 3.67121, its = 9
i = 241 (of 248), d = 5.70459, its = 9
i = 242 (of 248), d = 5.72122, its = 8
i = 243 (of 248), d = 8.92034, its = 10
i = 244 (of 248), d = 9.61382, its = 11
i = 245 (of 248), d = 5.41901, its = 9
i = 246 (of 248), d = 6.95801, its = 10
i = 247 (of 248), d = 5.80054, its = 9
i = 248 (of 248), d = 2.96218, its = 13
[1] "Fri Feb 09 13:39:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:39:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:39:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:39:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:39:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:40:02 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 2055.117833 
iter  10 value 45.582649
iter  20 value 7.484084
iter  30 value 2.122760
iter  40 value 1.753765
iter  50 value 1.563779
iter  60 value 1.343846
iter  70 value 1.133012
iter  80 value 1.012962
iter  90 value 0.959031
iter 100 value 0.881147
final  value 0.881147 
stopped after 100 iterations
# weights:  77
initial  value 694.867128 
iter  10 value 3.053565
iter  20 value 0.692519
iter  30 value 0.242785
iter  40 value 0.213535
iter  50 value 0.199447
iter  60 value 0.190491
iter  70 value 0.185030
iter  80 value 0.180932
iter  90 value 0.177882
iter 100 value 0.173365
final  value 0.173365 
stopped after 100 iterations
# weights:  77
initial  value 1149.135024 
iter  10 value 25.710571
iter  20 value 2.125039
iter  30 value 1.235372
iter  40 value 1.142042
iter  50 value 0.978734
iter  60 value 0.819562
iter  70 value 0.730770
iter  80 value 0.659082
iter  90 value 0.572455
iter 100 value 0.508655
final  value 0.508655 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 1087.684266 
iter  10 value 3.741417
iter  20 value 0.591168
iter  30 value 0.170674
iter  40 value 0.051542
iter  50 value 0.041478
iter  60 value 0.037920
iter  70 value 0.037028
iter  80 value 0.035639
iter  90 value 0.034314
iter 100 value 0.033572
final  value 0.033572 
stopped after 100 iterations
# weights:  53
initial  value 1745.830252 
iter  10 value 3.257662
iter  20 value 0.750997
iter  30 value 0.176659
iter  40 value 0.051887
iter  50 value 0.037085
iter  60 value 0.033374
iter  70 value 0.032450
iter  80 value 0.030645
iter  90 value 0.028947
iter 100 value 0.027753
final  value 0.027753 
stopped after 100 iterations
# weights:  53
initial  value 1064.838413 
iter  10 value 15.852578
iter  20 value 1.014224
iter  30 value 0.057078
iter  40 value 0.027380
iter  50 value 0.023017
iter  60 value 0.022091
iter  70 value 0.021366
iter  80 value 0.020741
iter  90 value 0.020223
iter 100 value 0.019755
final  value 0.019755 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00421; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 1027.417993 
iter  10 value 5.260361
iter  20 value 1.247861
iter  30 value 0.070619
iter  40 value 0.032506
iter  50 value 0.025096
iter  60 value 0.023047
iter  70 value 0.022690
iter  80 value 0.022211
iter  90 value 0.021808
iter 100 value 0.021467
final  value 0.021467 
stopped after 100 iterations
# weights:  49
initial  value 1089.691359 
iter  10 value 7.049628
iter  20 value 1.164526
iter  30 value 0.147588
iter  40 value 0.037932
iter  50 value 0.023853
iter  60 value 0.020437
iter  70 value 0.019590
iter  80 value 0.019206
iter  90 value 0.018704
iter 100 value 0.018380
final  value 0.018380 
stopped after 100 iterations
# weights:  49
initial  value 2181.524730 
iter  10 value 22.381245
iter  20 value 0.671290
iter  30 value 0.202037
iter  40 value 0.081409
iter  50 value 0.050435
iter  60 value 0.039237
iter  70 value 0.037364
iter  80 value 0.036465
iter  90 value 0.035540
iter 100 value 0.034698
final  value 0.034698 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00447; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 1108.159734 
iter  10 value 207.209103
iter  20 value 2.631132
iter  30 value 1.735204
iter  40 value 1.048318
iter  50 value 0.910326
iter  60 value 0.856599
iter  70 value 0.837449
iter  80 value 0.783607
iter  90 value 0.679772
iter 100 value 0.584993
final  value 0.584993 
stopped after 100 iterations
# weights:  9
initial  value 1007.258294 
iter  10 value 66.241235
iter  20 value 7.216869
iter  30 value 1.183378
iter  40 value 0.456165
iter  50 value 0.344727
iter  60 value 0.314897
iter  70 value 0.299062
iter  80 value 0.272306
iter  90 value 0.267614
iter 100 value 0.266806
final  value 0.266806 
stopped after 100 iterations
# weights:  9
initial  value 1157.894174 
iter  10 value 242.752962
iter  20 value 46.249926
iter  30 value 17.129975
iter  40 value 2.062842
iter  50 value 0.742082
iter  60 value 0.432655
iter  70 value 0.388645
iter  80 value 0.384483
iter  90 value 0.380312
iter 100 value 0.377676
final  value 0.377676 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0177; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 985.146109 
iter  10 value 3.063101
iter  20 value 0.858363
iter  30 value 0.606254
iter  40 value 0.544569
iter  50 value 0.516944
iter  60 value 0.508876
iter  70 value 0.504626
iter  80 value 0.502151
iter  90 value 0.499448
iter 100 value 0.497377
final  value 0.497377 
stopped after 100 iterations
# weights:  77
initial  value 800.050673 
iter  10 value 3.710801
iter  20 value 1.043901
iter  30 value 0.656029
iter  40 value 0.568553
iter  50 value 0.512543
iter  60 value 0.492483
iter  70 value 0.475765
iter  80 value 0.464203
iter  90 value 0.459831
iter 100 value 0.457518
final  value 0.457518 
stopped after 100 iterations
# weights:  77
initial  value 1251.721960 
iter  10 value 5.747690
iter  20 value 1.343555
iter  30 value 0.683029
iter  40 value 0.597212
iter  50 value 0.542268
iter  60 value 0.519709
iter  70 value 0.506747
iter  80 value 0.496567
iter  90 value 0.490531
iter 100 value 0.480590
final  value 0.480590 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.013; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 1483.486108 
iter  10 value 117.941465
iter  20 value 96.186172
iter  30 value 66.966159
iter  40 value 41.650182
iter  50 value 31.325553
iter  60 value 26.845716
iter  70 value 23.984089
iter  80 value 22.721743
iter  90 value 22.223328
iter 100 value 21.929878
final  value 21.929878 
stopped after 100 iterations
# weights:  73
initial  value 982.109057 
iter  10 value 31.712990
iter  20 value 26.644337
iter  30 value 22.469172
iter  40 value 21.295876
iter  50 value 20.674097
iter  60 value 20.555912
iter  70 value 20.502228
iter  80 value 20.449899
iter  90 value 20.425871
iter 100 value 20.417880
final  value 20.417880 
stopped after 100 iterations
# weights:  73
initial  value 1162.587316 
iter  10 value 122.151668
iter  20 value 97.829916
iter  30 value 71.322727
iter  40 value 43.878136
iter  50 value 31.048454
iter  60 value 26.770571
iter  70 value 24.102599
iter  80 value 22.966959
iter  90 value 22.414946
iter 100 value 21.933447
final  value 21.933447 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0695; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 1459.586792 
iter  10 value 3.563537
iter  20 value 0.456492
iter  30 value 0.137335
iter  40 value 0.076422
iter  50 value 0.069296
iter  60 value 0.067361
iter  70 value 0.066482
iter  80 value 0.065483
iter  90 value 0.064886
iter 100 value 0.064088
final  value 0.064088 
stopped after 100 iterations
# weights:  53
initial  value 1297.414885 
iter  10 value 7.013280
iter  20 value 1.067238
iter  30 value 0.312913
iter  40 value 0.072910
iter  50 value 0.064347
iter  60 value 0.060753
iter  70 value 0.058871
iter  80 value 0.056727
iter  90 value 0.054372
iter 100 value 0.051512
final  value 0.051512 
stopped after 100 iterations
# weights:  53
initial  value 1168.716758 
iter  10 value 28.447095
iter  20 value 5.777698
iter  30 value 1.011815
iter  40 value 0.268560
iter  50 value 0.181461
iter  60 value 0.172374
iter  70 value 0.164774
iter  80 value 0.160262
iter  90 value 0.155605
iter 100 value 0.150496
final  value 0.150496 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00504; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 1497.874987 
iter  10 value 8.136736
iter  20 value 0.825447
iter  30 value 0.272059
iter  40 value 0.154858
iter  50 value 0.135247
iter  60 value 0.132281
iter  70 value 0.131498
iter  80 value 0.125278
iter  90 value 0.119734
iter 100 value 0.115825
final  value 0.115825 
stopped after 100 iterations
# weights:  33
initial  value 1502.081017 
iter  10 value 12.384504
iter  20 value 2.756247
iter  30 value 0.763558
iter  40 value 0.311804
iter  50 value 0.235902
iter  60 value 0.212206
iter  70 value 0.200395
iter  80 value 0.178449
iter  90 value 0.139202
iter 100 value 0.111455
final  value 0.111455 
stopped after 100 iterations
# weights:  33
initial  value 1251.321756 
iter  10 value 8.733350
iter  20 value 0.871172
iter  30 value 0.233539
iter  40 value 0.144135
iter  50 value 0.134698
iter  60 value 0.129882
iter  70 value 0.126277
iter  80 value 0.116588
iter  90 value 0.107351
iter 100 value 0.099357
final  value 0.099357 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00886; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 1308.059039 
iter  10 value 2.679876
iter  20 value 0.766573
iter  30 value 0.195556
iter  40 value 0.110186
iter  50 value 0.096261
iter  60 value 0.090292
iter  70 value 0.088335
iter  80 value 0.087106
iter  90 value 0.085822
iter 100 value 0.084566
final  value 0.084566 
stopped after 100 iterations
# weights:  77
initial  value 1390.867385 
iter  10 value 11.538744
iter  20 value 0.522298
iter  30 value 0.238779
iter  40 value 0.192254
iter  50 value 0.182446
iter  60 value 0.178038
iter  70 value 0.172803
iter  80 value 0.168420
iter  90 value 0.163525
iter 100 value 0.160400
final  value 0.160400 
stopped after 100 iterations
# weights:  77
initial  value 2159.450423 
iter  10 value 6.428711
iter  20 value 0.345054
iter  30 value 0.195085
iter  40 value 0.157322
iter  50 value 0.146140
iter  60 value 0.141224
iter  70 value 0.138232
iter  80 value 0.135832
iter  90 value 0.134121
iter 100 value 0.132894
final  value 0.132894 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00577; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 1058.898822 
iter  10 value 24.487263
iter  20 value 5.694847
iter  30 value 0.692141
iter  40 value 0.128839
iter  50 value 0.087521
iter  60 value 0.032403
iter  70 value 0.016954
iter  80 value 0.010846
iter  90 value 0.009495
iter 100 value 0.006293
final  value 0.006293 
stopped after 100 iterations
# weights:  21
initial  value 1053.166648 
iter  10 value 16.446398
iter  20 value 1.649898
iter  30 value 0.167146
iter  40 value 0.040056
iter  50 value 0.023859
iter  60 value 0.005520
iter  70 value 0.003536
iter  80 value 0.003309
iter  90 value 0.003298
iter 100 value 0.003290
final  value 0.003290 
stopped after 100 iterations
# weights:  21
initial  value 863.785966 
iter  10 value 18.993060
iter  20 value 3.593740
iter  30 value 0.551804
iter  40 value 0.077649
iter  50 value 0.035833
iter  60 value 0.015735
iter  70 value 0.008145
iter  80 value 0.005011
iter  90 value 0.004569
iter 100 value 0.004148
final  value 0.004148 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0026; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 1592.224624 
iter  10 value 168.597815
iter  20 value 131.794813
iter  30 value 95.439944
iter  40 value 70.787161
iter  50 value 55.717023
iter  60 value 49.145682
iter  70 value 39.725189
iter  80 value 31.033063
iter  90 value 23.401405
iter 100 value 20.667270
final  value 20.667270 
stopped after 100 iterations
# weights:  73
initial  value 1424.855050 
iter  10 value 184.093616
iter  20 value 105.804213
iter  30 value 78.695706
iter  40 value 62.691765
iter  50 value 45.076236
iter  60 value 30.255166
iter  70 value 23.196525
iter  80 value 19.768347
iter  90 value 18.573445
iter 100 value 17.624850
final  value 17.624850 
stopped after 100 iterations
# weights:  73
initial  value 1583.313226 
iter  10 value 40.819421
iter  20 value 24.397336
iter  30 value 21.672446
iter  40 value 19.111372
iter  50 value 18.200161
iter  60 value 17.834346
iter  70 value 17.464461
iter  80 value 17.148206
iter  90 value 16.893631
iter 100 value 16.840386
final  value 16.840386 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0712; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 1103.466968 
iter  10 value 22.528096
iter  20 value 18.803347
iter  30 value 17.969268
iter  40 value 17.831834
iter  50 value 17.756932
iter  60 value 17.738295
iter  70 value 17.735817
iter  80 value 17.734048
iter  90 value 17.732789
final  value 17.732771 
converged
# weights:  45
initial  value 1335.424841 
iter  10 value 125.616883
iter  20 value 90.349265
iter  30 value 56.882050
iter  40 value 37.247318
iter  50 value 26.733574
iter  60 value 21.216301
iter  70 value 18.642717
iter  80 value 17.484806
iter  90 value 17.026743
iter 100 value 16.312544
final  value 16.312544 
stopped after 100 iterations
# weights:  45
initial  value 1999.357937 
iter  10 value 36.324072
iter  20 value 29.660105
iter  30 value 24.112144
iter  40 value 19.707785
iter  50 value 18.080992
iter  60 value 17.557404
iter  70 value 17.382438
iter  80 value 17.325098
iter  90 value 17.313161
iter 100 value 17.302749
final  value 17.302749 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 1542.182544 
iter  10 value 116.709797
iter  20 value 12.048866
iter  30 value 2.355811
iter  40 value 2.100144
iter  50 value 1.830046
iter  60 value 1.621842
iter  70 value 1.581923
iter  80 value 1.567460
iter  90 value 1.562841
iter 100 value 1.553988
final  value 1.553988 
stopped after 100 iterations
# weights:  21
initial  value 1294.608964 
iter  10 value 12.491392
iter  20 value 3.124823
iter  30 value 1.918952
iter  40 value 1.756784
iter  50 value 1.698981
iter  60 value 1.529463
iter  70 value 1.466632
iter  80 value 1.414071
iter  90 value 1.377011
iter 100 value 1.308725
final  value 1.308725 
stopped after 100 iterations
# weights:  21
initial  value 1295.552436 
iter  10 value 18.580008
iter  20 value 6.978893
iter  30 value 3.136244
iter  40 value 2.536492
iter  50 value 2.275615
iter  60 value 1.761433
iter  70 value 1.610466
iter  80 value 1.554700
iter  90 value 1.546827
iter 100 value 1.508925
final  value 1.508925 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0261; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1734.569539 
iter  10 value 8.460384
iter  20 value 1.626651
iter  30 value 0.676268
iter  40 value 0.553304
iter  50 value 0.522520
iter  60 value 0.508538
iter  70 value 0.478411
iter  80 value 0.460305
iter  90 value 0.436146
iter 100 value 0.419410
final  value 0.419410 
stopped after 100 iterations
# weights:  53
initial  value 1035.398660 
iter  10 value 2.828417
iter  20 value 0.772681
iter  30 value 0.356076
iter  40 value 0.258332
iter  50 value 0.236782
iter  60 value 0.226002
iter  70 value 0.218307
iter  80 value 0.210956
iter  90 value 0.205741
iter 100 value 0.201393
final  value 0.201393 
stopped after 100 iterations
# weights:  53
initial  value 1928.565219 
iter  10 value 13.131115
iter  20 value 1.756963
iter  30 value 0.548556
iter  40 value 0.420123
iter  50 value 0.388231
iter  60 value 0.362493
iter  70 value 0.344873
iter  80 value 0.329689
iter  90 value 0.306646
iter 100 value 0.288898
final  value 0.288898 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 2003.777874 
iter  10 value 4.220337
iter  20 value 2.977334
iter  30 value 2.823573
iter  40 value 2.731263
iter  50 value 2.654086
iter  60 value 2.593312
iter  70 value 2.529262
iter  80 value 2.489336
iter  90 value 2.473715
iter 100 value 2.417291
final  value 2.417291 
stopped after 100 iterations
# weights:  45
initial  value 1933.007083 
iter  10 value 7.153002
iter  20 value 2.998510
iter  30 value 2.678521
iter  40 value 2.526532
iter  50 value 2.429596
iter  60 value 2.350756
iter  70 value 2.314772
iter  80 value 2.291128
iter  90 value 2.246936
iter 100 value 2.156871
final  value 2.156871 
stopped after 100 iterations
# weights:  45
initial  value 1688.804070 
iter  10 value 6.604984
iter  20 value 3.616178
iter  30 value 3.022415
iter  40 value 2.754876
iter  50 value 2.598699
iter  60 value 2.465649
iter  70 value 2.358463
iter  80 value 2.293084
iter  90 value 2.261109
iter 100 value 2.193081
final  value 2.193081 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0289; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 1108.344491 
iter  10 value 4.341171
iter  20 value 1.049466
iter  30 value 0.195645
iter  40 value 0.136408
iter  50 value 0.126091
iter  60 value 0.122390
iter  70 value 0.118331
iter  80 value 0.117384
iter  90 value 0.113868
iter 100 value 0.110421
final  value 0.110421 
stopped after 100 iterations
# weights:  37
initial  value 1079.956895 
iter  10 value 10.890182
iter  20 value 1.174800
iter  30 value 0.356382
iter  40 value 0.202473
iter  50 value 0.177760
iter  60 value 0.163282
iter  70 value 0.151423
iter  80 value 0.146837
iter  90 value 0.144662
iter 100 value 0.137817
final  value 0.137817 
stopped after 100 iterations
# weights:  37
initial  value 1722.644604 
iter  10 value 8.406899
iter  20 value 1.121248
iter  30 value 0.373329
iter  40 value 0.225426
iter  50 value 0.171359
iter  60 value 0.159580
iter  70 value 0.154703
iter  80 value 0.150577
iter  90 value 0.139035
iter 100 value 0.124337
final  value 0.124337 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0094; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 1280.656198 
iter  10 value 60.537067
iter  20 value 34.084838
iter  30 value 27.996945
iter  40 value 25.315374
iter  50 value 25.159333
iter  60 value 25.126998
final  value 25.126901 
converged
# weights:  21
initial  value 1126.529876 
iter  10 value 66.779199
iter  20 value 45.101933
iter  30 value 28.500280
iter  40 value 24.602072
iter  50 value 22.698549
iter  60 value 22.272125
iter  70 value 22.259997
final  value 22.259957 
converged
# weights:  21
initial  value 1450.721515 
iter  10 value 28.313995
iter  20 value 25.960620
iter  30 value 25.080950
iter  40 value 24.445853
iter  50 value 24.332790
iter  60 value 24.326727
final  value 24.326726 
converged
[Tune-y] 17: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 1212.656975 
iter  10 value 16.301504
iter  20 value 1.080015
iter  30 value 0.180294
iter  40 value 0.025110
iter  50 value 0.009676
iter  60 value 0.003133
iter  70 value 0.002039
iter  80 value 0.001937
iter  90 value 0.001860
iter 100 value 0.001771
final  value 0.001771 
stopped after 100 iterations
# weights:  57
initial  value 1861.611180 
iter  10 value 4.857890
iter  20 value 0.368648
iter  30 value 0.097809
iter  40 value 0.029819
iter  50 value 0.011033
iter  60 value 0.004838
iter  70 value 0.003542
iter  80 value 0.002542
iter  90 value 0.002112
iter 100 value 0.001822
final  value 0.001822 
stopped after 100 iterations
# weights:  57
initial  value 2013.623634 
iter  10 value 7.588357
iter  20 value 1.139318
iter  30 value 0.184693
iter  40 value 0.046169
iter  50 value 0.012540
iter  60 value 0.004886
iter  70 value 0.002758
iter  80 value 0.002169
iter  90 value 0.001953
iter 100 value 0.001767
final  value 0.001767 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0028; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 1236.188511 
iter  10 value 16.575413
iter  20 value 2.427276
iter  30 value 1.189568
iter  40 value 0.539883
iter  50 value 0.260108
iter  60 value 0.214511
iter  70 value 0.199262
iter  80 value 0.191820
iter  90 value 0.173836
iter 100 value 0.144367
final  value 0.144367 
stopped after 100 iterations
# weights:  17
initial  value 762.704005 
iter  10 value 43.051083
iter  20 value 3.405132
iter  30 value 1.210700
iter  40 value 0.557954
iter  50 value 0.254053
iter  60 value 0.230854
iter  70 value 0.214190
iter  80 value 0.194577
iter  90 value 0.170473
iter 100 value 0.155981
final  value 0.155981 
stopped after 100 iterations
# weights:  17
initial  value 1222.961585 
iter  10 value 13.827939
iter  20 value 3.234374
iter  30 value 0.362021
iter  40 value 0.270045
iter  50 value 0.174899
iter  60 value 0.162686
iter  70 value 0.156535
iter  80 value 0.154792
iter  90 value 0.150133
iter 100 value 0.140648
final  value 0.140648 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0132; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 1126.497456 
iter  10 value 5.450586
iter  20 value 2.398704
iter  30 value 2.076104
iter  40 value 1.915709
iter  50 value 1.779192
iter  60 value 1.710542
iter  70 value 1.610359
iter  80 value 1.484707
iter  90 value 1.457499
iter 100 value 1.442048
final  value 1.442048 
stopped after 100 iterations
# weights:  29
initial  value 1380.795889 
iter  10 value 25.551873
iter  20 value 2.946474
iter  30 value 2.120734
iter  40 value 1.869483
iter  50 value 1.606711
iter  60 value 1.506208
iter  70 value 1.471098
iter  80 value 1.433971
iter  90 value 1.415659
iter 100 value 1.405517
final  value 1.405517 
stopped after 100 iterations
# weights:  29
initial  value 1010.721783 
iter  10 value 13.728250
iter  20 value 2.472497
iter  30 value 1.693907
iter  40 value 1.557610
iter  50 value 1.479626
iter  60 value 1.436626
iter  70 value 1.367942
iter  80 value 1.315472
iter  90 value 1.308494
iter 100 value 1.306465
final  value 1.306465 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0281; time: 0.0 min
[Tune] Result: size=5; decay=1.79e-05 : rmse.test.rmse=0.0026
# weights:  21
initial  value 2587.329306 
iter  10 value 32.381812
iter  20 value 8.144918
iter  30 value 2.437661
iter  40 value 0.927443
iter  50 value 0.474610
iter  60 value 0.202168
iter  70 value 0.071951
iter  80 value 0.021054
iter  90 value 0.009470
iter 100 value 0.009206
final  value 0.009206 
stopped after 100 iterations
[1] "Fri Feb 09 13:40:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 683 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 412
 number of selected nodes                               : 85 
[1] "Fri Feb 09 13:40:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:40:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:40:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:40:54 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.164; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.164
[1] "Fri Feb 09 13:41:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:41:51 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.861; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.897; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.61; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.625; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.677; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.799; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.603; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.588; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=1.05; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.511
[1] "Fri Feb 09 13:41:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:41:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:42:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7240 0.3270 0.1480 0.0671 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01900101 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1482 
    Y 1           0.31 
    Y 2       -0.04699 
    Y 3         -2.103 
    Y 4        -0.4739 
    Y 5         -0.282 
[1] "Fri Feb 09 13:42:30 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.276; time: 0.7 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.268; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:43:36] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.497; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.306; time: 0.6 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.357; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:14] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.361; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.402; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.324; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:44:20] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.268
[1] "Fri Feb 09 13:44:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:44:30 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.487577], n=(308,444)
**GROW** @depth 1: [1,0.500388], n=(171,136)
**GROW** @depth 1: [2,0.561963], n=(148,297)
**GROW** @depth 2: [2,0.244325], n=(15,156)
**GROW** @depth 2: [1,0.724539], n=(108,12)
**GROW** @depth 2: [2,0.74954], n=(220,36)
**GROW** @depth 2: [2,0.263497], n=(15,190)
**GROW** @depth 3: [1,0.616804], n=(57,51)
**GROW** @depth 3: [2,0.410276], n=(57,133)
**GROW** @depth 3: [2,0.623773], n=(138,83)
**GROW** @depth 3: [1,0.244303], n=(18,138)
**GROW** @depth 4: [1,0.3153], n=(19,66)
**GROW** @depth 5: [2,0.556442], n=(57,38)
**PRUNE** @depth 4: [1,0.718803]
**GROW** @depth 3: [2,0.515337], n=(82,48)
**GROW** @depth 4: [1,0.291118], n=(16,123)
**PRUNE** @depth 3: [2,0.258742]
**GROW** @depth 4: [1,0.398543], n=(53,69)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,18,16,53,69,61,57,38,66,33,44,57,84,77,64)
**GROW** @depth 3: [2,0.720859], n=(43,21)
**GROW** @depth 6: [1,0.429081], n=(20,18)
**GROW** @depth 4: [2,0.644325], n=(47,15)
**GROW** @depth 4: [1,0.562548], n=(43,41)
**PRUNE** @depth 5: [1,0.292978]
**GROW** @depth 6: [2,0.615491], n=(16,33)
**GROW** @depth 5: [2,0.570706], n=(21,24)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(16,18,68,71,21,24,16,60,20,35,17,34,33,45,56,38,43,74,42,21)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 5: [2,0.639724]
**GROW** @depth 5: [2,0.365644], n=(19,52)
**GROW** @depth 6: [2,0.42684], n=(24,28)
**GROW** @depth 5: [1,0.417455], n=(36,25)
**GROW** @depth 4: [2,0.464571], n=(28,27)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(16,18,69,19,23,28,22,39,32,22,22,35,17,35,33,43,29,28,40,39,77,44,22)
**PRUNE** @depth 3: [1,0.561928]
**GROW** @depth 5: [2,0.649233], n=(25,14)
**PRUNE** @depth 5: [2,0.651994]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(18,17,69,19,23,28,21,39,35,24,19,37,15,34,33,43,29,27,25,77,86,34)
**GROW** @depth 3: [2,0.631135], n=(49,37)
**PRUNE** @depth 4: [2,0.517791]
**PRUNE** @depth 3: [2,0.631135]
**GROW** @depth 5: [1,0.375756], n=(17,25)
**GROW** @depth 7: [2,0.66411], n=(16,17)
**GROW** @depth 7: [2,0.673926], n=(22,15)
**PRUNE** @depth 7: [2,0.675153]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(16,18,16,26,46,23,29,19,40,34,22,21,37,18,16,17,33,44,29,53,76,86,33)
**GROW** @depth 5: [2,0.27592], n=(11,33)
**GROW** @depth 6: [1,0.319485], n=(23,23)
**GROW** @depth 6: [2,0.347546], n=(12,21)
**GROW** @depth 3: [1,0.660208], n=(28,48)
**GROW** @depth 7: [2,0.425767], n=(12,11)
**GROW** @depth 7: [2,0.690337], n=(26,11)
**GROW** @depth 6: [1,0.445202], n=(12,14)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(16,18,16,12,14,23,12,11,23,29,19,40,34,22,21,26,11,17,17,18,32,11,12,21,29,43,63,23,86,33)
**GROW** @depth 4: [1,0.57867], n=(46,17)
**GROW** @depth 5: [2,0.628988], n=(22,18)
**PRUNE** @depth 5: [2,0.627607]
**PRUNE** @depth 7: [2,0.690337]
**PRUNE** @depth 7: [2,0.429755]
**PRUNE** @depth 6: [1,0.444427]
**PRUNE** @depth 4: [2,0.41365]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(17,17,16,25,24,22,23,28,19,40,34,22,19,39,17,17,18,32,12,12,51,43,45,20,23,84,33)
Grow: 12.05%, Prune: 4.167%, Change: 83.17%, Swap: 25.34%

[1] "Fri Feb 09 13:44:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:44:37 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:44:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9604 	 alpha= 0.9029 	 beta= 134622.2 
[1] "Fri Feb 09 13:44:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:44:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:44:42 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:44:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:44:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:44:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:44:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:44:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:44:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:44:48 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:44:51 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.591; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.162; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.336; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.184; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.332; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.22; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.173; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.193; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.238; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.204; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.569; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.109; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.448; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.158; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.109
[1] "Fri Feb 09 13:44:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:44:57 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.142; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0379; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.124; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0241; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0146; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.16; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.167; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0877; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00187; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0503; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.378; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00352; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0202; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0145; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.236; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.184; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.00855; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00187
[1] "Fri Feb 09 13:45:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:45:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:45:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:45:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=============                                                         |  18%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:45:19 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 655 is not positive definite

[1] "Fri Feb 09 13:45:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 3.49409, its = 9
i = 2 (of 248), d = 6.77343, its = 10
i = 3 (of 248), d = 4.01003, its = 7
i = 4 (of 248), d = 4.32307, its = 9
i = 5 (of 248), d = 8.26459, its = 10
i = 6 (of 248), d = 10.2359, its = 11
i = 7 (of 248), d = 6.10698, its = 9
i = 8 (of 248), d = 5.83321, its = 10
i = 9 (of 248), d = 4.58613, its = 9
i = 10 (of 248), d = 4.33885, its = 11
i = 11 (of 248), d = 5.49781, its = 10
i = 12 (of 248), d = 7.70851, its = 12
i = 13 (of 248), d = 5.38771, its = 10
i = 14 (of 248), d = 20.1092, its = 12
i = 15 (of 248), d = 3.54782, its = 9
i = 16 (of 248), d = 7.50848, its = 10
i = 17 (of 248), d = 2.84755, its = 9
i = 18 (of 248), d = 5.40925, its = 9
i = 19 (of 248), d = 8.09757, its = 10
i = 20 (of 248), d = 7.89641, its = 10
i = 21 (of 248), d = 3.12442, its = 8
i = 22 (of 248), d = 4.66892, its = 9
i = 23 (of 248), d = 4.83238, its = 10
i = 24 (of 248), d = 4.47923, its = 9
i = 25 (of 248), d = 3.86967, its = 9
i = 26 (of 248), d = 7.37521, its = 11
i = 27 (of 248), d = 19.7381, its = 12
i = 28 (of 248), d = 2.64601, its = 20
i = 29 (of 248), d = 5.41776, its = 10
i = 30 (of 248), d = 3.17781, its = 8
i = 31 (of 248), d = 7.49643, its = 10
i = 32 (of 248), d = 11.5347, its = 11
i = 33 (of 248), d = 4.95352, its = 9
i = 34 (of 248), d = 3.40327, its = 8
i = 35 (of 248), d = 4.19039, its = 11
i = 36 (of 248), d = 6.77818, its = 10
i = 37 (of 248), d = 12.2391, its = 11
i = 38 (of 248), d = 2.97519, its = 10
i = 39 (of 248), d = 10.1126, its = 9
i = 40 (of 248), d = 6.56718, its = 9
i = 41 (of 248), d = 6.69603, its = 10
i = 42 (of 248), d = 7.53094, its = 10
i = 43 (of 248), d = 5.66388, its = 10
i = 44 (of 248), d = 8.91389, its = 9
i = 45 (of 248), d = 3.21178, its = 8
i = 46 (of 248), d = 6.33534, its = 10
i = 47 (of 248), d = 7.26433, its = 11
i = 48 (of 248), d = 6.99554, its = 10
i = 49 (of 248), d = 8.504, its = 10
i = 50 (of 248), d = 5.50681, its = 8
i = 51 (of 248), d = 4.01024, its = 9
i = 52 (of 248), d = 6.88616, its = 10
i = 53 (of 248), d = 4.68561, its = 9
i = 54 (of 248), d = 7.99977, its = 10
i = 55 (of 248), d = 8.85692, its = 11
i = 56 (of 248), d = 7.76065, its = 10
i = 57 (of 248), d = 8.6847, its = 10
i = 58 (of 248), d = 12.4242, its = 12
i = 59 (of 248), d = 5.89458, its = 31
i = 60 (of 248), d = 7.95977, its = 10
i = 61 (of 248), d = 3.45768, its = 9
i = 62 (of 248), d = 7.43399, its = 11
i = 63 (of 248), d = 3.76836, its = 9
i = 64 (of 248), d = 3.12107, its = 8
i = 65 (of 248), d = 4.40636, its = 9
i = 66 (of 248), d = 3.34612, its = 9
i = 67 (of 248), d = 2.0289, its = 8
i = 68 (of 248), d = 4.35035, its = 8
i = 69 (of 248), d = 5.51343, its = 9
i = 70 (of 248), d = 10.8864, its = 11
i = 71 (of 248), d = 4.04211, its = 8
i = 72 (of 248), d = 7.53905, its = 11
i = 73 (of 248), d = 10.2437, its = 11
i = 74 (of 248), d = 8.30193, its = 10
i = 75 (of 248), d = 9.77261, its = 10
i = 76 (of 248), d = 9.59367, its = 10
i = 77 (of 248), d = 10.7642, its = 10
i = 78 (of 248), d = 3.57617, its = 8
i = 79 (of 248), d = 5.9249, its = 7
i = 80 (of 248), d = 7.35745, its = 9
i = 81 (of 248), d = 5.30028, its = 9
i = 82 (of 248), d = 4.83952, its = 8
i = 83 (of 248), d = 7.02828, its = 10
i = 84 (of 248), d = 10.5698, its = 11
i = 85 (of 248), d = 4.93702, its = 9
i = 86 (of 248), d = 6.72976, its = 10
i = 87 (of 248), d = 6.99738, its = 10
i = 88 (of 248), d = 11.6517, its = 11
i = 89 (of 248), d = 3.16074, its = 9
i = 90 (of 248), d = 10.2028, its = 10
i = 91 (of 248), d = 3.4893, its = 8
i = 92 (of 248), d = 3.66509, its = 8
i = 93 (of 248), d = 2.79268, its = 12
i = 94 (of 248), d = 10.3587, its = 11
i = 95 (of 248), d = 7.72127, its = 10
i = 96 (of 248), d = 6.29509, its = 10
i = 97 (of 248), d = 6.49686, its = 10
i = 98 (of 248), d = 6.17103, its = 10
i = 99 (of 248), d = 6.49109, its = 34
i = 100 (of 248), d = 3.62267, its = 8
i = 101 (of 248), d = 8.23526, its = 11
i = 102 (of 248), d = 8.23195, its = 10
i = 103 (of 248), d = 4.18016, its = 8
i = 104 (of 248), d = 3.58982, its = 8
i = 105 (of 248), d = 9.03568, its = 10
i = 106 (of 248), d = 5.44107, its = 9
i = 107 (of 248), d = 3.87741, its = 9
i = 108 (of 248), d = 3.44484, its = 9
i = 109 (of 248), d = 26.8805, its = 37
i = 110 (of 248), d = 5.87608, its = 27
i = 111 (of 248), d = 6.98191, its = 10
i = 112 (of 248), d = 6.98029, its = 10
i = 113 (of 248), d = 10.2036, its = 11
i = 114 (of 248), d = 3.64576, its = 9
i = 115 (of 248), d = 8.8888, its = 10
i = 116 (of 248), d = 4.32719, its = 9
i = 117 (of 248), d = 5.42137, its = 9
i = 118 (of 248), d = 10.7031, its = 11
i = 119 (of 248), d = 5.41758, its = 10
i = 120 (of 248), d = 5.74004, its = 10
i = 121 (of 248), d = 6.21438, its = 11
i = 122 (of 248), d = 5.62906, its = 10
i = 123 (of 248), d = 5.81461, its = 9
i = 124 (of 248), d = 6.20206, its = 10
i = 125 (of 248), d = 12.7129, its = 11
i = 126 (of 248), d = 3.05992, its = 8
i = 127 (of 248), d = 4.04766, its = 9
i = 128 (of 248), d = 5.91748, its = 10
i = 129 (of 248), d = 5.98651, its = 10
i = 130 (of 248), d = 3.216, its = 8
i = 131 (of 248), d = 3.35568, its = 9
i = 132 (of 248), d = 8.75668, its = 10
i = 133 (of 248), d = 5.82311, its = 9
i = 134 (of 248), d = 9.77101, its = 11
i = 135 (of 248), d = 3.59358, its = 28
i = 136 (of 248), d = 11.0771, its = 11
i = 137 (of 248), d = 8.77767, its = 11
i = 138 (of 248), d = 3.21367, its = 7
i = 139 (of 248), d = 5.42268, its = 8
i = 140 (of 248), d = 7.76109, its = 10
i = 141 (of 248), d = 8.531, its = 11
i = 142 (of 248), d = 4.88538, its = 8
i = 143 (of 248), d = 4.24182, its = 24
i = 144 (of 248), d = 4.71536, its = 9
i = 145 (of 248), d = 8.81425, its = 10
i = 146 (of 248), d = 8.35265, its = 11
i = 147 (of 248), d = 7.92796, its = 10
i = 148 (of 248), d = 8.53143, its = 10
i = 149 (of 248), d = 3.6328, its = 8
i = 150 (of 248), d = 3.93175, its = 8
i = 151 (of 248), d = 2.93272, its = 8
i = 152 (of 248), d = 8.94738, its = 10
i = 153 (of 248), d = 8.9763, its = 10
i = 154 (of 248), d = 5.45554, its = 9
i = 155 (of 248), d = 2.72827, its = 9
i = 156 (of 248), d = 2.77657, its = 8
i = 157 (of 248), d = 6.98401, its = 10
i = 158 (of 248), d = 7.09638, its = 11
i = 159 (of 248), d = 5.82646, its = 9
i = 160 (of 248), d = 6.26959, its = 8
i = 161 (of 248), d = 4.04768, its = 9
i = 162 (of 248), d = 8.85395, its = 11
i = 163 (of 248), d = 4.63106, its = 9
i = 164 (of 248), d = 7.62919, its = 10
i = 165 (of 248), d = 7.44411, its = 10
i = 166 (of 248), d = 10.0009, its = 11
i = 167 (of 248), d = 5.12861, its = 10
i = 168 (of 248), d = 4.28255, its = 10
i = 169 (of 248), d = 2.9368, its = 8
i = 170 (of 248), d = 2.30646, its = 8
i = 171 (of 248), d = 6.78627, its = 10
i = 172 (of 248), d = 8.64797, its = 10
i = 173 (of 248), d = 8.09422, its = 11
i = 174 (of 248), d = 15.8529, its = 12
i = 175 (of 248), d = 5.7967, its = 10
i = 176 (of 248), d = 7.78197, its = 10
i = 177 (of 248), d = 4.46263, its = 10
i = 178 (of 248), d = 5.13053, its = 10
i = 179 (of 248), d = 5.07654, its = 9
i = 180 (of 248), d = 4.04505, its = 9
i = 181 (of 248), d = 5.2425, its = 10
i = 182 (of 248), d = 8.38997, its = 8
i = 183 (of 248), d = 11.3447, its = 10
i = 184 (of 248), d = 6.24679, its = 10
i = 185 (of 248), d = 7.02635, its = 11
i = 186 (of 248), d = 4.19228, its = 12
i = 187 (of 248), d = 6.7352, its = 9
i = 188 (of 248), d = 5.90571, its = 9
i = 189 (of 248), d = 4.4542, its = 8
i = 190 (of 248), d = 3.47426, its = 8
i = 191 (of 248), d = 4.03502, its = 9
i = 192 (of 248), d = 8.47345, its = 10
i = 193 (of 248), d = 8.72932, its = 10
i = 194 (of 248), d = 8.82954, its = 11
i = 195 (of 248), d = 3.67416, its = 9
i = 196 (of 248), d = 4.62593, its = 9
i = 197 (of 248), d = 11.7411, its = 11
i = 198 (of 248), d = 3.50918, its = 9
i = 199 (of 248), d = 14.1848, its = 12
i = 200 (of 248), d = 4.1531, its = 9
i = 201 (of 248), d = 9.33629, its = 11
i = 202 (of 248), d = 2.68059, its = 8
i = 203 (of 248), d = 3.52483, its = 9
i = 204 (of 248), d = 3.5043, its = 8
i = 205 (of 248), d = 6.81131, its = 10
i = 206 (of 248), d = 8.64457, its = 10
i = 207 (of 248), d = 8.31676, its = 10
i = 208 (of 248), d = 7.8941, its = 10
i = 209 (of 248), d = 8.98465, its = 11
i = 210 (of 248), d = 6.76254, its = 9
i = 211 (of 248), d = 5.7591, its = 10
i = 212 (of 248), d = 4.00275, its = 9
i = 213 (of 248), d = 6.07597, its = 10
i = 214 (of 248), d = 7.40553, its = 10
i = 215 (of 248), d = 9.45492, its = 10
i = 216 (of 248), d = 11.1935, its = 11
i = 217 (of 248), d = 7.04532, its = 10
i = 218 (of 248), d = 4.01165, its = 9
i = 219 (of 248), d = 10.2559, its = 10
i = 220 (of 248), d = 5.09439, its = 9
i = 221 (of 248), d = 9.98895, its = 11
i = 222 (of 248), d = 7.89483, its = 11
i = 223 (of 248), d = 2.83806, its = 8
i = 224 (of 248), d = 10.4706, its = 12
i = 225 (of 248), d = 10.9185, its = 10
i = 226 (of 248), d = 12.7541, its = 11
i = 227 (of 248), d = 7.92854, its = 11
i = 228 (of 248), d = 3.73237, its = 9
i = 229 (of 248), d = 3.96222, its = 8
i = 230 (of 248), d = 4.95027, its = 10
i = 231 (of 248), d = 9.33227, its = 10
i = 232 (of 248), d = 7.71534, its = 10
i = 233 (of 248), d = 7.03665, its = 10
i = 234 (of 248), d = 3.85621, its = 9
i = 235 (of 248), d = 6.23121, its = 10
i = 236 (of 248), d = 8.15212, its = 10
i = 237 (of 248), d = 11.4325, its = 14
i = 238 (of 248), d = 3.69379, its = 9
i = 239 (of 248), d = 4.82578, its = 12
i = 240 (of 248), d = 5.66907, its = 10
i = 241 (of 248), d = 7.30938, its = 9
i = 242 (of 248), d = 6.5545, its = 9
i = 243 (of 248), d = 4.49685, its = 9
i = 244 (of 248), d = 9.47173, its = 10
i = 245 (of 248), d = 3.27536, its = 8
i = 246 (of 248), d = 7.64276, its = 10
i = 247 (of 248), d = 7.22358, its = 10
i = 248 (of 248), d = 5.91959, its = 10
[1] "Fri Feb 09 13:46:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:46:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:46:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:46:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:46:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:46:21 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 510.466394 
iter  10 value 40.081670
iter  20 value 6.311636
iter  30 value 3.409803
iter  40 value 3.326984
final  value 3.326977 
converged
# weights:  5
initial  value 493.401298 
iter  10 value 17.782336
iter  20 value 3.808644
iter  30 value 3.119690
final  value 3.110358 
converged
# weights:  5
initial  value 466.748710 
iter  10 value 10.079553
iter  20 value 3.994905
iter  30 value 3.627981
final  value 3.627812 
converged
[Tune-y] 1: rmse.test.rmse=0.0549; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 333.046560 
iter  10 value 1.439504
iter  20 value 0.157440
iter  30 value 0.099603
iter  40 value 0.086591
iter  50 value 0.081891
iter  60 value 0.079953
iter  70 value 0.077974
iter  80 value 0.076558
iter  90 value 0.075542
iter 100 value 0.074689
final  value 0.074689 
stopped after 100 iterations
# weights:  57
initial  value 610.432687 
iter  10 value 2.494653
iter  20 value 0.157278
iter  30 value 0.093332
iter  40 value 0.084844
iter  50 value 0.080702
iter  60 value 0.077487
iter  70 value 0.074528
iter  80 value 0.073020
iter  90 value 0.071739
iter 100 value 0.070380
final  value 0.070380 
stopped after 100 iterations
# weights:  57
initial  value 3112.812170 
iter  10 value 138.344530
iter  20 value 7.136996
iter  30 value 2.938779
iter  40 value 2.005041
iter  50 value 1.851184
iter  60 value 1.716779
iter  70 value 1.475692
iter  80 value 1.086711
iter  90 value 0.903206
iter 100 value 0.810532
final  value 0.810532 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00979; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 706.894223 
iter  10 value 1.661281
iter  20 value 0.607180
iter  30 value 0.502081
iter  40 value 0.479655
iter  50 value 0.454916
iter  60 value 0.434378
iter  70 value 0.408426
iter  80 value 0.373410
iter  90 value 0.354741
iter 100 value 0.328007
final  value 0.328007 
stopped after 100 iterations
# weights:  41
initial  value 1622.589717 
iter  10 value 1.732727
iter  20 value 0.531612
iter  30 value 0.484163
iter  40 value 0.453260
iter  50 value 0.441863
iter  60 value 0.434919
iter  70 value 0.427166
iter  80 value 0.420152
iter  90 value 0.412580
iter 100 value 0.398093
final  value 0.398093 
stopped after 100 iterations
# weights:  41
initial  value 451.177326 
iter  10 value 2.989351
iter  20 value 0.782964
iter  30 value 0.610470
iter  40 value 0.553389
iter  50 value 0.531753
iter  60 value 0.511260
iter  70 value 0.490651
iter  80 value 0.463530
iter  90 value 0.436681
iter 100 value 0.404979
final  value 0.404979 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.016; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 757.843541 
iter  10 value 41.045829
iter  20 value 5.067781
iter  30 value 1.731801
iter  40 value 0.707904
iter  50 value 0.513680
iter  60 value 0.485353
final  value 0.485009 
converged
# weights:  5
initial  value 539.665293 
iter  10 value 116.390876
iter  20 value 58.081826
iter  30 value 12.593251
iter  40 value 3.472475
iter  50 value 1.224170
iter  60 value 0.630762
iter  70 value 0.459073
iter  80 value 0.449464
final  value 0.449453 
converged
# weights:  5
initial  value 830.998609 
iter  10 value 113.299704
iter  20 value 18.469751
iter  30 value 3.116797
iter  40 value 1.259245
iter  50 value 0.688497
iter  60 value 0.552505
iter  70 value 0.526953
final  value 0.526048 
converged
[Tune-y] 4: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 495.941839 
iter  10 value 6.111544
iter  20 value 0.085890
iter  30 value 0.028924
iter  40 value 0.024277
iter  50 value 0.022960
iter  60 value 0.021808
iter  70 value 0.021163
iter  80 value 0.020504
iter  90 value 0.019988
iter 100 value 0.019554
final  value 0.019554 
stopped after 100 iterations
# weights:  61
initial  value 516.250782 
iter  10 value 4.529701
iter  20 value 0.216461
iter  30 value 0.027687
iter  40 value 0.020792
iter  50 value 0.019779
iter  60 value 0.019027
iter  70 value 0.018367
iter  80 value 0.017966
iter  90 value 0.017482
iter 100 value 0.017110
final  value 0.017110 
stopped after 100 iterations
# weights:  61
initial  value 2031.548432 
iter  10 value 5.514165
iter  20 value 0.195475
iter  30 value 0.035430
iter  40 value 0.027219
iter  50 value 0.024706
iter  60 value 0.023817
iter  70 value 0.023462
iter  80 value 0.023001
iter  90 value 0.022629
iter 100 value 0.022225
final  value 0.022225 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00432; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 801.331981 
iter  10 value 6.046677
iter  20 value 0.761058
iter  30 value 0.679243
iter  40 value 0.634274
iter  50 value 0.613534
iter  60 value 0.594258
iter  70 value 0.573831
iter  80 value 0.555999
iter  90 value 0.544195
iter 100 value 0.533868
final  value 0.533868 
stopped after 100 iterations
# weights:  57
initial  value 663.429841 
iter  10 value 8.352909
iter  20 value 2.059776
iter  30 value 1.756419
iter  40 value 1.427566
iter  50 value 1.177201
iter  60 value 1.078307
iter  70 value 0.946738
iter  80 value 0.853271
iter  90 value 0.772954
iter 100 value 0.697990
final  value 0.697990 
stopped after 100 iterations
# weights:  57
initial  value 388.128272 
iter  10 value 2.718645
iter  20 value 0.643044
iter  30 value 0.551480
iter  40 value 0.516789
iter  50 value 0.491990
iter  60 value 0.479037
iter  70 value 0.467352
iter  80 value 0.461494
iter  90 value 0.454347
iter 100 value 0.446668
final  value 0.446668 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0112; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 711.920943 
iter  10 value 44.579830
iter  20 value 8.347184
iter  30 value 5.101410
iter  40 value 3.833305
iter  50 value 3.532072
iter  60 value 3.497230
iter  70 value 3.477371
iter  80 value 3.477066
final  value 3.477066 
converged
# weights:  13
initial  value 545.920747 
iter  10 value 15.100525
iter  20 value 5.061536
iter  30 value 4.177878
iter  40 value 3.404416
iter  50 value 3.182245
iter  60 value 3.164511
iter  70 value 3.153640
final  value 3.153596 
converged
# weights:  13
initial  value 599.452084 
iter  10 value 15.259191
iter  20 value 6.759710
iter  30 value 6.378698
iter  40 value 5.576756
iter  50 value 4.911152
iter  60 value 4.887325
iter  70 value 4.799446
iter  80 value 4.721137
iter  90 value 4.705518
iter 100 value 4.695246
final  value 4.695246 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0404; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 1816.667900 
iter  10 value 6.973212
iter  20 value 1.101451
iter  30 value 0.959944
iter  40 value 0.892926
iter  50 value 0.824144
iter  60 value 0.709748
iter  70 value 0.647218
iter  80 value 0.594124
iter  90 value 0.558263
iter 100 value 0.532518
final  value 0.532518 
stopped after 100 iterations
# weights:  73
initial  value 683.940485 
iter  10 value 5.854827
iter  20 value 1.763349
iter  30 value 1.248895
iter  40 value 1.064960
iter  50 value 0.966764
iter  60 value 0.877626
iter  70 value 0.790703
iter  80 value 0.746832
iter  90 value 0.716199
iter 100 value 0.688456
final  value 0.688456 
stopped after 100 iterations
# weights:  73
initial  value 579.075279 
iter  10 value 1.322159
iter  20 value 0.475384
iter  30 value 0.348072
iter  40 value 0.310775
iter  50 value 0.304797
iter  60 value 0.299918
iter  70 value 0.297321
iter  80 value 0.295629
iter  90 value 0.294228
iter 100 value 0.292833
final  value 0.292833 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0129; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 639.443338 
iter  10 value 8.219539
iter  20 value 0.896045
iter  30 value 0.395191
iter  40 value 0.342307
iter  50 value 0.319817
iter  60 value 0.307122
iter  70 value 0.295233
iter  80 value 0.281792
iter  90 value 0.265839
iter 100 value 0.259124
final  value 0.259124 
stopped after 100 iterations
# weights:  57
initial  value 1351.583144 
iter  10 value 14.068593
iter  20 value 0.919596
iter  30 value 0.477042
iter  40 value 0.406081
iter  50 value 0.349729
iter  60 value 0.324844
iter  70 value 0.305906
iter  80 value 0.289641
iter  90 value 0.264972
iter 100 value 0.244575
final  value 0.244575 
stopped after 100 iterations
# weights:  57
initial  value 1253.933498 
iter  10 value 2.219239
iter  20 value 0.379390
iter  30 value 0.252609
iter  40 value 0.230797
iter  50 value 0.216472
iter  60 value 0.208028
iter  70 value 0.201374
iter  80 value 0.194452
iter  90 value 0.189199
iter 100 value 0.185130
final  value 0.185130 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00922; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 768.304777 
iter  10 value 5.227593
iter  20 value 1.372198
iter  30 value 0.465340
iter  40 value 0.163876
iter  50 value 0.108899
iter  60 value 0.073139
iter  70 value 0.058582
iter  80 value 0.048532
iter  90 value 0.045603
iter 100 value 0.042955
final  value 0.042955 
stopped after 100 iterations
# weights:  5
initial  value 772.025905 
iter  10 value 119.284059
iter  20 value 40.371468
iter  30 value 4.123222
iter  40 value 0.975630
iter  50 value 0.308814
iter  60 value 0.166411
iter  70 value 0.108506
iter  80 value 0.073110
iter  90 value 0.056189
iter 100 value 0.049288
final  value 0.049288 
stopped after 100 iterations
# weights:  5
initial  value 562.412652 
iter  10 value 60.437057
iter  20 value 12.301392
iter  30 value 2.386381
iter  40 value 0.938166
iter  50 value 0.427531
iter  60 value 0.221140
iter  70 value 0.133323
iter  80 value 0.080322
iter  90 value 0.060394
iter 100 value 0.053168
final  value 0.053168 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0105; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 868.964710 
iter  10 value 2.896577
iter  20 value 0.480388
iter  30 value 0.198359
iter  40 value 0.150128
iter  50 value 0.144635
iter  60 value 0.140389
iter  70 value 0.138031
iter  80 value 0.128431
iter  90 value 0.117762
iter 100 value 0.114700
final  value 0.114700 
stopped after 100 iterations
# weights:  33
initial  value 698.039666 
iter  10 value 2.292286
iter  20 value 0.357735
iter  30 value 0.144338
iter  40 value 0.136147
iter  50 value 0.134395
iter  60 value 0.132325
iter  70 value 0.131004
iter  80 value 0.121115
iter  90 value 0.114015
iter 100 value 0.109714
final  value 0.109714 
stopped after 100 iterations
# weights:  33
initial  value 474.111411 
iter  10 value 3.251741
iter  20 value 0.602717
iter  30 value 0.210094
iter  40 value 0.173527
iter  50 value 0.160419
iter  60 value 0.154333
iter  70 value 0.151356
iter  80 value 0.141424
iter  90 value 0.135804
iter 100 value 0.128826
final  value 0.128826 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0107; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 553.176754 
iter  10 value 14.335531
iter  20 value 6.979343
iter  30 value 6.391301
iter  40 value 6.287507
iter  50 value 6.071060
iter  60 value 5.710482
iter  70 value 5.283949
iter  80 value 4.355466
iter  90 value 4.034712
iter 100 value 4.032797
final  value 4.032797 
stopped after 100 iterations
# weights:  17
initial  value 620.898160 
iter  10 value 63.018334
iter  20 value 14.317107
iter  30 value 6.487459
iter  40 value 5.551656
iter  50 value 3.759259
iter  60 value 3.542076
iter  70 value 3.487706
iter  80 value 3.471462
iter  90 value 3.468863
final  value 3.468855 
converged
# weights:  17
initial  value 563.023625 
iter  10 value 8.620680
iter  20 value 5.014229
iter  30 value 4.587452
iter  40 value 4.448133
iter  50 value 4.153633
iter  60 value 4.085941
iter  70 value 4.067401
iter  80 value 4.051726
iter  90 value 4.049348
final  value 4.049309 
converged
[Tune-y] 12: rmse.test.rmse=0.0412; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 531.642525 
iter  10 value 19.816072
iter  20 value 2.578682
iter  30 value 0.396180
iter  40 value 0.107472
iter  50 value 0.031491
iter  60 value 0.020317
iter  70 value 0.014925
iter  80 value 0.013145
iter  90 value 0.012962
iter 100 value 0.011851
final  value 0.011851 
stopped after 100 iterations
# weights:  13
initial  value 496.195918 
iter  10 value 16.995345
iter  20 value 4.364624
iter  30 value 0.563073
iter  40 value 0.123572
iter  50 value 0.015412
iter  60 value 0.005062
iter  70 value 0.004277
iter  80 value 0.004183
iter  90 value 0.004161
iter 100 value 0.004139
final  value 0.004139 
stopped after 100 iterations
# weights:  13
initial  value 508.076803 
iter  10 value 71.240143
iter  20 value 16.696489
iter  30 value 3.504179
iter  40 value 0.951801
iter  50 value 0.329788
iter  60 value 0.192171
iter  70 value 0.134659
iter  80 value 0.100976
iter  90 value 0.095184
iter 100 value 0.084738
final  value 0.084738 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00531; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1001.969811 
iter  10 value 40.527850
iter  20 value 3.952576
iter  30 value 1.031365
iter  40 value 0.409279
iter  50 value 0.227005
iter  60 value 0.211541
iter  70 value 0.138297
iter  80 value 0.112717
iter  90 value 0.102074
iter 100 value 0.087329
final  value 0.087329 
stopped after 100 iterations
# weights:  25
initial  value 680.165410 
iter  10 value 8.386334
iter  20 value 0.463210
iter  30 value 0.058023
iter  40 value 0.037085
iter  50 value 0.031176
iter  60 value 0.027590
iter  70 value 0.024462
iter  80 value 0.023478
iter  90 value 0.022732
iter 100 value 0.021190
final  value 0.021190 
stopped after 100 iterations
# weights:  25
initial  value 557.831816 
iter  10 value 11.334051
iter  20 value 2.535763
iter  30 value 0.342092
iter  40 value 0.170389
iter  50 value 0.095996
iter  60 value 0.084703
iter  70 value 0.072937
iter  80 value 0.063931
iter  90 value 0.054509
iter 100 value 0.046601
final  value 0.046601 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00565; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 1649.038674 
iter  10 value 4.137377
iter  20 value 1.137673
iter  30 value 0.461704
iter  40 value 0.201685
iter  50 value 0.102297
iter  60 value 0.082293
iter  70 value 0.066405
iter  80 value 0.055225
iter  90 value 0.050687
iter 100 value 0.046927
final  value 0.046927 
stopped after 100 iterations
# weights:  25
initial  value 1099.851733 
iter  10 value 1.940533
iter  20 value 0.190897
iter  30 value 0.076617
iter  40 value 0.052446
iter  50 value 0.046104
iter  60 value 0.041854
iter  70 value 0.040008
iter  80 value 0.038928
iter  90 value 0.037875
iter 100 value 0.037229
final  value 0.037229 
stopped after 100 iterations
# weights:  25
initial  value 742.650687 
iter  10 value 3.568494
iter  20 value 0.767621
iter  30 value 0.229569
iter  40 value 0.058963
iter  50 value 0.053159
iter  60 value 0.049699
iter  70 value 0.046796
iter  80 value 0.044687
iter  90 value 0.043275
iter 100 value 0.042802
final  value 0.042802 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0085; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 422.558026 
iter  10 value 78.477388
iter  20 value 74.284481
iter  30 value 73.499228
iter  40 value 73.061982
iter  50 value 72.882995
iter  60 value 72.785353
iter  70 value 72.763363
final  value 72.762979 
converged
# weights:  65
initial  value 1280.711576 
iter  10 value 289.549081
iter  20 value 139.604808
iter  30 value 88.380356
iter  40 value 75.553777
iter  50 value 74.039198
iter  60 value 73.484116
iter  70 value 73.161171
iter  80 value 72.907110
iter  90 value 72.768753
iter 100 value 72.714687
final  value 72.714687 
stopped after 100 iterations
# weights:  65
initial  value 2884.109079 
iter  10 value 293.348262
iter  20 value 123.549815
iter  30 value 81.999075
iter  40 value 77.169418
iter  50 value 75.518617
iter  60 value 74.348542
iter  70 value 73.697525
iter  80 value 73.335714
iter  90 value 73.150713
iter 100 value 72.999876
final  value 72.999876 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0891; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 455.618052 
iter  10 value 1.855964
iter  20 value 0.379118
iter  30 value 0.042769
iter  40 value 0.027192
iter  50 value 0.025487
iter  60 value 0.025159
iter  70 value 0.024708
iter  80 value 0.023975
iter  90 value 0.023149
iter 100 value 0.022458
final  value 0.022458 
stopped after 100 iterations
# weights:  29
initial  value 453.253917 
iter  10 value 2.247438
iter  20 value 0.321836
iter  30 value 0.152255
iter  40 value 0.078963
iter  50 value 0.044577
iter  60 value 0.034930
iter  70 value 0.030213
iter  80 value 0.028158
iter  90 value 0.026004
iter 100 value 0.024278
final  value 0.024278 
stopped after 100 iterations
# weights:  29
initial  value 1018.428229 
iter  10 value 16.599273
iter  20 value 4.084324
iter  30 value 1.584385
iter  40 value 0.461352
iter  50 value 0.191189
iter  60 value 0.173465
iter  70 value 0.163920
iter  80 value 0.146973
iter  90 value 0.117203
iter 100 value 0.072539
final  value 0.072539 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.007; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 896.114637 
iter  10 value 5.838898
iter  20 value 2.164469
iter  30 value 1.847040
iter  40 value 1.716664
iter  50 value 1.691286
iter  60 value 1.661270
iter  70 value 1.631180
iter  80 value 1.600535
iter  90 value 1.590073
iter 100 value 1.586482
final  value 1.586482 
stopped after 100 iterations
# weights:  21
initial  value 902.008908 
iter  10 value 13.144327
iter  20 value 2.891753
iter  30 value 1.671348
iter  40 value 1.592282
iter  50 value 1.538749
iter  60 value 1.422859
iter  70 value 1.393202
iter  80 value 1.376828
iter  90 value 1.364518
iter 100 value 1.359928
final  value 1.359928 
stopped after 100 iterations
# weights:  21
initial  value 773.470364 
iter  10 value 11.549559
iter  20 value 3.101335
iter  30 value 2.275586
iter  40 value 1.976217
iter  50 value 1.895601
iter  60 value 1.765211
iter  70 value 1.755682
iter  80 value 1.696202
iter  90 value 1.584399
iter 100 value 1.519440
final  value 1.519440 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0256; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 1752.988097 
iter  10 value 5.874127
iter  20 value 1.515259
iter  30 value 1.237265
iter  40 value 1.050505
iter  50 value 0.901512
iter  60 value 0.821513
iter  70 value 0.733729
iter  80 value 0.671130
iter  90 value 0.635908
iter 100 value 0.607325
final  value 0.607325 
stopped after 100 iterations
# weights:  65
initial  value 475.660537 
iter  10 value 1.031222
iter  20 value 0.520613
iter  30 value 0.453134
iter  40 value 0.427876
iter  50 value 0.419857
iter  60 value 0.411955
iter  70 value 0.406474
iter  80 value 0.404732
iter  90 value 0.403349
iter 100 value 0.402509
final  value 0.402509 
stopped after 100 iterations
# weights:  65
initial  value 968.791235 
iter  10 value 12.016561
iter  20 value 1.420257
iter  30 value 1.129553
iter  40 value 0.995538
iter  50 value 0.914151
iter  60 value 0.864185
iter  70 value 0.808693
iter  80 value 0.746907
iter  90 value 0.704011
iter 100 value 0.671184
final  value 0.671184 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0151; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 667.923400 
iter  10 value 7.198532
iter  20 value 0.183830
iter  30 value 0.038374
iter  40 value 0.014580
iter  50 value 0.010431
iter  60 value 0.009933
iter  70 value 0.009584
iter  80 value 0.009365
iter  90 value 0.009181
iter 100 value 0.008861
final  value 0.008861 
stopped after 100 iterations
# weights:  69
initial  value 768.517058 
iter  10 value 1.620948
iter  20 value 0.139081
iter  30 value 0.029821
iter  40 value 0.012998
iter  50 value 0.010980
iter  60 value 0.010551
iter  70 value 0.010386
iter  80 value 0.010157
iter  90 value 0.010012
iter 100 value 0.009869
final  value 0.009869 
stopped after 100 iterations
# weights:  69
initial  value 1020.438714 
iter  10 value 1.347321
iter  20 value 0.107057
iter  30 value 0.025963
iter  40 value 0.013810
iter  50 value 0.011292
iter  60 value 0.010644
iter  70 value 0.010332
iter  80 value 0.010122
iter  90 value 0.009987
iter 100 value 0.009853
final  value 0.009853 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00367; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.00367
# weights:  69
initial  value 1104.206987 
iter  10 value 1.942942
iter  20 value 0.252758
iter  30 value 0.031023
iter  40 value 0.013428
iter  50 value 0.010277
iter  60 value 0.009423
iter  70 value 0.009054
iter  80 value 0.008899
iter  90 value 0.008780
iter 100 value 0.008684
final  value 0.008684 
stopped after 100 iterations
[1] "Fri Feb 09 13:46:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 718 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 437
 number of selected nodes                               : 76 
[1] "Fri Feb 09 13:46:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:46:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:47:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:47:12 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.13; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.145; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.128; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.136; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.155; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.145; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.146; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.146; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.142; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.133; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.127; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.127
[1] "Fri Feb 09 13:48:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:48:07 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.428; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.498; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.684; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.466; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.627; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.425; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.348; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.445; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.415; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.857; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.409; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.453; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.857; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.554; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.714; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.857; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.367; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.493; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.652; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.348
[1] "Fri Feb 09 13:48:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:48:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:48:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01500106 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1         0.2159 
    Y 2        0.07896 
    Y 3        -0.3264 
    Y 4        -0.1936 
    Y 5         0.9505 
[1] "Fri Feb 09 13:48:51 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.29; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.237; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:55] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:55] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:48:55] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.473; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.264; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.233; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.288; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.352; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:26] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:26] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:49:26] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.321; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:50:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.224
[1] "Fri Feb 09 13:50:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:50:19 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.441327], n=(296,456)
**GROW** @depth 1: [2,0.488497], n=(200,256)
**GROW** @depth 1: [1,0.224306], n=(36,260)
**GROW** @depth 2: [1,0.726864], n=(231,22)
**GROW** @depth 2: [2,0.341564], n=(51,152)
**GROW** @depth 3: [1,0.673694], n=(206,25)
**GROW** @depth 2: [1,0.337932], n=(116,145)
**PRUNE** @depth 3: [1,0.731359]
**GROW** @depth 1: [2,0.568712], n=(24,13)
**GROW** @depth 4: [2,0.605828], n=(24,23)
**GROW** @depth 3: [2,0.499693], n=(51,66)
**GROW** @depth 4: [1,0.563169], n=(61,50)
**GROW** @depth 3: [1,0.36661], n=(92,59)
**GROW** @depth 4: [2,0.580215], n=(38,58)
**PRUNE** @depth 5: [1,0.673384]
**GROW** @depth 5: [2,0.802914], n=(60,13)
**GROW** @depth 4: [1,0.270346], n=(19,78)
**GROW** @depth 5: [1,0.310184], n=(18,22)
**PRUNE** @depth 4: [1,0.339792]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(19,13,110,18,23,16,19,78,51,153,113,66,60,13)
**PRUNE** @depth 5: [1,0.371105]
**GROW** @depth 3: [2,0.245399], n=(14,96)
**GROW** @depth 3: [1,0.718803], n=(142,11)
**GROW** @depth 5: [2,0.369632], n=(35,57)
**GROW** @depth 4: [2,0.673466], n=(77,36)
**GROW** @depth 4: [2,0.394172], n=(35,108)
**GROW** @depth 6: [1,0.718803], n=(56,11)
**GROW** @depth 5: [2,0.581442], n=(46,31)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(24,13,12,35,33,43,37,19,78,50,33,109,12,46,31,37,56,11,60,13)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [1,0.600527], n=(33,17)
**GROW** @depth 4: [1,0.324136], n=(15,20)
**GROW** @depth 6: [2,0.770706], n=(62,17)
**GROW** @depth 6: [2,0.47684], n=(14,19)
**PRUNE** @depth 4: [1,0.324446]
**GROW** @depth 6: [1,0.642381], n=(31,25)
**GROW** @depth 5: [1,0.652612], n=(32,28)
**PRUNE** @depth 7: [1,0.642381]
**GROW** @depth 5: [1,0.585491], n=(82,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(23,13,12,35,15,19,43,37,19,61,17,33,17,32,82,28,12,46,31,37,56,11,32,28,13)
**GROW** @depth 7: [1,0.509533], n=(39,43)
**GROW** @depth 5: [1,0.354364], n=(24,37)
**PRUNE** @depth 5: [1,0.354364]
**GROW** @depth 6: [1,0.642536], n=(31,25)
**GROW** @depth 5: [1,0.355139], n=(24,37)
**GROW** @depth 7: [2,0.469632], n=(31,12)
**PRUNE** @depth 6: [1,0.355139]
**PRUNE** @depth 7: [2,0.470245]
**GROW** @depth 6: [1,0.376531], n=(33,28)
**GROW** @depth 7: [1,0.652612], n=(17,12)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(21,13,13,35,15,19,45,36,20,33,28,16,33,17,50,21,43,17,12,12,45,31,37,33,24,11,31,28,13)
**GROW** @depth 6: [1,0.392497], n=(29,16)
**PRUNE** @depth 7: [1,0.652612]
**GROW** @depth 4: [1,0.336692], n=(18,17)
**GROW** @depth 7: [1,0.657107], n=(18,11)
**GROW** @depth 4: [1,0.492637], n=(20,25)
**PRUNE** @depth 7: [1,0.658503]
**PRUNE** @depth 6: [1,0.392497]
**PRUNE** @depth 4: [1,0.493877]
**GROW** @depth 5: [2,0.617025], n=(12,19)
**GROW** @depth 6: [2,0.659202], n=(15,14)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(20,13,13,19,16,16,19,45,36,20,32,15,14,17,33,17,50,23,42,27,12,45,12,19,36,34,24,12,31,27,13)
**GROW** @depth 4: [2,0.409816], n=(16,34)
**PRUNE** @depth 5: [2,0.580215]
**PRUNE** @depth 6: [2,0.659202]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(17,13,13,18,18,17,21,44,36,19,32,28,19,32,16,17,34,23,42,27,13,57,19,36,34,24,11,31,28,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(18,13,12,20,17,17,20,44,36,19,32,27,19,33,17,16,34,24,41,27,12,56,19,37,36,23,11,31,28,13)
Grow: 11.92%, Prune: 4.237%, Change: 83.83%, Swap: 20.85%

[1] "Fri Feb 09 13:50:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:50:27 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:50:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 6.6899 	 alpha= 0.2787 	 beta= 43351.17 
[1] "Fri Feb 09 13:50:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:50:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 13:50:32 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:50:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:50:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:50:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:50:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:50:36 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:50:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:50:38 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:50:41 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.316; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.182; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.124; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.28; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.306; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.182; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.954; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.195; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.164; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.166; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse= 0.2; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.124
[1] "Fri Feb 09 13:50:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:50:51 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0125; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0114; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0375; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0223; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0411; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.00162; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0705; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.361; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0367; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00145; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0305; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.204; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00145
[1] "Fri Feb 09 13:50:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===================================                                   |  50%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:51:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==================================                                    |  48%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:51:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:51:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============                                                       |  22%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:51:15 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 449 is not positive definite

[1] "Fri Feb 09 13:51:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 3.17847, its = 8
i = 2 (of 248), d = 3.03451, its = 8
i = 3 (of 248), d = 9.11511, its = 11
i = 4 (of 248), d = 4.24318, its = 9
i = 5 (of 248), d = 5.48227, its = 10
i = 6 (of 248), d = 3.52971, its = 9
i = 7 (of 248), d = 4.2848, its = 9
i = 8 (of 248), d = 13.5905, its = 12
i = 9 (of 248), d = 5.74879, its = 10
i = 10 (of 248), d = 6.88618, its = 10
i = 11 (of 248), d = 6.07448, its = 9
i = 12 (of 248), d = 2.78072, its = 8
i = 13 (of 248), d = 3.72053, its = 8
i = 14 (of 248), d = 4.77482, its = 9
i = 15 (of 248), d = 6.32097, its = 9
i = 16 (of 248), d = 2.79716, its = 8
i = 17 (of 248), d = 9.29322, its = 11
i = 18 (of 248), d = 5.52128, its = 9
i = 19 (of 248), d = 8.53326, its = 10
i = 20 (of 248), d = 8.85958, its = 11
i = 21 (of 248), d = 7.43632, its = 9
i = 22 (of 248), d = 6.49826, its = 10
i = 23 (of 248), d = 3.12423, its = 8
i = 24 (of 248), d = 6.69111, its = 10
i = 25 (of 248), d = 4.87911, its = 9
i = 26 (of 248), d = 6.66769, its = 10
i = 27 (of 248), d = 4.29255, its = 8
i = 28 (of 248), d = 3.40372, its = 9
i = 29 (of 248), d = 6.30776, its = 9
i = 30 (of 248), d = 9.68512, its = 10
i = 31 (of 248), d = 5.51842, its = 10
i = 32 (of 248), d = 7.35439, its = 10
i = 33 (of 248), d = 4.52822, its = 9
i = 34 (of 248), d = 4.78419, its = 8
i = 35 (of 248), d = 5.51984, its = 9
i = 36 (of 248), d = 10.3919, its = 11
i = 37 (of 248), d = 12.5644, its = 11
i = 38 (of 248), d = 7.17854, its = 9
i = 39 (of 248), d = 3.63455, its = 8
i = 40 (of 248), d = 8.76654, its = 11
i = 41 (of 248), d = 10.9898, its = 11
i = 42 (of 248), d = 9.69678, its = 11
i = 43 (of 248), d = 7.47354, its = 10
i = 44 (of 248), d = 7.45604, its = 10
i = 45 (of 248), d = 6.08066, its = 10
i = 46 (of 248), d = 4.3497, its = 9
i = 47 (of 248), d = 6.13663, its = 11
i = 48 (of 248), d = 9.03714, its = 11
i = 49 (of 248), d = 4.96437, its = 9
i = 50 (of 248), d = 8.93484, its = 11
i = 51 (of 248), d = 2.79135, its = 9
i = 52 (of 248), d = 4.84627, its = 9
i = 53 (of 248), d = 6.46024, its = 10
i = 54 (of 248), d = 8.28198, its = 10
i = 55 (of 248), d = 5.59684, its = 7
i = 56 (of 248), d = 6.11097, its = 9
i = 57 (of 248), d = 7.76203, its = 10
i = 58 (of 248), d = 9.919, its = 11
i = 59 (of 248), d = 5.76401, its = 9
i = 60 (of 248), d = 5.81984, its = 11
i = 61 (of 248), d = 6.11596, its = 10
i = 62 (of 248), d = 9.77458, its = 10
i = 63 (of 248), d = 6.44518, its = 10
i = 64 (of 248), d = 10.8726, its = 11
i = 65 (of 248), d = 4.27826, its = 9
i = 66 (of 248), d = 11.859, its = 11
i = 67 (of 248), d = 7.50645, its = 10
i = 68 (of 248), d = 9.39758, its = 10
i = 69 (of 248), d = 3.4604, its = 9
i = 70 (of 248), d = 19.2178, its = 12
i = 71 (of 248), d = 5.01295, its = 8
i = 72 (of 248), d = 8.53903, its = 11
i = 73 (of 248), d = 3.37741, its = 9
i = 74 (of 248), d = 5.57618, its = 9
i = 75 (of 248), d = 18.7999, its = 12
i = 76 (of 248), d = 4.1098, its = 8
i = 77 (of 248), d = 5.14207, its = 9
i = 78 (of 248), d = 7.06605, its = 10
i = 79 (of 248), d = 5.4875, its = 11
i = 80 (of 248), d = 2.78727, its = 8
i = 81 (of 248), d = 9.14947, its = 11
i = 82 (of 248), d = 3.79976, its = 8
i = 83 (of 248), d = 9.45352, its = 27
i = 84 (of 248), d = 7.89486, its = 11
i = 85 (of 248), d = 3.37387, its = 8
i = 86 (of 248), d = 4.97778, its = 8
i = 87 (of 248), d = 10.1263, its = 11
i = 88 (of 248), d = 3.04953, its = 8
i = 89 (of 248), d = 2.80359, its = 8
i = 90 (of 248), d = 9.29822, its = 10
i = 91 (of 248), d = 6.47911, its = 10
i = 92 (of 248), d = 6.99984, its = 10
i = 93 (of 248), d = 10.52, its = 10
i = 94 (of 248), d = 7.78267, its = 10
i = 95 (of 248), d = 8.54995, its = 10
i = 96 (of 248), d = 6.5832, its = 10
i = 97 (of 248), d = 9.94352, its = 11
i = 98 (of 248), d = 12.0056, its = 11
i = 99 (of 248), d = 10.2214, its = 10
i = 100 (of 248), d = 3.61819, its = 9
i = 101 (of 248), d = 6.0317, its = 9
i = 102 (of 248), d = 5.22344, its = 9
i = 103 (of 248), d = 7.66249, its = 10
i = 104 (of 248), d = 5.11962, its = 9
i = 105 (of 248), d = 3.23358, its = 9
i = 106 (of 248), d = 2.83424, its = 7
i = 107 (of 248), d = 9.05822, its = 10
i = 108 (of 248), d = 5.05466, its = 8
i = 109 (of 248), d = 3.23831, its = 8
i = 110 (of 248), d = 7.06361, its = 10
i = 111 (of 248), d = 9.47698, its = 11
i = 112 (of 248), d = 4.65939, its = 8
i = 113 (of 248), d = 4.40339, its = 9
i = 114 (of 248), d = 10.512, its = 11
i = 115 (of 248), d = 4.34011, its = 9
i = 116 (of 248), d = 9.17055, its = 10
i = 117 (of 248), d = 10.071, its = 11
i = 118 (of 248), d = 3.89619, its = 9
i = 119 (of 248), d = 3.78769, its = 8
i = 120 (of 248), d = 5.8717, its = 9
i = 121 (of 248), d = 13.4807, its = 11
i = 122 (of 248), d = 10.7514, its = 11
i = 123 (of 248), d = 6.059, its = 10
i = 124 (of 248), d = 3.45711, its = 9
i = 125 (of 248), d = 5.19452, its = 10
i = 126 (of 248), d = 6.05115, its = 10
i = 127 (of 248), d = 5.07457, its = 9
i = 128 (of 248), d = 3.06008, its = 8
i = 129 (of 248), d = 3.21891, its = 9
i = 130 (of 248), d = 7.625, its = 10
i = 131 (of 248), d = 11.7214, its = 15
i = 132 (of 248), d = 8.66847, its = 11
i = 133 (of 248), d = 2.85353, its = 16
i = 134 (of 248), d = 8.27628, its = 10
i = 135 (of 248), d = 8.71045, its = 10
i = 136 (of 248), d = 6.28392, its = 10
i = 137 (of 248), d = 4.74815, its = 9
i = 138 (of 248), d = 3.61628, its = 9
i = 139 (of 248), d = 9.8105, its = 11
i = 140 (of 248), d = 3.85736, its = 8
i = 141 (of 248), d = 6.05523, its = 10
i = 142 (of 248), d = 3.81116, its = 9
i = 143 (of 248), d = 7.09605, its = 32
i = 144 (of 248), d = 3.87958, its = 9
i = 145 (of 248), d = 8.12864, its = 10
i = 146 (of 248), d = 5.5942, its = 8
i = 147 (of 248), d = 4.26103, its = 8
i = 148 (of 248), d = 4.40074, its = 9
i = 149 (of 248), d = 5.43418, its = 8
i = 150 (of 248), d = 8.0087, its = 10
i = 151 (of 248), d = 6.10295, its = 11
i = 152 (of 248), d = 7.8272, its = 10
i = 153 (of 248), d = 7.73895, its = 10
i = 154 (of 248), d = 10.6823, its = 11
i = 155 (of 248), d = 3.8838, its = 10
i = 156 (of 248), d = 3.50403, its = 8
i = 157 (of 248), d = 8.26492, its = 11
i = 158 (of 248), d = 3.66995, its = 8
i = 159 (of 248), d = 4.6776, its = 9
i = 160 (of 248), d = 3.68057, its = 8
i = 161 (of 248), d = 6.80126, its = 10
i = 162 (of 248), d = 6.55098, its = 10
i = 163 (of 248), d = 8.88368, its = 11
i = 164 (of 248), d = 4.39813, its = 9
i = 165 (of 248), d = 3.41968, its = 8
i = 166 (of 248), d = 2.83511, its = 7
i = 167 (of 248), d = 4.52054, its = 9
i = 168 (of 248), d = 3.80922, its = 7
i = 169 (of 248), d = 10.6296, its = 11
i = 170 (of 248), d = 6.74019, its = 10
i = 171 (of 248), d = 6.62406, its = 9
i = 172 (of 248), d = 6.52085, its = 10
i = 173 (of 248), d = 9.18998, its = 10
i = 174 (of 248), d = 6.92922, its = 10
i = 175 (of 248), d = 7.8013, its = 10
i = 176 (of 248), d = 10.4553, its = 10
i = 177 (of 248), d = 7.09413, its = 9
i = 178 (of 248), d = 4.99782, its = 9
i = 179 (of 248), d = 4.09515, its = 9
i = 180 (of 248), d = 3.08526, its = 8
i = 181 (of 248), d = 3.44827, its = 10
i = 182 (of 248), d = 14.1377, its = 11
i = 183 (of 248), d = 5.59556, its = 9
i = 184 (of 248), d = 5.5213, its = 10
i = 185 (of 248), d = 6.25929, its = 9
i = 186 (of 248), d = 4.6969, its = 9
i = 187 (of 248), d = 3.66635, its = 9
i = 188 (of 248), d = 5.04082, its = 9
i = 189 (of 248), d = 7.33262, its = 10
i = 190 (of 248), d = 5.65965, its = 10
i = 191 (of 248), d = 6.10413, its = 9
i = 192 (of 248), d = 3.08865, its = 8
i = 193 (of 248), d = 10.2896, its = 9
i = 194 (of 248), d = 5.21024, its = 9
i = 195 (of 248), d = 5.49078, its = 8
i = 196 (of 248), d = 4.19255, its = 8
i = 197 (of 248), d = 9.32014, its = 11
i = 198 (of 248), d = 5.49361, its = 10
i = 199 (of 248), d = 7.14677, its = 10
i = 200 (of 248), d = 3.26857, its = 8
i = 201 (of 248), d = 6.14177, its = 10
i = 202 (of 248), d = 3.21846, its = 8
i = 203 (of 248), d = 6.95383, its = 10
i = 204 (of 248), d = 5.69205, its = 10
i = 205 (of 248), d = 5.76097, its = 9
i = 206 (of 248), d = 3.54467, its = 7
i = 207 (of 248), d = 9.843, its = 11
i = 208 (of 248), d = 3.93968, its = 8
i = 209 (of 248), d = 6.88677, its = 10
i = 210 (of 248), d = 4.10659, its = 9
i = 211 (of 248), d = 4.87139, its = 9
i = 212 (of 248), d = 3.43837, its = 8
i = 213 (of 248), d = 6.88342, its = 9
i = 214 (of 248), d = 4.19802, its = 9
i = 215 (of 248), d = 5.73643, its = 9
i = 216 (of 248), d = 5.97962, its = 10
i = 217 (of 248), d = 7.92961, its = 11
i = 218 (of 248), d = 3.01495, its = 10
i = 219 (of 248), d = 3.9529, its = 9
i = 220 (of 248), d = 5.50571, its = 9
i = 221 (of 248), d = 7.10781, its = 10
i = 222 (of 248), d = 4.94116, its = 9
i = 223 (of 248), d = 12.1701, its = 11
i = 224 (of 248), d = 8.78268, its = 10
i = 225 (of 248), d = 12.2173, its = 11
i = 226 (of 248), d = 10.4979, its = 11
i = 227 (of 248), d = 24.5337, its = 12
i = 228 (of 248), d = 4.06658, its = 9
i = 229 (of 248), d = 6.16473, its = 9
i = 230 (of 248), d = 7.29307, its = 9
i = 231 (of 248), d = 7.60349, its = 10
i = 232 (of 248), d = 5.59124, its = 10
i = 233 (of 248), d = 6.95001, its = 10
i = 234 (of 248), d = 8.41291, its = 11
i = 235 (of 248), d = 3.9376, its = 9
i = 236 (of 248), d = 5.01979, its = 9
i = 237 (of 248), d = 8.54049, its = 9
i = 238 (of 248), d = 6.79209, its = 9
i = 239 (of 248), d = 4.48553, its = 9
i = 240 (of 248), d = 3.30462, its = 7
i = 241 (of 248), d = 6.65334, its = 9
i = 242 (of 248), d = 4.96061, its = 8
i = 243 (of 248), d = 6.51474, its = 9
i = 244 (of 248), d = 9.10625, its = 10
i = 245 (of 248), d = 5.75804, its = 8
i = 246 (of 248), d = 5.54249, its = 10
i = 247 (of 248), d = 5.07719, its = 9
i = 248 (of 248), d = 4.19741, its = 8
[1] "Fri Feb 09 13:52:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:52:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 13:52:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:52:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:52:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:52:14 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 2059.107360 
iter  10 value 57.549229
iter  20 value 5.931687
iter  30 value 1.884772
iter  40 value 1.598659
iter  50 value 1.462781
iter  60 value 1.248415
iter  70 value 1.106022
iter  80 value 1.010076
iter  90 value 0.931735
iter 100 value 0.871570
final  value 0.871570 
stopped after 100 iterations
# weights:  77
initial  value 694.288862 
iter  10 value 2.867235
iter  20 value 0.853913
iter  30 value 0.267385
iter  40 value 0.226277
iter  50 value 0.204254
iter  60 value 0.189725
iter  70 value 0.184750
iter  80 value 0.180560
iter  90 value 0.177605
iter 100 value 0.175211
final  value 0.175211 
stopped after 100 iterations
# weights:  77
initial  value 1151.493816 
iter  10 value 42.267469
iter  20 value 3.622895
iter  30 value 1.384762
iter  40 value 1.236641
iter  50 value 1.123246
iter  60 value 1.000910
iter  70 value 0.853605
iter  80 value 0.768480
iter  90 value 0.693700
iter 100 value 0.622928
final  value 0.622928 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00953; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 1086.036379 
iter  10 value 3.792822
iter  20 value 0.681447
iter  30 value 0.206917
iter  40 value 0.115046
iter  50 value 0.060307
iter  60 value 0.045519
iter  70 value 0.040893
iter  80 value 0.038120
iter  90 value 0.036723
iter 100 value 0.036147
final  value 0.036147 
stopped after 100 iterations
# weights:  53
initial  value 1742.010134 
iter  10 value 5.322500
iter  20 value 1.071291
iter  30 value 0.154854
iter  40 value 0.040985
iter  50 value 0.034966
iter  60 value 0.033425
iter  70 value 0.031997
iter  80 value 0.031182
iter  90 value 0.030051
iter 100 value 0.028628
final  value 0.028628 
stopped after 100 iterations
# weights:  53
initial  value 1065.781462 
iter  10 value 23.369837
iter  20 value 0.817760
iter  30 value 0.104800
iter  40 value 0.039096
iter  50 value 0.030680
iter  60 value 0.028382
iter  70 value 0.026659
iter  80 value 0.025392
iter  90 value 0.024758
iter 100 value 0.023907
final  value 0.023907 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00514; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 1029.991470 
iter  10 value 4.754137
iter  20 value 1.583951
iter  30 value 0.150017
iter  40 value 0.047310
iter  50 value 0.026864
iter  60 value 0.022419
iter  70 value 0.021567
iter  80 value 0.021249
iter  90 value 0.020563
iter 100 value 0.020319
final  value 0.020319 
stopped after 100 iterations
# weights:  49
initial  value 1090.191963 
iter  10 value 7.096662
iter  20 value 0.994473
iter  30 value 0.196270
iter  40 value 0.059492
iter  50 value 0.037454
iter  60 value 0.027536
iter  70 value 0.025334
iter  80 value 0.024774
iter  90 value 0.024225
iter 100 value 0.023770
final  value 0.023770 
stopped after 100 iterations
# weights:  49
initial  value 2184.755478 
iter  10 value 9.462232
iter  20 value 0.821824
iter  30 value 0.160337
iter  40 value 0.071208
iter  50 value 0.050472
iter  60 value 0.042625
iter  70 value 0.041063
iter  80 value 0.039919
iter  90 value 0.038950
iter 100 value 0.038020
final  value 0.038020 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00488; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 1107.226400 
iter  10 value 140.966703
iter  20 value 16.577704
iter  30 value 1.335484
iter  40 value 0.640204
iter  50 value 0.408120
iter  60 value 0.388795
iter  70 value 0.347603
iter  80 value 0.335613
iter  90 value 0.312168
iter 100 value 0.310345
final  value 0.310345 
stopped after 100 iterations
# weights:  9
initial  value 1009.986906 
iter  10 value 84.791684
iter  20 value 11.006711
iter  30 value 2.601348
iter  40 value 0.541720
iter  50 value 0.381695
iter  60 value 0.326395
iter  70 value 0.303887
iter  80 value 0.273654
iter  90 value 0.268617
iter 100 value 0.267607
final  value 0.267607 
stopped after 100 iterations
# weights:  9
initial  value 1157.010072 
iter  10 value 240.104858
iter  20 value 19.738081
iter  30 value 3.607821
iter  40 value 1.154979
iter  50 value 0.858623
iter  60 value 0.847632
final  value 0.847354 
converged
[Tune-y] 4: rmse.test.rmse=0.0198; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 982.423835 
iter  10 value 2.330919
iter  20 value 0.925802
iter  30 value 0.699364
iter  40 value 0.566546
iter  50 value 0.526863
iter  60 value 0.513618
iter  70 value 0.505544
iter  80 value 0.501783
iter  90 value 0.498688
iter 100 value 0.496131
final  value 0.496131 
stopped after 100 iterations
# weights:  77
initial  value 801.591598 
iter  10 value 3.643018
iter  20 value 0.818149
iter  30 value 0.647699
iter  40 value 0.572979
iter  50 value 0.510829
iter  60 value 0.486073
iter  70 value 0.475623
iter  80 value 0.470329
iter  90 value 0.467250
iter 100 value 0.464092
final  value 0.464092 
stopped after 100 iterations
# weights:  77
initial  value 1253.193197 
iter  10 value 4.123890
iter  20 value 0.799514
iter  30 value 0.565290
iter  40 value 0.521381
iter  50 value 0.495907
iter  60 value 0.475562
iter  70 value 0.458067
iter  80 value 0.448053
iter  90 value 0.439312
iter 100 value 0.431897
final  value 0.431897 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0132; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 1485.209561 
iter  10 value 138.873315
iter  20 value 33.723589
iter  30 value 22.551988
iter  40 value 21.598488
iter  50 value 21.428858
iter  60 value 21.345077
iter  70 value 21.314756
iter  80 value 21.255412
iter  90 value 21.163696
iter 100 value 21.148429
final  value 21.148429 
stopped after 100 iterations
# weights:  73
initial  value 983.060552 
iter  10 value 41.705338
iter  20 value 28.955057
iter  30 value 24.416697
iter  40 value 21.848414
iter  50 value 21.056635
iter  60 value 20.720161
iter  70 value 20.596543
iter  80 value 20.530347
iter  90 value 20.504006
iter 100 value 20.495256
final  value 20.495256 
stopped after 100 iterations
# weights:  73
initial  value 1157.731247 
iter  10 value 130.018150
iter  20 value 103.195258
iter  30 value 69.241981
iter  40 value 48.107833
iter  50 value 34.319120
iter  60 value 25.117862
iter  70 value 23.391322
iter  80 value 22.365620
iter  90 value 21.914673
iter 100 value 21.596030
final  value 21.596030 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0681; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 1459.705201 
iter  10 value 4.952543
iter  20 value 0.492068
iter  30 value 0.121467
iter  40 value 0.077426
iter  50 value 0.071057
iter  60 value 0.069273
iter  70 value 0.068052
iter  80 value 0.066555
iter  90 value 0.065373
iter 100 value 0.064555
final  value 0.064555 
stopped after 100 iterations
# weights:  53
initial  value 1296.270455 
iter  10 value 17.919930
iter  20 value 1.461913
iter  30 value 0.172466
iter  40 value 0.078682
iter  50 value 0.068048
iter  60 value 0.063756
iter  70 value 0.060746
iter  80 value 0.058179
iter  90 value 0.056561
iter 100 value 0.054258
final  value 0.054258 
stopped after 100 iterations
# weights:  53
initial  value 1170.522818 
iter  10 value 36.515587
iter  20 value 3.082403
iter  30 value 0.488654
iter  40 value 0.244527
iter  50 value 0.198781
iter  60 value 0.185568
iter  70 value 0.176464
iter  80 value 0.171013
iter  90 value 0.163819
iter 100 value 0.154294
final  value 0.154294 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00554; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 1496.918498 
iter  10 value 8.082356
iter  20 value 0.900954
iter  30 value 0.380656
iter  40 value 0.201222
iter  50 value 0.157564
iter  60 value 0.149989
iter  70 value 0.144829
iter  80 value 0.136115
iter  90 value 0.130106
iter 100 value 0.125553
final  value 0.125553 
stopped after 100 iterations
# weights:  33
initial  value 1498.116537 
iter  10 value 15.005380
iter  20 value 2.821170
iter  30 value 0.673950
iter  40 value 0.295972
iter  50 value 0.221420
iter  60 value 0.204107
iter  70 value 0.193090
iter  80 value 0.179436
iter  90 value 0.147199
iter 100 value 0.134075
final  value 0.134075 
stopped after 100 iterations
# weights:  33
initial  value 1243.913210 
iter  10 value 19.600566
iter  20 value 1.637378
iter  30 value 0.452185
iter  40 value 0.221176
iter  50 value 0.175036
iter  60 value 0.160471
iter  70 value 0.150108
iter  80 value 0.138220
iter  90 value 0.118947
iter 100 value 0.104482
final  value 0.104482 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0112; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 1310.946240 
iter  10 value 5.151788
iter  20 value 0.801360
iter  30 value 0.465762
iter  40 value 0.108209
iter  50 value 0.091919
iter  60 value 0.085653
iter  70 value 0.081845
iter  80 value 0.078850
iter  90 value 0.077302
iter 100 value 0.076376
final  value 0.076376 
stopped after 100 iterations
# weights:  77
initial  value 1394.087963 
iter  10 value 8.090627
iter  20 value 0.476955
iter  30 value 0.233715
iter  40 value 0.196121
iter  50 value 0.187137
iter  60 value 0.182342
iter  70 value 0.178128
iter  80 value 0.175071
iter  90 value 0.171714
iter 100 value 0.167026
final  value 0.167026 
stopped after 100 iterations
# weights:  77
initial  value 2163.444456 
iter  10 value 3.264707
iter  20 value 0.264820
iter  30 value 0.157660
iter  40 value 0.144582
iter  50 value 0.139809
iter  60 value 0.136201
iter  70 value 0.133903
iter  80 value 0.131920
iter  90 value 0.129561
iter 100 value 0.127297
final  value 0.127297 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00754; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 1061.474489 
iter  10 value 18.185548
iter  20 value 5.299294
iter  30 value 0.504591
iter  40 value 0.089288
iter  50 value 0.072724
iter  60 value 0.024978
iter  70 value 0.007461
iter  80 value 0.005442
iter  90 value 0.004885
iter 100 value 0.003930
final  value 0.003930 
stopped after 100 iterations
# weights:  21
initial  value 1052.005484 
iter  10 value 19.862313
iter  20 value 1.727485
iter  30 value 0.685568
iter  40 value 0.241623
iter  50 value 0.114454
iter  60 value 0.025899
iter  70 value 0.013349
iter  80 value 0.008297
iter  90 value 0.006775
iter 100 value 0.006097
final  value 0.006097 
stopped after 100 iterations
# weights:  21
initial  value 863.706047 
iter  10 value 17.207416
iter  20 value 2.027476
iter  30 value 0.259192
iter  40 value 0.075081
iter  50 value 0.046068
iter  60 value 0.010709
iter  70 value 0.002398
iter  80 value 0.002306
iter  90 value 0.002276
iter 100 value 0.002263
final  value 0.002263 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00312; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 1585.462442 
iter  10 value 151.987302
iter  20 value 128.333279
iter  30 value 87.640022
iter  40 value 68.295014
iter  50 value 58.093791
iter  60 value 49.377116
iter  70 value 39.780501
iter  80 value 30.520066
iter  90 value 22.879129
iter 100 value 19.752647
final  value 19.752647 
stopped after 100 iterations
# weights:  73
initial  value 1423.586579 
iter  10 value 146.538498
iter  20 value 98.456792
iter  30 value 68.712666
iter  40 value 50.908883
iter  50 value 38.269957
iter  60 value 27.750448
iter  70 value 21.856299
iter  80 value 19.411229
iter  90 value 18.006728
iter 100 value 17.323147
final  value 17.323147 
stopped after 100 iterations
# weights:  73
initial  value 1585.104595 
iter  10 value 40.789325
iter  20 value 25.400130
iter  30 value 21.832171
iter  40 value 19.510876
iter  50 value 18.431008
iter  60 value 17.976552
iter  70 value 17.651786
iter  80 value 17.366120
iter  90 value 17.187916
iter 100 value 17.127994
final  value 17.127994 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0663; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 1104.659273 
iter  10 value 24.479428
iter  20 value 19.146469
iter  30 value 18.228801
iter  40 value 17.954756
iter  50 value 17.843255
iter  60 value 17.790318
iter  70 value 17.782345
iter  80 value 17.773900
iter  90 value 17.772726
iter 100 value 17.772669
final  value 17.772669 
stopped after 100 iterations
# weights:  45
initial  value 1336.885511 
iter  10 value 107.793382
iter  20 value 71.591326
iter  30 value 46.260263
iter  40 value 32.465177
iter  50 value 24.151489
iter  60 value 19.855897
iter  70 value 17.544687
iter  80 value 16.493083
iter  90 value 16.075872
iter 100 value 15.895184
final  value 15.895184 
stopped after 100 iterations
# weights:  45
initial  value 2001.820601 
iter  10 value 37.148107
iter  20 value 29.879572
iter  30 value 23.767454
iter  40 value 20.443559
iter  50 value 18.591116
iter  60 value 17.687894
iter  70 value 17.131882
iter  80 value 16.918825
iter  90 value 16.885791
iter 100 value 16.868036
final  value 16.868036 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0813; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 1541.413041 
iter  10 value 59.909412
iter  20 value 4.057515
iter  30 value 2.078402
iter  40 value 1.929045
iter  50 value 1.853391
iter  60 value 1.644874
iter  70 value 1.557734
iter  80 value 1.513305
iter  90 value 1.500019
iter 100 value 1.440993
final  value 1.440993 
stopped after 100 iterations
# weights:  21
initial  value 1297.864914 
iter  10 value 23.644390
iter  20 value 3.597794
iter  30 value 1.980136
iter  40 value 1.774696
iter  50 value 1.686268
iter  60 value 1.591392
iter  70 value 1.519615
iter  80 value 1.462777
iter  90 value 1.433249
iter 100 value 1.323601
final  value 1.323601 
stopped after 100 iterations
# weights:  21
initial  value 1300.626581 
iter  10 value 15.541171
iter  20 value 3.089289
iter  30 value 1.949472
iter  40 value 1.820981
iter  50 value 1.726592
iter  60 value 1.419845
iter  70 value 1.356048
iter  80 value 1.349702
iter  90 value 1.346892
iter 100 value 1.338839
final  value 1.338839 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0272; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1722.065862 
iter  10 value 8.315839
iter  20 value 1.630587
iter  30 value 0.650936
iter  40 value 0.529499
iter  50 value 0.487862
iter  60 value 0.471360
iter  70 value 0.443240
iter  80 value 0.428776
iter  90 value 0.413054
iter 100 value 0.400970
final  value 0.400970 
stopped after 100 iterations
# weights:  53
initial  value 1039.115573 
iter  10 value 2.437287
iter  20 value 0.671866
iter  30 value 0.279183
iter  40 value 0.233589
iter  50 value 0.220739
iter  60 value 0.210607
iter  70 value 0.202323
iter  80 value 0.196008
iter  90 value 0.192690
iter 100 value 0.190730
final  value 0.190730 
stopped after 100 iterations
# weights:  53
initial  value 1927.826321 
iter  10 value 13.633691
iter  20 value 1.619092
iter  30 value 0.557957
iter  40 value 0.447035
iter  50 value 0.396045
iter  60 value 0.365846
iter  70 value 0.346563
iter  80 value 0.332082
iter  90 value 0.317887
iter 100 value 0.311934
final  value 0.311934 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0105; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 2006.203332 
iter  10 value 6.921526
iter  20 value 3.342837
iter  30 value 2.857376
iter  40 value 2.735340
iter  50 value 2.623935
iter  60 value 2.547754
iter  70 value 2.485358
iter  80 value 2.455601
iter  90 value 2.443546
iter 100 value 2.412961
final  value 2.412961 
stopped after 100 iterations
# weights:  45
initial  value 1932.629383 
iter  10 value 5.734800
iter  20 value 2.818411
iter  30 value 2.614702
iter  40 value 2.518379
iter  50 value 2.412418
iter  60 value 2.350500
iter  70 value 2.320109
iter  80 value 2.287207
iter  90 value 2.248406
iter 100 value 2.159986
final  value 2.159986 
stopped after 100 iterations
# weights:  45
initial  value 1688.311840 
iter  10 value 6.617459
iter  20 value 3.579270
iter  30 value 3.112264
iter  40 value 2.907971
iter  50 value 2.677762
iter  60 value 2.553482
iter  70 value 2.478045
iter  80 value 2.390086
iter  90 value 2.353787
iter 100 value 2.232972
final  value 2.232972 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0292; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 1106.984455 
iter  10 value 10.192187
iter  20 value 0.625732
iter  30 value 0.190932
iter  40 value 0.150462
iter  50 value 0.140391
iter  60 value 0.135286
iter  70 value 0.132098
iter  80 value 0.128760
iter  90 value 0.121562
iter 100 value 0.116493
final  value 0.116493 
stopped after 100 iterations
# weights:  37
initial  value 1078.815280 
iter  10 value 9.866866
iter  20 value 0.753074
iter  30 value 0.341161
iter  40 value 0.191683
iter  50 value 0.172262
iter  60 value 0.168159
iter  70 value 0.165848
iter  80 value 0.164220
iter  90 value 0.152990
iter 100 value 0.146094
final  value 0.146094 
stopped after 100 iterations
# weights:  37
initial  value 1713.633573 
iter  10 value 9.357630
iter  20 value 1.842008
iter  30 value 0.571120
iter  40 value 0.266346
iter  50 value 0.200716
iter  60 value 0.175075
iter  70 value 0.166516
iter  80 value 0.162827
iter  90 value 0.148681
iter 100 value 0.135770
final  value 0.135770 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.00995; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 1281.997037 
iter  10 value 44.860136
iter  20 value 36.274694
iter  30 value 29.994633
iter  40 value 26.795823
iter  50 value 25.346467
iter  60 value 25.167241
iter  70 value 25.154971
final  value 25.154952 
converged
# weights:  21
initial  value 1128.881837 
iter  10 value 66.061203
iter  20 value 44.313343
iter  30 value 30.510187
iter  40 value 25.052459
iter  50 value 22.730042
iter  60 value 22.320487
iter  70 value 22.292485
final  value 22.291650 
converged
# weights:  21
initial  value 1452.810106 
iter  10 value 31.008639
iter  20 value 26.603197
iter  30 value 25.309322
iter  40 value 24.458726
iter  50 value 24.362780
iter  60 value 24.355658
final  value 24.355654 
converged
[Tune-y] 17: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 1213.768466 
iter  10 value 4.788184
iter  20 value 0.734387
iter  30 value 0.089692
iter  40 value 0.017335
iter  50 value 0.003790
iter  60 value 0.001707
iter  70 value 0.001276
iter  80 value 0.001150
iter  90 value 0.001115
iter 100 value 0.001101
final  value 0.001101 
stopped after 100 iterations
# weights:  57
initial  value 1864.814907 
iter  10 value 11.844101
iter  20 value 0.834975
iter  30 value 0.069324
iter  40 value 0.026409
iter  50 value 0.010858
iter  60 value 0.006455
iter  70 value 0.003701
iter  80 value 0.002664
iter  90 value 0.001714
iter 100 value 0.001434
final  value 0.001434 
stopped after 100 iterations
# weights:  57
initial  value 2018.579107 
iter  10 value 9.607902
iter  20 value 1.447153
iter  30 value 0.240727
iter  40 value 0.071023
iter  50 value 0.018220
iter  60 value 0.007226
iter  70 value 0.003390
iter  80 value 0.002292
iter  90 value 0.002007
iter 100 value 0.001806
final  value 0.001806 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00173; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 1238.681761 
iter  10 value 33.615867
iter  20 value 4.551308
iter  30 value 1.227926
iter  40 value 1.055282
iter  50 value 0.725004
iter  60 value 0.541132
iter  70 value 0.431319
iter  80 value 0.365798
iter  90 value 0.284451
iter 100 value 0.228674
final  value 0.228674 
stopped after 100 iterations
# weights:  17
initial  value 764.088289 
iter  10 value 56.144878
iter  20 value 2.696033
iter  30 value 1.361862
iter  40 value 0.588420
iter  50 value 0.300857
iter  60 value 0.250688
iter  70 value 0.188419
iter  80 value 0.176533
iter  90 value 0.161927
iter 100 value 0.147170
final  value 0.147170 
stopped after 100 iterations
# weights:  17
initial  value 1222.679732 
iter  10 value 12.113847
iter  20 value 2.810467
iter  30 value 0.585859
iter  40 value 0.205238
iter  50 value 0.144858
iter  60 value 0.139220
iter  70 value 0.136970
iter  80 value 0.135730
iter  90 value 0.130311
iter 100 value 0.127138
final  value 0.127138 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0108; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 1122.869512 
iter  10 value 6.392482
iter  20 value 2.271157
iter  30 value 1.995862
iter  40 value 1.802020
iter  50 value 1.664122
iter  60 value 1.623160
iter  70 value 1.539198
iter  80 value 1.470679
iter  90 value 1.443825
iter 100 value 1.403907
final  value 1.403907 
stopped after 100 iterations
# weights:  29
initial  value 1381.100484 
iter  10 value 47.076578
iter  20 value 4.244438
iter  30 value 2.912317
iter  40 value 2.340347
iter  50 value 1.989426
iter  60 value 1.771840
iter  70 value 1.560357
iter  80 value 1.378000
iter  90 value 1.317789
iter 100 value 1.303498
final  value 1.303498 
stopped after 100 iterations
# weights:  29
initial  value 1011.632898 
iter  10 value 12.881202
iter  20 value 4.780186
iter  30 value 2.288219
iter  40 value 1.614330
iter  50 value 1.517720
iter  60 value 1.477613
iter  70 value 1.404486
iter  80 value 1.372737
iter  90 value 1.361898
iter 100 value 1.348418
final  value 1.348418 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0256; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00173
# weights:  57
initial  value 1762.998025 
iter  10 value 7.224220
iter  20 value 0.835011
iter  30 value 0.306911
iter  40 value 0.115156
iter  50 value 0.038095
iter  60 value 0.021285
iter  70 value 0.009324
iter  80 value 0.006584
iter  90 value 0.005337
iter 100 value 0.004447
final  value 0.004447 
stopped after 100 iterations
[1] "Fri Feb 09 13:52:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 685 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 412
 number of selected nodes                               : 78 
[1] "Fri Feb 09 13:52:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:52:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:53:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:53:09 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.168; time: 0.1 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.165
[1] "Fri Feb 09 13:54:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 13:54:08 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.861; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.897; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.61; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.625; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.677; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.799; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.603; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.588; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=1.05; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.511
[1] "Fri Feb 09 13:54:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 13:54:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:54:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7240 0.3270 0.1480 0.0671 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01800084 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1482 
    Y 1           0.31 
    Y 2       -0.04699 
    Y 3         -2.103 
    Y 4        -0.4739 
    Y 5         -0.282 
[1] "Fri Feb 09 13:54:47 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.274; time: 0.6 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.268; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:55:51] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.497; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.306; time: 0.6 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.357; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:29] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.361; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.402; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:33] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:33] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:33] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.324; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:34] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:34] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:34] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [13:56:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.268
[1] "Fri Feb 09 13:56:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 13:56:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.488812], n=(300,452)
**GROW** @depth 1: [1,0.500114], n=(160,140)
**GROW** @depth 1: [2,0.567883], n=(156,296)
**GROW** @depth 2: [2,0.246122], n=(14,146)
**GROW** @depth 2: [1,0.230413], n=(16,235)
**GROW** @depth 2: [2,0.27937], n=(19,194)
**GROW** @depth 3: [2,0.414448], n=(58,136)
**GROW** @depth 2: [1,0.669075], n=(91,36)
**GROW** @depth 3: [2,0.359012], n=(44,102)
**GROW** @depth 4: [1,0.582686], n=(35,56)
**GROW** @depth 5: [2,0.775259], n=(44,13)
**GROW** @depth 3: [1,0.445423], n=(163,72)
**PRUNE** @depth 3: [2,0.27937]
**GROW** @depth 4: [2,0.541703], n=(34,129)
**GROW** @depth 5: [1,0.34451], n=(51,79)
**GROW** @depth 4: [2,0.734512], n=(54,15)
**GROW** @depth 6: [2,0.643687], n=(65,42)
**PRUNE** @depth 4: [1,0.447758]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(13,44,106,16,47,40,65,42,40,80,133,34,42,31,19)
**PRUNE** @depth 3: [2,0.242253]
**GROW** @depth 3: [2,0.494335], n=(66,67)
**GROW** @depth 4: [2,0.85143], n=(29,11)
**PRUNE** @depth 4: [2,0.85143]
**GROW** @depth 3: [1,0.60441], n=(47,33)
**GROW** @depth 7: [2,0.587465], n=(35,30)
**GROW** @depth 5: [2,0.642619], n=(27,13)
**GROW** @depth 8: [1,0.423613], n=(23,12)
**PRUNE** @depth 8: [1,0.423613]
**GROW** @depth 4: [1,0.379269], n=(15,25)
**GROW** @depth 4: [2,0.493875], n=(41,37)
**GROW** @depth 4: [2,0.328014], n=(22,25)
**GROW** @depth 7: [1,0.425952], n=(23,12)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(57,104,16,49,40,23,12,32,40,15,25,22,25,40,37,57,32,36,27,15,28,20)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [2,0.640482], n=(16,20)
**PRUNE** @depth 8: [2,0.58777]
**GROW** @depth 6: [2,0.633917], n=(27,13)
**GROW** @depth 4: [1,0.685004], n=(31,25)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(57,106,16,46,27,13,37,32,39,15,25,23,25,41,37,30,16,41,15,20,28,14,27,22)
**GROW** @depth 3: [2,0.431058], n=(63,45)
**GROW** @depth 8: [2,0.583794], n=(21,17)
**GROW** @depth 8: [2,0.579205], n=(12,21)
**PRUNE** @depth 3: [2,0.431673]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(54,111,16,41,29,13,20,17,13,21,39,15,25,23,24,41,37,29,17,42,16,20,26,14,27,22)
**GROW** @depth 7: [1,0.41722], n=(18,21)
**PRUNE** @depth 7: [1,0.41722]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(53,109,16,43,27,13,20,17,13,21,39,15,24,23,26,41,38,30,16,42,15,21,27,13,29,21)
**PRUNE** @depth 6: [2,0.633917]
**PRUNE** @depth 4: [1,0.382709]
**GROW** @depth 4: [1,0.355322], n=(14,24)
**GROW** @depth 5: [2,0.598779], n=(26,26)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(53,109,16,26,26,31,20,17,13,22,37,14,24,25,24,43,38,26,17,43,15,23,27,18,24,21)
**PRUNE** @depth 8: [2,0.584253]
**GROW** @depth 3: [1,0.350152], n=(40,68)
**GROW** @depth 7: [1,0.417844], n=(19,19)
**PRUNE** @depth 7: [1,0.417844]
**GROW** @depth 6: [1,0.422054], n=(17,15)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(52,37,72,16,24,27,17,15,37,13,20,39,14,24,26,24,42,38,29,16,41,16,23,26,19,24,21)
Grow: 11.52%, Prune: 3.571%, Change: 83.13%, Swap: 17.28%

[1] "Fri Feb 09 13:56:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 13:56:52 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 13:56:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9364 	 alpha= 0.9037 	 beta= 169076.7 
[1] "Fri Feb 09 13:56:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 13:56:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [2,0.517186], n=(353,399)
**GROW** @depth 1: [1,0.487392], n=(183,170)
**GROW** @depth 2: [2,0.252929], n=(14,157)
**GROW** @depth 1: [2,0.761126], n=(339,58)
**GROW** @depth 3: [1,0.226621], n=(15,152)
**GROW** @depth 2: [1,0.500114], n=(32,27)
**GROW** @depth 3: [2,0.389361], n=(52,107)
**GROW** @depth 2: [1,0.500114], n=(15,14)
**GROW** @depth 4: [2,0.387051], n=(51,102)
**PRUNE** @depth 2: [1,0.50151]
**GROW** @depth 2: [2,0.784678], n=(192,28)
**GROW** @depth 3: [2,0.655895], n=(133,59)
**GROW** @depth 4: [2,0.673892], n=(120,29)
**GROW** @depth 5: [1,0.241311], n=(13,120)
**PRUNE** @depth 5: [1,0.241311]
**GROW** @depth 5: [2,0.583794], n=(62,58)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(27,17,49,104,54,105,133,60,27,63,57,30,26)
**GROW** @depth 5: [1,0.384116], n=(22,38)
**GROW** @depth 5: [1,0.243205], n=(15,119)
**GROW** @depth 5: [2,0.585171], n=(66,53)
**GROW** @depth 5: [1,0.349525], n=(30,74)
**GROW** @depth 5: [2,0.318446], n=(21,33)
**GROW** @depth 2: [1,0.507556], n=(15,12)
**PRUNE** @depth 4: [2,0.314433]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,12,18,46,33,74,53,105,16,65,52,21,38,28,63,58,28,27)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 2: [1,0.507401]
**GROW** @depth 2: [1,0.483201], n=(14,13)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(14,13,16,51,33,70,52,105,19,65,50,20,41,26,63,58,31,25)
**GROW** @depth 6: [2,0.318292], n=(21,31)
**GROW** @depth 5: [2,0.444431], n=(41,63)
**PRUNE** @depth 5: [2,0.444431]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(14,13,16,52,33,68,21,30,105,19,66,50,22,42,23,64,57,33,24)
**PRUNE** @depth 5: [2,0.31644]
**PRUNE** @depth 5: [2,0.58777]
**PRUNE** @depth 5: [2,0.657268]
**GROW** @depth 4: [1,0.689748], n=(91,14)
**PRUNE** @depth 4: [1,0.689748]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(14,11,16,52,33,66,52,105,28,72,100,25,65,57,37,19)
**GROW** @depth 6: [2,0.302544], n=(19,31)
**PRUNE** @depth 5: [2,0.302544]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(14,13,15,53,33,64,49,104,29,71,103,23,68,55,38,20)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(14,13,17,51,33,64,50,103,30,70,103,23,68,55,38,20)
Grow: 7.418%, Prune: 2.695%, Change: 66.81%, Swap: 32.04%

[1] "Fri Feb 09 13:56:59 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 13:57:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 13:57:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 13:57:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 13:57:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 13:57:04 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 13:57:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 13:57:06 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 13:57:09 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.302; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.854; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.241; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.285; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.485; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.264; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.474; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.317; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.245; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.283; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.345; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.294; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.823; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.228; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.158; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.65; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.22; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.234; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.158
[1] "Fri Feb 09 13:57:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 13:57:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0388; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.126; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0254; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.162; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0885; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00791; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0513; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.414; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00845; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0149; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0216; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0164; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.185; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0115; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00791
[1] "Fri Feb 09 13:57:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:57:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:57:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:57:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============                                                       |  22%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 13:57:39 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 662 is not positive definite

[1] "Fri Feb 09 13:57:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 4.46292, its = 8
i = 2 (of 248), d = 5.42892, its = 9
i = 3 (of 248), d = 5.18975, its = 10
i = 4 (of 248), d = 6.11893, its = 9
i = 5 (of 248), d = 6.26578, its = 11
i = 6 (of 248), d = 13.4729, its = 11
i = 7 (of 248), d = 6.64006, its = 11
i = 8 (of 248), d = 5.0299, its = 8
i = 9 (of 248), d = 2.82768, its = 8
i = 10 (of 248), d = 2.99057, its = 8
i = 11 (of 248), d = 3.90987, its = 9
i = 12 (of 248), d = 5.60345, its = 10
i = 13 (of 248), d = 5.61381, its = 10
i = 14 (of 248), d = 10.9405, its = 11
i = 15 (of 248), d = 6.01632, its = 9
i = 16 (of 248), d = 6.31502, its = 18
i = 17 (of 248), d = 4.15037, its = 9
i = 18 (of 248), d = 4.88668, its = 10
i = 19 (of 248), d = 7.08884, its = 10
i = 20 (of 248), d = 8.18563, its = 10
i = 21 (of 248), d = 4.53158, its = 9
i = 22 (of 248), d = 6.40838, its = 9
i = 23 (of 248), d = 5.85263, its = 9
i = 24 (of 248), d = 4.79378, its = 9
i = 25 (of 248), d = 4.80055, its = 26
i = 26 (of 248), d = 8.79316, its = 11
i = 27 (of 248), d = 15.5006, its = 14
i = 28 (of 248), d = 3.39118, its = 8
i = 29 (of 248), d = 6.1577, its = 10
i = 30 (of 248), d = 2.98686, its = 8
i = 31 (of 248), d = 7.22088, its = 9
i = 32 (of 248), d = 12.0828, its = 11
i = 33 (of 248), d = 6.63433, its = 10
i = 34 (of 248), d = 2.9096, its = 8
i = 35 (of 248), d = 3.03231, its = 7
i = 36 (of 248), d = 7.24404, its = 10
i = 37 (of 248), d = 11.3168, its = 11
i = 38 (of 248), d = 3.87692, its = 10
i = 39 (of 248), d = 10.4003, its = 10
i = 40 (of 248), d = 5.85564, its = 9
i = 41 (of 248), d = 8.4039, its = 10
i = 42 (of 248), d = 7.10968, its = 10
i = 43 (of 248), d = 5.29845, its = 9
i = 44 (of 248), d = 7.71764, its = 10
i = 45 (of 248), d = 3.54431, its = 8
i = 46 (of 248), d = 5.66892, its = 9
i = 47 (of 248), d = 7.26808, its = 10
i = 48 (of 248), d = 9.37632, its = 10
i = 49 (of 248), d = 7.91166, its = 10
i = 50 (of 248), d = 5.42822, its = 9
i = 51 (of 248), d = 4.02298, its = 9
i = 52 (of 248), d = 7.72477, its = 10
i = 53 (of 248), d = 5.71328, its = 9
i = 54 (of 248), d = 9.47428, its = 10
i = 55 (of 248), d = 9.45178, its = 11
i = 56 (of 248), d = 7.9265, its = 10
i = 57 (of 248), d = 7.6185, its = 10
i = 58 (of 248), d = 13.099, its = 11
i = 59 (of 248), d = 4.15979, its = 27
i = 60 (of 248), d = 7.82444, its = 10
i = 61 (of 248), d = 4.22455, its = 9
i = 62 (of 248), d = 6.61227, its = 11
i = 63 (of 248), d = 4.20632, its = 9
i = 64 (of 248), d = 4.32852, its = 26
i = 65 (of 248), d = 4.71616, its = 9
i = 66 (of 248), d = 3.07494, its = 9
i = 67 (of 248), d = 2.86771, its = 8
i = 68 (of 248), d = 4.33799, its = 9
i = 69 (of 248), d = 4.74011, its = 9
i = 70 (of 248), d = 12.5029, its = 12
i = 71 (of 248), d = 2.9123, its = 9
i = 72 (of 248), d = 7.65189, its = 11
i = 73 (of 248), d = 10.5756, its = 11
i = 74 (of 248), d = 6.90795, its = 10
i = 75 (of 248), d = 9.53151, its = 10
i = 76 (of 248), d = 11.1589, its = 11
i = 77 (of 248), d = 9.70406, its = 10
i = 78 (of 248), d = 3.64706, its = 8
i = 79 (of 248), d = 5.8599, its = 10
i = 80 (of 248), d = 7.45009, its = 9
i = 81 (of 248), d = 4.19255, its = 9
i = 82 (of 248), d = 4.86839, its = 8
i = 83 (of 248), d = 7.3174, its = 10
i = 84 (of 248), d = 11.181, its = 11
i = 85 (of 248), d = 3.68567, its = 8
i = 86 (of 248), d = 5.62444, its = 10
i = 87 (of 248), d = 7.04347, its = 10
i = 88 (of 248), d = 13.6155, its = 11
i = 89 (of 248), d = 3.99712, its = 8
i = 90 (of 248), d = 9.44415, its = 11
i = 91 (of 248), d = 3.07787, its = 9
i = 92 (of 248), d = 8.11041, its = 10
i = 93 (of 248), d = 3.72884, its = 8
i = 94 (of 248), d = 10.1241, its = 11
i = 95 (of 248), d = 8.98837, its = 11
i = 96 (of 248), d = 8.15696, its = 10
i = 97 (of 248), d = 6.04672, its = 10
i = 98 (of 248), d = 7.74238, its = 9
i = 99 (of 248), d = 4.38707, its = 8
i = 100 (of 248), d = 4.49659, its = 9
i = 101 (of 248), d = 7.55923, its = 10
i = 102 (of 248), d = 8.8127, its = 10
i = 103 (of 248), d = 6.86597, its = 10
i = 104 (of 248), d = 4.22393, its = 9
i = 105 (of 248), d = 8.28909, its = 10
i = 106 (of 248), d = 6.23872, its = 9
i = 107 (of 248), d = 5.06003, its = 30
i = 108 (of 248), d = 3.73163, its = 9
i = 109 (of 248), d = 7.54388, its = 10
i = 110 (of 248), d = 2.95786, its = 8
i = 111 (of 248), d = 8.82252, its = 10
i = 112 (of 248), d = 8.57423, its = 10
i = 113 (of 248), d = 10.6717, its = 11
i = 114 (of 248), d = 3.88195, its = 9
i = 115 (of 248), d = 9.24743, its = 10
i = 116 (of 248), d = 5.00912, its = 9
i = 117 (of 248), d = 5.39284, its = 9
i = 118 (of 248), d = 9.95487, its = 11
i = 119 (of 248), d = 4.51062, its = 9
i = 120 (of 248), d = 5.141, its = 9
i = 121 (of 248), d = 7.96075, its = 12
i = 122 (of 248), d = 5.58993, its = 9
i = 123 (of 248), d = 4.39023, its = 9
i = 124 (of 248), d = 5.86406, its = 10
i = 125 (of 248), d = 12.5528, its = 11
i = 126 (of 248), d = 3.03933, its = 8
i = 127 (of 248), d = 3.71762, its = 9
i = 128 (of 248), d = 6.06075, its = 10
i = 129 (of 248), d = 5.59458, its = 10
i = 130 (of 248), d = 3.01143, its = 8
i = 131 (of 248), d = 3.36008, its = 8
i = 132 (of 248), d = 8.37083, its = 10
i = 133 (of 248), d = 5.68837, its = 9
i = 134 (of 248), d = 9.96501, its = 11
i = 135 (of 248), d = 4.22315, its = 31
i = 136 (of 248), d = 10.4597, its = 11
i = 137 (of 248), d = 7.74072, its = 10
i = 138 (of 248), d = 4.51957, its = 10
i = 139 (of 248), d = 5.39615, its = 9
i = 140 (of 248), d = 6.24042, its = 37
i = 141 (of 248), d = 8.04553, its = 11
i = 142 (of 248), d = 3.67774, its = 9
i = 143 (of 248), d = 3.45541, its = 9
i = 144 (of 248), d = 4.60702, its = 9
i = 145 (of 248), d = 16.6667, its = 13
i = 146 (of 248), d = 8.20964, its = 11
i = 147 (of 248), d = 6.55854, its = 9
i = 148 (of 248), d = 8.60539, its = 9
i = 149 (of 248), d = 3.7783, its = 8
i = 150 (of 248), d = 3.92468, its = 8
i = 151 (of 248), d = 3.72735, its = 29
i = 152 (of 248), d = 8.6359, its = 11
i = 153 (of 248), d = 6.41592, its = 10
i = 154 (of 248), d = 6.33672, its = 11
i = 155 (of 248), d = 2.89722, its = 8
i = 156 (of 248), d = 3.55269, its = 8
i = 157 (of 248), d = 6.85492, its = 10
i = 158 (of 248), d = 7.79349, its = 11
i = 159 (of 248), d = 5.57659, its = 10
i = 160 (of 248), d = 6.39517, its = 10
i = 161 (of 248), d = 4.81654, its = 10
i = 162 (of 248), d = 11.1559, its = 12
i = 163 (of 248), d = 5.0728, its = 10
i = 164 (of 248), d = 7.99983, its = 10
i = 165 (of 248), d = 7.69503, its = 10
i = 166 (of 248), d = 7.84181, its = 9
i = 167 (of 248), d = 5.41527, its = 10
i = 168 (of 248), d = 4.3794, its = 10
i = 169 (of 248), d = 3.1328, its = 9
i = 170 (of 248), d = 2.7488, its = 8
i = 171 (of 248), d = 6.5397, its = 9
i = 172 (of 248), d = 8.88673, its = 9
i = 173 (of 248), d = 7.31971, its = 10
i = 174 (of 248), d = 19.2121, its = 12
i = 175 (of 248), d = 7.30955, its = 10
i = 176 (of 248), d = 8.34265, its = 10
i = 177 (of 248), d = 3.8838, its = 9
i = 178 (of 248), d = 4.89614, its = 9
i = 179 (of 248), d = 3.78355, its = 9
i = 180 (of 248), d = 4.68271, its = 9
i = 181 (of 248), d = 4.69621, its = 10
i = 182 (of 248), d = 6.36782, its = 11
i = 183 (of 248), d = 10.2392, its = 10
i = 184 (of 248), d = 7.0686, its = 10
i = 185 (of 248), d = 8.34899, its = 9
i = 186 (of 248), d = 3.07269, its = 8
i = 187 (of 248), d = 6.98279, its = 10
i = 188 (of 248), d = 5.3462, its = 9
i = 189 (of 248), d = 2.86775, its = 8
i = 190 (of 248), d = 3.16531, its = 10
i = 191 (of 248), d = 4.17526, its = 9
i = 192 (of 248), d = 8.07769, its = 10
i = 193 (of 248), d = 8.7031, its = 10
i = 194 (of 248), d = 9.58475, its = 11
i = 195 (of 248), d = 5.56867, its = 9
i = 196 (of 248), d = 5.79398, its = 10
i = 197 (of 248), d = 11.9683, its = 11
i = 198 (of 248), d = 4.62356, its = 8
i = 199 (of 248), d = 12.7379, its = 12
i = 200 (of 248), d = 3.90352, its = 8
i = 201 (of 248), d = 9.66487, its = 11
i = 202 (of 248), d = 3.56841, its = 8
i = 203 (of 248), d = 4.22551, its = 9
i = 204 (of 248), d = 2.93558, its = 8
i = 205 (of 248), d = 7.90038, its = 9
i = 206 (of 248), d = 8.79258, its = 11
i = 207 (of 248), d = 9.8236, its = 10
i = 208 (of 248), d = 6.87174, its = 9
i = 209 (of 248), d = 8.74256, its = 11
i = 210 (of 248), d = 6.61552, its = 11
i = 211 (of 248), d = 6.44212, its = 10
i = 212 (of 248), d = 4.55518, its = 9
i = 213 (of 248), d = 7.12438, its = 11
i = 214 (of 248), d = 6.80217, its = 11
i = 215 (of 248), d = 9.07234, its = 11
i = 216 (of 248), d = 11.3194, its = 11
i = 217 (of 248), d = 8.16463, its = 10
i = 218 (of 248), d = 5.73416, its = 9
i = 219 (of 248), d = 8.83681, its = 10
i = 220 (of 248), d = 6.93821, its = 9
i = 221 (of 248), d = 23.9234, its = 29
i = 222 (of 248), d = 7.51358, its = 10
i = 223 (of 248), d = 5.16893, its = 9
i = 224 (of 248), d = 9.45781, its = 11
i = 225 (of 248), d = 11.2766, its = 10
i = 226 (of 248), d = 12.4344, its = 11
i = 227 (of 248), d = 8.9385, its = 11
i = 228 (of 248), d = 3.61196, its = 8
i = 229 (of 248), d = 3.32622, its = 9
i = 230 (of 248), d = 5.4832, its = 10
i = 231 (of 248), d = 11.6065, its = 11
i = 232 (of 248), d = 7.59441, its = 10
i = 233 (of 248), d = 6.89873, its = 10
i = 234 (of 248), d = 4.08305, its = 9
i = 235 (of 248), d = 6.23523, its = 10
i = 236 (of 248), d = 7.44404, its = 10
i = 237 (of 248), d = 11.9827, its = 10
i = 238 (of 248), d = 3.44502, its = 9
i = 239 (of 248), d = 4.33358, its = 9
i = 240 (of 248), d = 5.22399, its = 28
i = 241 (of 248), d = 7.1624, its = 9
i = 242 (of 248), d = 6.39925, its = 10
i = 243 (of 248), d = 5.48934, its = 9
i = 244 (of 248), d = 8.70202, its = 10
i = 245 (of 248), d = 4.19366, its = 9
i = 246 (of 248), d = 7.14053, its = 10
i = 247 (of 248), d = 5.38266, its = 10
i = 248 (of 248), d = 6.05034, its = 10
[1] "Fri Feb 09 13:58:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 13:58:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 13:58:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 13:58:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 13:58:58 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 1062.015955 
iter  10 value 129.118596
iter  20 value 17.681636
iter  30 value 7.365067
iter  40 value 6.977657
final  value 6.976731 
converged
# weights:  5
initial  value 1048.493571 
iter  10 value 56.280696
iter  20 value 11.224203
iter  30 value 6.636160
final  value 6.503986 
converged
# weights:  5
initial  value 995.986527 
iter  10 value 48.139850
iter  20 value 12.852194
iter  30 value 7.934508
iter  40 value 7.640198
final  value 7.640038 
converged
[Tune-y] 1: rmse.test.rmse=0.0801; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 796.067212 
iter  10 value 7.727185
iter  20 value 0.843791
iter  30 value 0.149388
iter  40 value 0.111398
iter  50 value 0.103063
iter  60 value 0.100477
iter  70 value 0.098245
iter  80 value 0.095806
iter  90 value 0.094556
iter 100 value 0.093431
final  value 0.093431 
stopped after 100 iterations
# weights:  57
initial  value 1176.588613 
iter  10 value 16.932737
iter  20 value 1.024775
iter  30 value 0.288818
iter  40 value 0.222529
iter  50 value 0.201052
iter  60 value 0.194154
iter  70 value 0.187738
iter  80 value 0.178844
iter  90 value 0.173839
iter 100 value 0.167428
final  value 0.167428 
stopped after 100 iterations
# weights:  57
initial  value 3735.072013 
iter  10 value 16.569407
iter  20 value 1.367102
iter  30 value 0.338091
iter  40 value 0.287296
iter  50 value 0.268011
iter  60 value 0.256627
iter  70 value 0.245555
iter  80 value 0.236207
iter  90 value 0.227467
iter 100 value 0.221202
final  value 0.221202 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0148; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 1304.489237 
iter  10 value 7.446251
iter  20 value 1.911648
iter  30 value 1.362350
iter  40 value 1.269529
iter  50 value 1.209902
iter  60 value 1.148493
iter  70 value 1.067435
iter  80 value 1.009677
iter  90 value 0.979587
iter 100 value 0.761075
final  value 0.761075 
stopped after 100 iterations
# weights:  41
initial  value 2209.467834 
iter  10 value 7.545414
iter  20 value 0.990132
iter  30 value 0.783530
iter  40 value 0.746463
iter  50 value 0.731216
iter  60 value 0.714290
iter  70 value 0.698203
iter  80 value 0.685975
iter  90 value 0.674107
iter 100 value 0.638876
final  value 0.638876 
stopped after 100 iterations
# weights:  41
initial  value 939.542351 
iter  10 value 8.093178
iter  20 value 2.381168
iter  30 value 0.935874
iter  40 value 0.746481
iter  50 value 0.715950
iter  60 value 0.700242
iter  70 value 0.693375
iter  80 value 0.684470
iter  90 value 0.675812
iter 100 value 0.639442
final  value 0.639442 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0238; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 1312.574353 
iter  10 value 78.564903
iter  20 value 14.823057
iter  30 value 4.185762
iter  40 value 1.600173
iter  50 value 1.146184
iter  60 value 1.051108
iter  70 value 1.041070
final  value 1.041031 
converged
# weights:  5
initial  value 1109.881021 
iter  10 value 256.955160
iter  20 value 70.817308
iter  30 value 14.743505
iter  40 value 4.510650
iter  50 value 2.226659
iter  60 value 1.231785
iter  70 value 0.995343
iter  80 value 0.949294
final  value 0.948630 
converged
# weights:  5
initial  value 1392.102975 
iter  10 value 255.981569
iter  20 value 48.578969
iter  30 value 5.737847
iter  40 value 2.254034
iter  50 value 1.327857
iter  60 value 1.130897
iter  70 value 1.127396
final  value 1.127351 
converged
[Tune-y] 4: rmse.test.rmse=0.0311; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 1023.004913 
iter  10 value 2.784870
iter  20 value 0.766294
iter  30 value 0.130501
iter  40 value 0.035421
iter  50 value 0.031424
iter  60 value 0.029635
iter  70 value 0.028855
iter  80 value 0.028123
iter  90 value 0.027014
iter 100 value 0.026468
final  value 0.026468 
stopped after 100 iterations
# weights:  61
initial  value 1053.444245 
iter  10 value 8.484213
iter  20 value 0.635811
iter  30 value 0.053155
iter  40 value 0.028895
iter  50 value 0.026350
iter  60 value 0.025463
iter  70 value 0.024600
iter  80 value 0.024212
iter  90 value 0.023987
iter 100 value 0.023782
final  value 0.023782 
stopped after 100 iterations
# weights:  61
initial  value 2576.902904 
iter  10 value 24.984905
iter  20 value 0.856691
iter  30 value 0.115200
iter  40 value 0.059636
iter  50 value 0.049671
iter  60 value 0.047393
iter  70 value 0.044716
iter  80 value 0.043040
iter  90 value 0.041852
iter 100 value 0.040367
final  value 0.040367 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0065; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 1342.743246 
iter  10 value 7.771656
iter  20 value 2.370159
iter  30 value 1.838697
iter  40 value 1.695492
iter  50 value 1.603951
iter  60 value 1.455784
iter  70 value 1.373009
iter  80 value 1.306255
iter  90 value 1.248981
iter 100 value 1.176984
final  value 1.176984 
stopped after 100 iterations
# weights:  57
initial  value 1273.047307 
iter  10 value 2.882192
iter  20 value 1.060229
iter  30 value 0.759348
iter  40 value 0.705178
iter  50 value 0.681122
iter  60 value 0.671910
iter  70 value 0.661366
iter  80 value 0.655151
iter  90 value 0.650524
iter 100 value 0.645262
final  value 0.645262 
stopped after 100 iterations
# weights:  57
initial  value 874.812337 
iter  10 value 7.511878
iter  20 value 2.000331
iter  30 value 1.567475
iter  40 value 1.466981
iter  50 value 1.339758
iter  60 value 1.268969
iter  70 value 1.226387
iter  80 value 1.191883
iter  90 value 1.166622
iter 100 value 1.139636
final  value 1.139636 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0196; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 1309.301717 
iter  10 value 166.289625
iter  20 value 10.010860
iter  30 value 9.259481
iter  40 value 6.977491
iter  50 value 6.202046
iter  60 value 6.122067
iter  70 value 6.095868
iter  80 value 6.080693
iter  90 value 6.074658
iter 100 value 6.065829
final  value 6.065829 
stopped after 100 iterations
# weights:  13
initial  value 1111.543714 
iter  10 value 43.252126
iter  20 value 12.545385
iter  30 value 9.227600
iter  40 value 7.468610
iter  50 value 6.084869
iter  60 value 5.493348
iter  70 value 5.384681
iter  80 value 5.380886
iter  90 value 5.380756
final  value 5.380747 
converged
# weights:  13
initial  value 1118.940707 
iter  10 value 63.835822
iter  20 value 18.469305
iter  30 value 14.135010
iter  40 value 11.286765
iter  50 value 9.351139
iter  60 value 8.676358
iter  70 value 8.517398
iter  80 value 8.420558
final  value 8.418917 
converged
[Tune-y] 7: rmse.test.rmse=0.0613; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2289.676761 
iter  10 value 24.040057
iter  20 value 2.789837
iter  30 value 2.093181
iter  40 value 1.867886
iter  50 value 1.673035
iter  60 value 1.522869
iter  70 value 1.392569
iter  80 value 1.272771
iter  90 value 1.193335
iter 100 value 1.122695
final  value 1.122695 
stopped after 100 iterations
# weights:  73
initial  value 1320.742933 
iter  10 value 12.045826
iter  20 value 2.678168
iter  30 value 1.879649
iter  40 value 1.741007
iter  50 value 1.616383
iter  60 value 1.481438
iter  70 value 1.395300
iter  80 value 1.321974
iter  90 value 1.262038
iter 100 value 1.173939
final  value 1.173939 
stopped after 100 iterations
# weights:  73
initial  value 1142.428058 
iter  10 value 3.404543
iter  20 value 1.094540
iter  30 value 0.755906
iter  40 value 0.684060
iter  50 value 0.625392
iter  60 value 0.601253
iter  70 value 0.584105
iter  80 value 0.576150
iter  90 value 0.568429
iter 100 value 0.560396
final  value 0.560396 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0199; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 1193.006939 
iter  10 value 11.730120
iter  20 value 1.687686
iter  30 value 0.446406
iter  40 value 0.347161
iter  50 value 0.324636
iter  60 value 0.313727
iter  70 value 0.306379
iter  80 value 0.302284
iter  90 value 0.299314
iter 100 value 0.296245
final  value 0.296245 
stopped after 100 iterations
# weights:  57
initial  value 1900.203288 
iter  10 value 5.388687
iter  20 value 0.882408
iter  30 value 0.360662
iter  40 value 0.333245
iter  50 value 0.319322
iter  60 value 0.310053
iter  70 value 0.304773
iter  80 value 0.298441
iter  90 value 0.294136
iter 100 value 0.290604
final  value 0.290604 
stopped after 100 iterations
# weights:  57
initial  value 1839.989977 
iter  10 value 3.433797
iter  20 value 0.765255
iter  30 value 0.466924
iter  40 value 0.439100
iter  50 value 0.421252
iter  60 value 0.410813
iter  70 value 0.401613
iter  80 value 0.395518
iter  90 value 0.387420
iter 100 value 0.379760
final  value 0.379760 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0141; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 1319.187368 
iter  10 value 16.992925
iter  20 value 6.440145
iter  30 value 1.977455
iter  40 value 0.806421
iter  50 value 0.402279
iter  60 value 0.232616
iter  70 value 0.167169
iter  80 value 0.121356
iter  90 value 0.108287
iter 100 value 0.100365
final  value 0.100365 
stopped after 100 iterations
# weights:  5
initial  value 1338.484040 
iter  10 value 228.011484
iter  20 value 21.746523
iter  30 value 3.815021
iter  40 value 1.410496
iter  50 value 0.491005
iter  60 value 0.321782
iter  70 value 0.197561
iter  80 value 0.141506
iter  90 value 0.122036
iter 100 value 0.113460
final  value 0.113460 
stopped after 100 iterations
# weights:  5
initial  value 1114.051752 
iter  10 value 103.478194
iter  20 value 16.843630
iter  30 value 4.071559
iter  40 value 1.423612
iter  50 value 0.761459
iter  60 value 0.429651
iter  70 value 0.253521
iter  80 value 0.197813
iter  90 value 0.157396
iter 100 value 0.143615
final  value 0.143615 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0155; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 1458.844121 
iter  10 value 11.430775
iter  20 value 0.874737
iter  30 value 0.463735
iter  40 value 0.373108
iter  50 value 0.356621
iter  60 value 0.347946
iter  70 value 0.335381
iter  80 value 0.296208
iter  90 value 0.243899
iter 100 value 0.224031
final  value 0.224031 
stopped after 100 iterations
# weights:  33
initial  value 1335.825005 
iter  10 value 11.868625
iter  20 value 0.652252
iter  30 value 0.282515
iter  40 value 0.252474
iter  50 value 0.244970
iter  60 value 0.233753
iter  70 value 0.226671
iter  80 value 0.214403
iter  90 value 0.194614
iter 100 value 0.185958
final  value 0.185958 
stopped after 100 iterations
# weights:  33
initial  value 979.886693 
iter  10 value 38.739771
iter  20 value 0.760297
iter  30 value 0.305812
iter  40 value 0.259262
iter  50 value 0.241395
iter  60 value 0.234087
iter  70 value 0.230977
iter  80 value 0.223007
iter  90 value 0.213159
iter 100 value 0.206251
final  value 0.206251 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0154; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 1120.525660 
iter  10 value 20.687574
iter  20 value 13.726202
iter  30 value 12.863231
iter  40 value 12.528321
iter  50 value 11.805699
iter  60 value 11.112580
iter  70 value 9.901393
iter  80 value 8.488924
iter  90 value 7.241771
iter 100 value 7.058337
final  value 7.058337 
stopped after 100 iterations
# weights:  17
initial  value 1225.015992 
iter  10 value 43.838997
iter  20 value 20.130521
iter  30 value 10.179251
iter  40 value 9.066852
iter  50 value 6.796071
iter  60 value 6.330672
iter  70 value 6.184335
iter  80 value 6.121685
iter  90 value 6.106877
iter 100 value 6.106142
final  value 6.106142 
stopped after 100 iterations
# weights:  17
initial  value 1028.877368 
iter  10 value 43.898806
iter  20 value 15.781788
iter  30 value 9.019395
iter  40 value 7.942450
iter  50 value 7.421941
iter  60 value 7.344299
iter  70 value 7.227398
iter  80 value 7.097348
iter  90 value 7.061607
iter 100 value 7.052176
final  value 7.052176 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0582; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 1099.042170 
iter  10 value 80.396097
iter  20 value 12.124434
iter  30 value 1.601320
iter  40 value 0.710341
iter  50 value 0.244827
iter  60 value 0.078279
iter  70 value 0.048284
iter  80 value 0.032287
iter  90 value 0.026597
iter 100 value 0.025856
final  value 0.025856 
stopped after 100 iterations
# weights:  13
initial  value 1053.299733 
iter  10 value 46.593764
iter  20 value 3.455291
iter  30 value 0.528992
iter  40 value 0.170580
iter  50 value 0.058224
iter  60 value 0.043324
iter  70 value 0.036941
iter  80 value 0.032353
iter  90 value 0.030548
iter 100 value 0.029740
final  value 0.029740 
stopped after 100 iterations
# weights:  13
initial  value 1060.644580 
iter  10 value 198.174518
iter  20 value 97.942884
iter  30 value 24.728255
iter  40 value 6.777633
iter  50 value 0.804213
iter  60 value 0.442888
iter  70 value 0.148645
iter  80 value 0.075468
iter  90 value 0.067792
iter 100 value 0.058760
final  value 0.058760 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00954; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1579.249082 
iter  10 value 21.327457
iter  20 value 3.135430
iter  30 value 0.565923
iter  40 value 0.274493
iter  50 value 0.086100
iter  60 value 0.067993
iter  70 value 0.053779
iter  80 value 0.051002
iter  90 value 0.042331
iter 100 value 0.038741
final  value 0.038741 
stopped after 100 iterations
# weights:  25
initial  value 1257.851841 
iter  10 value 32.834190
iter  20 value 2.636891
iter  30 value 0.330859
iter  40 value 0.110993
iter  50 value 0.053971
iter  60 value 0.041785
iter  70 value 0.036820
iter  80 value 0.035188
iter  90 value 0.033449
iter 100 value 0.031397
final  value 0.031397 
stopped after 100 iterations
# weights:  25
initial  value 1128.934554 
iter  10 value 124.162015
iter  20 value 13.958552
iter  30 value 1.761746
iter  40 value 0.615364
iter  50 value 0.427232
iter  60 value 0.334269
iter  70 value 0.170133
iter  80 value 0.104143
iter  90 value 0.083057
iter 100 value 0.068892
final  value 0.068892 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0131; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2210.173755 
iter  10 value 7.359122
iter  20 value 3.029388
iter  30 value 0.736286
iter  40 value 0.107699
iter  50 value 0.083062
iter  60 value 0.081431
iter  70 value 0.075823
iter  80 value 0.069809
iter  90 value 0.066035
iter 100 value 0.063860
final  value 0.063860 
stopped after 100 iterations
# weights:  25
initial  value 1596.613644 
iter  10 value 9.446894
iter  20 value 0.520755
iter  30 value 0.215399
iter  40 value 0.127140
iter  50 value 0.104335
iter  60 value 0.096647
iter  70 value 0.088249
iter  80 value 0.081275
iter  90 value 0.074640
iter 100 value 0.071543
final  value 0.071543 
stopped after 100 iterations
# weights:  25
initial  value 1345.721104 
iter  10 value 9.397689
iter  20 value 1.749006
iter  30 value 0.385842
iter  40 value 0.173641
iter  50 value 0.154579
iter  60 value 0.144348
iter  70 value 0.123185
iter  80 value 0.105502
iter  90 value 0.091384
iter 100 value 0.083748
final  value 0.083748 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0147; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 887.390049 
iter  10 value 123.174570
iter  20 value 115.079413
iter  30 value 113.118045
iter  40 value 112.042342
iter  50 value 111.426425
iter  60 value 111.253531
iter  70 value 111.200630
iter  80 value 111.137121
iter  90 value 111.115480
final  value 111.114569 
converged
# weights:  65
initial  value 1863.267599 
iter  10 value 801.573315
iter  20 value 352.535363
iter  30 value 210.400868
iter  40 value 135.747424
iter  50 value 115.700682
iter  60 value 113.022991
iter  70 value 111.539813
iter  80 value 110.590350
iter  90 value 110.102277
iter 100 value 109.825486
final  value 109.825486 
stopped after 100 iterations
# weights:  65
initial  value 3336.872459 
iter  10 value 587.172231
iter  20 value 271.980213
iter  30 value 156.734807
iter  40 value 126.464994
iter  50 value 118.528264
iter  60 value 115.641297
iter  70 value 113.814195
iter  80 value 112.241835
iter  90 value 111.884367
iter 100 value 111.805795
final  value 111.805795 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 980.087047 
iter  10 value 6.689116
iter  20 value 0.922324
iter  30 value 0.131891
iter  40 value 0.060960
iter  50 value 0.056670
iter  60 value 0.055170
iter  70 value 0.053716
iter  80 value 0.048497
iter  90 value 0.045780
iter 100 value 0.043272
final  value 0.043272 
stopped after 100 iterations
# weights:  29
initial  value 950.791747 
iter  10 value 21.223575
iter  20 value 2.138636
iter  30 value 0.468898
iter  40 value 0.089229
iter  50 value 0.058572
iter  60 value 0.055016
iter  70 value 0.051071
iter  80 value 0.046364
iter  90 value 0.044417
iter 100 value 0.041638
final  value 0.041638 
stopped after 100 iterations
# weights:  29
initial  value 1575.803269 
iter  10 value 64.562639
iter  20 value 14.895498
iter  30 value 2.150802
iter  40 value 0.713000
iter  50 value 0.429122
iter  60 value 0.205840
iter  70 value 0.181334
iter  80 value 0.140370
iter  90 value 0.125117
iter 100 value 0.095778
final  value 0.095778 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0121; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 1469.574129 
iter  10 value 9.197763
iter  20 value 4.480119
iter  30 value 3.469805
iter  40 value 3.197746
iter  50 value 3.097175
iter  60 value 2.886417
iter  70 value 2.868372
iter  80 value 2.853232
iter  90 value 2.838021
iter 100 value 2.801832
final  value 2.801832 
stopped after 100 iterations
# weights:  21
initial  value 1536.117951 
iter  10 value 63.809906
iter  20 value 11.583832
iter  30 value 3.423943
iter  40 value 2.949846
iter  50 value 2.740404
iter  60 value 2.504416
iter  70 value 2.490019
iter  80 value 2.485435
iter  90 value 2.484053
iter 100 value 2.480036
final  value 2.480036 
stopped after 100 iterations
# weights:  21
initial  value 1336.342666 
iter  10 value 67.124464
iter  20 value 10.039967
iter  30 value 4.974177
iter  40 value 4.115041
iter  50 value 3.782022
iter  60 value 3.141182
iter  70 value 2.942706
iter  80 value 2.781869
iter  90 value 2.681028
iter 100 value 2.583220
final  value 2.583220 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2217.220200 
iter  10 value 5.829577
iter  20 value 1.309254
iter  30 value 0.898725
iter  40 value 0.852077
iter  50 value 0.822815
iter  60 value 0.796763
iter  70 value 0.784490
iter  80 value 0.773157
iter  90 value 0.762911
iter 100 value 0.754430
final  value 0.754430 
stopped after 100 iterations
# weights:  65
initial  value 1004.460944 
iter  10 value 15.718283
iter  20 value 2.117159
iter  30 value 1.387511
iter  40 value 1.186546
iter  50 value 1.057656
iter  60 value 0.979075
iter  70 value 0.925972
iter  80 value 0.888743
iter  90 value 0.868624
iter 100 value 0.840746
final  value 0.840746 
stopped after 100 iterations
# weights:  65
initial  value 1484.729742 
iter  10 value 32.516375
iter  20 value 2.681084
iter  30 value 1.843704
iter  40 value 1.410749
iter  50 value 1.199853
iter  60 value 1.122596
iter  70 value 1.034584
iter  80 value 0.995091
iter  90 value 0.969562
iter 100 value 0.947145
final  value 0.947145 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0186; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 1177.537774 
iter  10 value 8.609523
iter  20 value 1.021864
iter  30 value 0.186961
iter  40 value 0.056229
iter  50 value 0.021853
iter  60 value 0.017379
iter  70 value 0.016631
iter  80 value 0.015896
iter  90 value 0.015421
iter 100 value 0.014930
final  value 0.014930 
stopped after 100 iterations
# weights:  69
initial  value 1407.413458 
iter  10 value 7.846658
iter  20 value 0.741886
iter  30 value 0.207916
iter  40 value 0.055675
iter  50 value 0.041517
iter  60 value 0.032416
iter  70 value 0.028208
iter  80 value 0.026642
iter  90 value 0.026281
iter 100 value 0.025796
final  value 0.025796 
stopped after 100 iterations
# weights:  69
initial  value 1679.295619 
iter  10 value 8.392986
iter  20 value 1.274901
iter  30 value 0.364279
iter  40 value 0.074171
iter  50 value 0.037110
iter  60 value 0.030231
iter  70 value 0.028446
iter  80 value 0.027531
iter  90 value 0.027016
iter 100 value 0.026495
final  value 0.026495 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00676; time: 0.0 min
[Tune] Result: size=15; decay=0.000446 : rmse.test.rmse=0.0065
# weights:  61
initial  value 3524.509892 
iter  10 value 82.742006
iter  20 value 4.020856
iter  30 value 0.752094
iter  40 value 0.254223
iter  50 value 0.133528
iter  60 value 0.113407
iter  70 value 0.108129
iter  80 value 0.104715
iter  90 value 0.103219
iter 100 value 0.100470
final  value 0.100470 
stopped after 100 iterations
[1] "Fri Feb 09 13:59:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 725 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 437
 number of selected nodes                               : 76 
[1] "Fri Feb 09 13:59:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 13:59:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 13:59:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 13:59:50 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.187; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.191; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.193; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.184; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.184
[1] "Fri Feb 09 14:00:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:00:44 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.72; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.988; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.673; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.941; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.615; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.499; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.643; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse= 0.6; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.585; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.592; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.662; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.802; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.531; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.712; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.941; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.499
[1] "Fri Feb 09 14:00:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:00:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:01:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01800108 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1          0.307 
    Y 2          0.111 
    Y 3        -0.4791 
    Y 4        -0.2854 
    Y 5          1.362 
[1] "Fri Feb 09 14:01:27 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:28] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:28] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:28] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.323; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.282; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:32] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.677; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:37] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:37] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:01:37] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.299; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.264; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.326; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.385; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.367; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.257; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:02:41] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.257
[1] "Fri Feb 09 14:02:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:02:55 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.551971], n=(496,256)
**GROW** @depth 1: [2,0.503999], n=(107,148)
**GROW** @depth 1: [1,0.501355], n=(174,158)
**GROW** @depth 2: [1,0.750826], n=(147,11)
**GROW** @depth 2: [2,0.248598], n=(15,159)
**GROW** @depth 2: [2,0.752764], n=(126,22)
**GROW** @depth 3: [2,0.36795], n=(47,115)
**GROW** @depth 3: [2,0.27937], n=(15,129)
**GROW** @depth 2: [1,0.294889], n=(57,214)
**PRUNE** @depth 3: [2,0.247514]
**GROW** @depth 3: [1,0.321762], n=(20,195)
**GROW** @depth 3: [2,0.619562], n=(72,54)
**GROW** @depth 3: [2,0.300845], n=(32,31)
**PRUNE** @depth 3: [2,0.300845]
**GROW** @depth 4: [1,0.638084], n=(96,33)
**PRUNE** @depth 3: [1,0.290956]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(64,110,18,98,32,12,77,193,72,54,22)
**GROW** @depth 3: [1,0.240363], n=(11,98)
**GROW** @depth 4: [1,0.400833], n=(35,23)
**PRUNE** @depth 4: [1,0.400833]
**GROW** @depth 4: [1,0.369413], n=(26,32)
**PRUNE** @depth 4: [1,0.369413]
**GROW** @depth 5: [2,0.402907], n=(35,62)
**GROW** @depth 3: [2,0.752764], n=(166,28)
**GROW** @depth 4: [1,0.367692], n=(31,67)
**GROW** @depth 4: [2,0.641703], n=(117,49)
**GROW** @depth 6: [2,0.439205], n=(34,34)
**GROW** @depth 7: [1,0.411136], n=(12,22)
**GROW** @depth 7: [1,0.56587], n=(38,23)
**GROW** @depth 5: [1,0.444178], n=(18,30)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,56,32,11,23,33,20,35,38,25,32,12,77,117,18,30,28,71,54,22)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [2,0.659099], n=(53,26)
**GROW** @depth 4: [1,0.439351], n=(54,61)
**GROW** @depth 5: [2,0.440896], n=(17,15)
**GROW** @depth 5: [1,0.370509], n=(24,31)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(17,57,18,14,11,23,34,21,35,36,27,32,11,53,26,25,30,59,18,27,29,72,55,22)
**PRUNE** @depth 6: [1,0.370509]
**GROW** @depth 4: [1,0.428913], n=(41,16)
**GROW** @depth 5: [1,0.38224], n=(33,24)
**PRUNE** @depth 6: [1,0.38224]
**PRUNE** @depth 5: [2,0.440896]
**GROW** @depth 5: [2,0.580888], n=(36,21)
**GROW** @depth 5: [2,0.443509], n=(20,13)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(17,40,16,20,13,11,23,34,21,34,37,28,33,11,53,26,56,36,21,19,28,28,70,55,22)
**GROW** @depth 5: [1,0.677654], n=(38,17)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(18,38,17,20,14,11,23,33,20,34,37,28,35,11,54,26,55,38,18,20,30,26,69,39,16,22)
**PRUNE** @depth 5: [2,0.583641]
**GROW** @depth 5: [1,0.617643], n=(20,19)
**PRUNE** @depth 5: [1,0.617643]
**GROW** @depth 5: [2,0.571556], n=(34,22)
**PRUNE** @depth 5: [2,0.571556]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(18,37,18,20,14,12,22,33,19,35,37,27,33,11,54,26,54,56,21,30,26,72,39,16,22)
**GROW** @depth 6: [1,0.438417], n=(11,22)
**GROW** @depth 5: [1,0.621641], n=(21,18)
**PRUNE** @depth 5: [1,0.621641]
**PRUNE** @depth 5: [2,0.446429]
**GROW** @depth 5: [2,0.419216], n=(11,22)
**PRUNE** @depth 5: [2,0.419216]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(18,37,18,33,12,22,12,22,20,34,35,29,31,13,54,26,53,56,22,30,26,72,40,15,22)
Grow: 11.41%, Prune: 3.955%, Change: 83.99%, Swap: 17.84%

[1] "Fri Feb 09 14:03:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:03:03 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:03:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.1978 	 alpha= 0.2975 	 beta= 38880.2 
[1] "Fri Feb 09 14:03:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:03:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [1,0.374733], n=(187,565)
**GROW** @depth 1: [1,0.687453], n=(500,64)
**GROW** @depth 2: [2,0.499858], n=(230,270)
**GROW** @depth 2: [2,0.499858], n=(27,39)
**GROW** @depth 3: [2,0.630863], n=(136,132)
**GROW** @depth 4: [1,0.618258], n=(111,25)
**GROW** @depth 3: [2,0.208033], n=(11,213)
**GROW** @depth 4: [1,0.478542], n=(73,136)
**GROW** @depth 5: [1,0.497478], n=(54,58)
**GROW** @depth 2: [2,0.504152], n=(77,114)
**GROW** @depth 5: [1,0.587928], n=(97,39)
**GROW** @depth 2: [2,0.739532], n=(100,14)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(77,100,14,13,72,96,39,56,57,99,58,30,41)
**GROW** @depth 4: [1,0.276002], n=(41,54)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(75,40,56,16,14,73,96,43,58,63,93,61,25,39)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [2,0.345445], n=(17,80)
**GROW** @depth 3: [2,0.283543], n=(12,63)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(12,66,37,57,16,13,31,58,78,44,62,59,93,64,24,38)
**GROW** @depth 4: [2,0.706807], n=(51,13)
**GROW** @depth 4: [2,0.503999], n=(25,39)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(12,25,38,38,59,16,14,28,60,74,45,63,62,94,54,14,23,33)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(12,25,38,38,57,16,13,30,59,73,48,65,59,95,57,14,22,31)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(12,24,37,38,59,16,14,29,59,74,46,66,59,93,57,16,22,31)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(12,24,37,39,60,16,12,30,62,70,48,67,60,91,59,15,21,29)
Grow: 5.263%, Prune: 0%, Change: 80.22%, Swap: 20.23%

[1] "Fri Feb 09 14:03:09 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:03:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:03:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:03:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:03:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:03:13 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:03:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:03:15 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:03:17 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.184; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.123; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.282; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.306; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.179; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.954; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.194; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.193; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.123
[1] "Fri Feb 09 14:03:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:03:28 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.082; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.015; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0141; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0384; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0238; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0341; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.042; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.00836; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.071; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.361; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0377; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00832; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0317; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.204; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00832
[1] "Fri Feb 09 14:03:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:03:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:03:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:03:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============                                                       |  22%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:03:51 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 455 is not positive definite

[1] "Fri Feb 09 14:03:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 2.71502, its = 8
i = 2 (of 248), d = 2.97359, its = 8
i = 3 (of 248), d = 8.36824, its = 11
i = 4 (of 248), d = 3.77436, its = 9
i = 5 (of 248), d = 4.37175, its = 9
i = 6 (of 248), d = 4.2886, its = 9
i = 7 (of 248), d = 5.63432, its = 11
i = 8 (of 248), d = 12.684, its = 11
i = 9 (of 248), d = 3.96481, its = 9
i = 10 (of 248), d = 4.54434, its = 9
i = 11 (of 248), d = 7.65255, its = 10
i = 12 (of 248), d = 2.23998, its = 7
i = 13 (of 248), d = 4.13789, its = 8
i = 14 (of 248), d = 3.34651, its = 8
i = 15 (of 248), d = 4.83751, its = 9
i = 16 (of 248), d = 3.98395, its = 9
i = 17 (of 248), d = 6.93485, its = 10
i = 18 (of 248), d = 6.58232, its = 8
i = 19 (of 248), d = 7.35777, its = 10
i = 20 (of 248), d = 9.18546, its = 10
i = 21 (of 248), d = 7.81702, its = 10
i = 22 (of 248), d = 5.92703, its = 10
i = 23 (of 248), d = 2.4959, its = 8
i = 24 (of 248), d = 6.10494, its = 10
i = 25 (of 248), d = 5.66677, its = 9
i = 26 (of 248), d = 6.67441, its = 10
i = 27 (of 248), d = 4.509, its = 8
i = 28 (of 248), d = 3.42973, its = 16
i = 29 (of 248), d = 5.62239, its = 10
i = 30 (of 248), d = 9.24259, its = 10
i = 31 (of 248), d = 4.32499, its = 9
i = 32 (of 248), d = 6.53172, its = 10
i = 33 (of 248), d = 4.98169, its = 8
i = 34 (of 248), d = 2.40799, its = 7
i = 35 (of 248), d = 5.58766, its = 10
i = 36 (of 248), d = 9.21264, its = 11
i = 37 (of 248), d = 12.5268, its = 11
i = 38 (of 248), d = 6.63216, its = 10
i = 39 (of 248), d = 4.97048, its = 9
i = 40 (of 248), d = 15.5708, its = 16
i = 41 (of 248), d = 12.1844, its = 11
i = 42 (of 248), d = 10.7181, its = 11
i = 43 (of 248), d = 7.05034, its = 10
i = 44 (of 248), d = 7.40701, its = 10
i = 45 (of 248), d = 6.67801, its = 10
i = 46 (of 248), d = 4.27754, its = 9
i = 47 (of 248), d = 6.68838, its = 11
i = 48 (of 248), d = 11.7162, its = 11
i = 49 (of 248), d = 4.75126, its = 9
i = 50 (of 248), d = 8.57179, its = 10
i = 51 (of 248), d = 3.2758, its = 10
i = 52 (of 248), d = 4.76188, its = 10
i = 53 (of 248), d = 6.42247, its = 10
i = 54 (of 248), d = 8.34971, its = 10
i = 55 (of 248), d = 4.6026, its = 9
i = 56 (of 248), d = 5.94353, its = 31
i = 57 (of 248), d = 5.36113, its = 10
i = 58 (of 248), d = 10.4524, its = 10
i = 59 (of 248), d = 5.06454, its = 11
i = 60 (of 248), d = 6.57962, its = 9
i = 61 (of 248), d = 6.13171, its = 10
i = 62 (of 248), d = 8.11933, its = 10
i = 63 (of 248), d = 7.75956, its = 10
i = 64 (of 248), d = 7.43209, its = 12
i = 65 (of 248), d = 5.32891, its = 9
i = 66 (of 248), d = 10.5153, its = 10
i = 67 (of 248), d = 7.27467, its = 10
i = 68 (of 248), d = 9.30888, its = 10
i = 69 (of 248), d = 3.0048, its = 8
i = 70 (of 248), d = 10.0053, its = 12
i = 71 (of 248), d = 4.61936, its = 8
i = 72 (of 248), d = 9.23762, its = 11
i = 73 (of 248), d = 3.03623, its = 9
i = 74 (of 248), d = 4.49225, its = 10
i = 75 (of 248), d = 15.8402, its = 13
i = 76 (of 248), d = 4.64324, its = 8
i = 77 (of 248), d = 5.67427, its = 9
i = 78 (of 248), d = 6.55915, its = 10
i = 79 (of 248), d = 6.33074, its = 10
i = 80 (of 248), d = 5.16702, its = 9
i = 81 (of 248), d = 8.42446, its = 11
i = 82 (of 248), d = 3.48629, its = 10
i = 83 (of 248), d = 9.37507, its = 27
i = 84 (of 248), d = 8.81777, its = 11
i = 85 (of 248), d = 4.12938, its = 9
i = 86 (of 248), d = 3.84924, its = 7
i = 87 (of 248), d = 10.5325, its = 11
i = 88 (of 248), d = 4.06142, its = 11
i = 89 (of 248), d = 2.84102, its = 8
i = 90 (of 248), d = 9.28658, its = 10
i = 91 (of 248), d = 6.69492, its = 10
i = 92 (of 248), d = 8.39367, its = 10
i = 93 (of 248), d = 10.5926, its = 10
i = 94 (of 248), d = 7.99664, its = 10
i = 95 (of 248), d = 8.4165, its = 11
i = 96 (of 248), d = 6.82704, its = 10
i = 97 (of 248), d = 9.81951, its = 11
i = 98 (of 248), d = 9.00321, its = 11
i = 99 (of 248), d = 11.5397, its = 11
i = 100 (of 248), d = 3.80104, its = 9
i = 101 (of 248), d = 5.22133, its = 8
i = 102 (of 248), d = 6.66451, its = 33
i = 103 (of 248), d = 8.01678, its = 11
i = 104 (of 248), d = 5.15734, its = 9
i = 105 (of 248), d = 3.02571, its = 8
i = 106 (of 248), d = 3.41017, its = 8
i = 107 (of 248), d = 7.49107, its = 10
i = 108 (of 248), d = 3.96251, its = 9
i = 109 (of 248), d = 3.04602, its = 9
i = 110 (of 248), d = 7.22763, its = 10
i = 111 (of 248), d = 9.00899, its = 10
i = 112 (of 248), d = 4.44131, its = 9
i = 113 (of 248), d = 5.59885, its = 10
i = 114 (of 248), d = 9.18685, its = 10
i = 115 (of 248), d = 4.23737, its = 9
i = 116 (of 248), d = 11.0039, its = 10
i = 117 (of 248), d = 13.1612, its = 13
i = 118 (of 248), d = 3.83397, its = 8
i = 119 (of 248), d = 3.33395, its = 8
i = 120 (of 248), d = 5.59404, its = 9
i = 121 (of 248), d = 11.6664, its = 11
i = 122 (of 248), d = 10.0876, its = 11
i = 123 (of 248), d = 5.02119, its = 9
i = 124 (of 248), d = 5.04663, its = 9
i = 125 (of 248), d = 6.07289, its = 8
i = 126 (of 248), d = 6.0207, its = 10
i = 127 (of 248), d = 4.21869, its = 9
i = 128 (of 248), d = 3.9873, its = 10
i = 129 (of 248), d = 4.72587, its = 9
i = 130 (of 248), d = 9.60314, its = 10
i = 131 (of 248), d = 8.97664, its = 11
i = 132 (of 248), d = 10.2732, its = 12
i = 133 (of 248), d = 4.8888, its = 24
i = 134 (of 248), d = 9.70957, its = 11
i = 135 (of 248), d = 9.09496, its = 10
i = 136 (of 248), d = 8.14782, its = 11
i = 137 (of 248), d = 5.18093, its = 8
i = 138 (of 248), d = 2.51152, its = 8
i = 139 (of 248), d = 9.19106, its = 10
i = 140 (of 248), d = 3.67462, its = 9
i = 141 (of 248), d = 4.48284, its = 10
i = 142 (of 248), d = 4.43432, its = 9
i = 143 (of 248), d = 6.80255, its = 9
i = 144 (of 248), d = 4.2782, its = 9
i = 145 (of 248), d = 7.54627, its = 11
i = 146 (of 248), d = 4.25235, its = 8
i = 147 (of 248), d = 3.67692, its = 8
i = 148 (of 248), d = 3.30265, its = 8
i = 149 (of 248), d = 3.45158, its = 8
i = 150 (of 248), d = 7.48564, its = 10
i = 151 (of 248), d = 3.76339, its = 8
i = 152 (of 248), d = 7.36573, its = 10
i = 153 (of 248), d = 8.11956, its = 10
i = 154 (of 248), d = 11.2849, its = 11
i = 155 (of 248), d = 4.98386, its = 9
i = 156 (of 248), d = 5.60621, its = 29
i = 157 (of 248), d = 8.9181, its = 11
i = 158 (of 248), d = 3.84715, its = 11
i = 159 (of 248), d = 3.52352, its = 8
i = 160 (of 248), d = 4.36642, its = 7
i = 161 (of 248), d = 7.31851, its = 10
i = 162 (of 248), d = 6.62793, its = 10
i = 163 (of 248), d = 8.79397, its = 11
i = 164 (of 248), d = 5.2958, its = 9
i = 165 (of 248), d = 3.28164, its = 9
i = 166 (of 248), d = 2.52369, its = 7
i = 167 (of 248), d = 5.10319, its = 9
i = 168 (of 248), d = 3.77797, its = 9
i = 169 (of 248), d = 9.83699, its = 11
i = 170 (of 248), d = 7.78363, its = 10
i = 171 (of 248), d = 6.0822, its = 10
i = 172 (of 248), d = 5.21033, its = 9
i = 173 (of 248), d = 12.3828, its = 11
i = 174 (of 248), d = 7.37683, its = 11
i = 175 (of 248), d = 8.13746, its = 10
i = 176 (of 248), d = 8.55867, its = 10
i = 177 (of 248), d = 6.33536, its = 9
i = 178 (of 248), d = 6.19, its = 9
i = 179 (of 248), d = 4.36498, its = 9
i = 180 (of 248), d = 2.95798, its = 8
i = 181 (of 248), d = 4.52837, its = 9
i = 182 (of 248), d = 12.4039, its = 11
i = 183 (of 248), d = 6.39494, its = 10
i = 184 (of 248), d = 5.86355, its = 10
i = 185 (of 248), d = 6.36953, its = 10
i = 186 (of 248), d = 3.98291, its = 9
i = 187 (of 248), d = 4.56159, its = 9
i = 188 (of 248), d = 5.35978, its = 13
i = 189 (of 248), d = 6.51582, its = 10
i = 190 (of 248), d = 7.81434, its = 10
i = 191 (of 248), d = 5.84742, its = 9
i = 192 (of 248), d = 4.43907, its = 28
i = 193 (of 248), d = 7.26621, its = 9
i = 194 (of 248), d = 8.78323, its = 12
i = 195 (of 248), d = 6.11092, its = 9
i = 196 (of 248), d = 4.88325, its = 9
i = 197 (of 248), d = 10.5139, its = 11
i = 198 (of 248), d = 4.80795, its = 10
i = 199 (of 248), d = 6.70111, its = 11
i = 200 (of 248), d = 4.04294, its = 8
i = 201 (of 248), d = 7.63572, its = 8
i = 202 (of 248), d = 3.1724, its = 9
i = 203 (of 248), d = 6.10926, its = 10
i = 204 (of 248), d = 5.84067, its = 10
i = 205 (of 248), d = 6.29801, its = 9
i = 206 (of 248), d = 3.43705, its = 9
i = 207 (of 248), d = 12.9475, its = 11
i = 208 (of 248), d = 3.47798, its = 7
i = 209 (of 248), d = 6.92676, its = 10
i = 210 (of 248), d = 4.06905, its = 9
i = 211 (of 248), d = 3.99487, its = 8
i = 212 (of 248), d = 4.00997, its = 9
i = 213 (of 248), d = 6.58129, its = 10
i = 214 (of 248), d = 4.39735, its = 9
i = 215 (of 248), d = 6.61326, its = 10
i = 216 (of 248), d = 6.48533, its = 9
i = 217 (of 248), d = 7.60105, its = 10
i = 218 (of 248), d = 3.65247, its = 8
i = 219 (of 248), d = 4.54693, its = 9
i = 220 (of 248), d = 5.62827, its = 8
i = 221 (of 248), d = 8.08754, its = 11
i = 222 (of 248), d = 3.33362, its = 9
i = 223 (of 248), d = 12.7928, its = 11
i = 224 (of 248), d = 8.339, its = 10
i = 225 (of 248), d = 13.0285, its = 11
i = 226 (of 248), d = 10.6774, its = 10
i = 227 (of 248), d = 21.4707, its = 12
i = 228 (of 248), d = 3.86644, its = 9
i = 229 (of 248), d = 5.77218, its = 9
i = 230 (of 248), d = 7.3998, its = 9
i = 231 (of 248), d = 5.95168, its = 10
i = 232 (of 248), d = 5.59713, its = 10
i = 233 (of 248), d = 7.12362, its = 11
i = 234 (of 248), d = 7.27292, its = 10
i = 235 (of 248), d = 3.8385, its = 9
i = 236 (of 248), d = 5.11725, its = 9
i = 237 (of 248), d = 6.74521, its = 10
i = 238 (of 248), d = 6.33912, its = 9
i = 239 (of 248), d = 5.63279, its = 10
i = 240 (of 248), d = 3.67579, its = 9
i = 241 (of 248), d = 5.51713, its = 11
i = 242 (of 248), d = 4.73266, its = 9
i = 243 (of 248), d = 4.54236, its = 9
i = 244 (of 248), d = 9.15233, its = 10
i = 245 (of 248), d = 5.24305, its = 9
i = 246 (of 248), d = 6.94244, its = 10
i = 247 (of 248), d = 3.95925, its = 9
i = 248 (of 248), d = 2.92761, its = 8
[1] "Fri Feb 09 14:04:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:04:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:04:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:04:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:05:04 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 2053.393306 
iter  10 value 49.153770
iter  20 value 6.677783
iter  30 value 2.149640
iter  40 value 1.744910
iter  50 value 1.545521
iter  60 value 1.336291
iter  70 value 1.131520
iter  80 value 0.969501
iter  90 value 0.914117
iter 100 value 0.854657
final  value 0.854657 
stopped after 100 iterations
# weights:  77
initial  value 695.711272 
iter  10 value 2.981028
iter  20 value 0.620486
iter  30 value 0.250864
iter  40 value 0.217740
iter  50 value 0.198461
iter  60 value 0.189719
iter  70 value 0.183686
iter  80 value 0.179584
iter  90 value 0.177300
iter 100 value 0.175221
final  value 0.175221 
stopped after 100 iterations
# weights:  77
initial  value 1149.219942 
iter  10 value 23.158127
iter  20 value 1.912704
iter  30 value 1.203296
iter  40 value 1.102012
iter  50 value 0.947203
iter  60 value 0.812948
iter  70 value 0.700708
iter  80 value 0.631841
iter  90 value 0.554776
iter 100 value 0.470700
final  value 0.470700 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0118; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 1088.058313 
iter  10 value 3.657040
iter  20 value 0.668043
iter  30 value 0.201254
iter  40 value 0.081181
iter  50 value 0.053020
iter  60 value 0.043426
iter  70 value 0.040117
iter  80 value 0.038010
iter  90 value 0.035708
iter 100 value 0.033379
final  value 0.033379 
stopped after 100 iterations
# weights:  53
initial  value 1745.293579 
iter  10 value 3.556079
iter  20 value 0.972712
iter  30 value 0.228568
iter  40 value 0.060096
iter  50 value 0.044795
iter  60 value 0.035940
iter  70 value 0.033193
iter  80 value 0.032003
iter  90 value 0.030704
iter 100 value 0.030122
final  value 0.030122 
stopped after 100 iterations
# weights:  53
initial  value 1064.044362 
iter  10 value 15.900622
iter  20 value 0.963448
iter  30 value 0.058164
iter  40 value 0.023783
iter  50 value 0.021920
iter  60 value 0.021294
iter  70 value 0.020844
iter  80 value 0.020623
iter  90 value 0.020339
iter 100 value 0.020147
final  value 0.020147 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00474; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 1027.489265 
iter  10 value 5.251728
iter  20 value 1.103185
iter  30 value 0.072738
iter  40 value 0.040208
iter  50 value 0.025944
iter  60 value 0.023735
iter  70 value 0.023033
iter  80 value 0.022465
iter  90 value 0.022128
iter 100 value 0.021928
final  value 0.021928 
stopped after 100 iterations
# weights:  49
initial  value 1090.532524 
iter  10 value 6.727769
iter  20 value 1.276845
iter  30 value 0.160294
iter  40 value 0.042771
iter  50 value 0.023840
iter  60 value 0.020107
iter  70 value 0.019076
iter  80 value 0.018754
iter  90 value 0.018549
iter 100 value 0.018290
final  value 0.018290 
stopped after 100 iterations
# weights:  49
initial  value 2181.247879 
iter  10 value 10.778278
iter  20 value 1.303207
iter  30 value 0.237240
iter  40 value 0.135161
iter  50 value 0.069275
iter  60 value 0.052252
iter  70 value 0.044880
iter  80 value 0.042249
iter  90 value 0.041050
iter 100 value 0.039408
final  value 0.039408 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00448; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 1108.194078 
iter  10 value 254.611893
iter  20 value 2.282152
iter  30 value 1.325771
iter  40 value 0.830529
iter  50 value 0.652499
iter  60 value 0.506933
iter  70 value 0.435168
iter  80 value 0.358478
iter  90 value 0.336365
iter 100 value 0.328553
final  value 0.328553 
stopped after 100 iterations
# weights:  9
initial  value 1007.396833 
iter  10 value 64.560916
iter  20 value 9.007066
iter  30 value 1.169606
iter  40 value 0.499050
iter  50 value 0.397714
iter  60 value 0.333391
iter  70 value 0.312062
iter  80 value 0.288794
iter  90 value 0.285778
iter 100 value 0.284782
final  value 0.284782 
stopped after 100 iterations
# weights:  9
initial  value 1157.908930 
iter  10 value 240.865398
iter  20 value 115.164801
iter  30 value 24.699361
iter  40 value 2.217044
iter  50 value 0.823332
iter  60 value 0.439729
iter  70 value 0.402866
iter  80 value 0.398649
iter  90 value 0.394621
iter 100 value 0.393571
final  value 0.393571 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0182; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 985.603178 
iter  10 value 3.049772
iter  20 value 0.952990
iter  30 value 0.607569
iter  40 value 0.550378
iter  50 value 0.525339
iter  60 value 0.519332
iter  70 value 0.516494
iter  80 value 0.513913
iter  90 value 0.511584
iter 100 value 0.509169
final  value 0.509169 
stopped after 100 iterations
# weights:  77
initial  value 800.136848 
iter  10 value 4.147637
iter  20 value 0.947053
iter  30 value 0.690863
iter  40 value 0.568762
iter  50 value 0.526405
iter  60 value 0.508215
iter  70 value 0.487906
iter  80 value 0.474394
iter  90 value 0.465937
iter 100 value 0.463166
final  value 0.463166 
stopped after 100 iterations
# weights:  77
initial  value 1251.871794 
iter  10 value 4.400898
iter  20 value 1.015818
iter  30 value 0.675938
iter  40 value 0.587581
iter  50 value 0.556005
iter  60 value 0.530816
iter  70 value 0.505533
iter  80 value 0.493870
iter  90 value 0.484162
iter 100 value 0.471168
final  value 0.471168 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0137; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 1483.958470 
iter  10 value 116.050732
iter  20 value 94.633977
iter  30 value 63.644798
iter  40 value 40.559804
iter  50 value 33.060059
iter  60 value 28.173729
iter  70 value 25.233501
iter  80 value 23.727817
iter  90 value 22.866783
iter 100 value 22.586428
final  value 22.586428 
stopped after 100 iterations
# weights:  73
initial  value 982.452934 
iter  10 value 31.699343
iter  20 value 26.608976
iter  30 value 22.955292
iter  40 value 21.409381
iter  50 value 20.718408
iter  60 value 20.571150
iter  70 value 20.508558
iter  80 value 20.463790
iter  90 value 20.449068
iter 100 value 20.445778
final  value 20.445778 
stopped after 100 iterations
# weights:  73
initial  value 1163.049559 
iter  10 value 121.343646
iter  20 value 97.327549
iter  30 value 65.556208
iter  40 value 45.807052
iter  50 value 31.378258
iter  60 value 26.420901
iter  70 value 23.362706
iter  80 value 22.159609
iter  90 value 21.772048
iter 100 value 21.602433
final  value 21.602433 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.07; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 1459.319591 
iter  10 value 3.545987
iter  20 value 0.496430
iter  30 value 0.133479
iter  40 value 0.079242
iter  50 value 0.070305
iter  60 value 0.068635
iter  70 value 0.067565
iter  80 value 0.066652
iter  90 value 0.065855
iter 100 value 0.064648
final  value 0.064648 
stopped after 100 iterations
# weights:  53
initial  value 1297.738907 
iter  10 value 6.933087
iter  20 value 1.128456
iter  30 value 0.201121
iter  40 value 0.068481
iter  50 value 0.058274
iter  60 value 0.054937
iter  70 value 0.052606
iter  80 value 0.050754
iter  90 value 0.048788
iter 100 value 0.047496
final  value 0.047496 
stopped after 100 iterations
# weights:  53
initial  value 1168.712341 
iter  10 value 27.563220
iter  20 value 4.741287
iter  30 value 1.067122
iter  40 value 0.267246
iter  50 value 0.199761
iter  60 value 0.183855
iter  70 value 0.176571
iter  80 value 0.165959
iter  90 value 0.159165
iter 100 value 0.152158
final  value 0.152158 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00625; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 1497.575959 
iter  10 value 7.968999
iter  20 value 0.842316
iter  30 value 0.328451
iter  40 value 0.192345
iter  50 value 0.148138
iter  60 value 0.138526
iter  70 value 0.134150
iter  80 value 0.126432
iter  90 value 0.121796
iter 100 value 0.117843
final  value 0.117843 
stopped after 100 iterations
# weights:  33
initial  value 1501.820333 
iter  10 value 12.441226
iter  20 value 2.772452
iter  30 value 0.689551
iter  40 value 0.331257
iter  50 value 0.236767
iter  60 value 0.219382
iter  70 value 0.203998
iter  80 value 0.185522
iter  90 value 0.140629
iter 100 value 0.120720
final  value 0.120720 
stopped after 100 iterations
# weights:  33
initial  value 1251.307746 
iter  10 value 9.469462
iter  20 value 0.918863
iter  30 value 0.254599
iter  40 value 0.143354
iter  50 value 0.135354
iter  60 value 0.130657
iter  70 value 0.128523
iter  80 value 0.121331
iter  90 value 0.108801
iter 100 value 0.102973
final  value 0.102973 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0097; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 1307.617673 
iter  10 value 2.928718
iter  20 value 0.886583
iter  30 value 0.288167
iter  40 value 0.132989
iter  50 value 0.107352
iter  60 value 0.094434
iter  70 value 0.090414
iter  80 value 0.086964
iter  90 value 0.084067
iter 100 value 0.082145
final  value 0.082145 
stopped after 100 iterations
# weights:  77
initial  value 1390.215404 
iter  10 value 11.983720
iter  20 value 0.646963
iter  30 value 0.240965
iter  40 value 0.198186
iter  50 value 0.183288
iter  60 value 0.178430
iter  70 value 0.174225
iter  80 value 0.169361
iter  90 value 0.166624
iter 100 value 0.163534
final  value 0.163534 
stopped after 100 iterations
# weights:  77
initial  value 2158.152193 
iter  10 value 5.608513
iter  20 value 0.344380
iter  30 value 0.198684
iter  40 value 0.154697
iter  50 value 0.143604
iter  60 value 0.139619
iter  70 value 0.136998
iter  80 value 0.134364
iter  90 value 0.132383
iter 100 value 0.130436
final  value 0.130436 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00607; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 1058.811993 
iter  10 value 22.855284
iter  20 value 5.568127
iter  30 value 0.955181
iter  40 value 0.143396
iter  50 value 0.088878
iter  60 value 0.044811
iter  70 value 0.028368
iter  80 value 0.021831
iter  90 value 0.014304
iter 100 value 0.010490
final  value 0.010490 
stopped after 100 iterations
# weights:  21
initial  value 1053.215068 
iter  10 value 16.167453
iter  20 value 2.028647
iter  30 value 0.198182
iter  40 value 0.060243
iter  50 value 0.045682
iter  60 value 0.023630
iter  70 value 0.019879
iter  80 value 0.014225
iter  90 value 0.008675
iter 100 value 0.005860
final  value 0.005860 
stopped after 100 iterations
# weights:  21
initial  value 863.879483 
iter  10 value 19.051381
iter  20 value 3.968242
iter  30 value 0.382064
iter  40 value 0.043896
iter  50 value 0.034860
iter  60 value 0.007725
iter  70 value 0.004370
iter  80 value 0.003477
iter  90 value 0.002843
iter 100 value 0.002759
final  value 0.002759 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00379; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 1592.600189 
iter  10 value 164.524814
iter  20 value 130.428857
iter  30 value 91.556660
iter  40 value 70.756015
iter  50 value 52.791257
iter  60 value 44.809343
iter  70 value 37.540525
iter  80 value 28.562866
iter  90 value 21.599983
iter 100 value 19.274377
final  value 19.274377 
stopped after 100 iterations
# weights:  73
initial  value 1424.707524 
iter  10 value 193.607297
iter  20 value 109.214056
iter  30 value 77.321327
iter  40 value 64.733329
iter  50 value 43.657915
iter  60 value 27.660382
iter  70 value 22.051821
iter  80 value 19.480240
iter  90 value 18.329255
iter 100 value 17.461124
final  value 17.461124 
stopped after 100 iterations
# weights:  73
initial  value 1583.042320 
iter  10 value 40.975683
iter  20 value 24.290867
iter  30 value 21.460558
iter  40 value 19.221701
iter  50 value 18.233238
iter  60 value 17.812719
iter  70 value 17.539851
iter  80 value 17.161341
iter  90 value 16.961615
iter 100 value 16.880460
final  value 16.880460 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0702; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 1103.071634 
iter  10 value 22.626955
iter  20 value 18.883955
iter  30 value 18.020850
iter  40 value 17.866115
iter  50 value 17.776744
iter  60 value 17.752919
iter  70 value 17.750065
iter  80 value 17.747746
iter  90 value 17.746529
final  value 17.746510 
converged
# weights:  45
initial  value 1336.144235 
iter  10 value 125.460807
iter  20 value 90.501728
iter  30 value 51.525347
iter  40 value 32.457131
iter  50 value 24.147021
iter  60 value 20.661229
iter  70 value 18.799524
iter  80 value 17.053576
iter  90 value 16.361181
iter 100 value 15.863967
final  value 15.863967 
stopped after 100 iterations
# weights:  45
initial  value 1999.157134 
iter  10 value 36.622760
iter  20 value 30.297513
iter  30 value 22.764017
iter  40 value 18.959109
iter  50 value 17.636249
iter  60 value 17.245798
iter  70 value 17.107711
iter  80 value 16.977906
iter  90 value 16.916971
iter 100 value 16.860035
final  value 16.860035 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0818; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 1541.612420 
iter  10 value 103.261712
iter  20 value 4.320981
iter  30 value 1.986146
iter  40 value 1.863236
iter  50 value 1.820093
iter  60 value 1.660103
iter  70 value 1.635758
iter  80 value 1.615328
iter  90 value 1.597196
iter 100 value 1.572687
final  value 1.572687 
stopped after 100 iterations
# weights:  21
initial  value 1294.343484 
iter  10 value 12.364725
iter  20 value 3.404926
iter  30 value 2.089398
iter  40 value 1.882590
iter  50 value 1.778924
iter  60 value 1.602991
iter  70 value 1.527402
iter  80 value 1.488902
iter  90 value 1.435010
iter 100 value 1.321971
final  value 1.321971 
stopped after 100 iterations
# weights:  21
initial  value 1295.501581 
iter  10 value 19.340023
iter  20 value 7.499892
iter  30 value 3.159952
iter  40 value 2.261300
iter  50 value 1.947111
iter  60 value 1.541599
iter  70 value 1.477797
iter  80 value 1.465355
iter  90 value 1.462174
iter 100 value 1.388350
final  value 1.388350 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0261; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1734.934065 
iter  10 value 8.174570
iter  20 value 1.788701
iter  30 value 0.702174
iter  40 value 0.565003
iter  50 value 0.518260
iter  60 value 0.490313
iter  70 value 0.471637
iter  80 value 0.444701
iter  90 value 0.430116
iter 100 value 0.418507
final  value 0.418507 
stopped after 100 iterations
# weights:  53
initial  value 1035.621842 
iter  10 value 2.806394
iter  20 value 0.693519
iter  30 value 0.351099
iter  40 value 0.260735
iter  50 value 0.234978
iter  60 value 0.224220
iter  70 value 0.217681
iter  80 value 0.213446
iter  90 value 0.209535
iter 100 value 0.206266
final  value 0.206266 
stopped after 100 iterations
# weights:  53
initial  value 1928.655248 
iter  10 value 13.270178
iter  20 value 1.612254
iter  30 value 0.537303
iter  40 value 0.428463
iter  50 value 0.393851
iter  60 value 0.366439
iter  70 value 0.348419
iter  80 value 0.334012
iter  90 value 0.325071
iter 100 value 0.318638
final  value 0.318638 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0117; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 2004.085446 
iter  10 value 4.180541
iter  20 value 3.060075
iter  30 value 2.867253
iter  40 value 2.787384
iter  50 value 2.686653
iter  60 value 2.623118
iter  70 value 2.551011
iter  80 value 2.521301
iter  90 value 2.502414
iter 100 value 2.452779
final  value 2.452779 
stopped after 100 iterations
# weights:  45
initial  value 1932.359043 
iter  10 value 7.104973
iter  20 value 3.000400
iter  30 value 2.709249
iter  40 value 2.556436
iter  50 value 2.421869
iter  60 value 2.366095
iter  70 value 2.333022
iter  80 value 2.308536
iter  90 value 2.258217
iter 100 value 2.164222
final  value 2.164222 
stopped after 100 iterations
# weights:  45
initial  value 1689.055449 
iter  10 value 6.524017
iter  20 value 3.552558
iter  30 value 3.012833
iter  40 value 2.827375
iter  50 value 2.657735
iter  60 value 2.538016
iter  70 value 2.458705
iter  80 value 2.406973
iter  90 value 2.368326
iter 100 value 2.285153
final  value 2.285153 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 1108.535015 
iter  10 value 4.317627
iter  20 value 0.892688
iter  30 value 0.187345
iter  40 value 0.133710
iter  50 value 0.123119
iter  60 value 0.120474
iter  70 value 0.118446
iter  80 value 0.117617
iter  90 value 0.115213
iter 100 value 0.113088
final  value 0.113088 
stopped after 100 iterations
# weights:  37
initial  value 1080.112782 
iter  10 value 9.722907
iter  20 value 0.929088
iter  30 value 0.339918
iter  40 value 0.209534
iter  50 value 0.191192
iter  60 value 0.187652
iter  70 value 0.183394
iter  80 value 0.181257
iter  90 value 0.168107
iter 100 value 0.154481
final  value 0.154481 
stopped after 100 iterations
# weights:  37
initial  value 1722.730030 
iter  10 value 9.380897
iter  20 value 1.260859
iter  30 value 0.369474
iter  40 value 0.245135
iter  50 value 0.183605
iter  60 value 0.170307
iter  70 value 0.165349
iter  80 value 0.160886
iter  90 value 0.153802
iter 100 value 0.145852
final  value 0.145852 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.00916; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 1280.571541 
iter  10 value 65.739077
iter  20 value 34.919537
iter  30 value 28.631178
iter  40 value 25.723602
iter  50 value 25.165026
iter  60 value 25.140170
final  value 25.140083 
converged
# weights:  21
initial  value 1126.688553 
iter  10 value 66.850548
iter  20 value 45.925037
iter  30 value 31.505538
iter  40 value 23.988539
iter  50 value 22.418177
iter  60 value 22.273287
iter  70 value 22.269996
final  value 22.269995 
converged
# weights:  21
initial  value 1450.953776 
iter  10 value 31.633870
iter  20 value 26.926848
iter  30 value 25.396941
iter  40 value 24.617506
iter  50 value 24.360902
iter  60 value 24.346153
final  value 24.346141 
converged
[Tune-y] 17: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 1212.770905 
iter  10 value 15.664754
iter  20 value 1.087774
iter  30 value 0.186401
iter  40 value 0.035779
iter  50 value 0.006880
iter  60 value 0.002432
iter  70 value 0.002014
iter  80 value 0.001855
iter  90 value 0.001768
iter 100 value 0.001716
final  value 0.001716 
stopped after 100 iterations
# weights:  57
initial  value 1863.014798 
iter  10 value 2.649502
iter  20 value 0.285979
iter  30 value 0.063034
iter  40 value 0.014640
iter  50 value 0.005662
iter  60 value 0.002832
iter  70 value 0.002110
iter  80 value 0.001757
iter  90 value 0.001546
iter 100 value 0.001426
final  value 0.001426 
stopped after 100 iterations
# weights:  57
initial  value 2013.860257 
iter  10 value 8.162232
iter  20 value 1.326887
iter  30 value 0.205526
iter  40 value 0.062279
iter  50 value 0.016202
iter  60 value 0.007766
iter  70 value 0.004417
iter  80 value 0.003236
iter  90 value 0.002480
iter 100 value 0.002063
final  value 0.002063 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00322; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 1236.552721 
iter  10 value 14.902602
iter  20 value 3.074305
iter  30 value 1.181179
iter  40 value 0.448493
iter  50 value 0.210349
iter  60 value 0.180136
iter  70 value 0.170831
iter  80 value 0.167110
iter  90 value 0.153193
iter 100 value 0.140434
final  value 0.140434 
stopped after 100 iterations
# weights:  17
initial  value 762.409580 
iter  10 value 45.538854
iter  20 value 3.044657
iter  30 value 1.337843
iter  40 value 0.536629
iter  50 value 0.219607
iter  60 value 0.185288
iter  70 value 0.166479
iter  80 value 0.159974
iter  90 value 0.146384
iter 100 value 0.136709
final  value 0.136709 
stopped after 100 iterations
# weights:  17
initial  value 1222.925353 
iter  10 value 13.472952
iter  20 value 4.305171
iter  30 value 0.551070
iter  40 value 0.277022
iter  50 value 0.167861
iter  60 value 0.159549
iter  70 value 0.157344
iter  80 value 0.154335
iter  90 value 0.146689
iter 100 value 0.141199
final  value 0.141199 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0119; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 1126.564392 
iter  10 value 4.646368
iter  20 value 2.368477
iter  30 value 2.036253
iter  40 value 1.842533
iter  50 value 1.715701
iter  60 value 1.671017
iter  70 value 1.563165
iter  80 value 1.488428
iter  90 value 1.470023
iter 100 value 1.454141
final  value 1.454141 
stopped after 100 iterations
# weights:  29
initial  value 1381.174696 
iter  10 value 24.889283
iter  20 value 2.865674
iter  30 value 2.150964
iter  40 value 1.823074
iter  50 value 1.574135
iter  60 value 1.505098
iter  70 value 1.471372
iter  80 value 1.433504
iter  90 value 1.418749
iter 100 value 1.408074
final  value 1.408074 
stopped after 100 iterations
# weights:  29
initial  value 1010.176404 
iter  10 value 12.344671
iter  20 value 3.032715
iter  30 value 1.697973
iter  40 value 1.532694
iter  50 value 1.469116
iter  60 value 1.437383
iter  70 value 1.392687
iter  80 value 1.337635
iter  90 value 1.329778
iter 100 value 1.326059
final  value 1.326059 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0287; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00322
# weights:  57
initial  value 1758.490021 
iter  10 value 92.873070
iter  20 value 5.987992
iter  30 value 1.549145
iter  40 value 0.292255
iter  50 value 0.097572
iter  60 value 0.037925
iter  70 value 0.020148
iter  80 value 0.012544
iter  90 value 0.009629
iter 100 value 0.006851
final  value 0.006851 
stopped after 100 iterations
[1] "Fri Feb 09 14:05:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 683 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 412
 number of selected nodes                               : 78 
[1] "Fri Feb 09 14:05:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:05:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:05:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:05:58 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.165
[1] "Fri Feb 09 14:06:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:06:54 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.861; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.897; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.61; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.625; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.677; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.799; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.603; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.588; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=1.05; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.511
[1] "Fri Feb 09 14:06:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:06:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:07:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7230 0.3270 0.1480 0.0671 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01800084 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1482 
    Y 1         0.3141 
    Y 2       -0.03879 
    Y 3         -2.109 
    Y 4        -0.4713 
    Y 5        -0.2783 
[1] "Fri Feb 09 14:07:28 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.276; time: 0.6 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.268; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:08:35] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.497; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.306; time: 0.6 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.357; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.361; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.402; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:18] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.324; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:09:19] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.268
[1] "Fri Feb 09 14:09:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:09:28 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.487577], n=(308,444)
**GROW** @depth 1: [1,0.500388], n=(171,136)
**GROW** @depth 1: [2,0.561963], n=(148,297)
**GROW** @depth 2: [2,0.244325], n=(15,156)
**GROW** @depth 2: [1,0.724539], n=(108,12)
**GROW** @depth 2: [2,0.74954], n=(220,36)
**GROW** @depth 2: [2,0.263497], n=(15,190)
**GROW** @depth 3: [1,0.616804], n=(57,51)
**GROW** @depth 3: [2,0.410276], n=(57,133)
**GROW** @depth 3: [2,0.623773], n=(138,83)
**GROW** @depth 4: [2,0.544785], n=(114,19)
**GROW** @depth 3: [1,0.249264], n=(18,138)
**GROW** @depth 5: [1,0.447062], n=(59,24)
**GROW** @depth 4: [1,0.337002], n=(37,101)
**GROW** @depth 4: [1,0.254224], n=(21,116)
**GROW** @depth 5: [1,0.3632], n=(50,66)
**GROW** @depth 6: [2,0.372853], n=(33,69)
**GROW** @depth 5: [2,0.443252], n=(28,87)
**PRUNE** @depth 4: [2,0.444325]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,18,36,33,70,21,49,66,57,24,37,14,58,115,20,57,50,12)
**PRUNE** @depth 4: [1,0.723144]
**GROW** @depth 5: [1,0.431096], n=(30,35)
**PRUNE** @depth 5: [1,0.3632]
**GROW** @depth 4: [1,0.724539], n=(48,12)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 6: [1,0.417455], n=(25,43)
**GROW** @depth 5: [1,0.37126], n=(25,32)
**GROW** @depth 4: [2,0.545706], n=(17,20)
**GROW** @depth 5: [1,0.342273], n=(34,46)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(14,18,36,35,25,43,20,34,46,16,20,25,32,24,37,14,58,115,22,58,60)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [1,0.42319], n=(18,17)
**GROW** @depth 7: [2,0.436043], n=(22,21)
**PRUNE** @depth 6: [1,0.42319]
**GROW** @depth 6: [2,0.554448], n=(25,21)
**PRUNE** @depth 6: [2,0.554755]
**PRUNE** @depth 3: [2,0.258742]
**GROW** @depth 3: [1,0.68408], n=(96,18)
**GROW** @depth 4: [1,0.595102], n=(65,31)
**GROW** @depth 3: [1,0.57464], n=(36,36)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,17,37,34,25,22,21,19,35,45,17,20,25,32,23,38,36,36,65,31,18,23,58,60)
**GROW** @depth 6: [2,0.406902], n=(21,16)
**PRUNE** @depth 3: [1,0.57464]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,18,19,17,33,25,23,21,19,36,44,17,20,25,32,22,38,72,66,30,18,23,59,60)
**GROW** @depth 5: [2,0.551994], n=(17,19)
**GROW** @depth 3: [1,0.68935], n=(40,20)
**GROW** @depth 5: [2,0.474847], n=(32,34)
**GROW** @depth 3: [1,0.618199], n=(11,11)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,19,17,34,25,22,24,18,17,19,44,14,20,25,32,24,37,72,32,35,30,18,11,11,58,40,20)
**GROW** @depth 6: [2,0.551534], n=(24,20)
**PRUNE** @depth 6: [2,0.551534]
**GROW** @depth 5: [2,0.488037], n=(16,14)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,19,33,25,23,24,18,16,19,45,14,20,25,32,23,38,72,32,34,16,14,18,13,11,54,43,20)
**GROW** @depth 6: [2,0.31273], n=(15,19)
**PRUNE** @depth 6: [2,0.31273]
**GROW** @depth 4: [1,0.390327], n=(15,23)
**GROW** @depth 3: [2,0.356902], n=(45,27)
**GROW** @depth 7: [2,0.427454], n=(11,14)
**PRUNE** @depth 3: [2,0.356902]
**PRUNE** @depth 7: [2,0.42684]
**GROW** @depth 6: [2,0.308589], n=(14,19)
**GROW** @depth 6: [2,0.678834], n=(18,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,20,14,19,24,23,24,18,16,19,46,14,20,24,19,13,24,14,23,71,33,34,16,14,18,12,11,54,44,20)
Grow: 12.18%, Prune: 3.55%, Change: 83.52%, Swap: 34.03%

[1] "Fri Feb 09 14:09:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:09:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:09:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9604 	 alpha= 0.9029 	 beta= 134622.2 
[1] "Fri Feb 09 14:09:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:09:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 14:09:40 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:09:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:09:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:09:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:09:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:09:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:09:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:09:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:09:48 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0315; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.0897; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.025; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.0511; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.0507; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0335; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0262; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0232; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.0295; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0361; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0275; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0305; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.0865; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0237; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0167; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.068; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0233; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0241; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.0167
[1] "Fri Feb 09 14:09:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:09:54 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0799; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0359; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.0963; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0213; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0144; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.128; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0786; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00182; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0435; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.146; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00345; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0122; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0188; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0138; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.123; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.00849; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00182
[1] "Fri Feb 09 14:09:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:10:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:10:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:10:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:10:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 659 is not positive definite

[1] "Fri Feb 09 14:10:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 0.66039, its = 12
i = 2 (of 248), d = 0.355024, its = 12
i = 3 (of 248), d = 0.694694, its = 12
i = 4 (of 248), d = 0.741364, its = 12
i = 5 (of 248), d = 0.6104, its = 13
i = 6 (of 248), d = 0.696194, its = 14
i = 7 (of 248), d = 0.957112, its = 35
i = 8 (of 248), d = 0.810861, its = 12
i = 9 (of 248), d = 0.685241, its = 12
i = 10 (of 248), d = 0.552939, its = 12
i = 11 (of 248), d = 0.743485, its = 14
i = 12 (of 248), d = 0.595115, its = 13
i = 13 (of 248), d = 0.46913, its = 14
i = 14 (of 248), d = 0.992816, its = 13
i = 15 (of 248), d = 0.39203, its = 13
i = 16 (of 248), d = 0.410301, its = 13
i = 17 (of 248), d = 0.698536, its = 26
i = 18 (of 248), d = 0.751418, its = 13
i = 19 (of 248), d = 0.277571, its = 12
i = 20 (of 248), d = 0.322609, its = 12
i = 21 (of 248), d = 0.66708, its = 12
i = 22 (of 248), d = 0.474842, its = 13
i = 23 (of 248), d = 0.910438, its = 12
i = 24 (of 248), d = 0.856174, its = 13
i = 25 (of 248), d = 0.688029, its = 35
i = 26 (of 248), d = 0.407762, its = 12
i = 27 (of 248), d = 0.531485, its = 12
i = 28 (of 248), d = 0.609353, its = 12
i = 29 (of 248), d = 0.282666, its = 12
i = 30 (of 248), d = 0.511283, its = 13
i = 31 (of 248), d = 0.863908, its = 14
i = 32 (of 248), d = 0.316345, its = 12
i = 33 (of 248), d = 0.848143, its = 12
i = 34 (of 248), d = 0.634367, its = 22
i = 35 (of 248), d = 0.593967, its = 12
i = 36 (of 248), d = 0.478415, its = 12
i = 37 (of 248), d = 1.08073, its = 24
i = 38 (of 248), d = 0.662992, its = 13
i = 39 (of 248), d = 0.584174, its = 13
i = 40 (of 248), d = 0.370979, its = 12
i = 41 (of 248), d = 0.567554, its = 13
i = 42 (of 248), d = 0.56178, its = 12
i = 43 (of 248), d = 0.428056, its = 13
i = 44 (of 248), d = 0.622945, its = 13
i = 45 (of 248), d = 0.607731, its = 12
i = 46 (of 248), d = 0.479407, its = 13
i = 47 (of 248), d = 0.581027, its = 12
i = 48 (of 248), d = 0.278052, its = 12
i = 49 (of 248), d = 0.24103, its = 11
i = 50 (of 248), d = 0.850633, its = 13
i = 51 (of 248), d = 0.805224, its = 12
i = 52 (of 248), d = 0.330616, its = 12
i = 53 (of 248), d = 0.537581, its = 16
i = 54 (of 248), d = 0.560391, its = 13
i = 55 (of 248), d = 0.296062, its = 12
i = 56 (of 248), d = 0.95705, its = 14
i = 57 (of 248), d = 0.579746, its = 13
i = 58 (of 248), d = 1.08073, its = 23
i = 59 (of 248), d = 0.49195, its = 12
i = 60 (of 248), d = 0.573371, its = 14
i = 61 (of 248), d = 0.401957, its = 13
i = 62 (of 248), d = 0.594254, its = 12
i = 63 (of 248), d = 0.672588, its = 13
i = 64 (of 248), d = 0.701812, its = 13
i = 65 (of 248), d = 0.308785, its = 12
i = 66 (of 248), d = 0.675174, its = 12
i = 67 (of 248), d = 0.575652, its = 12
i = 68 (of 248), d = 0.551304, its = 13
i = 69 (of 248), d = 0.775982, its = 13
i = 70 (of 248), d = 0.393082, its = 12
i = 71 (of 248), d = 0.573127, its = 12
i = 72 (of 248), d = 0.861417, its = 13
i = 73 (of 248), d = 0.441146, its = 13
i = 74 (of 248), d = 0.818639, its = 14
i = 75 (of 248), d = 0.544246, its = 13
i = 76 (of 248), d = 1.08073, its = 25
i = 77 (of 248), d = 0.485646, its = 12
i = 78 (of 248), d = 0.415266, its = 13
i = 79 (of 248), d = 0.898982, its = 12
i = 80 (of 248), d = 0.585649, its = 14
i = 81 (of 248), d = 0.764778, its = 13
i = 82 (of 248), d = 0.677974, its = 12
i = 83 (of 248), d = 0.329467, its = 12
i = 84 (of 248), d = 1.08073, its = 18
i = 85 (of 248), d = 0.480824, its = 13
i = 86 (of 248), d = 0.735859, its = 13
i = 87 (of 248), d = 0.797697, its = 12
i = 88 (of 248), d = 0.866257, its = 13
i = 89 (of 248), d = 0.746902, its = 14
i = 90 (of 248), d = 0.512578, its = 12
i = 91 (of 248), d = 0.335407, its = 12
i = 92 (of 248), d = 0.631573, its = 12
i = 93 (of 248), d = 0.357105, its = 12
i = 94 (of 248), d = 0.372013, its = 12
i = 95 (of 248), d = 0.756281, its = 14
i = 96 (of 248), d = 0.602358, its = 14
i = 97 (of 248), d = 0.357281, its = 12
i = 98 (of 248), d = 0.791402, its = 14
i = 99 (of 248), d = 0.715571, its = 14
i = 100 (of 248), d = 0.596183, its = 14
i = 101 (of 248), d = 1.02788, its = 13
i = 102 (of 248), d = 0.369927, its = 12
i = 103 (of 248), d = 0.600676, its = 11
i = 104 (of 248), d = 0.526553, its = 13
i = 105 (of 248), d = 0.355039, its = 12
i = 106 (of 248), d = 0.333912, its = 12
i = 107 (of 248), d = 0.370591, its = 13
i = 108 (of 248), d = 0.457284, its = 14
i = 109 (of 248), d = 0.929308, its = 13
i = 110 (of 248), d = 0.668437, its = 14
i = 111 (of 248), d = 0.43337, its = 13
i = 112 (of 248), d = 0.503773, its = 13
i = 113 (of 248), d = 1.08073, its = 23
i = 114 (of 248), d = 0.654527, its = 12
i = 115 (of 248), d = 0.632609, its = 14
i = 116 (of 248), d = 0.730331, its = 14
i = 117 (of 248), d = 0.802012, its = 13
i = 118 (of 248), d = 0.353807, its = 12
i = 119 (of 248), d = 0.643513, its = 26
i = 120 (of 248), d = 0.77356, its = 13
i = 121 (of 248), d = 0.813713, its = 13
i = 122 (of 248), d = 0.86785, its = 14
i = 123 (of 248), d = 0.717995, its = 12
i = 124 (of 248), d = 0.693147, its = 16
i = 125 (of 248), d = 1.08073, its = 27
i = 126 (of 248), d = 0.80652, its = 27
i = 127 (of 248), d = 0.68645, its = 12
i = 128 (of 248), d = 0.65993, its = 13
i = 129 (of 248), d = 0.662884, its = 13
i = 130 (of 248), d = 0.692712, its = 14
i = 131 (of 248), d = 0.617571, its = 12
i = 132 (of 248), d = 0.337434, its = 12
i = 133 (of 248), d = 0.475838, its = 14
i = 134 (of 248), d = 1.08073, its = 57
i = 135 (of 248), d = 0.584949, its = 13
i = 136 (of 248), d = 0.688639, its = 14
i = 137 (of 248), d = 0.371873, its = 13
i = 138 (of 248), d = 0.391201, its = 14
i = 139 (of 248), d = 0.476386, its = 12
i = 140 (of 248), d = 0.960268, its = 13
i = 141 (of 248), d = 1.0228, its = 13
i = 142 (of 248), d = 0.783993, its = 12
i = 143 (of 248), d = 0.648957, its = 14
i = 144 (of 248), d = 0.356278, its = 13
i = 145 (of 248), d = 0.53448, its = 12
i = 146 (of 248), d = 0.900985, its = 13
i = 147 (of 248), d = 0.466957, its = 13
i = 148 (of 248), d = 0.354083, its = 12
i = 149 (of 248), d = 0.58834, its = 12
i = 150 (of 248), d = 0.66753, its = 12
i = 151 (of 248), d = 0.672824, its = 12
i = 152 (of 248), d = 1.01017, its = 15
i = 153 (of 248), d = 0.328966, its = 12
i = 154 (of 248), d = 0.411702, its = 13
i = 155 (of 248), d = 0.505053, its = 12
i = 156 (of 248), d = 0.623085, its = 12
i = 157 (of 248), d = 0.56679, its = 12
i = 158 (of 248), d = 0.807654, its = 13
i = 159 (of 248), d = 0.274427, its = 12
i = 160 (of 248), d = 0.293046, its = 12
i = 161 (of 248), d = 0.673561, its = 12
i = 162 (of 248), d = 1.01866, its = 13
i = 163 (of 248), d = 0.656169, its = 12
i = 164 (of 248), d = 0.935783, its = 14
i = 165 (of 248), d = 0.917912, its = 13
i = 166 (of 248), d = 0.846361, its = 13
i = 167 (of 248), d = 0.284394, its = 12
i = 168 (of 248), d = 0.599651, its = 12
i = 169 (of 248), d = 0.665147, its = 12
i = 170 (of 248), d = 0.579305, its = 12
i = 171 (of 248), d = 0.272256, its = 11
i = 172 (of 248), d = 0.375238, its = 12
i = 173 (of 248), d = 0.733994, its = 12
i = 174 (of 248), d = 0.405965, its = 12
i = 175 (of 248), d = 0.791839, its = 13
i = 176 (of 248), d = 0.38474, its = 12
i = 177 (of 248), d = 0.75291, its = 13
i = 178 (of 248), d = 0.873867, its = 12
i = 179 (of 248), d = 0.680256, its = 12
i = 180 (of 248), d = 0.788564, its = 13
i = 181 (of 248), d = 0.619343, its = 11
i = 182 (of 248), d = 0.912248, its = 37
i = 183 (of 248), d = 0.618282, its = 13
i = 184 (of 248), d = 0.305786, its = 12
i = 185 (of 248), d = 0.362952, its = 12
i = 186 (of 248), d = 0.610672, its = 12
i = 187 (of 248), d = 0.386348, its = 12
i = 188 (of 248), d = 0.846119, its = 13
i = 189 (of 248), d = 0.503816, its = 9
i = 190 (of 248), d = 0.630704, its = 12
i = 191 (of 248), d = 0.70213, its = 29
i = 192 (of 248), d = 0.403133, its = 12
i = 193 (of 248), d = 1.08073, its = 24
i = 194 (of 248), d = 0.28287, its = 12
i = 195 (of 248), d = 0.297902, its = 12
i = 196 (of 248), d = 0.747437, its = 13
i = 197 (of 248), d = 0.317531, its = 11
i = 198 (of 248), d = 0.703752, its = 13
i = 199 (of 248), d = 1.08073, its = 20
i = 200 (of 248), d = 0.63064, its = 12
i = 201 (of 248), d = 0.265698, its = 11
i = 202 (of 248), d = 0.537352, its = 12
i = 203 (of 248), d = 0.843252, its = 12
i = 204 (of 248), d = 0.629031, its = 12
i = 205 (of 248), d = 0.535238, its = 13
i = 206 (of 248), d = 0.44339, its = 12
i = 207 (of 248), d = 0.418342, its = 12
i = 208 (of 248), d = 0.521584, its = 14
i = 209 (of 248), d = 0.599162, its = 13
i = 210 (of 248), d = 0.798432, its = 14
i = 211 (of 248), d = 0.65657, its = 13
i = 212 (of 248), d = 0.719538, its = 12
i = 213 (of 248), d = 0.798389, its = 13
i = 214 (of 248), d = 0.889779, its = 13
i = 215 (of 248), d = 0.715638, its = 14
i = 216 (of 248), d = 0.379616, its = 12
i = 217 (of 248), d = 1.00323, its = 14
i = 218 (of 248), d = 0.470351, its = 13
i = 219 (of 248), d = 1.08073, its = 24
i = 220 (of 248), d = 0.873923, its = 15
i = 221 (of 248), d = 0.870151, its = 13
i = 222 (of 248), d = 0.942824, its = 13
i = 223 (of 248), d = 0.63719, its = 13
i = 224 (of 248), d = 0.234204, its = 12
i = 225 (of 248), d = 0.40969, its = 12
i = 226 (of 248), d = 1.01229, its = 14
i = 227 (of 248), d = 0.905366, its = 13
i = 228 (of 248), d = 0.618551, its = 12
i = 229 (of 248), d = 0.332434, its = 12
i = 230 (of 248), d = 0.819264, its = 13
i = 231 (of 248), d = 0.419255, its = 12
i = 232 (of 248), d = 0.262874, its = 12
i = 233 (of 248), d = 0.321471, its = 12
i = 234 (of 248), d = 0.695287, its = 28
i = 235 (of 248), d = 0.859136, its = 13
i = 236 (of 248), d = 0.970175, its = 14
i = 237 (of 248), d = 0.412909, its = 12
i = 238 (of 248), d = 0.695686, its = 12
i = 239 (of 248), d = 0.364347, its = 12
i = 240 (of 248), d = 0.834689, its = 15
i = 241 (of 248), d = 0.763427, its = 14
i = 242 (of 248), d = 0.922519, its = 16
i = 243 (of 248), d = 0.661213, its = 13
i = 244 (of 248), d = 0.390269, its = 12
i = 245 (of 248), d = 0.766149, its = 12
i = 246 (of 248), d = 0.550883, its = 13
i = 247 (of 248), d = 0.343168, its = 12
i = 248 (of 248), d = 0.939855, its = 14
[1] "Fri Feb 09 14:11:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:11:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:11:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:11:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:11:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:11:20 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 64.982262 
iter  10 value 8.011895
iter  20 value 0.870248
iter  30 value 0.384582
iter  40 value 0.324524
iter  50 value 0.323855
final  value 0.323855 
converged
# weights:  5
initial  value 125.295806 
iter  10 value 11.616680
iter  20 value 0.804450
iter  30 value 0.333808
iter  40 value 0.278913
iter  50 value 0.278299
iter  50 value 0.278299
iter  50 value 0.278299
final  value 0.278299 
converged
# weights:  5
initial  value 57.245405 
iter  10 value 6.677238
iter  20 value 1.548439
iter  30 value 0.369461
iter  40 value 0.278533
iter  50 value 0.277942
final  value 0.277941 
converged
[Tune-y] 1: rmse.test.rmse=0.00463; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 56.012785 
iter  10 value 0.045375
iter  20 value 0.025774
iter  30 value 0.021540
iter  40 value 0.017960
iter  50 value 0.017606
iter  60 value 0.017446
iter  70 value 0.017391
iter  80 value 0.017342
iter  90 value 0.017229
iter 100 value 0.017177
final  value 0.017177 
stopped after 100 iterations
# weights:  57
initial  value 14.372414 
iter  10 value 0.031663
iter  20 value 0.022746
iter  30 value 0.019549
iter  40 value 0.018030
iter  50 value 0.017726
iter  60 value 0.017616
iter  70 value 0.017525
iter  80 value 0.017476
iter  90 value 0.017437
iter 100 value 0.017417
final  value 0.017417 
stopped after 100 iterations
# weights:  57
initial  value 1389.763982 
iter  10 value 0.671712
iter  20 value 0.352312
iter  30 value 0.213624
iter  40 value 0.175407
iter  50 value 0.128533
iter  60 value 0.109658
iter  70 value 0.093809
iter  80 value 0.084326
iter  90 value 0.073033
iter 100 value 0.067463
final  value 0.067463 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0012; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 20.487424 
iter  10 value 0.175091
iter  20 value 0.125387
iter  30 value 0.120980
iter  40 value 0.120177
iter  50 value 0.119783
iter  60 value 0.119493
iter  70 value 0.119392
iter  80 value 0.119347
iter  90 value 0.119112
iter 100 value 0.119018
final  value 0.119018 
stopped after 100 iterations
# weights:  41
initial  value 2003.521764 
iter  10 value 0.195802
iter  20 value 0.139968
iter  30 value 0.134947
iter  40 value 0.127699
iter  50 value 0.123642
iter  60 value 0.122008
iter  70 value 0.121237
iter  80 value 0.120789
iter  90 value 0.120046
iter 100 value 0.119715
final  value 0.119715 
stopped after 100 iterations
# weights:  41
initial  value 9.380428 
iter  10 value 0.169710
iter  20 value 0.129537
iter  30 value 0.126412
iter  40 value 0.123957
iter  50 value 0.122395
iter  60 value 0.121384
iter  70 value 0.120333
iter  80 value 0.119536
iter  90 value 0.118739
iter 100 value 0.118523
final  value 0.118523 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00199; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 755.397768 
iter  10 value 9.152723
iter  20 value 0.380560
iter  30 value 0.027272
iter  40 value 0.022259
iter  50 value 0.018205
iter  60 value 0.017201
iter  70 value 0.017086
iter  80 value 0.017079
final  value 0.017079 
converged
# weights:  5
initial  value 78.618991 
iter  10 value 7.245317
iter  20 value 3.184628
iter  30 value 1.187489
iter  40 value 0.337283
iter  50 value 0.093340
iter  60 value 0.038095
iter  70 value 0.023577
iter  80 value 0.021068
iter  90 value 0.020620
iter 100 value 0.020549
final  value 0.020549 
stopped after 100 iterations
# weights:  5
initial  value 50.758231 
iter  10 value 1.539153
iter  20 value 0.136420
iter  30 value 0.025397
iter  40 value 0.021624
iter  50 value 0.021244
iter  60 value 0.021227
iter  70 value 0.021227
iter  80 value 0.021227
final  value 0.021227 
converged
[Tune-y] 4: rmse.test.rmse=0.00228; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 20.053774 
iter  10 value 0.041611
iter  20 value 0.008476
iter  30 value 0.007286
iter  40 value 0.005306
iter  50 value 0.004833
iter  60 value 0.004647
iter  70 value 0.004315
iter  80 value 0.004234
iter  90 value 0.004179
iter 100 value 0.004121
final  value 0.004121 
stopped after 100 iterations
# weights:  61
initial  value 17.446902 
iter  10 value 0.020870
iter  20 value 0.008920
iter  30 value 0.007218
iter  40 value 0.006740
iter  50 value 0.005800
iter  60 value 0.005547
iter  70 value 0.005209
iter  80 value 0.004840
iter  90 value 0.004776
iter 100 value 0.004584
final  value 0.004584 
stopped after 100 iterations
# weights:  61
initial  value 2582.063601 
iter  10 value 3.782511
iter  20 value 0.799770
iter  30 value 0.547005
iter  40 value 0.485802
iter  50 value 0.439427
iter  60 value 0.391216
iter  70 value 0.301543
iter  80 value 0.222895
iter  90 value 0.146274
iter 100 value 0.106716
final  value 0.106716 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00121; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 75.118734 
iter  10 value 0.384271
iter  20 value 0.176516
iter  30 value 0.155878
iter  40 value 0.154741
iter  50 value 0.154090
iter  60 value 0.153393
iter  70 value 0.152953
iter  80 value 0.152780
iter  90 value 0.152710
iter 100 value 0.152630
final  value 0.152630 
stopped after 100 iterations
# weights:  57
initial  value 398.518965 
iter  10 value 1.806296
iter  20 value 1.038611
iter  30 value 0.656696
iter  40 value 0.460809
iter  50 value 0.363952
iter  60 value 0.338188
iter  70 value 0.322119
iter  80 value 0.303919
iter  90 value 0.285327
iter 100 value 0.259910
final  value 0.259910 
stopped after 100 iterations
# weights:  57
initial  value 106.592234 
iter  10 value 0.316882
iter  20 value 0.176812
iter  30 value 0.160764
iter  40 value 0.158604
iter  50 value 0.156923
iter  60 value 0.156175
iter  70 value 0.155494
iter  80 value 0.155105
iter  90 value 0.154713
iter 100 value 0.154236
final  value 0.154236 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0019; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 16.932386 
iter  10 value 1.582898
iter  20 value 1.205781
iter  30 value 1.124526
iter  40 value 1.058886
iter  50 value 1.014321
iter  60 value 1.003280
iter  70 value 0.996384
iter  80 value 0.995867
iter  90 value 0.995776
iter 100 value 0.995772
final  value 0.995772 
stopped after 100 iterations
# weights:  13
initial  value 151.576192 
iter  10 value 4.608249
iter  20 value 1.231694
iter  30 value 1.058204
iter  40 value 1.036286
iter  50 value 1.020382
iter  60 value 1.016028
iter  70 value 1.014442
iter  80 value 1.014263
final  value 1.014260 
converged
# weights:  13
initial  value 536.709396 
iter  10 value 2.923926
iter  20 value 1.110672
iter  30 value 1.037661
iter  40 value 1.018291
iter  50 value 1.009114
iter  60 value 1.008786
iter  70 value 1.008784
final  value 1.008784 
converged
[Tune-y] 7: rmse.test.rmse=0.00816; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2243.777998 
iter  10 value 10.040824
iter  20 value 6.708845
iter  30 value 5.146212
iter  40 value 2.411202
iter  50 value 1.608084
iter  60 value 1.324749
iter  70 value 1.000439
iter  80 value 0.608972
iter  90 value 0.470043
iter 100 value 0.409355
final  value 0.409355 
stopped after 100 iterations
# weights:  73
initial  value 200.738296 
iter  10 value 2.048335
iter  20 value 0.502728
iter  30 value 0.203720
iter  40 value 0.167363
iter  50 value 0.158522
iter  60 value 0.152526
iter  70 value 0.148964
iter  80 value 0.146416
iter  90 value 0.144691
iter 100 value 0.144173
final  value 0.144173 
stopped after 100 iterations
# weights:  73
initial  value 362.131523 
iter  10 value 5.474008
iter  20 value 0.990000
iter  30 value 0.412794
iter  40 value 0.271165
iter  50 value 0.228656
iter  60 value 0.215178
iter  70 value 0.209207
iter  80 value 0.206265
iter  90 value 0.204371
iter 100 value 0.201926
final  value 0.201926 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00212; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 527.998963 
iter  10 value 0.169623
iter  20 value 0.115836
iter  30 value 0.091992
iter  40 value 0.076304
iter  50 value 0.069295
iter  60 value 0.066129
iter  70 value 0.064245
iter  80 value 0.063194
iter  90 value 0.062579
iter 100 value 0.061927
final  value 0.061927 
stopped after 100 iterations
# weights:  57
initial  value 378.226758 
iter  10 value 25.487750
iter  20 value 12.877487
iter  30 value 0.761659
iter  40 value 0.221833
iter  50 value 0.145573
iter  60 value 0.115952
iter  70 value 0.098320
iter  80 value 0.089695
iter  90 value 0.084616
iter 100 value 0.082406
final  value 0.082406 
stopped after 100 iterations
# weights:  57
initial  value 1488.055591 
iter  10 value 1.819925
iter  20 value 0.493430
iter  30 value 0.366266
iter  40 value 0.299662
iter  50 value 0.233257
iter  60 value 0.172850
iter  70 value 0.134121
iter  80 value 0.116316
iter  90 value 0.106314
iter 100 value 0.101969
final  value 0.101969 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0016; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 44.067483 
iter  10 value 0.203695
iter  20 value 0.019927
iter  30 value 0.002199
iter  40 value 0.001823
iter  50 value 0.001495
iter  60 value 0.001375
iter  70 value 0.001283
iter  80 value 0.001192
iter  90 value 0.001092
iter 100 value 0.000988
final  value 0.000988 
stopped after 100 iterations
# weights:  5
initial  value 750.434533 
iter  10 value 11.676225
iter  20 value 2.005848
iter  30 value 0.286027
iter  40 value 0.027102
iter  50 value 0.001268
iter  60 value 0.001189
iter  70 value 0.001128
iter  80 value 0.001086
iter  90 value 0.001016
iter 100 value 0.000918
final  value 0.000918 
stopped after 100 iterations
# weights:  5
initial  value 21.219949 
iter  10 value 2.336735
iter  20 value 0.446405
iter  30 value 0.077972
iter  40 value 0.007533
iter  50 value 0.001241
iter  60 value 0.001191
iter  70 value 0.001153
iter  80 value 0.001120
iter  90 value 0.001102
iter 100 value 0.001053
final  value 0.001053 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0013; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 40.913888 
iter  10 value 0.083768
iter  20 value 0.047135
iter  30 value 0.038630
iter  40 value 0.036624
iter  50 value 0.035668
iter  60 value 0.035079
iter  70 value 0.034699
iter  80 value 0.033274
iter  90 value 0.032920
iter 100 value 0.032723
final  value 0.032723 
stopped after 100 iterations
# weights:  33
initial  value 201.552499 
iter  10 value 0.100900
iter  20 value 0.051400
iter  30 value 0.043006
iter  40 value 0.040245
iter  50 value 0.038394
iter  60 value 0.037048
iter  70 value 0.036689
iter  80 value 0.034681
iter  90 value 0.034094
iter 100 value 0.033925
final  value 0.033925 
stopped after 100 iterations
# weights:  33
initial  value 9.866276 
iter  10 value 0.052216
iter  20 value 0.038930
iter  30 value 0.034853
iter  40 value 0.034553
iter  50 value 0.034209
iter  60 value 0.034041
iter  70 value 0.033967
iter  80 value 0.033809
iter  90 value 0.033492
iter 100 value 0.033288
final  value 0.033288 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00148; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 269.266483 
iter  10 value 1.906853
iter  20 value 1.399073
iter  30 value 1.376504
iter  40 value 1.296231
iter  50 value 1.244134
iter  60 value 1.243005
iter  70 value 1.242487
iter  80 value 1.237973
iter  90 value 1.225991
iter 100 value 1.221906
final  value 1.221906 
stopped after 100 iterations
# weights:  17
initial  value 296.184289 
iter  10 value 7.330326
iter  20 value 2.839710
iter  30 value 1.455278
iter  40 value 1.315460
iter  50 value 1.281405
iter  60 value 1.273844
iter  70 value 1.265089
iter  80 value 1.250626
iter  90 value 1.231246
iter 100 value 1.226205
final  value 1.226205 
stopped after 100 iterations
# weights:  17
initial  value 561.766350 
iter  10 value 9.889119
iter  20 value 1.399104
iter  30 value 1.319454
iter  40 value 1.293960
iter  50 value 1.270739
iter  60 value 1.264730
iter  70 value 1.260966
iter  80 value 1.242795
iter  90 value 1.224870
iter 100 value 1.219543
final  value 1.219543 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.00932; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 174.520341 
iter  10 value 4.182069
iter  20 value 0.524887
iter  30 value 0.014436
iter  40 value 0.009424
iter  50 value 0.006127
iter  60 value 0.005436
iter  70 value 0.004879
iter  80 value 0.004614
iter  90 value 0.003837
iter 100 value 0.003552
final  value 0.003552 
stopped after 100 iterations
# weights:  13
initial  value 142.146557 
iter  10 value 12.143163
iter  20 value 12.130504
final  value 12.129855 
converged
# weights:  13
initial  value 135.416939 
iter  10 value 11.718564
iter  20 value 11.707699
iter  30 value 11.349820
iter  40 value 5.106919
iter  50 value 0.876914
iter  60 value 0.041559
iter  70 value 0.031341
iter  80 value 0.021179
iter  90 value 0.016989
iter 100 value 0.015357
final  value 0.015357 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0869; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 109.084314 
iter  10 value 0.018885
iter  20 value 0.007602
iter  30 value 0.006799
iter  40 value 0.006137
iter  50 value 0.005687
iter  60 value 0.005444
iter  70 value 0.004840
iter  80 value 0.004622
iter  90 value 0.004559
iter 100 value 0.004527
final  value 0.004527 
stopped after 100 iterations
# weights:  25
initial  value 569.196721 
iter  10 value 0.013461
iter  20 value 0.006227
iter  30 value 0.005727
iter  40 value 0.005528
iter  50 value 0.005305
iter  60 value 0.005148
iter  70 value 0.004643
iter  80 value 0.004553
iter  90 value 0.004496
iter 100 value 0.004466
final  value 0.004466 
stopped after 100 iterations
# weights:  25
initial  value 253.530422 
iter  10 value 0.141723
iter  20 value 0.020569
iter  30 value 0.017492
iter  40 value 0.012536
iter  50 value 0.009747
iter  60 value 0.008232
iter  70 value 0.006189
iter  80 value 0.005538
iter  90 value 0.005173
iter 100 value 0.004897
final  value 0.004897 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.000767; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2089.501968 
iter  10 value 0.023961
iter  20 value 0.011351
iter  30 value 0.010231
iter  40 value 0.009758
iter  50 value 0.009521
iter  60 value 0.009301
iter  70 value 0.009038
iter  80 value 0.008899
iter  90 value 0.008852
iter 100 value 0.008842
final  value 0.008842 
stopped after 100 iterations
# weights:  25
initial  value 1285.290040 
iter  10 value 0.024609
iter  20 value 0.012887
iter  30 value 0.011431
iter  40 value 0.010979
iter  50 value 0.010514
iter  60 value 0.009606
iter  70 value 0.009257
iter  80 value 0.009055
iter  90 value 0.008966
iter 100 value 0.008939
final  value 0.008939 
stopped after 100 iterations
# weights:  25
initial  value 17.003529 
iter  10 value 0.015402
iter  20 value 0.011401
iter  30 value 0.010297
iter  40 value 0.009781
iter  50 value 0.009513
iter  60 value 0.009290
iter  70 value 0.009064
iter  80 value 0.009015
iter  90 value 0.008965
iter 100 value 0.008943
final  value 0.008943 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.000788; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 157.554372 
iter  10 value 12.902231
iter  20 value 12.339822
iter  30 value 12.212724
final  value 12.205294 
converged
# weights:  65
initial  value 1456.806774 
iter  10 value 94.435320
iter  20 value 20.558970
iter  30 value 15.075505
iter  40 value 13.697472
iter  50 value 12.882802
iter  60 value 12.678528
iter  70 value 12.567050
iter  80 value 12.548762
iter  90 value 12.535392
final  value 12.535378 
converged
# weights:  65
initial  value 3518.565498 
iter  10 value 86.511600
iter  20 value 34.672347
iter  30 value 25.015099
iter  40 value 16.964270
iter  50 value 13.869553
iter  60 value 12.395136
iter  70 value 12.128299
iter  80 value 12.117237
final  value 12.111984 
converged
[Tune-y] 16: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 208.685725 
iter  10 value 0.024715
iter  20 value 0.010915
iter  30 value 0.007557
iter  40 value 0.006531
iter  50 value 0.006050
iter  60 value 0.005889
iter  70 value 0.005619
iter  80 value 0.005411
iter  90 value 0.005267
iter 100 value 0.005228
final  value 0.005228 
stopped after 100 iterations
# weights:  29
initial  value 317.682796 
iter  10 value 0.025711
iter  20 value 0.008370
iter  30 value 0.006622
iter  40 value 0.006213
iter  50 value 0.005810
iter  60 value 0.005608
iter  70 value 0.005438
iter  80 value 0.005273
iter  90 value 0.005228
iter 100 value 0.005195
final  value 0.005195 
stopped after 100 iterations
# weights:  29
initial  value 1163.963086 
iter  10 value 0.060980
iter  20 value 0.010137
iter  30 value 0.006938
iter  40 value 0.006242
iter  50 value 0.005889
iter  60 value 0.005639
iter  70 value 0.005429
iter  80 value 0.005278
iter  90 value 0.005237
iter 100 value 0.005191
final  value 0.005191 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00065; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 950.489959 
iter  10 value 0.884889
iter  20 value 0.627529
iter  30 value 0.514062
iter  40 value 0.491909
iter  50 value 0.476957
iter  60 value 0.469221
iter  70 value 0.465567
iter  80 value 0.465336
iter  90 value 0.464984
iter 100 value 0.464831
final  value 0.464831 
stopped after 100 iterations
# weights:  21
initial  value 827.863586 
iter  10 value 1.304129
iter  20 value 0.570524
iter  30 value 0.495475
iter  40 value 0.476735
iter  50 value 0.468249
iter  60 value 0.464891
iter  70 value 0.464054
iter  80 value 0.463694
iter  90 value 0.463413
iter 100 value 0.463347
final  value 0.463347 
stopped after 100 iterations
# weights:  21
initial  value 768.202029 
iter  10 value 6.677906
iter  20 value 2.671995
iter  30 value 0.984929
iter  40 value 0.691515
iter  50 value 0.493384
iter  60 value 0.469617
iter  70 value 0.466641
iter  80 value 0.466089
iter  90 value 0.465629
iter 100 value 0.465559
final  value 0.465559 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0046; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2150.660379 
iter  10 value 7.895406
iter  20 value 4.707519
iter  30 value 3.042770
iter  40 value 1.889320
iter  50 value 0.347395
iter  60 value 0.206670
iter  70 value 0.193565
iter  80 value 0.186407
iter  90 value 0.181771
iter 100 value 0.179412
final  value 0.179412 
stopped after 100 iterations
# weights:  65
initial  value 224.113118 
iter  10 value 0.496241
iter  20 value 0.291246
iter  30 value 0.218035
iter  40 value 0.192025
iter  50 value 0.184763
iter  60 value 0.182080
iter  70 value 0.180577
iter  80 value 0.179741
iter  90 value 0.179049
iter 100 value 0.178382
final  value 0.178382 
stopped after 100 iterations
# weights:  65
initial  value 1143.533755 
iter  10 value 6.263902
iter  20 value 1.322059
iter  30 value 0.655848
iter  40 value 0.430276
iter  50 value 0.364769
iter  60 value 0.313649
iter  70 value 0.230051
iter  80 value 0.201281
iter  90 value 0.192084
iter 100 value 0.189519
final  value 0.189519 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00242; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 49.970314 
iter  10 value 0.046770
iter  20 value 0.006489
iter  30 value 0.004142
iter  40 value 0.002810
iter  50 value 0.002671
iter  60 value 0.002319
iter  70 value 0.002141
iter  80 value 0.002113
iter  90 value 0.002005
iter 100 value 0.001983
final  value 0.001983 
stopped after 100 iterations
# weights:  69
initial  value 60.988274 
iter  10 value 0.016952
iter  20 value 0.004428
iter  30 value 0.003589
iter  40 value 0.003329
iter  50 value 0.002998
iter  60 value 0.002882
iter  70 value 0.002653
iter  80 value 0.002301
iter  90 value 0.002088
iter 100 value 0.001983
final  value 0.001983 
stopped after 100 iterations
# weights:  69
initial  value 18.651330 
iter  10 value 0.014444
iter  20 value 0.003886
iter  30 value 0.003174
iter  40 value 0.002900
iter  50 value 0.002599
iter  60 value 0.002360
iter  70 value 0.002305
iter  80 value 0.002130
iter  90 value 0.002004
iter 100 value 0.001982
final  value 0.001982 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.000408; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.000408
# weights:  69
initial  value 1249.558082 
iter  10 value 0.681975
iter  20 value 0.024086
iter  30 value 0.015291
iter  40 value 0.011955
iter  50 value 0.009718
iter  60 value 0.009469
iter  70 value 0.009014
iter  80 value 0.008007
iter  90 value 0.007219
iter 100 value 0.006995
final  value 0.006995 
stopped after 100 iterations
[1] "Fri Feb 09 14:11:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 696 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 419
 number of selected nodes                               : 69 
[1] "Fri Feb 09 14:11:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:11:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:12:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:12:08 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0228; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0198; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0227; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0194; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.0201; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0233; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0206; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.0198; time: 0.0 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0214; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.0226; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0236; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0219; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0221; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0221; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0216; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0229; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0202; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0193; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.0193
[1] "Fri Feb 09 14:13:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:13:01 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.0651; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.0757; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.104; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.0707; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.0953; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.0645; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.0528; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.0676; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.063; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.061; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.0621; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.0688; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.0842; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.108; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.0558; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.0748; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.099; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.0528
[1] "Fri Feb 09 14:13:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:13:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:13:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 2 
Runtime: 0.02200103 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1         0.5528 
    Y 2          0.532 
    Y 3         0.4704 
    Y 4         0.4905 
    Y 5         0.6643 
[1] "Fri Feb 09 14:13:47 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:50] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:50] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.0875; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:57] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:57] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:13:57] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.143; time: 0.1 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.154; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:14:22] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:14:22] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:14:22] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.154; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.153; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:15:00] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939 : rmse.test.rmse=0.0875
[1] "Fri Feb 09 14:15:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:15:04 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.441327], n=(296,456)
**GROW** @depth 1: [2,0.488497], n=(200,256)
**GROW** @depth 1: [1,0.224306], n=(36,260)
**GROW** @depth 2: [1,0.726864], n=(231,22)
**GROW** @depth 2: [2,0.312883], n=(41,162)
**GROW** @depth 3: [1,0.673694], n=(206,25)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 4: [1,0.588281], n=(136,73)
**GROW** @depth 2: [1,0.302589], n=(76,184)
**GROW** @depth 3: [2,0.499847], n=(79,105)
**GROW** @depth 4: [2,0.75138], n=(90,15)
**GROW** @depth 3: [2,0.428834], n=(81,82)
**GROW** @depth 5: [2,0.620706], n=(72,64)
**GROW** @depth 4: [1,0.668889], n=(71,11)
**GROW** @depth 4: [1,0.554178], n=(50,21)
**GROW** @depth 3: [1,0.336692], n=(47,60)
**GROW** @depth 2: [2,0.517485], n=(18,20)
**PRUNE** @depth 3: [1,0.334832]
**GROW** @depth 5: [2,0.615491], n=(50,38)
**GROW** @depth 4: [1,0.495737], n=(25,25)
**GROW** @depth 4: [2,0.663037], n=(32,14)
**GROW** @depth 5: [1,0.615563], n=(77,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,107,32,14,51,38,15,41,81,25,25,21,12,72,33,77,27,43)
**GROW** @depth 6: [1,0.519299], n=(44,28)
**GROW** @depth 3: [2,0.257822], n=(15,92)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(18,20,15,92,33,14,51,35,15,41,80,25,24,23,12,48,25,34,79,27,41)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [1,0.541466], n=(49,30)
**PRUNE** @depth 5: [1,0.541466]
**GROW** @depth 5: [2,0.559969], n=(12,22)
**GROW** @depth 4: [1,0.329251], n=(42,50)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(18,19,15,42,50,13,22,16,51,33,13,41,80,26,25,23,13,47,24,35,79,27,40)
**GROW** @depth 5: [2,0.379294], n=(17,23)
**GROW** @depth 5: [2,0.555368], n=(25,22)
**PRUNE** @depth 5: [2,0.554908]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(16,19,16,18,17,59,13,22,16,50,33,13,41,77,27,25,24,14,47,24,34,80,29,38)
**GROW** @depth 6: [2,0.557975], n=(13,11)
**PRUNE** @depth 6: [2,0.557975]
**PRUNE** @depth 5: [1,0.324446]
**GROW** @depth 5: [1,0.329251], n=(17,16)
**GROW** @depth 5: [1,0.330491], n=(25,34)
**PRUNE** @depth 5: [1,0.330491]
**GROW** @depth 5: [2,0.809356], n=(66,13)
**GROW** @depth 6: [1,0.52736], n=(40,26)
**GROW** @depth 6: [1,0.387072], n=(33,17)
**PRUNE** @depth 6: [1,0.386142]
**GROW** @depth 5: [2,0.693558], n=(16,11)
**GROW** @depth 5: [1,0.630755], n=(13,22)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,19,18,34,25,34,13,22,16,50,33,14,41,47,57,25,24,14,47,24,13,22,40,27,14,16,11,37)
**GROW** @depth 7: [2,0.713497], n=(25,15)
**PRUNE** @depth 6: [1,0.587041]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(16,19,18,33,25,34,13,22,16,50,33,13,41,47,58,25,23,14,47,37,22,25,15,26,14,16,13,37)
**PRUNE** @depth 3: [2,0.27592]
**GROW** @depth 5: [1,0.568749], n=(28,30)
**GROW** @depth 3: [2,0.228528], n=(11,40)
**GROW** @depth 4: [1,0.324446], n=(19,21)
**PRUNE** @depth 4: [1,0.324446]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,13,39,25,34,12,23,16,50,33,13,41,47,27,30,25,24,14,47,37,22,26,22,19,13,18,13,35)
Grow: 9.949%, Prune: 3.198%, Change: 83.98%, Swap: 27.08%

[1] "Fri Feb 09 14:15:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:15:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:15:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 6.6899 	 alpha= 0.2787 	 beta= 43351.17 
[1] "Fri Feb 09 14:15:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:15:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 14:15:17 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:15:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:15:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:15:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:15:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:15:21 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:15:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:15:23 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:15:26 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.311; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.184; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.279; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.235; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.207; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.306; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.182; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.954; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=1.01; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.171; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.197; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.125
[1] "Fri Feb 09 14:15:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:15:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0816; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0125; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0114; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0375; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0223; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0411; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.00162; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=1.42; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0705; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.361; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0367; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.98; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00145; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0305; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.204; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00145
[1] "Fri Feb 09 14:15:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:15:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:15:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:15:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============                                                       |  22%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:15:57 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 469 is not positive definite

[1] "Fri Feb 09 14:16:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 0.0831641, its = 14
i = 2 (of 248), d = 0.0723574, its = 31
i = 3 (of 248), d = 0.233375, its = 12
i = 4 (of 248), d = 0.095905, its = 9
i = 5 (of 248), d = 0.132757, its = 10
i = 6 (of 248), d = 0.10724, its = 9
i = 7 (of 248), d = 0.112599, its = 10
i = 8 (of 248), d = 0.301862, its = 12
i = 9 (of 248), d = 0.118418, its = 10
i = 10 (of 248), d = 0.117512, its = 10
i = 11 (of 248), d = 0.172271, its = 10
i = 12 (of 248), d = 0.0688141, its = 7
i = 13 (of 248), d = 0.0810934, its = 8
i = 14 (of 248), d = 0.0764655, its = 9
i = 15 (of 248), d = 0.114547, its = 9
i = 16 (of 248), d = 0.0851128, its = 8
i = 17 (of 248), d = 0.17454, its = 11
i = 18 (of 248), d = 0.171283, its = 10
i = 19 (of 248), d = 0.186395, its = 10
i = 20 (of 248), d = 0.230008, its = 12
i = 21 (of 248), d = 0.173009, its = 10
i = 22 (of 248), d = 0.140068, its = 10
i = 23 (of 248), d = 0.0718243, its = 8
i = 24 (of 248), d = 0.124382, its = 10
i = 25 (of 248), d = 0.126531, its = 10
i = 26 (of 248), d = 0.170989, its = 11
i = 27 (of 248), d = 0.105494, its = 10
i = 28 (of 248), d = 0.0812849, its = 14
i = 29 (of 248), d = 0.126144, its = 10
i = 30 (of 248), d = 0.2337, its = 10
i = 31 (of 248), d = 0.103257, its = 9
i = 32 (of 248), d = 0.170878, its = 10
i = 33 (of 248), d = 0.119393, its = 9
i = 34 (of 248), d = 0.0701622, its = 8
i = 35 (of 248), d = 0.104395, its = 9
i = 36 (of 248), d = 0.276316, its = 12
i = 37 (of 248), d = 0.307295, its = 12
i = 38 (of 248), d = 0.14123, its = 9
i = 39 (of 248), d = 0.10499, its = 10
i = 40 (of 248), d = 0.402886, its = 40
i = 41 (of 248), d = 0.277552, its = 11
i = 42 (of 248), d = 0.264759, its = 11
i = 43 (of 248), d = 0.164969, its = 10
i = 44 (of 248), d = 0.180204, its = 10
i = 45 (of 248), d = 0.133666, its = 11
i = 46 (of 248), d = 0.102856, its = 10
i = 47 (of 248), d = 0.294023, its = 12
i = 48 (of 248), d = 0.224104, its = 11
i = 49 (of 248), d = 0.114716, its = 9
i = 50 (of 248), d = 0.248252, its = 11
i = 51 (of 248), d = 0.0871211, its = 9
i = 52 (of 248), d = 0.118925, its = 10
i = 53 (of 248), d = 0.179146, its = 11
i = 54 (of 248), d = 0.191524, its = 10
i = 55 (of 248), d = 0.0983302, its = 8
i = 56 (of 248), d = 0.152825, its = 10
i = 57 (of 248), d = 0.125562, its = 9
i = 58 (of 248), d = 0.268593, its = 11
i = 59 (of 248), d = 0.121468, its = 11
i = 60 (of 248), d = 0.155637, its = 9
i = 61 (of 248), d = 0.207757, its = 11
i = 62 (of 248), d = 0.231398, its = 10
i = 63 (of 248), d = 0.193075, its = 11
i = 64 (of 248), d = 0.162677, its = 11
i = 65 (of 248), d = 0.118547, its = 10
i = 66 (of 248), d = 0.280762, its = 11
i = 67 (of 248), d = 0.175351, its = 11
i = 68 (of 248), d = 0.192435, its = 10
i = 69 (of 248), d = 0.0769418, its = 9
i = 70 (of 248), d = 0.217344, its = 12
i = 71 (of 248), d = 0.106551, its = 9
i = 72 (of 248), d = 0.211601, its = 11
i = 73 (of 248), d = 0.0896952, its = 10
i = 74 (of 248), d = 0.0941949, its = 10
i = 75 (of 248), d = 0.300901, its = 12
i = 76 (of 248), d = 0.137342, its = 14
i = 77 (of 248), d = 0.134084, its = 10
i = 78 (of 248), d = 0.168937, its = 10
i = 79 (of 248), d = 0.116619, its = 10
i = 80 (of 248), d = 0.068376, its = 9
i = 81 (of 248), d = 0.20546, its = 11
i = 82 (of 248), d = 0.09969, its = 9
i = 83 (of 248), d = 0.128004, its = 9
i = 84 (of 248), d = 0.226112, its = 12
i = 85 (of 248), d = 0.0937561, its = 10
i = 86 (of 248), d = 0.103224, its = 9
i = 87 (of 248), d = 0.253017, its = 12
i = 88 (of 248), d = 0.0489416, its = 8
i = 89 (of 248), d = 0.0670164, its = 8
i = 90 (of 248), d = 0.221489, its = 10
i = 91 (of 248), d = 0.14636, its = 10
i = 92 (of 248), d = 0.164106, its = 9
i = 93 (of 248), d = 0.274318, its = 11
i = 94 (of 248), d = 0.186643, its = 11
i = 95 (of 248), d = 0.2142, its = 11
i = 96 (of 248), d = 0.20301, its = 11
i = 97 (of 248), d = 0.226412, its = 11
i = 98 (of 248), d = 0.502182, its = 29
i = 99 (of 248), d = 0.247544, its = 10
i = 100 (of 248), d = 0.0908215, its = 9
i = 101 (of 248), d = 0.140785, its = 9
i = 102 (of 248), d = 0.0953535, its = 9
i = 103 (of 248), d = 0.191968, its = 11
i = 104 (of 248), d = 0.121463, its = 9
i = 105 (of 248), d = 0.0717972, its = 8
i = 106 (of 248), d = 0.108842, its = 9
i = 107 (of 248), d = 0.178933, its = 11
i = 108 (of 248), d = 0.114067, its = 10
i = 109 (of 248), d = 0.094424, its = 9
i = 110 (of 248), d = 0.163311, its = 10
i = 111 (of 248), d = 0.206597, its = 11
i = 112 (of 248), d = 0.162071, its = 10
i = 113 (of 248), d = 0.13238, its = 11
i = 114 (of 248), d = 0.220725, its = 10
i = 115 (of 248), d = 0.100291, its = 9
i = 116 (of 248), d = 0.257246, its = 10
i = 117 (of 248), d = 0.236395, its = 11
i = 118 (of 248), d = 0.084534, its = 9
i = 119 (of 248), d = 0.0762523, its = 9
i = 120 (of 248), d = 0.132808, its = 9
i = 121 (of 248), d = 0.296049, its = 11
i = 122 (of 248), d = 0.260073, its = 11
i = 123 (of 248), d = 0.113362, its = 10
i = 124 (of 248), d = 0.119764, its = 9
i = 125 (of 248), d = 0.12761, its = 10
i = 126 (of 248), d = 0.157931, its = 10
i = 127 (of 248), d = 0.111112, its = 10
i = 128 (of 248), d = 0.091411, its = 9
i = 129 (of 248), d = 0.112749, its = 9
i = 130 (of 248), d = 0.180608, its = 10
i = 131 (of 248), d = 0.146975, its = 11
i = 132 (of 248), d = 0.220282, its = 11
i = 133 (of 248), d = 0.0812991, its = 24
i = 134 (of 248), d = 0.219279, its = 11
i = 135 (of 248), d = 0.215031, its = 11
i = 136 (of 248), d = 0.206419, its = 12
i = 137 (of 248), d = 0.114712, its = 15
i = 138 (of 248), d = 0.052968, its = 9
i = 139 (of 248), d = 0.218135, its = 11
i = 140 (of 248), d = 0.121015, its = 9
i = 141 (of 248), d = 0.106804, its = 10
i = 142 (of 248), d = 0.0918552, its = 8
i = 143 (of 248), d = 0.149861, its = 8
i = 144 (of 248), d = 0.0842301, its = 10
i = 145 (of 248), d = 0.187931, its = 11
i = 146 (of 248), d = 0.10149, its = 8
i = 147 (of 248), d = 0.0710755, its = 8
i = 148 (of 248), d = 0.0976666, its = 8
i = 149 (of 248), d = 0.10704, its = 9
i = 150 (of 248), d = 0.186889, its = 10
i = 151 (of 248), d = 0.0911634, its = 9
i = 152 (of 248), d = 0.153292, its = 10
i = 153 (of 248), d = 0.196903, its = 10
i = 154 (of 248), d = 0.260058, its = 11
i = 155 (of 248), d = 0.0930243, its = 9
i = 156 (of 248), d = 0.132294, its = 28
i = 157 (of 248), d = 0.191188, its = 10
i = 158 (of 248), d = 0.0778335, its = 30
i = 159 (of 248), d = 0.097931, its = 9
i = 160 (of 248), d = 0.0751368, its = 8
i = 161 (of 248), d = 0.174, its = 10
i = 162 (of 248), d = 0.130847, its = 10
i = 163 (of 248), d = 0.238028, its = 12
i = 164 (of 248), d = 0.11999, its = 9
i = 165 (of 248), d = 0.0891957, its = 9
i = 166 (of 248), d = 0.0986145, its = 8
i = 167 (of 248), d = 0.116971, its = 10
i = 168 (of 248), d = 0.111561, its = 15
i = 169 (of 248), d = 0.248037, its = 11
i = 170 (of 248), d = 0.18162, its = 9
i = 171 (of 248), d = 0.136058, its = 10
i = 172 (of 248), d = 0.125896, its = 9
i = 173 (of 248), d = 0.239849, its = 11
i = 174 (of 248), d = 0.164603, its = 10
i = 175 (of 248), d = 0.184834, its = 11
i = 176 (of 248), d = 0.213205, its = 10
i = 177 (of 248), d = 0.1501, its = 10
i = 178 (of 248), d = 0.148832, its = 9
i = 179 (of 248), d = 0.107752, its = 9
i = 180 (of 248), d = 0.0771879, its = 9
i = 181 (of 248), d = 0.0718244, its = 9
i = 182 (of 248), d = 0.336324, its = 12
i = 183 (of 248), d = 0.136519, its = 28
i = 184 (of 248), d = 0.152913, its = 11
i = 185 (of 248), d = 0.154385, its = 10
i = 186 (of 248), d = 0.0941068, its = 10
i = 187 (of 248), d = 0.137368, its = 10
i = 188 (of 248), d = 0.109891, its = 9
i = 189 (of 248), d = 0.161057, its = 10
i = 190 (of 248), d = 0.198044, its = 11
i = 191 (of 248), d = 0.115211, its = 9
i = 192 (of 248), d = 0.101492, its = 34
i = 193 (of 248), d = 0.204874, its = 10
i = 194 (of 248), d = 0.113803, its = 10
i = 195 (of 248), d = 0.144893, its = 9
i = 196 (of 248), d = 0.0688915, its = 9
i = 197 (of 248), d = 0.230435, its = 12
i = 198 (of 248), d = 0.107724, its = 10
i = 199 (of 248), d = 0.147504, its = 10
i = 200 (of 248), d = 0.084158, its = 9
i = 201 (of 248), d = 0.137829, its = 10
i = 202 (of 248), d = 0.0787041, its = 8
i = 203 (of 248), d = 0.152121, its = 10
i = 204 (of 248), d = 0.137563, its = 10
i = 205 (of 248), d = 0.146352, its = 9
i = 206 (of 248), d = 0.0821772, its = 9
i = 207 (of 248), d = 0.270675, its = 11
i = 208 (of 248), d = 0.0878656, its = 8
i = 209 (of 248), d = 0.139962, its = 10
i = 210 (of 248), d = 0.102779, its = 9
i = 211 (of 248), d = 0.0801303, its = 9
i = 212 (of 248), d = 0.0875875, its = 9
i = 213 (of 248), d = 0.15983, its = 10
i = 214 (of 248), d = 0.0999293, its = 9
i = 215 (of 248), d = 0.128211, its = 10
i = 216 (of 248), d = 0.149468, its = 10
i = 217 (of 248), d = 0.202483, its = 11
i = 218 (of 248), d = 0.113874, its = 9
i = 219 (of 248), d = 0.0823461, its = 9
i = 220 (of 248), d = 0.155326, its = 9
i = 221 (of 248), d = 0.185217, its = 11
i = 222 (of 248), d = 0.076397, its = 8
i = 223 (of 248), d = 0.30375, its = 11
i = 224 (of 248), d = 0.201829, its = 9
i = 225 (of 248), d = 0.282236, its = 11
i = 226 (of 248), d = 0.261707, its = 11
i = 227 (of 248), d = 0.528558, its = 12
i = 228 (of 248), d = 0.0819503, its = 9
i = 229 (of 248), d = 0.137185, its = 9
i = 230 (of 248), d = 0.164897, its = 10
i = 231 (of 248), d = 0.197807, its = 10
i = 232 (of 248), d = 0.121823, its = 10
i = 233 (of 248), d = 0.182071, its = 20
i = 234 (of 248), d = 0.180613, its = 11
i = 235 (of 248), d = 0.0962074, its = 10
i = 236 (of 248), d = 0.145155, its = 10
i = 237 (of 248), d = 0.190819, its = 11
i = 238 (of 248), d = 0.1404, its = 10
i = 239 (of 248), d = 0.118658, its = 10
i = 240 (of 248), d = 0.0927975, its = 17
i = 241 (of 248), d = 0.135104, its = 10
i = 242 (of 248), d = 0.133583, its = 9
i = 243 (of 248), d = 0.213629, its = 10
i = 244 (of 248), d = 0.228378, its = 11
i = 245 (of 248), d = 0.120238, its = 10
i = 246 (of 248), d = 0.164711, its = 10
i = 247 (of 248), d = 0.119318, its = 9
i = 248 (of 248), d = 0.091792, its = 8
[1] "Fri Feb 09 14:16:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:16:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:16:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:16:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:16:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:16:57 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 1771.147567 
iter  10 value 7.845236
iter  20 value 4.495743
iter  30 value 1.875767
iter  40 value 0.988990
iter  50 value 0.817135
iter  60 value 0.734295
iter  70 value 0.702339
iter  80 value 0.658448
iter  90 value 0.617567
iter 100 value 0.585954
final  value 0.585954 
stopped after 100 iterations
# weights:  77
initial  value 920.358364 
iter  10 value 5.225719
iter  20 value 1.079908
iter  30 value 0.930704
iter  40 value 0.752154
iter  50 value 0.679237
iter  60 value 0.638811
iter  70 value 0.615289
iter  80 value 0.597944
iter  90 value 0.585284
iter 100 value 0.570490
final  value 0.570490 
stopped after 100 iterations
# weights:  77
initial  value 1106.857748 
iter  10 value 10.326094
iter  20 value 1.595584
iter  30 value 1.208328
iter  40 value 1.099600
iter  50 value 1.017549
iter  60 value 0.946975
iter  70 value 0.818588
iter  80 value 0.740780
iter  90 value 0.694381
iter 100 value 0.656640
final  value 0.656640 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0106; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 1096.126253 
iter  10 value 14.998844
iter  20 value 1.447192
iter  30 value 0.316300
iter  40 value 0.113867
iter  50 value 0.104473
iter  60 value 0.101502
iter  70 value 0.095353
iter  80 value 0.091501
iter  90 value 0.088625
iter 100 value 0.085210
final  value 0.085210 
stopped after 100 iterations
# weights:  53
initial  value 1671.459368 
iter  10 value 20.497183
iter  20 value 0.515436
iter  30 value 0.132543
iter  40 value 0.072445
iter  50 value 0.065622
iter  60 value 0.062095
iter  70 value 0.058989
iter  80 value 0.056871
iter  90 value 0.056191
iter 100 value 0.054343
final  value 0.054343 
stopped after 100 iterations
# weights:  53
initial  value 1332.834759 
iter  10 value 136.415052
iter  20 value 5.783433
iter  30 value 0.535942
iter  40 value 0.371777
iter  50 value 0.302359
iter  60 value 0.245824
iter  70 value 0.170350
iter  80 value 0.135223
iter  90 value 0.117762
iter 100 value 0.106063
final  value 0.106063 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00556; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 1074.376291 
iter  10 value 14.661713
iter  20 value 0.537011
iter  30 value 0.186886
iter  40 value 0.162274
iter  50 value 0.147412
iter  60 value 0.136562
iter  70 value 0.131963
iter  80 value 0.124908
iter  90 value 0.114228
iter 100 value 0.109410
final  value 0.109410 
stopped after 100 iterations
# weights:  49
initial  value 1174.867173 
iter  10 value 9.529723
iter  20 value 0.495655
iter  30 value 0.139588
iter  40 value 0.116843
iter  50 value 0.106798
iter  60 value 0.098164
iter  70 value 0.084815
iter  80 value 0.077647
iter  90 value 0.073236
iter 100 value 0.067346
final  value 0.067346 
stopped after 100 iterations
# weights:  49
initial  value 2179.449185 
iter  10 value 1.009102
iter  20 value 0.233778
iter  30 value 0.083734
iter  40 value 0.080700
iter  50 value 0.079269
iter  60 value 0.077716
iter  70 value 0.075210
iter  80 value 0.073529
iter  90 value 0.072229
iter 100 value 0.070572
final  value 0.070572 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00409; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 1098.402597 
iter  10 value 25.260446
iter  20 value 3.023393
iter  30 value 1.678101
iter  40 value 0.680043
iter  50 value 0.637072
iter  60 value 0.537113
iter  70 value 0.511347
iter  80 value 0.479086
iter  90 value 0.471644
iter 100 value 0.460935
final  value 0.460935 
stopped after 100 iterations
# weights:  9
initial  value 1022.038234 
iter  10 value 9.829465
iter  20 value 1.857734
iter  30 value 1.101514
iter  40 value 0.664682
iter  50 value 0.554421
iter  60 value 0.500487
iter  70 value 0.468602
iter  80 value 0.444282
iter  90 value 0.440254
iter 100 value 0.439909
final  value 0.439909 
stopped after 100 iterations
# weights:  9
initial  value 1125.212174 
iter  10 value 138.596313
iter  20 value 23.855450
iter  30 value 8.295601
iter  40 value 1.344310
iter  50 value 0.760746
iter  60 value 0.592614
iter  70 value 0.560825
iter  80 value 0.560047
iter  90 value 0.559725
final  value 0.559665 
converged
[Tune-y] 4: rmse.test.rmse=0.0121; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 1076.729536 
iter  10 value 8.793483
iter  20 value 2.318326
iter  30 value 1.829324
iter  40 value 1.683873
iter  50 value 1.640912
iter  60 value 1.603770
iter  70 value 1.572019
iter  80 value 1.546531
iter  90 value 1.530002
iter 100 value 1.513689
final  value 1.513689 
stopped after 100 iterations
# weights:  77
initial  value 939.758626 
iter  10 value 18.864806
iter  20 value 3.498875
iter  30 value 2.853716
iter  40 value 2.368509
iter  50 value 1.992001
iter  60 value 1.773919
iter  70 value 1.673970
iter  80 value 1.615352
iter  90 value 1.579069
iter 100 value 1.548107
final  value 1.548107 
stopped after 100 iterations
# weights:  77
initial  value 1167.871201 
iter  10 value 110.285938
iter  20 value 7.243189
iter  30 value 3.741262
iter  40 value 2.879367
iter  50 value 2.509193
iter  60 value 2.243722
iter  70 value 1.927755
iter  80 value 1.761447
iter  90 value 1.662828
iter 100 value 1.613137
final  value 1.613137 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 1290.321846 
iter  10 value 376.473117
iter  20 value 167.877453
iter  30 value 138.879998
iter  40 value 133.770507
iter  50 value 130.911123
iter  60 value 128.682347
iter  70 value 127.458626
iter  80 value 126.936839
iter  90 value 126.554272
iter 100 value 126.110571
final  value 126.110571 
stopped after 100 iterations
# weights:  73
initial  value 978.390897 
iter  10 value 174.004168
iter  20 value 137.829646
iter  30 value 127.380844
iter  40 value 125.713653
iter  50 value 125.439516
iter  60 value 125.085787
iter  70 value 124.883575
iter  80 value 124.741545
iter  90 value 124.562805
iter 100 value 124.416441
final  value 124.416441 
stopped after 100 iterations
# weights:  73
initial  value 1123.376116 
iter  10 value 189.520599
iter  20 value 149.091888
iter  30 value 129.744791
iter  40 value 127.809052
iter  50 value 127.145652
iter  60 value 126.745344
iter  70 value 126.008598
iter  80 value 125.464993
iter  90 value 125.284990
iter 100 value 125.235752
final  value 125.235752 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.105; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 1429.020591 
iter  10 value 69.052360
iter  20 value 0.845863
iter  30 value 0.717149
iter  40 value 0.465182
iter  50 value 0.353494
iter  60 value 0.285424
iter  70 value 0.234184
iter  80 value 0.209397
iter  90 value 0.188408
iter 100 value 0.167421
final  value 0.167421 
stopped after 100 iterations
# weights:  53
initial  value 1092.043761 
iter  10 value 100.178855
iter  20 value 1.725046
iter  30 value 0.478789
iter  40 value 0.419917
iter  50 value 0.333925
iter  60 value 0.265783
iter  70 value 0.234216
iter  80 value 0.200872
iter  90 value 0.186512
iter 100 value 0.169877
final  value 0.169877 
stopped after 100 iterations
# weights:  53
initial  value 1180.289317 
iter  10 value 10.063816
iter  20 value 0.476999
iter  30 value 0.337800
iter  40 value 0.320933
iter  50 value 0.278842
iter  60 value 0.253706
iter  70 value 0.231115
iter  80 value 0.207974
iter  90 value 0.188626
iter 100 value 0.181384
final  value 0.181384 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00451; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 1618.154297 
iter  10 value 3.846048
iter  20 value 1.368927
iter  30 value 1.198157
iter  40 value 1.037086
iter  50 value 0.764842
iter  60 value 0.565743
iter  70 value 0.477314
iter  80 value 0.389500
iter  90 value 0.321629
iter 100 value 0.292724
final  value 0.292724 
stopped after 100 iterations
# weights:  33
initial  value 1394.134823 
iter  10 value 22.191846
iter  20 value 0.737096
iter  30 value 0.314867
iter  40 value 0.284183
iter  50 value 0.277275
iter  60 value 0.272788
iter  70 value 0.267762
iter  80 value 0.256357
iter  90 value 0.240290
iter 100 value 0.232472
final  value 0.232472 
stopped after 100 iterations
# weights:  33
initial  value 1124.246928 
iter  10 value 74.307768
iter  20 value 3.348975
iter  30 value 0.510337
iter  40 value 0.431088
iter  50 value 0.372365
iter  60 value 0.351937
iter  70 value 0.324717
iter  80 value 0.296939
iter  90 value 0.266914
iter 100 value 0.252733
final  value 0.252733 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00505; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 1171.600096 
iter  10 value 19.259809
iter  20 value 0.841163
iter  30 value 0.613177
iter  40 value 0.508466
iter  50 value 0.427757
iter  60 value 0.363638
iter  70 value 0.331713
iter  80 value 0.307952
iter  90 value 0.292560
iter 100 value 0.282347
final  value 0.282347 
stopped after 100 iterations
# weights:  77
initial  value 1355.176463 
iter  10 value 8.425858
iter  20 value 0.663311
iter  30 value 0.455808
iter  40 value 0.407125
iter  50 value 0.379141
iter  60 value 0.361842
iter  70 value 0.341778
iter  80 value 0.321795
iter  90 value 0.303377
iter 100 value 0.285200
final  value 0.285200 
stopped after 100 iterations
# weights:  77
initial  value 2098.829574 
iter  10 value 8.668020
iter  20 value 0.522487
iter  30 value 0.410920
iter  40 value 0.347469
iter  50 value 0.285749
iter  60 value 0.260012
iter  70 value 0.250437
iter  80 value 0.241632
iter  90 value 0.236112
iter 100 value 0.232089
final  value 0.232089 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00594; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 1086.487757 
iter  10 value 125.473609
iter  20 value 2.790750
iter  30 value 0.417215
iter  40 value 0.210217
iter  50 value 0.065224
iter  60 value 0.022780
iter  70 value 0.011603
iter  80 value 0.008342
iter  90 value 0.008192
iter 100 value 0.007872
final  value 0.007872 
stopped after 100 iterations
# weights:  21
initial  value 1005.084212 
iter  10 value 173.985428
iter  20 value 3.715526
iter  30 value 0.087205
iter  40 value 0.029408
iter  50 value 0.024234
iter  60 value 0.018500
iter  70 value 0.017194
iter  80 value 0.016028
iter  90 value 0.015507
iter 100 value 0.015210
final  value 0.015210 
stopped after 100 iterations
# weights:  21
initial  value 1060.335092 
iter  10 value 17.630872
iter  20 value 0.662280
iter  30 value 0.157994
iter  40 value 0.029158
iter  50 value 0.015157
iter  60 value 0.009261
iter  70 value 0.006694
iter  80 value 0.005486
iter  90 value 0.004735
iter 100 value 0.004451
final  value 0.004451 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00228; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 1423.309039 
iter  10 value 322.722350
iter  20 value 146.526286
iter  30 value 113.886972
iter  40 value 106.631526
iter  50 value 103.379856
iter  60 value 101.647467
iter  70 value 101.085041
iter  80 value 100.773302
iter  90 value 100.623488
iter 100 value 100.481738
final  value 100.481738 
stopped after 100 iterations
# weights:  73
initial  value 1307.910505 
iter  10 value 206.342162
iter  20 value 116.746639
iter  30 value 105.050654
iter  40 value 102.370717
iter  50 value 101.574829
iter  60 value 101.089178
iter  70 value 100.841281
iter  80 value 100.575638
iter  90 value 100.435777
iter 100 value 100.346668
final  value 100.346668 
stopped after 100 iterations
# weights:  73
initial  value 1552.521752 
iter  10 value 208.004549
iter  20 value 120.586395
iter  30 value 105.731951
iter  40 value 103.462227
iter  50 value 102.257359
iter  60 value 101.475763
iter  70 value 100.940593
iter  80 value 100.707386
iter  90 value 100.548730
iter 100 value 100.340751
final  value 100.340751 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0844; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 1315.395801 
iter  10 value 269.997532
iter  20 value 145.276484
iter  30 value 101.397824
iter  40 value 92.991925
iter  50 value 90.200942
iter  60 value 88.959258
iter  70 value 88.317917
iter  80 value 88.078628
iter  90 value 87.848088
iter 100 value 87.369923
final  value 87.369923 
stopped after 100 iterations
# weights:  45
initial  value 1027.409096 
iter  10 value 282.195401
iter  20 value 147.443440
iter  30 value 109.332175
iter  40 value 97.651774
iter  50 value 92.387209
iter  60 value 90.743902
iter  70 value 89.703689
iter  80 value 88.581970
iter  90 value 87.844438
iter 100 value 87.046025
final  value 87.046025 
stopped after 100 iterations
# weights:  45
initial  value 2000.040715 
iter  10 value 423.323325
iter  20 value 143.132428
iter  30 value 101.484361
iter  40 value 92.674120
iter  50 value 89.442889
iter  60 value 88.688165
iter  70 value 88.380207
iter  80 value 88.000709
iter  90 value 87.596994
iter 100 value 87.059689
final  value 87.059689 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0735; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 1468.286316 
iter  10 value 53.869308
iter  20 value 6.596709
iter  30 value 5.738216
iter  40 value 5.079855
iter  50 value 4.464945
iter  60 value 3.809343
iter  70 value 3.625244
iter  80 value 3.569845
iter  90 value 3.545276
iter 100 value 3.475189
final  value 3.475189 
stopped after 100 iterations
# weights:  21
initial  value 1244.464729 
iter  10 value 58.530959
iter  20 value 5.437941
iter  30 value 4.198692
iter  40 value 4.019493
iter  50 value 3.912915
iter  60 value 3.666025
iter  70 value 3.422831
iter  80 value 3.378284
iter  90 value 3.364119
iter 100 value 3.321148
final  value 3.321148 
stopped after 100 iterations
# weights:  21
initial  value 1151.869660 
iter  10 value 318.541659
iter  20 value 39.580785
iter  30 value 8.566758
iter  40 value 4.890296
iter  50 value 4.405905
iter  60 value 3.867216
iter  70 value 3.603601
iter  80 value 3.404841
iter  90 value 3.376808
iter 100 value 3.371921
final  value 3.371921 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0317; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1618.590535 
iter  10 value 44.320609
iter  20 value 2.826610
iter  30 value 1.573612
iter  40 value 1.118311
iter  50 value 0.997522
iter  60 value 0.918586
iter  70 value 0.823450
iter  80 value 0.771592
iter  90 value 0.730366
iter 100 value 0.704420
final  value 0.704420 
stopped after 100 iterations
# weights:  53
initial  value 998.936024 
iter  10 value 25.613343
iter  20 value 1.348014
iter  30 value 1.151696
iter  40 value 1.033558
iter  50 value 0.905204
iter  60 value 0.809665
iter  70 value 0.731613
iter  80 value 0.688642
iter  90 value 0.650244
iter 100 value 0.623842
final  value 0.623842 
stopped after 100 iterations
# weights:  53
initial  value 1895.012675 
iter  10 value 16.924905
iter  20 value 2.189602
iter  30 value 1.589941
iter  40 value 1.318977
iter  50 value 1.056999
iter  60 value 0.963766
iter  70 value 0.857114
iter  80 value 0.769920
iter  90 value 0.721762
iter 100 value 0.686922
final  value 0.686922 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.012; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 2028.490800 
iter  10 value 18.951303
iter  20 value 14.489854
iter  30 value 9.088122
iter  40 value 7.779660
iter  50 value 7.489974
iter  60 value 7.313980
iter  70 value 7.255396
iter  80 value 7.228477
iter  90 value 7.205714
iter 100 value 7.133962
final  value 7.133962 
stopped after 100 iterations
# weights:  45
initial  value 2030.979832 
iter  10 value 28.306776
iter  20 value 16.203763
iter  30 value 9.105859
iter  40 value 7.884405
iter  50 value 7.539574
iter  60 value 7.351601
iter  70 value 7.236774
iter  80 value 7.146904
iter  90 value 7.077084
iter 100 value 7.017470
final  value 7.017470 
stopped after 100 iterations
# weights:  45
initial  value 1702.427841 
iter  10 value 107.456647
iter  20 value 31.364821
iter  30 value 14.730675
iter  40 value 10.085392
iter  50 value 8.468421
iter  60 value 7.906720
iter  70 value 7.518116
iter  80 value 7.444936
iter  90 value 7.372138
iter 100 value 7.163852
final  value 7.163852 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0246; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 1099.435922 
iter  10 value 55.259943
iter  20 value 1.073908
iter  30 value 0.790384
iter  40 value 0.691240
iter  50 value 0.639795
iter  60 value 0.574024
iter  70 value 0.520333
iter  80 value 0.477257
iter  90 value 0.378044
iter 100 value 0.322017
final  value 0.322017 
stopped after 100 iterations
# weights:  37
initial  value 1039.857731 
iter  10 value 19.111910
iter  20 value 1.337151
iter  30 value 0.968969
iter  40 value 0.921465
iter  50 value 0.798289
iter  60 value 0.644444
iter  70 value 0.568724
iter  80 value 0.545338
iter  90 value 0.458849
iter 100 value 0.381596
final  value 0.381596 
stopped after 100 iterations
# weights:  37
initial  value 1645.179048 
iter  10 value 18.297935
iter  20 value 0.692572
iter  30 value 0.510483
iter  40 value 0.422144
iter  50 value 0.355828
iter  60 value 0.334117
iter  70 value 0.323035
iter  80 value 0.314298
iter  90 value 0.306986
iter 100 value 0.287623
final  value 0.287623 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0083; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 1120.249951 
iter  10 value 185.375413
iter  20 value 96.795267
iter  30 value 94.540572
iter  40 value 93.325328
iter  50 value 93.018422
iter  60 value 92.817456
iter  70 value 92.795623
final  value 92.795532 
converged
# weights:  21
initial  value 1058.531721 
iter  10 value 300.237297
iter  20 value 155.672345
iter  30 value 99.150776
iter  40 value 95.478930
iter  50 value 94.438881
iter  60 value 93.811779
iter  70 value 91.828821
iter  80 value 90.668681
iter  90 value 90.629112
iter 100 value 90.622605
final  value 90.622605 
stopped after 100 iterations
# weights:  21
initial  value 1491.965509 
iter  10 value 155.218442
iter  20 value 99.840930
iter  30 value 95.980983
iter  40 value 93.759736
iter  50 value 92.284761
iter  60 value 91.729072
iter  70 value 91.531540
iter  80 value 91.488115
final  value 91.488095 
converged
[Tune-y] 17: rmse.test.rmse=0.0883; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 1259.132716 
iter  10 value 10.627087
iter  20 value 0.669043
iter  30 value 0.022221
iter  40 value 0.010346
iter  50 value 0.004998
iter  60 value 0.004637
iter  70 value 0.004093
iter  80 value 0.003920
iter  90 value 0.003871
iter 100 value 0.003747
final  value 0.003747 
stopped after 100 iterations
# weights:  57
initial  value 1612.240631 
iter  10 value 18.388654
iter  20 value 0.268011
iter  30 value 0.029778
iter  40 value 0.011280
iter  50 value 0.005813
iter  60 value 0.005100
iter  70 value 0.004844
iter  80 value 0.004705
iter  90 value 0.004608
iter 100 value 0.004530
final  value 0.004530 
stopped after 100 iterations
# weights:  57
initial  value 2013.844877 
iter  10 value 4.763356
iter  20 value 0.417201
iter  30 value 0.021621
iter  40 value 0.004738
iter  50 value 0.004051
iter  60 value 0.003900
iter  70 value 0.003814
iter  80 value 0.003795
iter  90 value 0.003745
iter 100 value 0.003644
final  value 0.003644 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00124; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 1307.286701 
iter  10 value 1.572846
iter  20 value 0.357457
iter  30 value 0.322577
iter  40 value 0.318083
iter  50 value 0.292576
iter  60 value 0.273065
iter  70 value 0.269835
iter  80 value 0.267494
iter  90 value 0.265880
iter 100 value 0.264814
final  value 0.264814 
stopped after 100 iterations
# weights:  17
initial  value 967.587856 
iter  10 value 36.512761
iter  20 value 0.902848
iter  30 value 0.686830
iter  40 value 0.596747
iter  50 value 0.491492
iter  60 value 0.351689
iter  70 value 0.294292
iter  80 value 0.279955
iter  90 value 0.265425
iter 100 value 0.262501
final  value 0.262501 
stopped after 100 iterations
# weights:  17
initial  value 1127.782737 
iter  10 value 339.110293
iter  20 value 7.267007
iter  30 value 1.177946
iter  40 value 0.968266
iter  50 value 0.549890
iter  60 value 0.388971
iter  70 value 0.345671
iter  80 value 0.321594
iter  90 value 0.296012
iter 100 value 0.284982
final  value 0.284982 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00675; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 1097.930069 
iter  10 value 95.641398
iter  20 value 15.568336
iter  30 value 10.681709
iter  40 value 7.801286
iter  50 value 5.570406
iter  60 value 4.802102
iter  70 value 4.264731
iter  80 value 3.721265
iter  90 value 3.603784
iter 100 value 3.581013
final  value 3.581013 
stopped after 100 iterations
# weights:  29
initial  value 1031.104501 
iter  10 value 10.669187
iter  20 value 5.292592
iter  30 value 4.641182
iter  40 value 4.389926
iter  50 value 3.992306
iter  60 value 3.803248
iter  70 value 3.687612
iter  80 value 3.525374
iter  90 value 3.476788
iter 100 value 3.449064
final  value 3.449064 
stopped after 100 iterations
# weights:  29
initial  value 1274.467171 
iter  10 value 16.677837
iter  20 value 8.254674
iter  30 value 5.856913
iter  40 value 4.501130
iter  50 value 4.079860
iter  60 value 3.915466
iter  70 value 3.743247
iter  80 value 3.682022
iter  90 value 3.646667
iter 100 value 3.624279
final  value 3.624279 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0264; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00124
# weights:  57
initial  value 1686.247595 
iter  10 value 48.613584
iter  20 value 3.155780
iter  30 value 0.378599
iter  40 value 0.020976
iter  50 value 0.011352
iter  60 value 0.008045
iter  70 value 0.007704
iter  80 value 0.006758
iter  90 value 0.006525
iter 100 value 0.006427
final  value 0.006427 
stopped after 100 iterations
[1] "Fri Feb 09 14:17:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 654 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 391
 number of selected nodes                               : 69 
[1] "Fri Feb 09 14:17:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:17:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:17:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:17:46 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.164; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.172; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.166; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.186; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.165; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.206; time: 0.1 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.213; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.164
[1] "Fri Feb 09 14:18:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:18:45 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.861; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.897; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.61; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.625; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.677; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=1.45; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.799; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=1.26; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.603; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.745; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.588; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=1.32; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=1.05; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.511
[1] "Fri Feb 09 14:18:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:18:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:19:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7240 0.3270 0.1480 0.0671 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.01700091 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1482 
    Y 1           0.31 
    Y 2       -0.04699 
    Y 3         -2.103 
    Y 4        -0.4739 
    Y 5         -0.282 
[1] "Fri Feb 09 14:19:26 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.276; time: 0.6 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.37; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.268; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:20:30] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.497; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.306; time: 0.6 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.357; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:07] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:07] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:08] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.361; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:11] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.402; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:12] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:12] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:12] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.324; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:21:13] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.268
[1] "Fri Feb 09 14:21:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:21:23 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.487577], n=(308,444)
**GROW** @depth 1: [1,0.500388], n=(171,136)
**GROW** @depth 1: [2,0.561963], n=(148,297)
**GROW** @depth 2: [2,0.244325], n=(15,156)
**GROW** @depth 2: [1,0.724539], n=(108,12)
**GROW** @depth 2: [2,0.74954], n=(220,36)
**GROW** @depth 2: [2,0.263497], n=(15,190)
**GROW** @depth 3: [1,0.616804], n=(57,51)
**GROW** @depth 3: [2,0.410276], n=(57,133)
**GROW** @depth 3: [2,0.623773], n=(138,83)
**GROW** @depth 4: [2,0.544785], n=(114,19)
**GROW** @depth 3: [1,0.249264], n=(18,138)
**GROW** @depth 5: [1,0.447062], n=(59,24)
**GROW** @depth 4: [1,0.337002], n=(37,101)
**GROW** @depth 4: [1,0.254224], n=(21,116)
**GROW** @depth 5: [1,0.3632], n=(50,66)
**GROW** @depth 6: [2,0.372853], n=(33,69)
**GROW** @depth 5: [2,0.443252], n=(28,87)
**PRUNE** @depth 4: [2,0.444325]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(15,18,36,33,70,21,49,66,57,24,37,14,58,115,20,57,50,12)
**PRUNE** @depth 4: [1,0.723144]
**GROW** @depth 5: [1,0.431096], n=(30,35)
**PRUNE** @depth 5: [1,0.3632]
**GROW** @depth 4: [1,0.724539], n=(48,12)
**PRUNE** @depth 4: [1,0.726864]
**GROW** @depth 6: [1,0.417455], n=(25,43)
**GROW** @depth 5: [1,0.37126], n=(25,32)
**GROW** @depth 4: [2,0.545706], n=(17,20)
**GROW** @depth 5: [1,0.342273], n=(34,46)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(14,18,36,35,25,43,20,34,46,16,20,25,32,24,37,14,58,115,22,58,60)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [1,0.42319], n=(18,17)
**GROW** @depth 7: [2,0.436043], n=(22,21)
**PRUNE** @depth 6: [1,0.42319]
**GROW** @depth 6: [2,0.554448], n=(25,21)
**PRUNE** @depth 6: [2,0.554755]
**PRUNE** @depth 3: [2,0.258742]
**GROW** @depth 3: [1,0.68408], n=(96,18)
**GROW** @depth 4: [1,0.595102], n=(65,31)
**GROW** @depth 3: [1,0.57464], n=(36,36)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,17,37,34,25,22,21,19,35,45,17,20,25,32,23,38,36,36,65,31,18,23,58,60)
**GROW** @depth 6: [2,0.406902], n=(21,16)
**PRUNE** @depth 3: [1,0.57464]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,18,19,17,33,25,23,21,19,36,44,17,20,25,32,22,38,72,66,30,18,23,59,60)
**GROW** @depth 5: [2,0.551994], n=(17,19)
**GROW** @depth 3: [1,0.68935], n=(40,20)
**GROW** @depth 5: [2,0.474847], n=(32,34)
**GROW** @depth 3: [1,0.618199], n=(11,11)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(15,19,19,17,34,25,22,24,18,17,19,44,14,20,25,32,24,37,72,32,35,30,18,11,11,58,40,20)
**GROW** @depth 6: [2,0.551534], n=(24,20)
**PRUNE** @depth 6: [2,0.551534]
**GROW** @depth 5: [2,0.488037], n=(16,14)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,19,33,25,23,24,18,16,19,45,14,20,25,32,23,38,72,32,34,16,14,18,13,11,54,43,20)
**GROW** @depth 6: [2,0.31273], n=(15,19)
**PRUNE** @depth 6: [2,0.31273]
**GROW** @depth 4: [1,0.390327], n=(15,23)
**GROW** @depth 3: [2,0.356902], n=(45,27)
**GROW** @depth 7: [2,0.427454], n=(11,14)
**PRUNE** @depth 3: [2,0.356902]
**PRUNE** @depth 7: [2,0.42684]
**GROW** @depth 6: [2,0.308589], n=(14,19)
**GROW** @depth 6: [2,0.678834], n=(18,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(13,19,19,20,14,19,24,23,24,18,16,19,46,14,20,24,19,13,24,14,23,71,33,34,16,14,18,12,11,54,44,20)
Grow: 12.18%, Prune: 3.55%, Change: 83.52%, Swap: 34.03%

[1] "Fri Feb 09 14:21:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:21:31 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:21:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.9604 	 alpha= 0.9029 	 beta= 134622.2 
[1] "Fri Feb 09 14:21:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:21:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
r=1000 d=[0]; n=752
r=2000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752
r=4000 d=[0]; mh=1 n=752
r=5000 d=[0]; mh=1 n=752
Grow: 0%, 

[1] "Fri Feb 09 14:21:37 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:21:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:21:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:21:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:21:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:21:42 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:21:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:21:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:21:46 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.299; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.854; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.237; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.288; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.486; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.482; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.25; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.279; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.344; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.294; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.823; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.225; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.647; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.227; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.159
[1] "Fri Feb 09 14:21:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:21:52 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.038; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.126; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0147; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.162; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.351; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0882; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.00191; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0507; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.414; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.00355; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0203; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0145; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.249; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.185; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.00855; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.00191
[1] "Fri Feb 09 14:21:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:22:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:22:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:22:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:22:13 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 
Warning in train(allmodel, regr.task) :
  Could not train learner regr.km: Error in chol.default(R) : 
  the leading minor of order 650 is not positive definite

[1] "Fri Feb 09 14:22:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 4.06062, its = 9
i = 2 (of 248), d = 5.59561, its = 9
i = 3 (of 248), d = 5.08335, its = 36
i = 4 (of 248), d = 4.31959, its = 10
i = 5 (of 248), d = 8.66797, its = 11
i = 6 (of 248), d = 9.22476, its = 11
i = 7 (of 248), d = 6.96056, its = 10
i = 8 (of 248), d = 5.78405, its = 10
i = 9 (of 248), d = 2.82828, its = 8
i = 10 (of 248), d = 3.58542, its = 9
i = 11 (of 248), d = 3.91119, its = 9
i = 12 (of 248), d = 6.67653, its = 11
i = 13 (of 248), d = 6.65577, its = 10
i = 14 (of 248), d = 18.0006, its = 12
i = 15 (of 248), d = 5.08072, its = 9
i = 16 (of 248), d = 5.6842, its = 9
i = 17 (of 248), d = 5.54425, its = 14
i = 18 (of 248), d = 4.97646, its = 10
i = 19 (of 248), d = 7.88293, its = 10
i = 20 (of 248), d = 9.49932, its = 11
i = 21 (of 248), d = 4.15586, its = 9
i = 22 (of 248), d = 7.22389, its = 15
i = 23 (of 248), d = 5.59233, its = 10
i = 24 (of 248), d = 4.34637, its = 9
i = 25 (of 248), d = 4.70336, its = 9
i = 26 (of 248), d = 13.2449, its = 12
i = 27 (of 248), d = 15.7467, its = 12
i = 28 (of 248), d = 3.39923, its = 8
i = 29 (of 248), d = 6.27373, its = 10
i = 30 (of 248), d = 2.78208, its = 8
i = 31 (of 248), d = 8.09619, its = 10
i = 32 (of 248), d = 14.9145, its = 10
i = 33 (of 248), d = 5.64515, its = 9
i = 34 (of 248), d = 2.86336, its = 9
i = 35 (of 248), d = 3.03631, its = 9
i = 36 (of 248), d = 17.6274, its = 12
i = 37 (of 248), d = 11.36, its = 11
i = 38 (of 248), d = 2.94845, its = 29
i = 39 (of 248), d = 10.6901, its = 10
i = 40 (of 248), d = 7.13968, its = 32
i = 41 (of 248), d = 5.95892, its = 11
i = 42 (of 248), d = 7.15049, its = 10
i = 43 (of 248), d = 3.96092, its = 8
i = 44 (of 248), d = 6.80626, its = 10
i = 45 (of 248), d = 3.61037, its = 27
i = 46 (of 248), d = 9.11059, its = 9
i = 47 (of 248), d = 7.51896, its = 11
i = 48 (of 248), d = 8.50283, its = 11
i = 49 (of 248), d = 7.93903, its = 10
i = 50 (of 248), d = 6.26179, its = 10
i = 51 (of 248), d = 5.02274, its = 9
i = 52 (of 248), d = 7.59173, its = 10
i = 53 (of 248), d = 5.87844, its = 10
i = 54 (of 248), d = 9.19949, its = 11
i = 55 (of 248), d = 8.75032, its = 11
i = 56 (of 248), d = 8.49651, its = 10
i = 57 (of 248), d = 6.74454, its = 10
i = 58 (of 248), d = 13.4282, its = 11
i = 59 (of 248), d = 3.94956, its = 9
i = 60 (of 248), d = 8.32363, its = 9
i = 61 (of 248), d = 4.15974, its = 9
i = 62 (of 248), d = 9.0354, its = 11
i = 63 (of 248), d = 5.31658, its = 9
i = 64 (of 248), d = 3.02685, its = 8
i = 65 (of 248), d = 4.50288, its = 9
i = 66 (of 248), d = 3.14815, its = 8
i = 67 (of 248), d = 2.72913, its = 7
i = 68 (of 248), d = 3.64896, its = 8
i = 69 (of 248), d = 4.86042, its = 9
i = 70 (of 248), d = 12.5643, its = 11
i = 71 (of 248), d = 3.07218, its = 8
i = 72 (of 248), d = 7.4185, its = 11
i = 73 (of 248), d = 9.14848, its = 11
i = 74 (of 248), d = 9.26015, its = 10
i = 75 (of 248), d = 8.81789, its = 10
i = 76 (of 248), d = 9.61306, its = 10
i = 77 (of 248), d = 11.4998, its = 10
i = 78 (of 248), d = 3.64832, its = 8
i = 79 (of 248), d = 6.82955, its = 10
i = 80 (of 248), d = 7.63584, its = 9
i = 81 (of 248), d = 5.06626, its = 9
i = 82 (of 248), d = 4.2032, its = 9
i = 83 (of 248), d = 7.51228, its = 10
i = 84 (of 248), d = 11.9114, its = 11
i = 85 (of 248), d = 5.99023, its = 8
i = 86 (of 248), d = 7.62554, its = 11
i = 87 (of 248), d = 5.00016, its = 10
i = 88 (of 248), d = 7.48296, its = 11
i = 89 (of 248), d = 2.98655, its = 8
i = 90 (of 248), d = 10.0606, its = 11
i = 91 (of 248), d = 5.23287, its = 9
i = 92 (of 248), d = 5.72188, its = 29
i = 93 (of 248), d = 4.14558, its = 9
i = 94 (of 248), d = 12.2238, its = 12
i = 95 (of 248), d = 8.44958, its = 11
i = 96 (of 248), d = 9.32383, its = 10
i = 97 (of 248), d = 7.21559, its = 10
i = 98 (of 248), d = 6.14066, its = 9
i = 99 (of 248), d = 7.09138, its = 9
i = 100 (of 248), d = 4.02193, its = 9
i = 101 (of 248), d = 7.68388, its = 10
i = 102 (of 248), d = 9.63203, its = 10
i = 103 (of 248), d = 3.99874, its = 9
i = 104 (of 248), d = 3.71878, its = 8
i = 105 (of 248), d = 8.75533, its = 10
i = 106 (of 248), d = 6.09809, its = 9
i = 107 (of 248), d = 5.84079, its = 28
i = 108 (of 248), d = 4.00687, its = 9
i = 109 (of 248), d = 7.88964, its = 11
i = 110 (of 248), d = 4.49193, its = 28
i = 111 (of 248), d = 8.59859, its = 10
i = 112 (of 248), d = 8.51451, its = 10
i = 113 (of 248), d = 10.6737, its = 11
i = 114 (of 248), d = 4.35651, its = 9
i = 115 (of 248), d = 9.68954, its = 10
i = 116 (of 248), d = 3.51529, its = 9
i = 117 (of 248), d = 5.28936, its = 9
i = 118 (of 248), d = 11.0726, its = 11
i = 119 (of 248), d = 4.28408, its = 9
i = 120 (of 248), d = 5.80389, its = 10
i = 121 (of 248), d = 10.0397, its = 13
i = 122 (of 248), d = 6.29393, its = 10
i = 123 (of 248), d = 4.5092, its = 9
i = 124 (of 248), d = 5.74653, its = 10
i = 125 (of 248), d = 12.5306, its = 11
i = 126 (of 248), d = 3.04591, its = 8
i = 127 (of 248), d = 3.95336, its = 9
i = 128 (of 248), d = 5.99044, its = 10
i = 129 (of 248), d = 4.3166, its = 10
i = 130 (of 248), d = 3.31603, its = 8
i = 131 (of 248), d = 3.98777, its = 7
i = 132 (of 248), d = 8.20159, its = 9
i = 133 (of 248), d = 5.7322, its = 8
i = 134 (of 248), d = 9.88552, its = 11
i = 135 (of 248), d = 5.58832, its = 10
i = 136 (of 248), d = 12.3091, its = 11
i = 137 (of 248), d = 18.0364, its = 13
i = 138 (of 248), d = 4.51819, its = 10
i = 139 (of 248), d = 5.70304, its = 10
i = 140 (of 248), d = 6.48362, its = 8
i = 141 (of 248), d = 8.39053, its = 10
i = 142 (of 248), d = 3.8459, its = 9
i = 143 (of 248), d = 3.46949, its = 7
i = 144 (of 248), d = 5.06264, its = 8
i = 145 (of 248), d = 13.4116, its = 10
i = 146 (of 248), d = 8.49637, its = 12
i = 147 (of 248), d = 5.50801, its = 9
i = 148 (of 248), d = 8.66247, its = 9
i = 149 (of 248), d = 3.75463, its = 8
i = 150 (of 248), d = 3.87732, its = 7
i = 151 (of 248), d = 2.92282, its = 8
i = 152 (of 248), d = 8.22595, its = 10
i = 153 (of 248), d = 6.86007, its = 10
i = 154 (of 248), d = 5.60159, its = 8
i = 155 (of 248), d = 2.98718, its = 8
i = 156 (of 248), d = 2.95659, its = 8
i = 157 (of 248), d = 7.92399, its = 10
i = 158 (of 248), d = 6.80291, its = 10
i = 159 (of 248), d = 5.53844, its = 10
i = 160 (of 248), d = 6.76254, its = 10
i = 161 (of 248), d = 4.6702, its = 9
i = 162 (of 248), d = 10.7582, its = 11
i = 163 (of 248), d = 4.90755, its = 10
i = 164 (of 248), d = 6.9048, its = 10
i = 165 (of 248), d = 7.21481, its = 10
i = 166 (of 248), d = 7.9281, its = 10
i = 167 (of 248), d = 5.36823, its = 10
i = 168 (of 248), d = 4.27691, its = 9
i = 169 (of 248), d = 3.12277, its = 27
i = 170 (of 248), d = 2.73406, its = 8
i = 171 (of 248), d = 6.31031, its = 9
i = 172 (of 248), d = 8.29583, its = 10
i = 173 (of 248), d = 7.43075, its = 10
i = 174 (of 248), d = 15.6516, its = 12
i = 175 (of 248), d = 5.59501, its = 10
i = 176 (of 248), d = 8.32317, its = 10
i = 177 (of 248), d = 4.46465, its = 9
i = 178 (of 248), d = 5.23207, its = 9
i = 179 (of 248), d = 4.10329, its = 9
i = 180 (of 248), d = 4.48251, its = 9
i = 181 (of 248), d = 4.45294, its = 10
i = 182 (of 248), d = 6.98059, its = 9
i = 183 (of 248), d = 9.64693, its = 10
i = 184 (of 248), d = 6.53938, its = 10
i = 185 (of 248), d = 7.88791, its = 9
i = 186 (of 248), d = 3.02055, its = 8
i = 187 (of 248), d = 6.82572, its = 10
i = 188 (of 248), d = 5.22565, its = 9
i = 189 (of 248), d = 3.8273, its = 8
i = 190 (of 248), d = 3.15354, its = 10
i = 191 (of 248), d = 4.15024, its = 9
i = 192 (of 248), d = 9.66064, its = 11
i = 193 (of 248), d = 8.78814, its = 10
i = 194 (of 248), d = 8.18343, its = 10
i = 195 (of 248), d = 5.53554, its = 9
i = 196 (of 248), d = 4.95235, its = 9
i = 197 (of 248), d = 13.3536, its = 11
i = 198 (of 248), d = 3.22114, its = 7
i = 199 (of 248), d = 13.474, its = 11
i = 200 (of 248), d = 4.05969, its = 8
i = 201 (of 248), d = 10.6388, its = 11
i = 202 (of 248), d = 3.54729, its = 8
i = 203 (of 248), d = 5.87373, its = 8
i = 204 (of 248), d = 2.94386, its = 8
i = 205 (of 248), d = 7.42242, its = 10
i = 206 (of 248), d = 9.48048, its = 9
i = 207 (of 248), d = 12.7507, its = 11
i = 208 (of 248), d = 7.77518, its = 10
i = 209 (of 248), d = 8.13536, its = 11
i = 210 (of 248), d = 6.62019, its = 11
i = 211 (of 248), d = 5.94097, its = 10
i = 212 (of 248), d = 4.16523, its = 9
i = 213 (of 248), d = 5.71183, its = 10
i = 214 (of 248), d = 6.60865, its = 11
i = 215 (of 248), d = 9.03542, its = 11
i = 216 (of 248), d = 11.0888, its = 11
i = 217 (of 248), d = 8.27823, its = 11
i = 218 (of 248), d = 3.75383, its = 8
i = 219 (of 248), d = 9.17587, its = 10
i = 220 (of 248), d = 6.51798, its = 10
i = 221 (of 248), d = 10.0255, its = 12
i = 222 (of 248), d = 7.40285, its = 10
i = 223 (of 248), d = 4.74101, its = 9
i = 224 (of 248), d = 11.69, its = 13
i = 225 (of 248), d = 11.1832, its = 9
i = 226 (of 248), d = 13.39, its = 11
i = 227 (of 248), d = 8.69434, its = 11
i = 228 (of 248), d = 2.71491, its = 8
i = 229 (of 248), d = 3.45702, its = 8
i = 230 (of 248), d = 6.16922, its = 10
i = 231 (of 248), d = 11.5007, its = 11
i = 232 (of 248), d = 8.78622, its = 11
i = 233 (of 248), d = 6.87143, its = 10
i = 234 (of 248), d = 3.19644, its = 9
i = 235 (of 248), d = 5.68409, its = 10
i = 236 (of 248), d = 8.56088, its = 10
i = 237 (of 248), d = 9.24348, its = 10
i = 238 (of 248), d = 4.10884, its = 9
i = 239 (of 248), d = 3.77698, its = 9
i = 240 (of 248), d = 5.28921, its = 9
i = 241 (of 248), d = 7.61001, its = 10
i = 242 (of 248), d = 6.18389, its = 9
i = 243 (of 248), d = 5.44042, its = 9
i = 244 (of 248), d = 8.10591, its = 10
i = 245 (of 248), d = 4.20907, its = 9
i = 246 (of 248), d = 7.1918, its = 10
i = 247 (of 248), d = 5.48545, its = 10
i = 248 (of 248), d = 5.3134, its = 8
[1] "Fri Feb 09 14:23:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:23:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:23:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:23:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:23:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:23:15 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 1059.299679 
iter  10 value 166.035282
iter  20 value 33.866874
iter  30 value 10.577263
iter  40 value 6.987356
final  value 6.942912 
converged
# weights:  5
initial  value 1049.961534 
iter  10 value 45.941276
iter  20 value 8.003829
iter  30 value 6.618324
iter  40 value 6.491324
final  value 6.490996 
converged
# weights:  5
initial  value 993.489860 
iter  10 value 47.879778
iter  20 value 10.724267
iter  30 value 7.674687
iter  40 value 7.573752
final  value 7.573732 
converged
[Tune-y] 1: rmse.test.rmse=0.0794; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 795.062946 
iter  10 value 9.369793
iter  20 value 0.790798
iter  30 value 0.170531
iter  40 value 0.120300
iter  50 value 0.107275
iter  60 value 0.103153
iter  70 value 0.099649
iter  80 value 0.097352
iter  90 value 0.095521
iter 100 value 0.094351
final  value 0.094351 
stopped after 100 iterations
# weights:  57
initial  value 1174.949250 
iter  10 value 4.071977
iter  20 value 0.410776
iter  30 value 0.226279
iter  40 value 0.189604
iter  50 value 0.170805
iter  60 value 0.164593
iter  70 value 0.157567
iter  80 value 0.152424
iter  90 value 0.147999
iter 100 value 0.144817
final  value 0.144817 
stopped after 100 iterations
# weights:  57
initial  value 3716.482392 
iter  10 value 16.152607
iter  20 value 1.931926
iter  30 value 0.386108
iter  40 value 0.287261
iter  50 value 0.263043
iter  60 value 0.251605
iter  70 value 0.243988
iter  80 value 0.236371
iter  90 value 0.230762
iter 100 value 0.224344
final  value 0.224344 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0113; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 1299.209726 
iter  10 value 22.037834
iter  20 value 2.379424
iter  30 value 1.545340
iter  40 value 1.380039
iter  50 value 1.261278
iter  60 value 1.202898
iter  70 value 1.139232
iter  80 value 1.079034
iter  90 value 1.042799
iter 100 value 0.872402
final  value 0.872402 
stopped after 100 iterations
# weights:  41
initial  value 2222.533024 
iter  10 value 2.622584
iter  20 value 0.776061
iter  30 value 0.702844
iter  40 value 0.679448
iter  50 value 0.665373
iter  60 value 0.643015
iter  70 value 0.633865
iter  80 value 0.625209
iter  90 value 0.610828
iter 100 value 0.585368
final  value 0.585368 
stopped after 100 iterations
# weights:  41
initial  value 935.920239 
iter  10 value 8.546391
iter  20 value 2.368333
iter  30 value 0.862699
iter  40 value 0.730774
iter  50 value 0.711723
iter  60 value 0.700867
iter  70 value 0.687203
iter  80 value 0.677236
iter  90 value 0.661764
iter 100 value 0.617696
final  value 0.617696 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0248; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 1316.117860 
iter  10 value 78.983110
iter  20 value 24.892326
iter  30 value 4.952670
iter  40 value 1.904938
iter  50 value 1.123698
iter  60 value 1.018164
iter  70 value 1.013433
final  value 1.013426 
converged
# weights:  5
initial  value 1110.400139 
iter  10 value 232.683563
iter  20 value 84.677444
iter  30 value 16.510366
iter  40 value 4.006123
iter  50 value 1.898151
iter  60 value 0.993999
iter  70 value 0.939028
final  value 0.938993 
converged
# weights:  5
initial  value 1385.044503 
iter  10 value 207.826676
iter  20 value 25.108306
iter  30 value 5.146213
iter  40 value 1.711663
iter  50 value 1.147555
iter  60 value 1.100007
iter  70 value 1.099203
iter  70 value 1.099203
iter  70 value 1.099203
final  value 1.099203 
converged
[Tune-y] 4: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 1020.048221 
iter  10 value 2.355711
iter  20 value 0.789603
iter  30 value 0.137414
iter  40 value 0.031280
iter  50 value 0.028385
iter  60 value 0.027569
iter  70 value 0.027108
iter  80 value 0.026518
iter  90 value 0.026042
iter 100 value 0.025710
final  value 0.025710 
stopped after 100 iterations
# weights:  61
initial  value 1052.873103 
iter  10 value 8.730371
iter  20 value 0.656174
iter  30 value 0.056394
iter  40 value 0.028387
iter  50 value 0.026259
iter  60 value 0.025479
iter  70 value 0.025074
iter  80 value 0.024565
iter  90 value 0.024408
iter 100 value 0.024179
final  value 0.024179 
stopped after 100 iterations
# weights:  61
initial  value 2587.819258 
iter  10 value 24.498339
iter  20 value 1.279699
iter  30 value 0.193446
iter  40 value 0.063728
iter  50 value 0.052295
iter  60 value 0.049081
iter  70 value 0.047382
iter  80 value 0.045355
iter  90 value 0.042982
iter 100 value 0.041332
final  value 0.041332 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00648; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 1335.730077 
iter  10 value 7.542435
iter  20 value 2.308015
iter  30 value 1.815130
iter  40 value 1.685106
iter  50 value 1.581875
iter  60 value 1.459760
iter  70 value 1.342972
iter  80 value 1.266394
iter  90 value 1.196870
iter 100 value 1.108447
final  value 1.108447 
stopped after 100 iterations
# weights:  57
initial  value 1277.081162 
iter  10 value 3.174011
iter  20 value 1.115915
iter  30 value 0.768605
iter  40 value 0.682055
iter  50 value 0.662407
iter  60 value 0.650360
iter  70 value 0.642732
iter  80 value 0.637903
iter  90 value 0.634352
iter 100 value 0.630405
final  value 0.630405 
stopped after 100 iterations
# weights:  57
initial  value 874.657135 
iter  10 value 3.649500
iter  20 value 1.522589
iter  30 value 0.953158
iter  40 value 0.834056
iter  50 value 0.775385
iter  60 value 0.750444
iter  70 value 0.739108
iter  80 value 0.728492
iter  90 value 0.715293
iter 100 value 0.707387
final  value 0.707387 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0192; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 1304.127944 
iter  10 value 238.850200
iter  20 value 18.425322
iter  30 value 8.184349
iter  40 value 6.811430
iter  50 value 6.190418
iter  60 value 6.007520
iter  70 value 6.003607
iter  80 value 5.992108
iter  90 value 5.988069
iter 100 value 5.987663
final  value 5.987663 
stopped after 100 iterations
# weights:  13
initial  value 1113.135716 
iter  10 value 53.570016
iter  20 value 14.304861
iter  30 value 10.426925
iter  40 value 8.529135
iter  50 value 7.284018
iter  60 value 7.009615
iter  70 value 5.991983
iter  80 value 5.580263
iter  90 value 5.575885
iter 100 value 5.567355
final  value 5.567355 
stopped after 100 iterations
# weights:  13
initial  value 1121.505293 
iter  10 value 63.681140
iter  20 value 18.071938
iter  30 value 12.428215
iter  40 value 10.011710
iter  50 value 8.339400
iter  60 value 7.089082
iter  70 value 6.444263
iter  80 value 6.285360
iter  90 value 6.264700
iter 100 value 6.234914
final  value 6.234914 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0586; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2300.069287 
iter  10 value 11.543952
iter  20 value 2.251229
iter  30 value 1.940041
iter  40 value 1.761021
iter  50 value 1.594632
iter  60 value 1.446408
iter  70 value 1.313268
iter  80 value 1.174100
iter  90 value 1.098835
iter 100 value 1.046376
final  value 1.046376 
stopped after 100 iterations
# weights:  73
initial  value 1322.270928 
iter  10 value 27.389055
iter  20 value 3.634736
iter  30 value 2.094054
iter  40 value 1.764892
iter  50 value 1.586455
iter  60 value 1.465668
iter  70 value 1.374564
iter  80 value 1.229616
iter  90 value 1.090356
iter 100 value 1.004134
final  value 1.004134 
stopped after 100 iterations
# weights:  73
initial  value 1143.225988 
iter  10 value 4.713373
iter  20 value 1.078122
iter  30 value 0.783790
iter  40 value 0.681878
iter  50 value 0.637268
iter  60 value 0.618878
iter  70 value 0.607357
iter  80 value 0.596624
iter  90 value 0.588236
iter 100 value 0.579940
final  value 0.579940 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0222; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 1195.438988 
iter  10 value 11.348608
iter  20 value 2.122117
iter  30 value 0.646214
iter  40 value 0.364069
iter  50 value 0.339067
iter  60 value 0.327534
iter  70 value 0.317782
iter  80 value 0.310097
iter  90 value 0.305637
iter 100 value 0.302006
final  value 0.302006 
stopped after 100 iterations
# weights:  57
initial  value 1892.055001 
iter  10 value 5.622247
iter  20 value 0.884850
iter  30 value 0.363349
iter  40 value 0.336919
iter  50 value 0.321460
iter  60 value 0.310808
iter  70 value 0.305669
iter  80 value 0.301821
iter  90 value 0.295862
iter 100 value 0.292548
final  value 0.292548 
stopped after 100 iterations
# weights:  57
initial  value 1846.816488 
iter  10 value 3.205859
iter  20 value 0.637868
iter  30 value 0.444574
iter  40 value 0.422861
iter  50 value 0.411218
iter  60 value 0.400602
iter  70 value 0.392906
iter  80 value 0.384074
iter  90 value 0.372391
iter 100 value 0.365694
final  value 0.365694 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0151; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 1312.372598 
iter  10 value 16.045385
iter  20 value 6.561075
iter  30 value 1.556571
iter  40 value 0.526128
iter  50 value 0.306483
iter  60 value 0.192842
iter  70 value 0.156905
iter  80 value 0.127967
iter  90 value 0.113457
iter 100 value 0.099616
final  value 0.099616 
stopped after 100 iterations
# weights:  5
initial  value 1345.331484 
iter  10 value 231.805090
iter  20 value 33.113431
iter  30 value 5.434750
iter  40 value 1.933399
iter  50 value 0.873705
iter  60 value 0.416175
iter  70 value 0.231380
iter  80 value 0.182473
iter  90 value 0.140714
iter 100 value 0.122295
final  value 0.122295 
stopped after 100 iterations
# weights:  5
initial  value 1110.254975 
iter  10 value 102.787496
iter  20 value 13.916057
iter  30 value 5.598325
iter  40 value 2.578710
iter  50 value 1.081945
iter  60 value 0.491927
iter  70 value 0.290481
iter  80 value 0.187913
iter  90 value 0.150540
iter 100 value 0.127075
final  value 0.127075 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0168; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 1452.011286 
iter  10 value 10.212630
iter  20 value 1.959693
iter  30 value 0.536483
iter  40 value 0.384990
iter  50 value 0.368272
iter  60 value 0.348120
iter  70 value 0.323537
iter  80 value 0.283043
iter  90 value 0.232649
iter 100 value 0.214712
final  value 0.214712 
stopped after 100 iterations
# weights:  33
initial  value 1337.934019 
iter  10 value 16.214636
iter  20 value 0.888267
iter  30 value 0.325440
iter  40 value 0.288924
iter  50 value 0.269126
iter  60 value 0.249216
iter  70 value 0.241791
iter  80 value 0.227899
iter  90 value 0.203358
iter 100 value 0.193330
final  value 0.193330 
stopped after 100 iterations
# weights:  33
initial  value 976.350753 
iter  10 value 34.641850
iter  20 value 0.982770
iter  30 value 0.317655
iter  40 value 0.283764
iter  50 value 0.258476
iter  60 value 0.247736
iter  70 value 0.243556
iter  80 value 0.231311
iter  90 value 0.210054
iter 100 value 0.200908
final  value 0.200908 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0152; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 1120.000951 
iter  10 value 25.633125
iter  20 value 14.093253
iter  30 value 12.958636
iter  40 value 12.645189
iter  50 value 11.655252
iter  60 value 10.964852
iter  70 value 9.836737
iter  80 value 8.361473
iter  90 value 7.409875
iter 100 value 7.075420
final  value 7.075420 
stopped after 100 iterations
# weights:  17
initial  value 1228.253836 
iter  10 value 41.380129
iter  20 value 19.087112
iter  30 value 11.244799
iter  40 value 9.384795
iter  50 value 6.749824
iter  60 value 6.012560
iter  70 value 5.833187
iter  80 value 5.718934
iter  90 value 5.707245
iter 100 value 5.706399
final  value 5.706399 
stopped after 100 iterations
# weights:  17
initial  value 1032.076214 
iter  10 value 45.493932
iter  20 value 15.851012
iter  30 value 8.579395
iter  40 value 7.895280
iter  50 value 6.756943
iter  60 value 6.574960
iter  70 value 6.545108
iter  80 value 6.543114
iter  90 value 6.542280
final  value 6.542280 
converged
[Tune-y] 12: rmse.test.rmse=0.0576; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 1097.845296 
iter  10 value 68.858338
iter  20 value 12.916233
iter  30 value 5.091353
iter  40 value 1.625206
iter  50 value 0.241197
iter  60 value 0.138772
iter  70 value 0.083309
iter  80 value 0.057792
iter  90 value 0.052160
iter 100 value 0.046816
final  value 0.046816 
stopped after 100 iterations
# weights:  13
initial  value 1055.069163 
iter  10 value 46.712859
iter  20 value 3.700017
iter  30 value 0.503396
iter  40 value 0.080564
iter  50 value 0.020798
iter  60 value 0.016904
iter  70 value 0.012142
iter  80 value 0.010418
iter  90 value 0.010046
iter 100 value 0.009488
final  value 0.009488 
stopped after 100 iterations
# weights:  13
initial  value 1059.318527 
iter  10 value 164.003470
iter  20 value 46.859156
iter  30 value 17.996639
iter  40 value 5.795364
iter  50 value 1.149867
iter  60 value 0.352912
iter  70 value 0.084741
iter  80 value 0.038738
iter  90 value 0.033544
iter 100 value 0.022324
final  value 0.022324 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00972; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1571.022701 
iter  10 value 19.126207
iter  20 value 3.538768
iter  30 value 1.174483
iter  40 value 0.101658
iter  50 value 0.053843
iter  60 value 0.050664
iter  70 value 0.047098
iter  80 value 0.044497
iter  90 value 0.039888
iter 100 value 0.035900
final  value 0.035900 
stopped after 100 iterations
# weights:  25
initial  value 1262.730054 
iter  10 value 29.177657
iter  20 value 2.394576
iter  30 value 0.211621
iter  40 value 0.095861
iter  50 value 0.047101
iter  60 value 0.043508
iter  70 value 0.037532
iter  80 value 0.035220
iter  90 value 0.033215
iter 100 value 0.031726
final  value 0.031726 
stopped after 100 iterations
# weights:  25
initial  value 1128.600761 
iter  10 value 75.780444
iter  20 value 7.945936
iter  30 value 1.065356
iter  40 value 0.255307
iter  50 value 0.207281
iter  60 value 0.186825
iter  70 value 0.108895
iter  80 value 0.075172
iter  90 value 0.061267
iter 100 value 0.053933
final  value 0.053933 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.011; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2219.409507 
iter  10 value 7.188421
iter  20 value 2.570428
iter  30 value 1.031284
iter  40 value 0.198387
iter  50 value 0.110280
iter  60 value 0.095951
iter  70 value 0.085718
iter  80 value 0.081564
iter  90 value 0.074303
iter 100 value 0.069798
final  value 0.069798 
stopped after 100 iterations
# weights:  25
initial  value 1605.964962 
iter  10 value 10.376920
iter  20 value 0.487462
iter  30 value 0.224382
iter  40 value 0.121274
iter  50 value 0.090012
iter  60 value 0.085943
iter  70 value 0.080913
iter  80 value 0.076899
iter  90 value 0.070542
iter 100 value 0.066590
final  value 0.066590 
stopped after 100 iterations
# weights:  25
initial  value 1340.448466 
iter  10 value 11.541293
iter  20 value 1.641391
iter  30 value 0.504012
iter  40 value 0.190931
iter  50 value 0.142952
iter  60 value 0.137991
iter  70 value 0.123218
iter  80 value 0.103750
iter  90 value 0.084363
iter 100 value 0.077020
final  value 0.077020 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0128; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 885.727697 
iter  10 value 152.959714
iter  20 value 121.360035
iter  30 value 113.367129
iter  40 value 111.587349
iter  50 value 110.976261
iter  60 value 110.748425
iter  70 value 110.556347
iter  80 value 110.515102
iter  90 value 110.512386
iter 100 value 110.512288
final  value 110.512288 
stopped after 100 iterations
# weights:  65
initial  value 1872.791730 
iter  10 value 770.746907
iter  20 value 391.448120
iter  30 value 211.244214
iter  40 value 142.623995
iter  50 value 121.395860
iter  60 value 117.687529
iter  70 value 113.964317
iter  80 value 111.958727
iter  90 value 110.507466
iter 100 value 110.004790
final  value 110.004790 
stopped after 100 iterations
# weights:  65
initial  value 3350.745474 
iter  10 value 582.008384
iter  20 value 229.387439
iter  30 value 138.034105
iter  40 value 119.632466
iter  50 value 116.067621
iter  60 value 113.411936
iter  70 value 112.742623
iter  80 value 112.043633
iter  90 value 111.696173
iter 100 value 111.551225
final  value 111.551225 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 980.186556 
iter  10 value 9.161239
iter  20 value 1.828905
iter  30 value 0.130051
iter  40 value 0.053560
iter  50 value 0.049443
iter  60 value 0.046252
iter  70 value 0.044961
iter  80 value 0.042915
iter  90 value 0.041600
iter 100 value 0.040264
final  value 0.040264 
stopped after 100 iterations
# weights:  29
initial  value 954.790840 
iter  10 value 15.166020
iter  20 value 2.348370
iter  30 value 0.132372
iter  40 value 0.066237
iter  50 value 0.054193
iter  60 value 0.052768
iter  70 value 0.051329
iter  80 value 0.048147
iter  90 value 0.044775
iter 100 value 0.041506
final  value 0.041506 
stopped after 100 iterations
# weights:  29
initial  value 1581.897680 
iter  10 value 62.526649
iter  20 value 14.657795
iter  30 value 4.566391
iter  40 value 0.930275
iter  50 value 0.226962
iter  60 value 0.166439
iter  70 value 0.153223
iter  80 value 0.134367
iter  90 value 0.093762
iter 100 value 0.067767
final  value 0.067767 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 1473.965847 
iter  10 value 9.253773
iter  20 value 4.934695
iter  30 value 3.391214
iter  40 value 3.218571
iter  50 value 3.055863
iter  60 value 2.880273
iter  70 value 2.854760
iter  80 value 2.830829
iter  90 value 2.821559
iter 100 value 2.809078
final  value 2.809078 
stopped after 100 iterations
# weights:  21
initial  value 1543.264991 
iter  10 value 66.279928
iter  20 value 5.739694
iter  30 value 2.966144
iter  40 value 2.702899
iter  50 value 2.625814
iter  60 value 2.545368
iter  70 value 2.525469
iter  80 value 2.508536
iter  90 value 2.500483
iter 100 value 2.491147
final  value 2.491147 
stopped after 100 iterations
# weights:  21
initial  value 1340.018927 
iter  10 value 77.350089
iter  20 value 10.427835
iter  30 value 4.658557
iter  40 value 3.991325
iter  50 value 3.643746
iter  60 value 3.011846
iter  70 value 2.913530
iter  80 value 2.809329
iter  90 value 2.668026
iter 100 value 2.556764
final  value 2.556764 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2226.507642 
iter  10 value 5.001791
iter  20 value 1.255271
iter  30 value 0.906367
iter  40 value 0.828434
iter  50 value 0.802606
iter  60 value 0.778837
iter  70 value 0.764471
iter  80 value 0.754568
iter  90 value 0.745368
iter 100 value 0.739529
final  value 0.739529 
stopped after 100 iterations
# weights:  65
initial  value 1007.644163 
iter  10 value 8.916714
iter  20 value 1.748869
iter  30 value 1.245679
iter  40 value 1.074772
iter  50 value 0.966563
iter  60 value 0.930666
iter  70 value 0.901788
iter  80 value 0.874673
iter  90 value 0.853258
iter 100 value 0.828639
final  value 0.828639 
stopped after 100 iterations
# weights:  65
initial  value 1490.629441 
iter  10 value 15.929648
iter  20 value 1.467284
iter  30 value 1.185676
iter  40 value 1.072035
iter  50 value 0.994227
iter  60 value 0.958870
iter  70 value 0.920931
iter  80 value 0.886604
iter  90 value 0.861335
iter 100 value 0.846263
final  value 0.846263 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0178; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 1171.248051 
iter  10 value 8.574906
iter  20 value 0.512210
iter  30 value 0.126268
iter  40 value 0.033881
iter  50 value 0.017827
iter  60 value 0.015225
iter  70 value 0.014711
iter  80 value 0.014500
iter  90 value 0.014114
iter 100 value 0.013866
final  value 0.013866 
stopped after 100 iterations
# weights:  69
initial  value 1406.448369 
iter  10 value 9.061825
iter  20 value 0.724619
iter  30 value 0.203684
iter  40 value 0.061498
iter  50 value 0.043801
iter  60 value 0.034507
iter  70 value 0.031231
iter  80 value 0.029284
iter  90 value 0.028550
iter 100 value 0.027798
final  value 0.027798 
stopped after 100 iterations
# weights:  69
initial  value 1671.142797 
iter  10 value 8.599604
iter  20 value 1.520770
iter  30 value 0.293586
iter  40 value 0.067036
iter  50 value 0.034308
iter  60 value 0.029797
iter  70 value 0.028586
iter  80 value 0.027425
iter  90 value 0.026622
iter 100 value 0.025896
final  value 0.025896 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00744; time: 0.0 min
[Tune] Result: size=15; decay=0.000446 : rmse.test.rmse=0.00648
# weights:  61
initial  value 3541.451880 
iter  10 value 92.304952
iter  20 value 4.652718
iter  30 value 0.959275
iter  40 value 0.235677
iter  50 value 0.131231
iter  60 value 0.112773
iter  70 value 0.106157
iter  80 value 0.103870
iter  90 value 0.099578
iter 100 value 0.096212
final  value 0.096212 
stopped after 100 iterations
[1] "Fri Feb 09 14:23:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 708 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 429
 number of selected nodes                               : 71 
[1] "Fri Feb 09 14:23:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:23:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:24:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:24:11 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.189; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.185; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.191; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.204; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.205; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.184; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.184
[1] "Fri Feb 09 14:25:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:25:05 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.619; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.72; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.989; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.673; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.907; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.614; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.503; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.643; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.599; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.58; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.591; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.655; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.801; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.531; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.712; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.943; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.503
[1] "Fri Feb 09 14:25:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:25:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:25:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7440 0.3350 0.1500 0.0676 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 2 
Runtime: 0.01900101 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1503 
    Y 1           0.31 
    Y 2          0.112 
    Y 3        -0.4739 
    Y 4         -0.282 
    Y 5          1.372 
[1] "Fri Feb 09 14:25:47 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:47] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.32; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:49] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.28; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.676; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:25:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.309; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.265; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.329; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.386; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:26:22] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:26:22] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:26:23] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.375; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.257; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:27:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.257
[1] "Fri Feb 09 14:27:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:27:15 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.358358], n=(269,483)
**GROW** @depth 1: [2,0.526527], n=(262,221)
**GROW** @depth 1: [1,0.186186], n=(146,123)
**GROW** @depth 2: [2,0.503504], n=(69,76)
**GROW** @depth 2: [1,0.766767], n=(126,94)
**GROW** @depth 3: [1,0.672673], n=(136,124)
**GROW** @depth 2: [2,0.22973], n=(38,31)
**GROW** @depth 2: [2,0.500501], n=(61,66)
**GROW** @depth 3: [2,0.15015], n=(34,90)
**GROW** @depth 3: [2,0.740741], n=(34,32)
**GROW** @depth 4: [2,0.847848], n=(13,19)
**GROW** @depth 3: [2,0.767267], n=(41,53)
**GROW** @depth 3: [2,0.761261], n=(45,31)
**GROW** @depth 3: [2,0.297297], n=(68,68)
**GROW** @depth 3: [1,0.561562], n=(71,55)
**GROW** @depth 4: [1,0.777778], n=(32,39)
**GROW** @depth 4: [2,0.880881], n=(26,26)
**GROW** @depth 5: [2,0.279279], n=(37,24)
**GROW** @depth 5: [1,0.89039], n=(18,21)
**GROW** @depth 4: [2,0.163163], n=(30,40)
**GROW** @depth 4: [2,0.913914], n=(39,26)
**GROW** @depth 4: [1,0.0890891], n=(26,19)
**PRUNE** @depth 4: [1,0.0890891]
**PRUNE** @depth 5: [1,0.89039]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(38,31,45,30,37,24,34,13,19,30,40,68,34,89,32,32,39,39,26,26,26)
**PRUNE** @depth 3: [2,0.881882]
**GROW** @depth 4: [1,0.891892], n=(18,21)
**PRUNE** @depth 4: [1,0.891892]
**GROW** @depth 5: [1,0.835836], n=(51,38)
**GROW** @depth 4: [1,0.871371], n=(13,26)
**GROW** @depth 6: [2,0.338839], n=(17,21)
**GROW** @depth 4: [1,0.835836], n=(17,17)
**GROW** @depth 4: [2,0.840841], n=(19,33)
**PRUNE** @depth 5: [2,0.34034]
**GROW** @depth 5: [1,0.937938], n=(15,11)
**PRUNE** @depth 5: [1,0.937938]
**GROW** @depth 4: [2,0.117117], n=(19,20)
**GROW** @depth 5: [2,0.33033], n=(26,23)
**PRUNE** @depth 3: [2,0.117117]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(39,30,45,30,37,23,35,13,19,30,40,67,16,18,28,23,38,32,33,12,26,37,28,20,33)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 3: [2,0.843844]
**GROW** @depth 5: [1,0.506507], n=(16,22)
**GROW** @depth 3: [1,0.102102], n=(18,12)
**GROW** @depth 5: [1,0.588589], n=(23,14)
**GROW** @depth 5: [1,0.0890891], n=(26,19)
**PRUNE** @depth 3: [1,0.1001]
**GROW** @depth 5: [2,0.641642], n=(14,20)
**PRUNE** @depth 4: [1,0.0890891]
**PRUNE** @depth 5: [2,0.641642]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(39,30,45,30,37,23,34,14,19,30,18,21,69,16,18,28,23,38,32,34,15,22,23,14,28,52)
**GROW** @depth 5: [2,0.286286], n=(15,23)
**GROW** @depth 5: [1,0.0900901], n=(27,18)
**GROW** @depth 5: [2,0.412412], n=(41,27)
**GROW** @depth 3: [2,0.885886], n=(16,14)
**PRUNE** @depth 3: [2,0.885886]
**GROW** @depth 4: [2,0.874374], n=(25,27)
**PRUNE** @depth 5: [2,0.411411]
**PRUNE** @depth 4: [2,0.874374]
**GROW** @depth 3: [2,0.887888], n=(16,14)
**GROW** @depth 6: [2,0.646146], n=(16,16)
**PRUNE** @depth 3: [2,0.888889]
**GROW** @depth 6: [2,0.432432], n=(12,11)
**GROW** @depth 4: [2,0.379379], n=(32,36)
**GROW** @depth 5: [1,0.953453], n=(14,11)
**PRUNE** @depth 6: [1,0.955956]
**PRUNE** @depth 6: [2,0.433433]
**PRUNE** @depth 5: [1,0.880881]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(39,30,27,19,30,37,24,32,14,19,30,18,21,32,36,16,18,28,23,15,23,33,16,17,41,21,16,27,50)
**GROW** @depth 6: [1,0.741742], n=(12,11)
**PRUNE** @depth 5: [2,0.646146]
**GROW** @depth 4: [1,0.0840841], n=(15,15)
**GROW** @depth 4: [2,0.885886], n=(27,25)
**GROW** @depth 5: [2,0.138138], n=(22,15)
**GROW** @depth 5: [1,0.0810811], n=(13,17)
**PRUNE** @depth 5: [1,0.0810811]
**GROW** @depth 5: [2,0.962462], n=(17,11)
**PRUNE** @depth 5: [2,0.962462]
**GROW** @depth 4: [2,0.411411], n=(13,11)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(39,30,27,19,15,15,22,15,13,11,32,14,19,30,18,22,32,35,17,17,28,12,11,15,23,32,33,39,23,14,28,27,25)
**PRUNE** @depth 5: [2,0.411411]
**GROW** @depth 5: [2,0.64014], n=(14,18)
**GROW** @depth 4: [1,0.888889], n=(19,21)
**GROW** @depth 5: [1,0.713714], n=(14,18)
**PRUNE** @depth 5: [1,0.713714]
**PRUNE** @depth 4: [1,0.835836]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(40,29,27,18,16,15,22,15,24,32,14,19,30,18,17,37,36,34,27,12,11,16,23,14,18,31,19,21,23,14,28,27,25)
**GROW** @depth 6: [1,0.75976], n=(14,13)
**GROW** @depth 5: [2,0.416917], n=(13,11)
**GROW** @depth 5: [2,0.376877], n=(15,15)
**PRUNE** @depth 4: [2,0.638639]
**PRUNE** @depth 5: [2,0.376877]
**PRUNE** @depth 6: [1,0.756757]
**PRUNE** @depth 5: [2,0.284284]
**PRUNE** @depth 4: [1,0.0840841]
**GROW** @depth 4: [1,0.802803], n=(16,19)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(39,30,28,18,30,22,28,11,32,14,19,29,18,17,36,36,16,19,28,12,11,16,23,32,31,19,21,23,14,28,27,25)
Grow: 17.44%, Prune: 8.406%, Change: 86.74%, Swap: 28.74%

[1] "Fri Feb 09 14:27:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:27:23 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:27:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.5291 	 alpha= 0.0158 	 beta= 69.5499 
[1] "Fri Feb 09 14:27:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:27:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [1,0.520521], n=(390,362)
**GROW** @depth 1: [2,0.516016], n=(193,169)
**GROW** @depth 2: [1,0.107608], n=(50,144)
**GROW** @depth 2: [2,0.811812], n=(97,72)
**GROW** @depth 3: [2,0.749249], n=(29,21)
**GROW** @depth 1: [2,0.258258], n=(193,196)
**GROW** @depth 2: [2,0.328829], n=(61,133)
**GROW** @depth 3: [1,0.756757], n=(38,60)
**GROW** @depth 3: [2,0.283283], n=(22,41)
**GROW** @depth 4: [1,0.0530531], n=(14,17)
**GROW** @depth 3: [1,0.781281], n=(32,40)
**GROW** @depth 4: [2,0.66967], n=(34,26)
**PRUNE** @depth 4: [1,0.0530531]
**PRUNE** @depth 4: [2,0.66967]
**GROW** @depth 4: [1,0.0530531], n=(13,16)
**GROW** @depth 4: [1,0.881882], n=(29,30)
**GROW** @depth 2: [1,0.455455], n=(89,104)
**GROW** @depth 3: [2,0.129129], n=(51,53)
**GROW** @depth 5: [1,0.837838], n=(42,11)
**GROW** @depth 5: [1,0.70971], n=(20,31)
**GROW** @depth 4: [2,0.184685], n=(65,24)
**GROW** @depth 4: [1,0.256256], n=(32,32)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(64,25,20,30,41,12,19,44,135,13,15,32,32,99,41,28,29,33,40)
**PRUNE** @depth 4: [1,0.0530531]
**GROW** @depth 4: [1,0.600601], n=(15,18)
**PRUNE** @depth 4: [1,0.600601]
**GROW** @depth 4: [2,0.904404], n=(18,15)
**GROW** @depth 4: [1,0.415415], n=(18,14)
**GROW** @depth 3: [1,0.226226], n=(39,59)
**GROW** @depth 3: [1,0.520521], n=(63,71)
**GROW** @depth 5: [1,0.942943], n=(16,12)
**GROW** @depth 4: [1,0.273774], n=(31,32)
**PRUNE** @depth 4: [1,0.942943]
**GROW** @depth 4: [1,0.896897], n=(53,18)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(63,25,20,31,41,11,19,45,31,32,53,18,28,33,18,13,39,59,41,30,29,16,16,41)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [1,0.044044], n=(12,16)
**GROW** @depth 5: [2,0.904404], n=(18,24)
**GROW** @depth 5: [1,0.70971], n=(35,19)
**PRUNE** @depth 5: [2,0.904404]
**GROW** @depth 5: [1,0.893894], n=(27,16)
**GROW** @depth 5: [1,0.942943], n=(14,12)
**GROW** @depth 5: [2,0.858859], n=(14,25)
**GROW** @depth 5: [2,0.0720721], n=(25,38)
**GROW** @depth 5: [1,0.107608], n=(12,13)
**PRUNE** @depth 5: [1,0.258258]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(26,37,26,20,31,41,11,19,44,31,31,34,23,15,12,14,54,13,14,11,13,59,41,31,15,12,16,16,27,15)
**GROW** @depth 6: [1,0.188689], n=(18,18)
**GROW** @depth 3: [1,0.494494], n=(21,24)
**GROW** @depth 4: [2,0.862362], n=(25,34)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(24,21,18,26,20,31,42,11,16,21,25,29,31,33,21,15,13,15,54,13,14,11,13,25,34,43,31,15,12,16,17,26,16)
**PRUNE** @depth 6: [1,0.514515]
**GROW** @depth 5: [1,0.864865], n=(19,12)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(24,21,18,26,20,19,12,42,11,16,21,25,29,64,22,14,13,15,53,13,15,11,13,25,34,40,34,16,12,16,17,26,15)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(23,22,19,25,20,18,12,42,12,17,22,25,29,63,22,13,13,15,54,13,14,11,13,23,35,40,34,16,13,17,17,26,14)
**PRUNE** @depth 5: [1,0.106106]
**GROW** @depth 5: [1,0.111111], n=(11,13)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(22,22,21,25,19,18,12,43,12,15,22,26,29,63,23,13,13,15,55,13,13,11,13,24,35,41,34,15,13,15,17,26,14)
Grow: 12.28%, Prune: 2.55%, Change: 81.78%, Swap: 26.1%

[1] "Fri Feb 09 14:27:30 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:27:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:27:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:27:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:27:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:27:34 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:27:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:27:36 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:27:39 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0301; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0383; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.0453; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0309; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0211; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.0403; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0389; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0477; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.053; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0312; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.0817; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=0.198; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0342; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0284; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.0417; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0284; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.036; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0211
[1] "Fri Feb 09 14:27:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:27:48 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.104; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0667; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0665; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0736; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.17; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0691; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0726; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0773; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0655; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.145; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0933; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0741; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0655; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0713; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.164; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0655
[1] "Fri Feb 09 14:27:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:28:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================                                                |  32%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:28:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:28:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==============                                                        |  20%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:28:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 
  - parameters upper bounds :  2 2 
  - best initial criterion value(s) :  1928.327 

N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -1928.3  |proj g|=      0.23813
At iterate     1  f =      -2025.6  |proj g|=        1.8826
At iterate     2  f =      -2036.8  |proj g|=        1.9204
At iterate     3  f =      -2041.1  |proj g|=       0.16307
At iterate     4  f =      -2042.7  |proj g|=       0.15502
At iterate     5  f =      -2042.9  |proj g|=        1.9161
At iterate     6  f =        -2043  |proj g|=       0.15131
At iterate     7  f =        -2043  |proj g|=      0.040477
At iterate     8  f =        -2043  |proj g|=     0.0044762

iterations 8
function evaluations 13
segments explored during Cauchy searches 9
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.00447622
final function value -2042.96

F = -2042.96
final  value -2042.961316 
converged
[1] "Fri Feb 09 14:28:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 0.650916, its = 22
i = 2 (of 248), d = 0.667437, its = 9
i = 3 (of 248), d = 0.165894, its = 9
i = 4 (of 248), d = 0.0739465, its = 23
i = 5 (of 248), d = 0.158455, its = 32
i = 6 (of 248), d = 0.721723, its = 11
i = 7 (of 248), d = 0.359246, its = 27
i = 8 (of 248), d = 0.24632, its = 9
i = 9 (of 248), d = 0.344011, its = 9
i = 10 (of 248), d = 0.492851, its = 9
i = 11 (of 248), d = 0.0493543, its = 7
i = 12 (of 248), d = 0.72296, its = 8
i = 13 (of 248), d = 0.786371, its = 23
i = 14 (of 248), d = 0.340732, its = 8
i = 15 (of 248), d = 0.0517307, its = 5
i = 16 (of 248), d = 0.804444, its = 20
i = 17 (of 248), d = 0.230281, its = 8
i = 18 (of 248), d = 0.948979, its = 23
i = 19 (of 248), d = 0.032987, its = 4
i = 20 (of 248), d = 0.931103, its = 11
i = 21 (of 248), d = 0.178429, its = 7
i = 22 (of 248), d = 0.0985016, its = 7
i = 23 (of 248), d = 0.204365, its = 8
i = 24 (of 248), d = 0.235148, its = 11
i = 25 (of 248), d = 0.438032, its = 10
i = 26 (of 248), d = 0.313548, its = 8
i = 27 (of 248), d = 0.349206, its = 27
i = 28 (of 248), d = 0.667574, its = 21
i = 29 (of 248), d = 0.419494, its = 10
i = 30 (of 248), d = 0.159898, its = 25
i = 31 (of 248), d = 0.36327, its = 10
i = 32 (of 248), d = 0.0751326, its = 7
i = 33 (of 248), d = 0.389392, its = 10
i = 34 (of 248), d = 0.180374, its = 7
i = 35 (of 248), d = 0.826736, its = 9
i = 36 (of 248), d = 0.578458, its = 21
i = 37 (of 248), d = 0.789957, its = 10
i = 38 (of 248), d = 0.0810076, its = 7
i = 39 (of 248), d = 0.0435214, its = 5
i = 40 (of 248), d = 0.520059, its = 8
i = 41 (of 248), d = 0.278186, its = 21
i = 42 (of 248), d = 0.41852, its = 9
i = 43 (of 248), d = 0.851272, its = 10
i = 44 (of 248), d = 0.146427, its = 8
i = 45 (of 248), d = 0.282786, its = 9
i = 46 (of 248), d = 0.897997, its = 19
i = 47 (of 248), d = 0.888421, its = 12
i = 48 (of 248), d = 0.276698, its = 9
i = 49 (of 248), d = 0.0738203, its = 7
i = 50 (of 248), d = 0.447066, its = 9
i = 51 (of 248), d = 0.720512, its = 10
i = 52 (of 248), d = 0.239094, its = 18
i = 53 (of 248), d = 0.528815, its = 26
i = 54 (of 248), d = 0.315759, its = 6
i = 55 (of 248), d = 0.0347765, its = 6
i = 56 (of 248), d = 0.312338, its = 8
i = 57 (of 248), d = 0.134953, its = 7
i = 58 (of 248), d = 0.122798, its = 7
i = 59 (of 248), d = 0.122232, its = 7
i = 60 (of 248), d = 0.0848352, its = 29
i = 61 (of 248), d = 0.199615, its = 7
i = 62 (of 248), d = 0.253494, its = 8
i = 63 (of 248), d = 0.159818, its = 9
i = 64 (of 248), d = 0.543797, its = 8
i = 65 (of 248), d = 0.335913, its = 9
i = 66 (of 248), d = 0.229841, its = 7
i = 67 (of 248), d = 0.100118, its = 7
i = 68 (of 248), d = 0.221505, its = 6
i = 69 (of 248), d = 0.122285, its = 7
i = 70 (of 248), d = 0.472155, its = 31
i = 71 (of 248), d = 0.467345, its = 21
i = 72 (of 248), d = 0.138921, its = 21
i = 73 (of 248), d = 0.237314, its = 7
i = 74 (of 248), d = 0.932577, its = 10
i = 75 (of 248), d = 0.291134, its = 8
i = 76 (of 248), d = 1.15407, its = 15
i = 77 (of 248), d = 0.364492, its = 8
i = 78 (of 248), d = 0.256349, its = 8
i = 79 (of 248), d = 0.21827, its = 9
i = 80 (of 248), d = 0.416291, its = 9
i = 81 (of 248), d = 0.607083, its = 10
i = 82 (of 248), d = 1.86775, its = 20
i = 83 (of 248), d = 0.259144, its = 5
i = 84 (of 248), d = 0.611574, its = 19
i = 85 (of 248), d = 0.429248, its = 11
i = 86 (of 248), d = 0.311927, its = 8
i = 87 (of 248), d = 0.750533, its = 23
i = 88 (of 248), d = 1.86775, its = 17
i = 89 (of 248), d = 0.929152, its = 11
i = 90 (of 248), d = 0.178848, its = 23
i = 91 (of 248), d = 0.611875, its = 24
i = 92 (of 248), d = 0.0277732, its = 7
i = 93 (of 248), d = 0.178748, its = 10
i = 94 (of 248), d = 0.283259, its = 9
i = 95 (of 248), d = 0.599193, its = 9
i = 96 (of 248), d = 0.240654, its = 9
i = 97 (of 248), d = 0.71817, its = 10
i = 98 (of 248), d = 0.61089, its = 8
i = 99 (of 248), d = 0.0343442, its = 4
i = 100 (of 248), d = 0.464961, its = 10
i = 101 (of 248), d = 0.362423, its = 9
i = 102 (of 248), d = 0.206778, its = 9
i = 103 (of 248), d = 0.101675, its = 6
i = 104 (of 248), d = 0.0697933, its = 5
i = 105 (of 248), d = 0.776756, its = 7
i = 106 (of 248), d = 0.964611, its = 8
i = 107 (of 248), d = 0.10603, its = 7
i = 108 (of 248), d = 0.269646, its = 6
i = 109 (of 248), d = 0.814566, its = 22
i = 110 (of 248), d = 0.519569, its = 23
i = 111 (of 248), d = 0.351348, its = 20
i = 112 (of 248), d = 0.356945, its = 8
i = 113 (of 248), d = 0.587911, its = 11
i = 114 (of 248), d = 0.528144, its = 22
i = 115 (of 248), d = 0.625622, its = 10
i = 116 (of 248), d = 0.129137, its = 7
i = 117 (of 248), d = 0.180025, its = 26
i = 118 (of 248), d = 0.0793322, its = 6
i = 119 (of 248), d = 0.874211, its = 10
i = 120 (of 248), d = 0.173984, its = 8
i = 121 (of 248), d = 0.837469, its = 10
i = 122 (of 248), d = 0.770156, its = 28
i = 123 (of 248), d = 0.339893, its = 9
i = 124 (of 248), d = 0.351574, its = 10
i = 125 (of 248), d = 0.328581, its = 9
i = 126 (of 248), d = 0.212952, its = 8
i = 127 (of 248), d = 0.278583, its = 29
i = 128 (of 248), d = 0.800042, its = 25
i = 129 (of 248), d = 0.702713, its = 10
i = 130 (of 248), d = 0.204126, its = 19
i = 131 (of 248), d = 1.02818, its = 26
i = 132 (of 248), d = 0.256789, its = 9
i = 133 (of 248), d = 0.696781, its = 11
i = 134 (of 248), d = 0.160527, its = 8
i = 135 (of 248), d = 0.107027, its = 7
i = 136 (of 248), d = 0.256138, its = 9
i = 137 (of 248), d = 0.489243, its = 8
i = 138 (of 248), d = 0.289636, its = 20
i = 139 (of 248), d = 0.1664, its = 11
i = 140 (of 248), d = 0.250826, its = 8
i = 141 (of 248), d = 0.0456578, its = 6
i = 142 (of 248), d = 0.895923, its = 10
i = 143 (of 248), d = 0.151419, its = 8
i = 144 (of 248), d = 0.552825, its = 10
i = 145 (of 248), d = 0.797475, its = 26
i = 146 (of 248), d = 0.172919, its = 8
i = 147 (of 248), d = 0.970241, its = 20
i = 148 (of 248), d = 0.440063, its = 24
i = 149 (of 248), d = 0.736911, its = 10
i = 150 (of 248), d = 0.216964, its = 9
i = 151 (of 248), d = 0.619287, its = 22
i = 152 (of 248), d = 0.388984, its = 27
i = 153 (of 248), d = 0.371136, its = 22
i = 154 (of 248), d = 0.318989, its = 8
i = 155 (of 248), d = 0.106838, its = 7
i = 156 (of 248), d = 0.770699, its = 10
i = 157 (of 248), d = 0.580877, its = 19
i = 158 (of 248), d = 0.450305, its = 10
i = 159 (of 248), d = 0.215923, its = 7
i = 160 (of 248), d = 1.47237, its = 32
i = 161 (of 248), d = 0.434914, its = 21
i = 162 (of 248), d = 0.0712919, its = 23
i = 163 (of 248), d = 0.325282, its = 8
i = 164 (of 248), d = 0.196474, its = 24
i = 165 (of 248), d = 0.425838, its = 10
i = 166 (of 248), d = 0.578269, its = 9
i = 167 (of 248), d = 0.367015, its = 20
i = 168 (of 248), d = 0.51388, its = 10
i = 169 (of 248), d = 0.10782, its = 6
i = 170 (of 248), d = 0.113793, its = 26
i = 171 (of 248), d = 0.0633315, its = 5
i = 172 (of 248), d = 0.126631, its = 8
i = 173 (of 248), d = 0.192319, its = 22
i = 174 (of 248), d = 0.245063, its = 8
i = 175 (of 248), d = 0.125696, its = 9
i = 176 (of 248), d = 0.214552, its = 22
i = 177 (of 248), d = 0.174986, its = 7
i = 178 (of 248), d = 0.372548, its = 9
i = 179 (of 248), d = 0.90576, its = 26
i = 180 (of 248), d = 0.973699, its = 22
i = 181 (of 248), d = 0.394899, its = 9
i = 182 (of 248), d = 0.455043, its = 10
i = 183 (of 248), d = 0.320607, its = 13
i = 184 (of 248), d = 0.285501, its = 8
i = 185 (of 248), d = 0.155308, its = 7
i = 186 (of 248), d = 0.22659, its = 8
i = 187 (of 248), d = 0.353855, its = 24
i = 188 (of 248), d = 0.295536, its = 9
i = 189 (of 248), d = 0.528152, its = 10
i = 190 (of 248), d = 0.281096, its = 10
i = 191 (of 248), d = 0.392444, its = 9
i = 192 (of 248), d = 1.44409, its = 12
i = 193 (of 248), d = 0.0491283, its = 6
i = 194 (of 248), d = 0.583718, its = 32
i = 195 (of 248), d = 0.235917, its = 8
i = 196 (of 248), d = 0.69211, its = 21
i = 197 (of 248), d = 0.220641, its = 6
i = 198 (of 248), d = 0.488424, its = 29
i = 199 (of 248), d = 0.191413, its = 9
i = 200 (of 248), d = 0.785889, its = 10
i = 201 (of 248), d = 0.324056, its = 7
i = 202 (of 248), d = 0.280097, its = 22
i = 203 (of 248), d = 0.261978, its = 24
i = 204 (of 248), d = 0.367688, its = 25
i = 205 (of 248), d = 0.252433, its = 9
i = 206 (of 248), d = 0.749605, its = 10
i = 207 (of 248), d = 0.710412, its = 24
i = 208 (of 248), d = 0.833579, its = 15
i = 209 (of 248), d = 0.202622, its = 24
i = 210 (of 248), d = 0.800183, its = 9
i = 211 (of 248), d = 0.277226, its = 8
i = 212 (of 248), d = 0.17222, its = 7
i = 213 (of 248), d = 0.635773, its = 23
i = 214 (of 248), d = 0.119515, its = 5
i = 215 (of 248), d = 0.215173, its = 8
i = 216 (of 248), d = 0.660921, its = 10
i = 217 (of 248), d = 0.971015, its = 12
i = 218 (of 248), d = 0.387334, its = 9
i = 219 (of 248), d = 0.277976, its = 8
i = 220 (of 248), d = 0.546636, its = 26
i = 221 (of 248), d = 0.612407, its = 9
i = 222 (of 248), d = 0.660663, its = 27
i = 223 (of 248), d = 0.628552, its = 10
i = 224 (of 248), d = 0.148998, its = 19
i = 225 (of 248), d = 0.637299, its = 30
i = 226 (of 248), d = 0.0459815, its = 4
i = 227 (of 248), d = 0.639685, its = 11
i = 228 (of 248), d = 0.602326, its = 19
i = 229 (of 248), d = 0.327202, its = 10
i = 230 (of 248), d = 0.0880473, its = 21
i = 231 (of 248), d = 0.290239, its = 21
i = 232 (of 248), d = 0.411704, its = 8
i = 233 (of 248), d = 0.27632, its = 20
i = 234 (of 248), d = 0.564057, its = 8
i = 235 (of 248), d = 0.541827, its = 10
i = 236 (of 248), d = 1.05598, its = 18
i = 237 (of 248), d = 0.272109, its = 23
i = 238 (of 248), d = 0.0433408, its = 4
i = 239 (of 248), d = 0.300637, its = 8
i = 240 (of 248), d = 0.49118, its = 27
i = 241 (of 248), d = 0.161075, its = 7
i = 242 (of 248), d = 0.633323, its = 10
i = 243 (of 248), d = 0.0692264, its = 7
i = 244 (of 248), d = 0.347978, its = 9
i = 245 (of 248), d = 0.176533, its = 10
i = 246 (of 248), d = 0.145782, its = 24
i = 247 (of 248), d = 0.111487, its = 23
i = 248 (of 248), d = 0.468138, its = 13
[1] "Fri Feb 09 14:29:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:29:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:29:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:29:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:29:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:29:59 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  77
initial  value 1326.406726 
iter  10 value 59.595183
iter  20 value 13.073116
iter  30 value 5.687430
iter  40 value 2.874098
iter  50 value 1.936564
iter  60 value 1.699415
iter  70 value 1.421942
iter  80 value 1.330164
iter  90 value 1.298043
iter 100 value 1.282934
final  value 1.282934 
stopped after 100 iterations
# weights:  77
initial  value 300.536421 
iter  10 value 2.111558
iter  20 value 1.369909
iter  30 value 1.300909
iter  40 value 1.273904
iter  50 value 1.216743
iter  60 value 0.954148
iter  70 value 0.790392
iter  80 value 0.719523
iter  90 value 0.693857
iter 100 value 0.688230
final  value 0.688230 
stopped after 100 iterations
# weights:  77
initial  value 127.128048 
iter  10 value 2.360440
iter  20 value 2.109923
iter  30 value 1.439269
iter  40 value 1.328717
iter  50 value 1.161154
iter  60 value 0.928022
iter  70 value 0.760228
iter  80 value 0.662358
iter  90 value 0.650860
iter 100 value 0.645796
final  value 0.645796 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0353; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  53
initial  value 75.877318 
iter  10 value 2.019981
iter  20 value 1.649202
iter  30 value 1.049918
iter  40 value 1.015177
iter  50 value 0.626033
iter  60 value 0.387988
iter  70 value 0.364035
iter  80 value 0.336241
iter  90 value 0.326244
iter 100 value 0.315184
final  value 0.315184 
stopped after 100 iterations
# weights:  53
initial  value 261.239096 
iter  10 value 1.977763
iter  20 value 1.907509
iter  30 value 1.615080
iter  40 value 1.065334
iter  50 value 0.931389
iter  60 value 0.487631
iter  70 value 0.379681
iter  80 value 0.335676
iter  90 value 0.324925
iter 100 value 0.319415
final  value 0.319415 
stopped after 100 iterations
# weights:  53
initial  value 63.657908 
iter  10 value 2.387906
iter  20 value 2.082008
iter  30 value 1.161879
iter  40 value 1.036872
iter  50 value 0.611692
iter  60 value 0.353154
iter  70 value 0.316298
iter  80 value 0.299824
iter  90 value 0.286543
iter 100 value 0.279734
final  value 0.279734 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0247; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  49
initial  value 134.129291 
iter  10 value 3.246053
iter  20 value 1.109787
iter  30 value 1.038781
iter  40 value 0.526288
iter  50 value 0.346366
iter  60 value 0.303687
iter  70 value 0.295999
iter  80 value 0.283216
iter  90 value 0.268208
iter 100 value 0.240102
final  value 0.240102 
stopped after 100 iterations
# weights:  49
initial  value 790.925576 
iter  10 value 2.052171
iter  20 value 1.777269
iter  30 value 1.128765
iter  40 value 0.968535
iter  50 value 0.609149
iter  60 value 0.465732
iter  70 value 0.391928
iter  80 value 0.358434
iter  90 value 0.343906
iter 100 value 0.336056
final  value 0.336056 
stopped after 100 iterations
# weights:  49
initial  value 1949.171586 
iter  10 value 2.247068
iter  20 value 1.568647
iter  30 value 1.113997
iter  40 value 1.090190
iter  50 value 1.072994
iter  60 value 0.956337
iter  70 value 0.777291
iter  80 value 0.726982
iter  90 value 0.660593
iter 100 value 0.549785
final  value 0.549785 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0287; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  9
initial  value 248.496248 
iter  10 value 3.032138
iter  20 value 2.232834
iter  30 value 2.132236
iter  40 value 2.014013
iter  50 value 1.533438
iter  60 value 1.162918
iter  70 value 1.143477
iter  80 value 1.141997
iter  90 value 1.139889
iter 100 value 1.139064
final  value 1.139064 
stopped after 100 iterations
# weights:  9
initial  value 428.795209 
iter  10 value 16.743008
iter  20 value 1.805683
iter  30 value 1.609899
iter  40 value 1.254394
iter  50 value 1.214766
iter  60 value 1.157190
iter  70 value 1.153640
iter  80 value 1.147437
iter  90 value 1.145509
iter 100 value 1.143912
final  value 1.143912 
stopped after 100 iterations
# weights:  9
initial  value 271.681191 
iter  10 value 2.340124
iter  20 value 1.300953
iter  30 value 1.226151
iter  40 value 1.213720
iter  50 value 1.210942
iter  60 value 1.208002
iter  70 value 1.207165
iter  80 value 1.206303
iter  90 value 1.206180
iter 100 value 1.206030
final  value 1.206030 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0475; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  77
initial  value 249.398532 
iter  10 value 2.485823
iter  20 value 2.218010
iter  30 value 1.822140
iter  40 value 1.610695
iter  50 value 1.559922
iter  60 value 1.541543
iter  70 value 1.527931
iter  80 value 1.518000
iter  90 value 1.506949
iter 100 value 1.497098
final  value 1.497098 
stopped after 100 iterations
# weights:  77
initial  value 135.935182 
iter  10 value 2.369627
iter  20 value 2.148629
iter  30 value 2.053014
iter  40 value 1.933993
iter  50 value 1.731837
iter  60 value 1.629964
iter  70 value 1.572325
iter  80 value 1.546961
iter  90 value 1.534039
iter 100 value 1.524740
final  value 1.524740 
stopped after 100 iterations
# weights:  77
initial  value 60.443127 
iter  10 value 2.519831
iter  20 value 2.257193
iter  30 value 1.791890
iter  40 value 1.570129
iter  50 value 1.554770
iter  60 value 1.552674
iter  70 value 1.552157
iter  80 value 1.551927
iter  90 value 1.550749
iter 100 value 1.548930
final  value 1.548930 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0483; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  73
initial  value 71.051481 
iter  10 value 15.110602
iter  20 value 13.648611
iter  30 value 13.442887
iter  40 value 13.354054
iter  50 value 13.342291
iter  60 value 13.326722
iter  70 value 13.305754
iter  80 value 13.301336
iter  90 value 13.293113
iter 100 value 13.286961
final  value 13.286961 
stopped after 100 iterations
# weights:  73
initial  value 185.094294 
iter  10 value 17.208303
iter  20 value 13.408235
iter  30 value 13.247445
iter  40 value 13.213980
iter  50 value 13.195423
iter  60 value 13.173127
iter  70 value 13.161531
iter  80 value 13.140026
iter  90 value 13.122494
iter 100 value 13.121504
final  value 13.121504 
stopped after 100 iterations
# weights:  73
initial  value 158.881456 
iter  10 value 23.197635
iter  20 value 14.792905
iter  30 value 14.112839
iter  40 value 13.669804
iter  50 value 13.619292
iter  60 value 13.582052
iter  70 value 13.553266
iter  80 value 13.529704
iter  90 value 13.515403
iter 100 value 13.508228
final  value 13.508228 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0787; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  53
initial  value 890.210192 
iter  10 value 2.362504
iter  20 value 1.986623
iter  30 value 1.227189
iter  40 value 1.107706
iter  50 value 1.096246
iter  60 value 1.069768
iter  70 value 1.005896
iter  80 value 0.586773
iter  90 value 0.404694
iter 100 value 0.369257
final  value 0.369257 
stopped after 100 iterations
# weights:  53
initial  value 498.773126 
iter  10 value 1.966048
iter  20 value 1.139425
iter  30 value 1.095365
iter  40 value 1.091050
iter  50 value 1.081390
iter  60 value 1.074719
iter  70 value 1.030330
iter  80 value 0.867235
iter  90 value 0.535655
iter 100 value 0.450479
final  value 0.450479 
stopped after 100 iterations
# weights:  53
initial  value 46.560051 
iter  10 value 2.243384
iter  20 value 1.794161
iter  30 value 1.139368
iter  40 value 1.083954
iter  50 value 0.818139
iter  60 value 0.621075
iter  70 value 0.450439
iter  80 value 0.379674
iter  90 value 0.354951
iter 100 value 0.340295
final  value 0.340295 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0263; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  33
initial  value 201.618169 
iter  10 value 2.003957
iter  20 value 1.355322
iter  30 value 1.158648
iter  40 value 1.105770
iter  50 value 1.051295
iter  60 value 0.674824
iter  70 value 0.497277
iter  80 value 0.444066
iter  90 value 0.406831
iter 100 value 0.400492
final  value 0.400492 
stopped after 100 iterations
# weights:  33
initial  value 121.643794 
iter  10 value 1.908041
iter  20 value 1.295196
iter  30 value 1.139526
iter  40 value 1.113932
iter  50 value 0.975612
iter  60 value 0.558723
iter  70 value 0.512559
iter  80 value 0.483774
iter  90 value 0.460996
iter 100 value 0.455586
final  value 0.455586 
stopped after 100 iterations
# weights:  33
initial  value 114.007095 
iter  10 value 2.341718
iter  20 value 1.957241
iter  30 value 1.252686
iter  40 value 1.197120
iter  50 value 0.985099
iter  60 value 0.664103
iter  70 value 0.533466
iter  80 value 0.483840
iter  90 value 0.428991
iter 100 value 0.418007
final  value 0.418007 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0258; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  77
initial  value 387.124090 
iter  10 value 2.335745
iter  20 value 1.911130
iter  30 value 1.131583
iter  40 value 1.118866
iter  50 value 1.110609
iter  60 value 1.078063
iter  70 value 0.775016
iter  80 value 0.637171
iter  90 value 0.517241
iter 100 value 0.450557
final  value 0.450557 
stopped after 100 iterations
# weights:  77
initial  value 1029.992647 
iter  10 value 10.059186
iter  20 value 4.722003
iter  30 value 2.653531
iter  40 value 1.971850
iter  50 value 1.526147
iter  60 value 1.254492
iter  70 value 1.190042
iter  80 value 0.970452
iter  90 value 0.866519
iter 100 value 0.795750
final  value 0.795750 
stopped after 100 iterations
# weights:  77
initial  value 1802.331291 
iter  10 value 3.328014
iter  20 value 1.670700
iter  30 value 1.371003
iter  40 value 1.120395
iter  50 value 0.899363
iter  60 value 0.673851
iter  70 value 0.630901
iter  80 value 0.597467
iter  90 value 0.571954
iter 100 value 0.549513
final  value 0.549513 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0302; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  21
initial  value 104.479697 
iter  10 value 2.384996
iter  20 value 1.629647
iter  30 value 1.013791
iter  40 value 0.799662
iter  50 value 0.744151
iter  60 value 0.677494
iter  70 value 0.510647
iter  80 value 0.381764
iter  90 value 0.325131
iter 100 value 0.279716
final  value 0.279716 
stopped after 100 iterations
# weights:  21
initial  value 347.208228 
iter  10 value 4.171525
iter  20 value 1.186203
iter  30 value 1.022868
iter  40 value 0.827587
iter  50 value 0.771115
iter  60 value 0.710366
iter  70 value 0.692376
iter  80 value 0.574868
iter  90 value 0.341555
iter 100 value 0.278445
final  value 0.278445 
stopped after 100 iterations
# weights:  21
initial  value 116.258505 
iter  10 value 2.126674
iter  20 value 1.317795
iter  30 value 1.094184
iter  40 value 1.070971
iter  50 value 1.056005
iter  60 value 0.803333
iter  70 value 0.650311
iter  80 value 0.627072
iter  90 value 0.595859
iter 100 value 0.575527
final  value 0.575527 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.0318; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  73
initial  value 113.294711 
iter  10 value 15.298039
iter  20 value 11.448382
iter  30 value 11.256595
iter  40 value 11.204144
iter  50 value 11.169407
iter  60 value 11.129083
iter  70 value 11.119020
iter  80 value 11.108621
iter  90 value 11.090067
iter 100 value 11.086500
final  value 11.086500 
stopped after 100 iterations
# weights:  73
initial  value 95.834726 
iter  10 value 11.228205
iter  20 value 11.020166
iter  30 value 10.989390
iter  40 value 10.970165
iter  50 value 10.951146
iter  60 value 10.942489
iter  70 value 10.925136
iter  80 value 10.923019
iter  90 value 10.922768
iter 100 value 10.921784
final  value 10.921784 
stopped after 100 iterations
# weights:  73
initial  value 1057.729142 
iter  10 value 72.907323
iter  20 value 19.350898
iter  30 value 13.598066
iter  40 value 12.949118
iter  50 value 12.280593
iter  60 value 11.980060
iter  70 value 11.791639
iter  80 value 11.687650
iter  90 value 11.595624
iter 100 value 11.553549
final  value 11.553549 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0741; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  45
initial  value 66.619092 
iter  10 value 10.621036
iter  20 value 10.030240
iter  30 value 9.906481
iter  40 value 9.860929
iter  50 value 9.828729
iter  60 value 9.822008
iter  70 value 9.820861
iter  80 value 9.817595
iter  90 value 9.816249
iter 100 value 9.816206
final  value 9.816206 
stopped after 100 iterations
# weights:  45
initial  value 147.597176 
iter  10 value 10.517387
iter  20 value 9.749703
iter  30 value 9.693751
iter  40 value 9.659662
iter  50 value 9.654013
iter  60 value 9.653840
iter  70 value 9.653518
iter  80 value 9.653222
final  value 9.653204 
converged
# weights:  45
initial  value 1693.667638 
iter  10 value 70.665184
iter  20 value 21.953788
iter  30 value 13.221505
iter  40 value 11.521927
iter  50 value 10.680631
iter  60 value 10.277561
iter  70 value 10.166564
iter  80 value 10.105284
iter  90 value 10.089182
iter 100 value 10.030234
final  value 10.030234 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0718; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  21
initial  value 931.912566 
iter  10 value 5.662594
iter  20 value 2.539334
iter  30 value 2.344039
iter  40 value 2.081529
iter  50 value 1.898376
iter  60 value 1.849082
iter  70 value 1.847504
iter  80 value 1.846963
iter  90 value 1.846633
iter 100 value 1.845883
final  value 1.845883 
stopped after 100 iterations
# weights:  21
initial  value 842.436993 
iter  10 value 3.546034
iter  20 value 2.439156
iter  30 value 2.242847
iter  40 value 2.011378
iter  50 value 1.832002
iter  60 value 1.811523
iter  70 value 1.807503
iter  80 value 1.806815
iter  90 value 1.806535
iter 100 value 1.806145
final  value 1.806145 
stopped after 100 iterations
# weights:  21
initial  value 309.725906 
iter  10 value 2.656334
iter  20 value 2.371749
iter  30 value 2.075656
iter  40 value 2.006990
iter  50 value 1.955098
iter  60 value 1.936441
iter  70 value 1.932388
iter  80 value 1.930921
iter  90 value 1.930182
iter 100 value 1.928174
final  value 1.928174 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0511; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  53
initial  value 1137.467154 
iter  10 value 3.492631
iter  20 value 1.688830
iter  30 value 1.470284
iter  40 value 1.110357
iter  50 value 0.973330
iter  60 value 0.895597
iter  70 value 0.757601
iter  80 value 0.696757
iter  90 value 0.668589
iter 100 value 0.645240
final  value 0.645240 
stopped after 100 iterations
# weights:  53
initial  value 86.965831 
iter  10 value 1.951143
iter  20 value 1.483993
iter  30 value 1.248817
iter  40 value 1.227313
iter  50 value 1.211229
iter  60 value 1.043068
iter  70 value 0.798285
iter  80 value 0.730542
iter  90 value 0.717590
iter 100 value 0.707101
final  value 0.707101 
stopped after 100 iterations
# weights:  53
initial  value 349.864409 
iter  10 value 2.339754
iter  20 value 1.679464
iter  30 value 1.384870
iter  40 value 1.286784
iter  50 value 1.182299
iter  60 value 0.836264
iter  70 value 0.689899
iter  80 value 0.653969
iter  90 value 0.642422
iter 100 value 0.636071
final  value 0.636071 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.0274; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  45
initial  value 440.913952 
iter  10 value 7.910137
iter  20 value 3.223413
iter  30 value 2.657453
iter  40 value 2.570947
iter  50 value 2.527022
iter  60 value 2.506354
iter  70 value 2.490632
iter  80 value 2.484594
iter  90 value 2.480737
iter 100 value 2.451428
final  value 2.451428 
stopped after 100 iterations
# weights:  45
initial  value 476.318351 
iter  10 value 3.284420
iter  20 value 2.630507
iter  30 value 2.481606
iter  40 value 2.445308
iter  50 value 2.406037
iter  60 value 2.394531
iter  70 value 2.388241
iter  80 value 2.386070
iter  90 value 2.382333
iter 100 value 2.352697
final  value 2.352697 
stopped after 100 iterations
# weights:  45
initial  value 1300.057571 
iter  10 value 5.085846
iter  20 value 3.593586
iter  30 value 3.091885
iter  40 value 2.768152
iter  50 value 2.682073
iter  60 value 2.635950
iter  70 value 2.620986
iter  80 value 2.600817
iter  90 value 2.592538
iter 100 value 2.563098
final  value 2.563098 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0581; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  37
initial  value 86.812955 
iter  10 value 5.153334
iter  20 value 1.938005
iter  30 value 1.173632
iter  40 value 0.845104
iter  50 value 0.776164
iter  60 value 0.669302
iter  70 value 0.555507
iter  80 value 0.530905
iter  90 value 0.483144
iter 100 value 0.456730
final  value 0.456730 
stopped after 100 iterations
# weights:  37
initial  value 450.483175 
iter  10 value 2.325326
iter  20 value 1.455856
iter  30 value 1.145651
iter  40 value 1.134857
iter  50 value 1.113150
iter  60 value 1.018471
iter  70 value 0.743807
iter  80 value 0.657590
iter  90 value 0.552145
iter 100 value 0.490708
final  value 0.490708 
stopped after 100 iterations
# weights:  37
initial  value 1166.587740 
iter  10 value 2.353256
iter  20 value 2.150068
iter  30 value 1.331150
iter  40 value 1.213576
iter  50 value 1.170119
iter  60 value 0.960714
iter  70 value 0.676774
iter  80 value 0.519177
iter  90 value 0.448522
iter 100 value 0.429805
final  value 0.429805 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0263; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  21
initial  value 142.314789 
iter  10 value 10.555799
iter  20 value 10.156841
iter  30 value 10.108559
iter  40 value 10.103755
iter  50 value 10.097503
iter  60 value 10.095859
final  value 10.095854 
converged
# weights:  21
initial  value 471.548015 
iter  10 value 10.285708
iter  20 value 10.080383
iter  30 value 10.023756
iter  40 value 9.981510
iter  50 value 9.943442
iter  60 value 9.940940
final  value 9.940935 
converged
# weights:  21
initial  value 996.599060 
iter  10 value 18.639347
iter  20 value 11.257482
iter  30 value 10.667657
iter  40 value 10.460637
iter  50 value 10.357891
iter  60 value 10.296024
iter  70 value 10.291342
final  value 10.291329 
converged
[Tune-y] 17: rmse.test.rmse=0.0723; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  57
initial  value 638.958689 
iter  10 value 2.095832
iter  20 value 1.302982
iter  30 value 1.033170
iter  40 value 1.010611
iter  50 value 0.998435
iter  60 value 0.901340
iter  70 value 0.731778
iter  80 value 0.439504
iter  90 value 0.290580
iter 100 value 0.254228
final  value 0.254228 
stopped after 100 iterations
# weights:  57
initial  value 221.206072 
iter  10 value 1.950456
iter  20 value 1.712061
iter  30 value 1.061585
iter  40 value 1.016001
iter  50 value 0.803332
iter  60 value 0.620587
iter  70 value 0.432421
iter  80 value 0.332294
iter  90 value 0.316732
iter 100 value 0.296615
final  value 0.296615 
stopped after 100 iterations
# weights:  57
initial  value 1759.690153 
iter  10 value 4.003943
iter  20 value 1.319672
iter  30 value 1.158604
iter  40 value 1.095115
iter  50 value 0.834480
iter  60 value 0.700984
iter  70 value 0.604423
iter  80 value 0.547105
iter  90 value 0.369781
iter 100 value 0.300856
final  value 0.300856 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0269; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  17
initial  value 741.692344 
iter  10 value 2.010690
iter  20 value 1.156914
iter  30 value 1.141113
iter  40 value 1.132189
iter  50 value 1.116661
iter  60 value 1.106187
iter  70 value 0.948986
iter  80 value 0.758273
iter  90 value 0.619806
iter 100 value 0.559036
final  value 0.559036 
stopped after 100 iterations
# weights:  17
initial  value 65.026227 
iter  10 value 2.063893
iter  20 value 1.762033
iter  30 value 1.268031
iter  40 value 1.128864
iter  50 value 1.115497
iter  60 value 1.113685
iter  70 value 1.095076
iter  80 value 0.871733
iter  90 value 0.569219
iter 100 value 0.530008
final  value 0.530008 
stopped after 100 iterations
# weights:  17
initial  value 236.548047 
iter  10 value 3.015401
iter  20 value 1.675574
iter  30 value 1.110174
iter  40 value 0.773899
iter  50 value 0.601781
iter  60 value 0.461312
iter  70 value 0.429344
iter  80 value 0.425943
iter  90 value 0.415312
iter 100 value 0.412746
final  value 0.412746 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0295; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  29
initial  value 227.140194 
iter  10 value 2.373068
iter  20 value 2.288804
iter  30 value 2.102996
iter  40 value 1.989690
iter  50 value 1.947181
iter  60 value 1.934733
iter  70 value 1.911637
iter  80 value 1.908997
iter  90 value 1.908673
iter 100 value 1.908436
final  value 1.908436 
stopped after 100 iterations
# weights:  29
initial  value 279.335873 
iter  10 value 2.626804
iter  20 value 2.099257
iter  30 value 1.926645
iter  40 value 1.893867
iter  50 value 1.884622
iter  60 value 1.877157
iter  70 value 1.851327
iter  80 value 1.848227
iter  90 value 1.848019
iter 100 value 1.847929
final  value 1.847929 
stopped after 100 iterations
# weights:  29
initial  value 46.245443 
iter  10 value 2.511335
iter  20 value 2.416344
iter  30 value 2.230131
iter  40 value 2.092436
iter  50 value 2.057679
iter  60 value 2.026089
iter  70 value 1.999082
iter  80 value 1.995159
iter  90 value 1.994198
iter 100 value 1.994036
final  value 1.994036 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0525; time: 0.0 min
[Tune] Result: size=13; decay=0.000354 : rmse.test.rmse=0.0247
# weights:  53
initial  value 82.359215 
iter  10 value 3.927883
iter  20 value 2.468986
iter  30 value 1.625192
iter  40 value 1.428353
iter  50 value 1.077713
iter  60 value 0.918344
iter  70 value 0.875237
iter  80 value 0.585648
iter  90 value 0.508209
iter 100 value 0.481724
final  value 0.481724 
stopped after 100 iterations
[1] "Fri Feb 09 14:30:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 700 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 414
 number of selected nodes                               : 65 
[1] "Fri Feb 09 14:30:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:30:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:30:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:30:48 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=2; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0276; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0265; time: 0.1 min
[Tune-x] 3: mtry=2; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0266; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0327; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0285; time: 0.0 min
[Tune-x] 6: mtry=2; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0308; time: 0.0 min
[Tune-x] 7: mtry=2; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0267; time: 0.1 min
[Tune-x] 8: mtry=1; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0319; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0274; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0301; time: 0.0 min
[Tune-x] 11: mtry=2; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0311; time: 0.0 min
[Tune-x] 12: mtry=2; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0313; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0339; time: 0.0 min
[Tune-x] 14: mtry=2; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0275; time: 0.0 min
[Tune-x] 15: mtry=2; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0293; time: 0.0 min
[Tune-x] 16: mtry=1; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0319; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0365; time: 0.0 min
[Tune-x] 18: mtry=2; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0263; time: 0.1 min
[Tune-x] 19: mtry=1; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0317; time: 0.0 min
[Tune-x] 20: mtry=1; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0338; time: 0.0 min
[Tune] Result: mtry=2; min.node.size=1 : rmse.test.rmse=0.0263
[1] "Fri Feb 09 14:31:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:31:44 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.163; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.111; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.0953; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.147; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.251; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.108; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.267; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.196; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.0953
[1] "Fri Feb 09 14:31:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:31:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:32:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.6930 0.3170 0.1450 0.0664 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 2 
Runtime: 0.01300097 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1451 
    Y 1         0.5637 
    Y 2          0.481 
    Y 3         0.1034 
    Y 4          0.416 
    Y 5         0.4436 
[1] "Fri Feb 09 14:32:13 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.166; time: 0.6 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:32:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:32:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:32:53] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.383537 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.133; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:17] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.380767 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.181; time: 0.7 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.257; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.496772 is too small that no feature can be included

[Tune-y] 9: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:33:58] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.351825 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.169; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.333946 is too small that no feature can be included

[Tune-y] 12: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.313328 is too small that no feature can be included

[Tune-y] 13: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.29; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:02] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:03] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.440964 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.253; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.202; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411008 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:34:04] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.329956 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.133
[1] "Fri Feb 09 14:34:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:34:13 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [2,0.52953], n=(396,356)
**GROW** @depth 1: [2,0.764765], n=(173,181)
**GROW** @depth 2: [1,0.497998], n=(203,195)
**GROW** @depth 2: [1,0.675175], n=(119,62)
**GROW** @depth 3: [2,0.299299], n=(39,43)
**GROW** @depth 2: [1,0.342843], n=(125,104)
**GROW** @depth 2: [1,0.825826], n=(86,90)
**GROW** @depth 3: [2,0.804805], n=(56,48)
**GROW** @depth 4: [2,0.139139], n=(18,22)
**GROW** @depth 3: [2,0.682182], n=(43,83)
**GROW** @depth 3: [2,0.444444], n=(47,35)
**GROW** @depth 3: [2,0.264264], n=(29,61)
**GROW** @depth 4: [2,0.904404], n=(23,23)
**GROW** @depth 4: [1,0.253754], n=(70,77)
**GROW** @depth 4: [2,0.83984], n=(44,40)
**GROW** @depth 3: [1,0.34034], n=(44,31)
**GROW** @depth 4: [2,0.221221], n=(21,27)
**GROW** @depth 5: [1,0.16016], n=(42,28)
**GROW** @depth 5: [2,0.3003], n=(35,43)
**GROW** @depth 5: [2,0.353353], n=(27,15)
**PRUNE** @depth 5: [2,0.353353]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(45,31,42,26,35,43,23,41,42,45,41,58,23,22,21,26,36,29,61,62)
**GROW** @depth 4: [2,0.0990991], n=(20,11)
**GROW** @depth 5: [1,0.139139], n=(15,27)
**PRUNE** @depth 4: [2,0.0990991]
**PRUNE** @depth 5: [2,0.678679]
**GROW** @depth 4: [2,0.666667], n=(27,30)
**PRUNE** @depth 4: [2,0.666667]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(45,31,40,28,35,46,24,40,34,53,41,57,23,22,20,25,37,30,59,62)

Sampling @ nn=0 pred locs:
**GROW** @depth 4: [2,0.038038], n=(14,31)
**GROW** @depth 4: [2,0.600601], n=(16,20)
**GROW** @depth 5: [2,0.407407], n=(13,15)
**PRUNE** @depth 4: [2,0.600601]
**PRUNE** @depth 5: [2,0.407407]
**GROW** @depth 4: [2,0.588088], n=(17,20)
**GROW** @depth 4: [1,0.536537], n=(35,23)
**GROW** @depth 2: [2,0.86987], n=(25,37)
**GROW** @depth 5: [1,0.163163], n=(15,16)
**GROW** @depth 6: [1,0.223223], n=(22,29)
**GROW** @depth 4: [1,0.912913], n=(28,31)
**PRUNE** @depth 4: [1,0.912913]
**GROW** @depth 7: [1,0.361862], n=(15,30)
**GROW** @depth 3: [2,0.0810811], n=(17,15)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(14,15,17,17,15,40,26,34,16,30,27,37,33,24,28,44,35,23,23,21,20,24,18,20,30,59,25,37)
**PRUNE** @depth 6: [1,0.535536]
**GROW** @depth 6: [1,0.36036], n=(19,21)
**PRUNE** @depth 7: [1,0.243243]
**GROW** @depth 3: [2,0.95996], n=(22,15)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,15,16,16,16,39,19,21,36,29,25,38,32,26,28,43,57,23,21,22,26,17,22,29,60,24,22,15)
**PRUNE** @depth 3: [2,0.958959]
**GROW** @depth 4: [1,0.0730731], n=(15,25)
**PRUNE** @depth 4: [1,0.0740741]
**GROW** @depth 6: [2,0.66016], n=(13,15)
**GROW** @depth 4: [1,0.922923], n=(31,29)
**GROW** @depth 6: [2,0.684184], n=(28,29)
**PRUNE** @depth 6: [2,0.807808]
**PRUNE** @depth 4: [1,0.927928]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,15,16,16,14,39,19,22,36,30,25,38,33,24,13,16,42,28,52,21,23,26,17,22,30,59,24,37)
**GROW** @depth 5: [2,0.928929], n=(26,16)
**GROW** @depth 4: [1,0.927928], n=(32,27)
**PRUNE** @depth 3: [2,0.0755756]
**GROW** @depth 5: [1,0.0690691], n=(19,15)
**GROW** @depth 4: [2,0.0680681], n=(11,12)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,15,16,31,37,20,22,38,31,23,38,19,15,24,13,17,25,17,28,51,20,11,12,25,18,21,29,33,25,26,37)
**PRUNE** @depth 7: [1,0.0690691]
**GROW** @depth 4: [2,0.0630631], n=(13,18)
**GROW** @depth 6: [2,0.418418], n=(22,18)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(15,15,16,12,20,37,20,21,37,31,22,22,18,35,23,13,17,25,17,28,51,20,11,12,26,17,21,29,33,25,26,37)
Grow: 13.94%, Prune: 4.438%, Change: 82.89%, Swap: 23.39%

[1] "Fri Feb 09 14:34:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:34:22 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:34:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 8 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 7.0345 	 alpha= 0.0252 	 beta= 170.0561 
[1] "Fri Feb 09 14:34:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:34:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [2,0.501502], n=(376,376)
**GROW** @depth 1: [1,0.501502], n=(192,184)
**PRUNE** @depth 1: [1,0.501502]
**GROW** @depth 1: [1,0.551552], n=(208,165)
**GROW** @depth 2: [1,0.275275], n=(104,102)
**GROW** @depth 2: [2,0.761261], n=(87,77)
**PRUNE** @depth 2: [1,0.275275]
**GROW** @depth 3: [2,0.880881], n=(39,38)
**GROW** @depth 3: [1,0.767768], n=(44,43)
**GROW** @depth 1: [1,0.468468], n=(184,197)
**GROW** @depth 2: [2,0.745245], n=(93,113)
**GROW** @depth 4: [2,0.877878], n=(15,11)
**GROW** @depth 4: [1,0.268268], n=(59,34)
**GROW** @depth 2: [1,0.733734], n=(113,82)
**GROW** @depth 3: [2,0.241742], n=(40,42)
**GROW** @depth 4: [1,0.368368], n=(11,23)
**GROW** @depth 3: [2,0.867868], n=(51,59)
**PRUNE** @depth 4: [1,0.36987]
**GROW** @depth 4: [2,0.930931], n=(29,30)
**PRUNE** @depth 4: [2,0.930931]
**GROW** @depth 4: [1,0.864865], n=(17,26)
**GROW** @depth 4: [1,0.872873], n=(26,39)
**PRUNE** @depth 3: [2,0.745245]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(185,86,67,17,26,100,64,38,48,15,12,26,39,29)
**GROW** @depth 4: [2,0.930931], n=(19,19)
**GROW** @depth 5: [1,0.935936], n=(13,13)
**PRUNE** @depth 5: [1,0.868869]
**GROW** @depth 5: [1,0.906907], n=(14,25)
**PRUNE** @depth 5: [1,0.872873]
**GROW** @depth 2: [2,0.0980981], n=(76,104)
**GROW** @depth 3: [1,0.494494], n=(58,47)
**GROW** @depth 5: [2,0.693694], n=(20,21)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(75,57,47,93,67,31,12,98,64,21,19,45,16,11,20,22,20,34)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [2,0.778779], n=(64,34)
**GROW** @depth 3: [1,0.564565], n=(41,32)
**PRUNE** @depth 3: [2,0.778779]
**PRUNE** @depth 3: [1,0.493493]
**GROW** @depth 3: [1,0.476476], n=(57,50)
**GROW** @depth 4: [1,0.246246], n=(26,31)
**GROW** @depth 5: [1,0.73974], n=(31,19)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(42,32,25,30,31,19,96,68,32,12,94,66,21,19,41,16,11,23,22,23,29)
**GROW** @depth 5: [2,0.15015], n=(12,19)
**PRUNE** @depth 2: [1,0.562563]
**GROW** @depth 5: [1,0.241241], n=(46,49)
**GROW** @depth 5: [1,0.37988], n=(31,35)
**PRUNE** @depth 5: [1,0.37988]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(74,25,30,13,18,18,48,48,69,34,12,93,66,21,19,39,16,11,24,23,20,31)
**GROW** @depth 4: [1,0.122122], n=(20,28)
**GROW** @depth 3: [1,0.720721], n=(52,22)
**PRUNE** @depth 3: [1,0.720721]
**GROW** @depth 3: [2,0.022022], n=(18,56)
**PRUNE** @depth 6: [2,0.15015]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(17,57,24,29,31,18,20,28,52,69,34,12,92,64,22,19,39,16,11,24,24,21,29)
**GROW** @depth 6: [2,0.171171], n=(16,15)
**GROW** @depth 3: [1,0.555556], n=(31,27)
**GROW** @depth 4: [1,0.256256], n=(19,12)
**PRUNE** @depth 4: [2,0.172172]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(16,19,12,27,24,29,31,18,20,28,51,68,35,12,93,64,22,19,40,15,11,24,24,20,30)
**GROW** @depth 4: [1,0.392893], n=(35,30)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(17,19,12,28,23,28,28,17,20,31,51,70,36,12,91,35,30,22,19,41,15,11,24,26,20,26)
Grow: 10.54%, Prune: 3.753%, Change: 79.76%, Swap: 42.9%

[1] "Fri Feb 09 14:34:30 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:34:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:34:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:34:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:34:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:34:33 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:34:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:34:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:34:37 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.303; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.853; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.236; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.301; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.485; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.263; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.476; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.253; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.282; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.339; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.291; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.823; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.21; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.16; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.645; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.219; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.223; time: 0.0 min
[Tune] Result: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10 : rmse.test.rmse=0.16
[1] "Fri Feb 09 14:34:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:34:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.34; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.309; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.331; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.308; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.347; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.462; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.35; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.319; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.311; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.511; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.308; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=1.46; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.393; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.358; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.307; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.307
[1] "Fri Feb 09 14:34:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:34:55 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |================================                                      |  46%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:34:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:35:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===============                                                       |  22%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:35:06 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 
  - parameters upper bounds :  2 2 
  - best initial criterion value(s) :  803.698 

N = 2, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       -803.7  |proj g|=        1.021
At iterate     1  f =      -806.24  |proj g|=        1.0954
At iterate     2  f =      -807.58  |proj g|=        1.1126
At iterate     3  f =      -807.64  |proj g|=        1.5207
At iterate     4  f =      -807.64  |proj g|=       0.35857
At iterate     5  f =      -807.64  |proj g|=       0.34991
Bad direction in the line search;
   refresh the lbfgs memory and restart the iteration.
Line search cannot locate an adequate point after 20 function
and gradient evaluations
final  value -807.644003 
stopped after 5 iterations
[1] "Fri Feb 09 14:36:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 0.0527754, its = 5
i = 2 (of 248), d = 0.114564, its = 6
i = 3 (of 248), d = 0.0191854, its = 21
i = 4 (of 248), d = 0.170989, its = 17
i = 5 (of 248), d = 0.0692158, its = 11
i = 6 (of 248), d = 0.202044, its = 23
i = 7 (of 248), d = 0.0377343, its = 4
i = 8 (of 248), d = 0.584073, its = 19
i = 9 (of 248), d = 0.0480588, its = 5
i = 10 (of 248), d = 0.107211, its = 6
i = 11 (of 248), d = 0.187357, its = 24
i = 12 (of 248), d = 0.0448452, its = 5
i = 13 (of 248), d = 0.0341529, its = 3
i = 14 (of 248), d = 0.466581, its = 21
i = 15 (of 248), d = 0.0446268, its = 4
i = 16 (of 248), d = 0.0384341, its = 3
i = 17 (of 248), d = 0.0630266, its = 8
i = 18 (of 248), d = 0.0309077, its = 6
i = 19 (of 248), d = 0.193139, its = 7
i = 20 (of 248), d = 1.24874, its = 16
i = 21 (of 248), d = 0.289933, its = 7
i = 22 (of 248), d = 0.117421, its = 25
i = 23 (of 248), d = 0.0839699, its = 24
i = 24 (of 248), d = 0.0523159, its = 6
i = 25 (of 248), d = 0.565861, its = 22
i = 26 (of 248), d = 0.400402, its = 21
i = 27 (of 248), d = 0.572085, its = 24
i = 28 (of 248), d = 0.119903, its = 5
i = 29 (of 248), d = 0.0389391, its = 4
i = 30 (of 248), d = 0.114706, its = 25
i = 31 (of 248), d = 0.261432, its = 22
i = 32 (of 248), d = 0.380063, its = 7
i = 33 (of 248), d = 0.118166, its = 28
i = 34 (of 248), d = 0.0938474, its = 6
i = 35 (of 248), d = 0.603762, its = 9
i = 36 (of 248), d = 0.0607215, its = 6
i = 37 (of 248), d = 0.0526689, its = 4
i = 38 (of 248), d = 0.308669, its = 26
i = 39 (of 248), d = 0.415346, its = 25
i = 40 (of 248), d = 0.188464, its = 20
i = 41 (of 248), d = 0.0418148, its = 4
i = 42 (of 248), d = 0.491585, its = 19
i = 43 (of 248), d = 0.0323937, its = 6
i = 44 (of 248), d = 0.0460407, its = 4
i = 45 (of 248), d = 0.281348, its = 20
i = 46 (of 248), d = 0.15545, its = 21
i = 47 (of 248), d = 0.192897, its = 6
i = 48 (of 248), d = 0.20137, its = 26
i = 49 (of 248), d = 0.919885, its = 18
i = 50 (of 248), d = 0.0503795, its = 4
i = 51 (of 248), d = 0.128965, its = 7
i = 52 (of 248), d = 0.812829, its = 19
i = 53 (of 248), d = 0.0706311, its = 6
i = 54 (of 248), d = 0.0785245, its = 4
i = 55 (of 248), d = 0.329422, its = 25
i = 56 (of 248), d = 0.548436, its = 10
i = 57 (of 248), d = 0.0389275, its = 4
i = 58 (of 248), d = 0.406577, its = 17
i = 59 (of 248), d = 0.455116, its = 22
i = 60 (of 248), d = 0.281102, its = 10
i = 61 (of 248), d = 0.031247, its = 5
i = 62 (of 248), d = 0.0337395, its = 5
i = 63 (of 248), d = 0.0694463, its = 6
i = 64 (of 248), d = 0.169786, its = 29
i = 65 (of 248), d = 0.0676242, its = 5
i = 66 (of 248), d = 0.4203, its = 8
i = 67 (of 248), d = 0.577487, its = 21
i = 68 (of 248), d = 0.101763, its = 4
i = 69 (of 248), d = 0.0488033, its = 5
i = 70 (of 248), d = 0.36011, its = 26
i = 71 (of 248), d = 0.587054, its = 16
i = 72 (of 248), d = 0.0408985, its = 3
i = 73 (of 248), d = 0.0606467, its = 5
i = 74 (of 248), d = 0.0512808, its = 6
i = 75 (of 248), d = 0.137541, its = 27
i = 76 (of 248), d = 0.553618, its = 8
i = 77 (of 248), d = 0.321876, its = 20
i = 78 (of 248), d = 0.0545864, its = 6
i = 79 (of 248), d = 0.0338999, its = 7
i = 80 (of 248), d = 0.0798181, its = 4
i = 81 (of 248), d = 0.0847009, its = 23
i = 82 (of 248), d = 0.136019, its = 20
i = 83 (of 248), d = 0.684006, its = 24
i = 84 (of 248), d = 0.376664, its = 8
i = 85 (of 248), d = 0.0411848, its = 4
i = 86 (of 248), d = 0.853667, its = 8
i = 87 (of 248), d = 0.323261, its = 20
i = 88 (of 248), d = 0.0434161, its = 4
i = 89 (of 248), d = 0.0333468, its = 7
i = 90 (of 248), d = 0.177417, its = 9
i = 91 (of 248), d = 0.126524, its = 7
i = 92 (of 248), d = 0.148276, its = 21
i = 93 (of 248), d = 0.118675, its = 4
i = 94 (of 248), d = 0.956768, its = 19
i = 95 (of 248), d = 0.74644, its = 22
i = 96 (of 248), d = 0.0732381, its = 23
i = 97 (of 248), d = 0.675886, its = 23
i = 98 (of 248), d = 0.032856, its = 4
i = 99 (of 248), d = 0.17681, its = 8
i = 100 (of 248), d = 0.138687, its = 7
i = 101 (of 248), d = 0.42908, its = 25
i = 102 (of 248), d = 0.536254, its = 20
i = 103 (of 248), d = 0.160656, its = 24
i = 104 (of 248), d = 0.0999595, its = 8
i = 105 (of 248), d = 0.259359, its = 21
i = 106 (of 248), d = 0.162183, its = 7
i = 107 (of 248), d = 0.0280533, its = 25
i = 108 (of 248), d = 0.196793, its = 6
i = 109 (of 248), d = 0.0706779, its = 24
i = 110 (of 248), d = 0.0980373, its = 29
i = 111 (of 248), d = 0.0324062, its = 6
i = 112 (of 248), d = 0.0782854, its = 4
i = 113 (of 248), d = 0.131401, its = 8
i = 114 (of 248), d = 0.0907374, its = 6
i = 115 (of 248), d = 0.519773, its = 18
i = 116 (of 248), d = 0.0689923, its = 21
i = 117 (of 248), d = 0.0562355, its = 6
i = 118 (of 248), d = 0.681529, its = 23
i = 119 (of 248), d = 0.146065, its = 7
i = 120 (of 248), d = 0.468324, its = 19
i = 121 (of 248), d = 0.129909, its = 7
i = 122 (of 248), d = 0.68334, its = 17
i = 123 (of 248), d = 0.042203, its = 4
i = 124 (of 248), d = 0.170629, its = 9
i = 125 (of 248), d = 0.0855587, its = 6
i = 126 (of 248), d = 0.590116, its = 25
i = 127 (of 248), d = 0.0748244, its = 5
i = 128 (of 248), d = 0.053793, its = 4
i = 129 (of 248), d = 0.0548892, its = 3
i = 130 (of 248), d = 0.392547, its = 9
i = 131 (of 248), d = 0.613936, its = 19
i = 132 (of 248), d = 0.331624, its = 27
i = 133 (of 248), d = 0.301907, its = 23
i = 134 (of 248), d = 0.0346249, its = 5
i = 135 (of 248), d = 0.164503, its = 25
i = 136 (of 248), d = 0.170486, its = 7
i = 137 (of 248), d = 0.261698, its = 25
i = 138 (of 248), d = 0.0536309, its = 4
i = 139 (of 248), d = 0.0546599, its = 4
i = 140 (of 248), d = 0.166456, its = 26
i = 141 (of 248), d = 0.760153, its = 17
i = 142 (of 248), d = 0.0322784, its = 24
i = 143 (of 248), d = 0.0611965, its = 6
i = 144 (of 248), d = 0.720308, its = 23
i = 145 (of 248), d = 0.122041, its = 22
i = 146 (of 248), d = 0.0353954, its = 5
i = 147 (of 248), d = 0.260038, its = 18
i = 148 (of 248), d = 0.194832, its = 21
i = 149 (of 248), d = 0.0489523, its = 5
i = 150 (of 248), d = 0.878517, its = 21
i = 151 (of 248), d = 0.164726, its = 23
i = 152 (of 248), d = 0.10272, its = 6
i = 153 (of 248), d = 0.0652348, its = 21
i = 154 (of 248), d = 0.201328, its = 29
i = 155 (of 248), d = 0.459692, its = 8
i = 156 (of 248), d = 0.355621, its = 19
i = 157 (of 248), d = 0.0308541, its = 7
i = 158 (of 248), d = 0.0456501, its = 6
i = 159 (of 248), d = 0.0409411, its = 4
i = 160 (of 248), d = 0.332821, its = 25
i = 161 (of 248), d = 0.0135069, its = 24
i = 162 (of 248), d = 0.102842, its = 6
i = 163 (of 248), d = 0.0438503, its = 6
i = 164 (of 248), d = 0.12246, its = 7
i = 165 (of 248), d = 0.0980615, its = 7
i = 166 (of 248), d = 0.175631, its = 21
i = 167 (of 248), d = 0.257088, its = 22
i = 168 (of 248), d = 0.0595438, its = 6
i = 169 (of 248), d = 0.0477127, its = 5
i = 170 (of 248), d = 0.385132, its = 22
i = 171 (of 248), d = 0.0708672, its = 22
i = 172 (of 248), d = 0.0541748, its = 21
i = 173 (of 248), d = 0.177139, its = 23
i = 174 (of 248), d = 1.13642, its = 20
i = 175 (of 248), d = 0.0698017, its = 21
i = 176 (of 248), d = 0.071504, its = 19
i = 177 (of 248), d = 0.0456866, its = 4
i = 178 (of 248), d = 0.82387, its = 17
i = 179 (of 248), d = 0.0440151, its = 5
i = 180 (of 248), d = 0.121256, its = 7
i = 181 (of 248), d = 0.122656, its = 11
i = 182 (of 248), d = 0.0403295, its = 4
i = 183 (of 248), d = 0.729842, its = 17
i = 184 (of 248), d = 0.175021, its = 21
i = 185 (of 248), d = 0.122236, its = 6
i = 186 (of 248), d = 0.487035, its = 23
i = 187 (of 248), d = 0.02952, its = 23
i = 188 (of 248), d = 0.0375292, its = 3
i = 189 (of 248), d = 0.092697, its = 7
i = 190 (of 248), d = 0.611994, its = 22
i = 191 (of 248), d = 0.221659, its = 22
i = 192 (of 248), d = 0.132739, its = 7
i = 193 (of 248), d = 0.131114, its = 7
i = 194 (of 248), d = 0.807058, its = 27
i = 195 (of 248), d = 0.107436, its = 31
i = 196 (of 248), d = 0.0447642, its = 5
i = 197 (of 248), d = 0.818849, its = 10
i = 198 (of 248), d = 0.579417, its = 24
i = 199 (of 248), d = 0.0494423, its = 5
i = 200 (of 248), d = 0.105875, its = 6
i = 201 (of 248), d = 0.556583, its = 23
i = 202 (of 248), d = 0.0332916, its = 5
i = 203 (of 248), d = 0.0406565, its = 4
i = 204 (of 248), d = 0.0526149, its = 6
i = 205 (of 248), d = 0.315962, its = 20
i = 206 (of 248), d = 0.338822, its = 27
i = 207 (of 248), d = 0.130406, its = 7
i = 208 (of 248), d = 0.0757881, its = 6
i = 209 (of 248), d = 0.0746032, its = 7
i = 210 (of 248), d = 0.0583, its = 7
i = 211 (of 248), d = 0.0888291, its = 5
i = 212 (of 248), d = 0.0273942, its = 10
i = 213 (of 248), d = 0.352914, its = 26
i = 214 (of 248), d = 0.142875, its = 7
i = 215 (of 248), d = 0.0702892, its = 8
i = 216 (of 248), d = 0.294039, its = 25
i = 217 (of 248), d = 0.045796, its = 4
i = 218 (of 248), d = 0.117281, its = 19
i = 219 (of 248), d = 0.10168, its = 20
i = 220 (of 248), d = 0.0460798, its = 6
i = 221 (of 248), d = 0.037717, its = 3
i = 222 (of 248), d = 0.0518959, its = 4
i = 223 (of 248), d = 0.0365478, its = 4
i = 224 (of 248), d = 0.32814, its = 9
i = 225 (of 248), d = 0.0457141, its = 5
i = 226 (of 248), d = 0.0488409, its = 6
i = 227 (of 248), d = 0.0865377, its = 7
i = 228 (of 248), d = 0.304688, its = 28
i = 229 (of 248), d = 0.0333622, its = 9
i = 230 (of 248), d = 0.104484, its = 8
i = 231 (of 248), d = 0.522728, its = 28
i = 232 (of 248), d = 0.10962, its = 26
i = 233 (of 248), d = 0.655643, its = 16
i = 234 (of 248), d = 0.18235, its = 9
i = 235 (of 248), d = 0.0559921, its = 5
i = 236 (of 248), d = 0.0402969, its = 4
i = 237 (of 248), d = 0.0590407, its = 6
i = 238 (of 248), d = 0.0901945, its = 27
i = 239 (of 248), d = 0.0666957, its = 6
i = 240 (of 248), d = 0.0543332, its = 5
i = 241 (of 248), d = 0.118574, its = 8
i = 242 (of 248), d = 0.119949, its = 6
i = 243 (of 248), d = 0.0551644, its = 5
i = 244 (of 248), d = 0.141923, its = 7
i = 245 (of 248), d = 0.187493, its = 6
i = 246 (of 248), d = 0.0800702, its = 5
i = 247 (of 248), d = 0.498806, its = 18
i = 248 (of 248), d = 0.0537282, its = 5
[1] "Fri Feb 09 14:37:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:37:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:37:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:37:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:37:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:37:34 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  5
initial  value 1079.024430 
iter  10 value 218.834804
iter  20 value 82.033044
iter  30 value 57.325088
iter  40 value 54.452039
iter  50 value 54.394407
iter  50 value 54.394407
iter  50 value 54.394407
final  value 54.394407 
converged
# weights:  5
initial  value 1082.294176 
iter  10 value 84.791948
iter  20 value 56.614470
iter  30 value 55.846258
final  value 55.843294 
converged
# weights:  5
initial  value 1058.061178 
iter  10 value 101.228354
iter  20 value 56.769074
iter  30 value 53.946786
iter  40 value 53.208262
final  value 53.194856 
converged
[Tune-y] 1: rmse.test.rmse=0.325; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  57
initial  value 1005.491843 
iter  10 value 51.238406
iter  20 value 45.811772
iter  30 value 37.986981
iter  40 value 20.947055
iter  50 value 15.774164
iter  60 value 13.522548
iter  70 value 12.284047
iter  80 value 11.565637
iter  90 value 10.794085
iter 100 value 10.060100
final  value 10.060100 
stopped after 100 iterations
# weights:  57
initial  value 1189.600692 
iter  10 value 71.878598
iter  20 value 49.413835
iter  30 value 39.960880
iter  40 value 28.555375
iter  50 value 17.199753
iter  60 value 14.509551
iter  70 value 12.779405
iter  80 value 11.731192
iter  90 value 11.088136
iter 100 value 10.451979
final  value 10.451979 
stopped after 100 iterations
# weights:  57
initial  value 3498.778482 
iter  10 value 193.941282
iter  20 value 53.678942
iter  30 value 44.058559
iter  40 value 34.943116
iter  50 value 29.008595
iter  60 value 28.308540
iter  70 value 25.997574
iter  80 value 20.287550
iter  90 value 16.361816
iter 100 value 13.159314
final  value 13.159314 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.14; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  41
initial  value 1184.032603 
iter  10 value 62.271254
iter  20 value 46.009675
iter  30 value 34.251275
iter  40 value 31.669308
iter  50 value 29.148725
iter  60 value 24.406579
iter  70 value 22.661632
iter  80 value 21.281426
iter  90 value 21.066789
iter 100 value 20.286123
final  value 20.286123 
stopped after 100 iterations
# weights:  41
initial  value 2235.578144 
iter  10 value 56.462977
iter  20 value 47.408869
iter  30 value 31.601176
iter  40 value 22.145832
iter  50 value 20.628098
iter  60 value 20.069677
iter  70 value 19.825003
iter  80 value 19.664845
iter  90 value 19.542473
iter 100 value 18.961743
final  value 18.961743 
stopped after 100 iterations
# weights:  41
initial  value 1120.333178 
iter  10 value 54.371839
iter  20 value 43.767803
iter  30 value 32.221827
iter  40 value 30.601931
iter  50 value 24.772775
iter  60 value 20.961573
iter  70 value 19.704867
iter  80 value 18.483897
iter  90 value 18.133452
iter 100 value 17.380534
final  value 17.380534 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.171; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  5
initial  value 1311.150034 
iter  10 value 124.028871
iter  20 value 62.104973
iter  30 value 52.661767
iter  40 value 48.995120
iter  50 value 48.052157
iter  60 value 47.858773
iter  70 value 47.791412
final  value 47.791080 
converged
# weights:  5
initial  value 1107.633273 
iter  10 value 264.693372
iter  20 value 139.255997
iter  30 value 65.372193
iter  40 value 53.514462
iter  50 value 50.436801
iter  60 value 49.910160
iter  70 value 49.822742
iter  80 value 49.806346
final  value 49.803199 
converged
# weights:  5
initial  value 1389.469702 
iter  10 value 626.126123
iter  20 value 83.044484
iter  30 value 52.702848
iter  40 value 48.025349
iter  50 value 46.888341
iter  60 value 46.202476
iter  70 value 45.894382
iter  80 value 45.829191
iter  90 value 45.810625
final  value 45.809686 
converged
[Tune-y] 4: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  61
initial  value 1126.810908 
iter  10 value 49.495919
iter  20 value 38.726548
iter  30 value 28.788807
iter  40 value 19.101642
iter  50 value 15.829570
iter  60 value 13.031937
iter  70 value 11.365672
iter  80 value 8.630321
iter  90 value 6.661030
iter 100 value 6.305181
final  value 6.305181 
stopped after 100 iterations
# weights:  61
initial  value 1146.405911 
iter  10 value 56.121942
iter  20 value 48.527761
iter  30 value 34.781649
iter  40 value 27.403122
iter  50 value 20.432474
iter  60 value 18.332009
iter  70 value 16.746027
iter  80 value 15.995902
iter  90 value 15.180533
iter 100 value 14.326968
final  value 14.326968 
stopped after 100 iterations
# weights:  61
initial  value 2551.368917 
iter  10 value 73.209682
iter  20 value 45.602639
iter  30 value 31.890377
iter  40 value 27.214435
iter  50 value 23.044578
iter  60 value 11.728755
iter  70 value 9.552116
iter  80 value 8.803717
iter  90 value 8.045406
iter 100 value 7.607538
final  value 7.607538 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  57
initial  value 1430.632656 
iter  10 value 77.246557
iter  20 value 46.386235
iter  30 value 29.157234
iter  40 value 24.453001
iter  50 value 22.914316
iter  60 value 21.930474
iter  70 value 21.253348
iter  80 value 20.938825
iter  90 value 20.730717
iter 100 value 20.649738
final  value 20.649738 
stopped after 100 iterations
# weights:  57
initial  value 1200.109408 
iter  10 value 74.243813
iter  20 value 52.521771
iter  30 value 45.885828
iter  40 value 35.219427
iter  50 value 26.712153
iter  60 value 25.227442
iter  70 value 24.567740
iter  80 value 24.316646
iter  90 value 24.130895
iter 100 value 23.955125
final  value 23.955125 
stopped after 100 iterations
# weights:  57
initial  value 1016.550244 
iter  10 value 50.834461
iter  20 value 45.960105
iter  30 value 36.338433
iter  40 value 27.472071
iter  50 value 21.280932
iter  60 value 20.704174
iter  70 value 20.249155
iter  80 value 19.971671
iter  90 value 19.804812
iter 100 value 19.655305
final  value 19.655305 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.174; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  13
initial  value 1199.188839 
iter  10 value 598.043210
iter  20 value 103.578120
iter  30 value 80.856596
iter  40 value 64.861154
iter  50 value 58.530169
iter  60 value 56.592273
iter  70 value 53.737973
iter  80 value 53.566416
iter  90 value 53.536879
final  value 53.536101 
converged
# weights:  13
initial  value 1099.233540 
iter  10 value 207.990777
iter  20 value 69.226157
iter  30 value 62.489196
iter  40 value 59.157690
iter  50 value 58.663878
iter  60 value 58.525473
iter  70 value 58.487995
final  value 58.486800 
converged
# weights:  13
initial  value 1155.907953 
iter  10 value 93.143353
iter  20 value 62.651122
iter  30 value 59.992026
iter  40 value 56.405439
iter  50 value 55.624125
iter  60 value 55.506238
iter  70 value 55.471746
iter  80 value 55.361953
iter  90 value 55.336469
final  value 55.335549 
converged
[Tune-y] 7: rmse.test.rmse=0.294; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  73
initial  value 2284.540380 
iter  10 value 53.100376
iter  20 value 41.503911
iter  30 value 30.889092
iter  40 value 26.950105
iter  50 value 24.854014
iter  60 value 23.530703
iter  70 value 22.928778
iter  80 value 22.540423
iter  90 value 22.111948
iter 100 value 21.780721
final  value 21.780721 
stopped after 100 iterations
# weights:  73
initial  value 1167.769295 
iter  10 value 57.926208
iter  20 value 50.923941
iter  30 value 49.563350
iter  40 value 43.068320
iter  50 value 28.167532
iter  60 value 22.917442
iter  70 value 21.855245
iter  80 value 21.431271
iter  90 value 21.123029
iter 100 value 20.966064
final  value 20.966064 
stopped after 100 iterations
# weights:  73
initial  value 1116.650384 
iter  10 value 54.091716
iter  20 value 42.211702
iter  30 value 32.337208
iter  40 value 29.306886
iter  50 value 24.232343
iter  60 value 22.989372
iter  70 value 22.422037
iter  80 value 22.106874
iter  90 value 21.886452
iter 100 value 21.617454
final  value 21.617454 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  57
initial  value 1187.417308 
iter  10 value 61.980441
iter  20 value 43.242820
iter  30 value 28.905966
iter  40 value 26.462173
iter  50 value 22.801461
iter  60 value 19.769343
iter  70 value 18.508800
iter  80 value 17.751877
iter  90 value 17.366562
iter 100 value 17.080474
final  value 17.080474 
stopped after 100 iterations
# weights:  57
initial  value 1999.096081 
iter  10 value 55.852548
iter  20 value 47.234802
iter  30 value 33.902562
iter  40 value 20.692429
iter  50 value 18.282424
iter  60 value 17.375073
iter  70 value 16.475111
iter  80 value 15.611751
iter  90 value 14.844253
iter 100 value 14.580603
final  value 14.580603 
stopped after 100 iterations
# weights:  57
initial  value 1779.649756 
iter  10 value 94.830536
iter  20 value 39.477790
iter  30 value 24.555151
iter  40 value 17.514468
iter  50 value 14.226520
iter  60 value 13.389432
iter  70 value 13.063824
iter  80 value 12.931837
iter  90 value 12.767258
iter 100 value 12.574143
final  value 12.574143 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  5
initial  value 1358.753565 
iter  10 value 226.029043
iter  20 value 60.130878
iter  30 value 51.688650
iter  40 value 48.666198
iter  50 value 47.457844
iter  60 value 47.101087
iter  70 value 46.826319
iter  80 value 46.621746
iter  90 value 46.486950
iter 100 value 46.422151
final  value 46.422151 
stopped after 100 iterations
# weights:  5
initial  value 1345.365871 
iter  10 value 247.869467
iter  20 value 73.945010
iter  30 value 54.866324
iter  40 value 51.629273
iter  50 value 50.052338
iter  60 value 49.421027
iter  70 value 49.030341
iter  80 value 48.809629
iter  90 value 48.711690
iter 100 value 48.607765
final  value 48.607765 
stopped after 100 iterations
# weights:  5
initial  value 1130.780103 
iter  10 value 278.823844
iter  20 value 115.398985
iter  30 value 59.025123
iter  40 value 49.009697
iter  50 value 46.510163
iter  60 value 45.350142
iter  70 value 44.856716
iter  80 value 44.594008
iter  90 value 44.416287
iter 100 value 44.342959
final  value 44.342959 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.312; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  33
initial  value 1380.577263 
iter  10 value 60.782530
iter  20 value 39.577788
iter  30 value 28.101909
iter  40 value 21.389886
iter  50 value 19.301901
iter  60 value 18.438718
iter  70 value 17.862121
iter  80 value 17.071344
iter  90 value 15.150165
iter 100 value 13.350557
final  value 13.350557 
stopped after 100 iterations
# weights:  33
initial  value 1171.557793 
iter  10 value 111.916179
iter  20 value 52.022347
iter  30 value 45.015424
iter  40 value 34.889457
iter  50 value 23.126686
iter  60 value 19.169627
iter  70 value 14.495375
iter  80 value 13.677990
iter  90 value 11.862418
iter 100 value 11.308912
final  value 11.308912 
stopped after 100 iterations
# weights:  33
initial  value 1131.357978 
iter  10 value 72.377543
iter  20 value 44.378894
iter  30 value 38.547552
iter  40 value 24.510620
iter  50 value 19.467597
iter  60 value 17.263643
iter  70 value 16.455416
iter  80 value 14.471774
iter  90 value 12.322516
iter 100 value 11.319476
final  value 11.319476 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.14; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  17
initial  value 1094.311898 
iter  10 value 89.511336
iter  20 value 71.102204
iter  30 value 63.759004
iter  40 value 62.457723
iter  50 value 60.084832
iter  60 value 59.215015
iter  70 value 58.550945
iter  80 value 58.201697
iter  90 value 58.076811
iter 100 value 58.038190
final  value 58.038190 
stopped after 100 iterations
# weights:  17
initial  value 1159.434725 
iter  10 value 125.635114
iter  20 value 78.097116
iter  30 value 63.973570
iter  40 value 61.849345
iter  50 value 60.246415
iter  60 value 60.064085
iter  70 value 59.814576
iter  80 value 59.497642
iter  90 value 59.198067
iter 100 value 59.051243
final  value 59.051243 
stopped after 100 iterations
# weights:  17
initial  value 1122.005190 
iter  10 value 66.741635
iter  20 value 58.139360
iter  30 value 56.586746
iter  40 value 55.744052
iter  50 value 54.957216
iter  60 value 54.047693
iter  70 value 53.812260
iter  80 value 53.697747
iter  90 value 53.686736
final  value 53.686661 
converged
[Tune-y] 12: rmse.test.rmse=0.307; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  13
initial  value 1074.766020 
iter  10 value 89.300227
iter  20 value 45.696489
iter  30 value 41.033225
iter  40 value 39.867216
iter  50 value 37.900474
iter  60 value 37.200658
iter  70 value 37.121371
iter  80 value 35.522637
iter  90 value 27.960842
iter 100 value 26.585652
final  value 26.585652 
stopped after 100 iterations
# weights:  13
initial  value 1083.783665 
iter  10 value 117.763286
iter  20 value 45.235411
iter  30 value 40.142682
iter  40 value 32.906164
iter  50 value 26.910508
iter  60 value 22.267717
iter  70 value 21.640118
iter  80 value 21.360469
iter  90 value 21.151374
iter 100 value 21.129763
final  value 21.129763 
stopped after 100 iterations
# weights:  13
initial  value 1059.211355 
iter  10 value 167.400661
iter  20 value 48.828926
iter  30 value 34.779359
iter  40 value 32.190257
iter  50 value 23.876709
iter  60 value 20.217737
iter  70 value 18.885990
iter  80 value 17.786562
iter  90 value 17.575401
iter 100 value 17.429827
final  value 17.429827 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.216; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  25
initial  value 1541.119326 
iter  10 value 108.511537
iter  20 value 41.782325
iter  30 value 32.126284
iter  40 value 28.272562
iter  50 value 26.572095
iter  60 value 25.722579
iter  70 value 16.986590
iter  80 value 13.679224
iter  90 value 13.373970
iter 100 value 13.017238
final  value 13.017238 
stopped after 100 iterations
# weights:  25
initial  value 1239.529471 
iter  10 value 76.333831
iter  20 value 49.111478
iter  30 value 32.850405
iter  40 value 25.964256
iter  50 value 15.679169
iter  60 value 12.505987
iter  70 value 9.296982
iter  80 value 7.637959
iter  90 value 7.455243
iter 100 value 6.921823
final  value 6.921823 
stopped after 100 iterations
# weights:  25
initial  value 1085.398270 
iter  10 value 54.999831
iter  20 value 41.221634
iter  30 value 33.940947
iter  40 value 27.906150
iter  50 value 24.884482
iter  60 value 24.100372
iter  70 value 23.006759
iter  80 value 21.359324
iter  90 value 20.789363
iter 100 value 19.961654
final  value 19.961654 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.176; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  25
initial  value 2223.136476 
iter  10 value 55.278054
iter  20 value 44.975426
iter  30 value 38.805357
iter  40 value 34.007686
iter  50 value 27.092072
iter  60 value 25.162194
iter  70 value 17.468944
iter  80 value 14.973603
iter  90 value 14.752194
iter 100 value 14.512305
final  value 14.512305 
stopped after 100 iterations
# weights:  25
initial  value 1626.450135 
iter  10 value 85.820313
iter  20 value 52.108083
iter  30 value 42.615564
iter  40 value 26.641132
iter  50 value 13.341626
iter  60 value 11.115547
iter  70 value 7.679852
iter  80 value 6.301783
iter  90 value 5.972491
iter 100 value 5.889969
final  value 5.889969 
stopped after 100 iterations
# weights:  25
initial  value 1207.264748 
iter  10 value 90.792192
iter  20 value 36.750215
iter  30 value 27.984523
iter  40 value 23.035378
iter  50 value 21.891891
iter  60 value 20.914300
iter  70 value 19.863727
iter  80 value 18.349267
iter  90 value 17.470492
iter 100 value 16.955778
final  value 16.955778 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.173; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  65
initial  value 1080.826406 
iter  10 value 423.492889
iter  20 value 407.246296
iter  30 value 402.806771
iter  40 value 400.683609
iter  50 value 399.238302
iter  60 value 398.311719
iter  70 value 396.979462
iter  80 value 396.151669
iter  90 value 395.964880
iter 100 value 395.921201
final  value 395.921201 
stopped after 100 iterations
# weights:  65
initial  value 1842.066391 
iter  10 value 588.922482
iter  20 value 431.893503
iter  30 value 418.107744
iter  40 value 412.846683
iter  50 value 409.543703
iter  60 value 407.183234
iter  70 value 404.902151
iter  80 value 403.198286
iter  90 value 401.385490
iter 100 value 399.778557
final  value 399.778557 
stopped after 100 iterations
# weights:  65
initial  value 3262.162099 
iter  10 value 553.550186
iter  20 value 427.501436
iter  30 value 405.401701
iter  40 value 399.161441
iter  50 value 397.380245
iter  60 value 395.955629
iter  70 value 393.027376
iter  80 value 390.909716
iter  90 value 390.655650
iter 100 value 390.648362
final  value 390.648362 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.423; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  29
initial  value 1047.239854 
iter  10 value 74.823397
iter  20 value 44.489820
iter  30 value 31.027629
iter  40 value 28.396875
iter  50 value 26.521537
iter  60 value 25.198432
iter  70 value 24.249170
iter  80 value 22.104104
iter  90 value 19.574107
iter 100 value 16.606969
final  value 16.606969 
stopped after 100 iterations
# weights:  29
initial  value 1070.529400 
iter  10 value 83.706843
iter  20 value 46.769047
iter  30 value 33.310387
iter  40 value 30.946756
iter  50 value 29.146170
iter  60 value 26.864754
iter  70 value 25.971317
iter  80 value 22.842133
iter  90 value 13.083674
iter 100 value 11.137338
final  value 11.137338 
stopped after 100 iterations
# weights:  29
initial  value 1562.415002 
iter  10 value 57.790107
iter  20 value 41.097069
iter  30 value 24.338790
iter  40 value 13.259624
iter  50 value 9.432915
iter  60 value 7.189368
iter  70 value 6.555244
iter  80 value 5.266142
iter  90 value 4.432568
iter 100 value 4.286402
final  value 4.286402 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  21
initial  value 1437.961406 
iter  10 value 56.783602
iter  20 value 52.566667
iter  30 value 51.409173
iter  40 value 48.890219
iter  50 value 41.162184
iter  60 value 33.378330
iter  70 value 31.930333
iter  80 value 31.749718
iter  90 value 31.670657
iter 100 value 31.519215
final  value 31.519215 
stopped after 100 iterations
# weights:  21
initial  value 1450.326503 
iter  10 value 124.183886
iter  20 value 78.284021
iter  30 value 61.490187
iter  40 value 55.846161
iter  50 value 55.098460
iter  60 value 53.376584
iter  70 value 52.752329
iter  80 value 48.766050
iter  90 value 45.443747
iter 100 value 43.837783
final  value 43.837783 
stopped after 100 iterations
# weights:  21
initial  value 1310.342364 
iter  10 value 95.057155
iter  20 value 59.568218
iter  30 value 46.152464
iter  40 value 41.330633
iter  50 value 40.233782
iter  60 value 39.024094
iter  70 value 38.113284
iter  80 value 37.919704
iter  90 value 37.851563
iter 100 value 37.739186
final  value 37.739186 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.226; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  65
initial  value 2195.227653 
iter  10 value 61.908403
iter  20 value 47.268091
iter  30 value 34.200072
iter  40 value 25.784608
iter  50 value 24.230771
iter  60 value 23.492398
iter  70 value 22.851899
iter  80 value 22.270930
iter  90 value 21.776885
iter 100 value 21.598726
final  value 21.598726 
stopped after 100 iterations
# weights:  65
initial  value 1068.863829 
iter  10 value 69.548916
iter  20 value 54.401829
iter  30 value 40.344701
iter  40 value 28.474979
iter  50 value 26.453801
iter  60 value 25.730289
iter  70 value 25.315479
iter  80 value 24.903393
iter  90 value 24.704795
iter 100 value 24.537733
final  value 24.537733 
stopped after 100 iterations
# weights:  65
initial  value 1504.844878 
iter  10 value 54.414138
iter  20 value 42.176587
iter  30 value 33.606803
iter  40 value 30.814958
iter  50 value 23.288124
iter  60 value 22.398769
iter  70 value 21.647625
iter  80 value 21.173659
iter  90 value 20.906628
iter 100 value 20.721333
final  value 20.721333 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.182; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  69
initial  value 1347.826726 
iter  10 value 62.100155
iter  20 value 44.544217
iter  30 value 29.766778
iter  40 value 18.403014
iter  50 value 15.095014
iter  60 value 13.484720
iter  70 value 10.975873
iter  80 value 8.029851
iter  90 value 6.197918
iter 100 value 5.413242
final  value 5.413242 
stopped after 100 iterations
# weights:  69
initial  value 1184.542407 
iter  10 value 62.328620
iter  20 value 45.643605
iter  30 value 33.363510
iter  40 value 15.544342
iter  50 value 9.550195
iter  60 value 7.679339
iter  70 value 6.235861
iter  80 value 4.667456
iter  90 value 3.917423
iter 100 value 3.707303
final  value 3.707303 
stopped after 100 iterations
# weights:  69
initial  value 1320.193210 
iter  10 value 53.630675
iter  20 value 34.153810
iter  30 value 27.035823
iter  40 value 21.623067
iter  50 value 14.708046
iter  60 value 11.641438
iter  70 value 8.033050
iter  80 value 6.408278
iter  90 value 4.885886
iter 100 value 4.448080
final  value 4.448080 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.113; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.113
# weights:  69
initial  value 1885.024651 
iter  10 value 93.835866
iter  20 value 69.145006
iter  30 value 62.892355
iter  40 value 42.541481
iter  50 value 27.260622
iter  60 value 16.744292
iter  70 value 10.485669
iter  80 value 7.273700
iter  90 value 5.875784
iter 100 value 5.179884
final  value 5.179884 
stopped after 100 iterations
[1] "Fri Feb 09 14:37:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1080
 total number of nodes after removal of identical nodes : 723 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 440
 number of selected nodes                               : 74 
[1] "Fri Feb 09 14:38:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:38:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:38:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:38:29 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   1  1 to 2   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 2: mtry=2; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.185; time: 0.1 min
[Tune-x] 3: mtry=1; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.209; time: 0.0 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.202; time: 0.0 min
[Tune-x] 5: mtry=2; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.181; time: 0.1 min
[Tune-x] 6: mtry=2; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 8: mtry=2; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 9: mtry=2; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.183; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.196; time: 0.0 min
[Tune-x] 11: mtry=1; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.207; time: 0.1 min
[Tune-x] 12: mtry=1; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.201; time: 0.0 min
[Tune-x] 14: mtry=1; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.202; time: 0.0 min
[Tune-x] 15: mtry=1; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.204; time: 0.0 min
[Tune-x] 16: mtry=2; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.208; time: 0.0 min
[Tune-x] 17: mtry=1; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 18: mtry=1; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 19: mtry=2; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.189; time: 0.0 min
[Tune-x] 20: mtry=2; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.179; time: 0.1 min
[Tune] Result: mtry=2; min.node.size=3 : rmse.test.rmse=0.179
[1] "Fri Feb 09 14:39:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:39:26 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.618; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.72; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.99; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.674; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.907; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.615; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.505; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.643; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse= 0.6; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.58; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.591; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.656; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.801; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=1.03; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=1.24; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.532; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.712; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.944; time: 0.0 min
[Tune] Result: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17 : rmse.test.rmse=0.505
[1] "Fri Feb 09 14:39:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:39:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:39:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.7220 0.3270 0.1480 0.0670 0.0304
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 2 
Runtime: 0.0580039 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.1481 
    Y 1         0.3024 
    Y 2          0.184 
    Y 3        -0.3915 
    Y 4        -0.3094 
    Y 5          1.383 
[1] "Fri Feb 09 14:39:52 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:52] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.494341 is too small that no feature can be included

[Tune-y] 1: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.315; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.304442 is too small that no feature can be included

[Tune-y] 3: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.357007 is too small that no feature can be included

[Tune-y] 4: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.434979 is too small that no feature can be included

[Tune-y] 5: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:54] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.38423 is too small that no feature can be included

[Tune-y] 6: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.28; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:39:56] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.499858 is too small that no feature can be included

[Tune-y] 8: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.675; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:01] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.411178 is too small that no feature can be included

[Tune-y] 10: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse= 0.3; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.263; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.328; time: 0.2 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.387; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:40:27] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.308971 is too small that no feature can be included

[Tune-y] 15: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.375; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.256; time: 0.6 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.361402 is too small that no feature can be included

[Tune-y] 18: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:05] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.465244 is too small that no feature can be included

[Tune-y] 19: rmse.test.rmse=  NA; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

Warning in train(learner, task, subset = train.i, weights = weights[train.i]) :
  Could not train learner regr.xgboost: Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [14:41:06] amalgamation/../src/tree/updater_colmaker.cc:162: Check failed: (n) > (0) colsample_bytree=0.369418 is too small that no feature can be included

[Tune-y] 20: rmse.test.rmse=  NA; time: 0.0 min
[Tune] Result: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526 : rmse.test.rmse=0.256
[1] "Fri Feb 09 14:41:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:41:20 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.594666], n=(505,247)
**GROW** @depth 1: [4,0.550019], n=(179,66)
**GROW** @depth 1: [6,0.489538], n=(447,59)
**PRUNE** @depth 1: [6,0.492889]
**GROW** @depth 1: [1,0.316428], n=(205,299)
**GROW** @depth 1: [6,0.432497], n=(182,24)
**PRUNE** @depth 1: [6,0.432199]
**GROW** @depth 2: [3,0.4875], n=(79,215)
**GROW** @depth 3: [5,0.255448], n=(37,145)
**GROW** @depth 2: [4,0.380667], n=(179,28)
**GROW** @depth 3: [2,0.498822], n=(133,82)
**GROW** @depth 4: [3,0.554167], n=(19,126)
**GROW** @depth 2: [5,0.222214], n=(132,47)
**GROW** @depth 3: [6,0.294512], n=(68,16)
**PRUNE** @depth 3: [6,0.29682]
**GROW** @depth 4: [2,0.397173], n=(50,81)
**GROW** @depth 2: [1,0.79309], n=(38,28)
**GROW** @depth 3: [6,0.154442], n=(15,69)
**GROW** @depth 4: [2,0.262538], n=(31,16)
**GROW** @depth 5: [7,0.469042], n=(61,20)
**PRUNE** @depth 5: [7,0.465994]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(131,32,14,29,17,68,49,82,84,40,16,142,20,28)
**GROW** @depth 4: [1,0.678723], n=(54,88)
**GROW** @depth 4: [2,0.227701], n=(100,29)
**GROW** @depth 5: [1,0.149525], n=(52,48)
**GROW** @depth 5: [3,0.779167], n=(67,20)
**GROW** @depth 4: [9,0.507495], n=(56,16)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(51,49,22,30,24,27,18,57,17,44,83,86,39,15,53,62,29,20,26)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 4: [2,0.260855]
**GROW** @depth 5: [3,0.779167], n=(38,13)
**GROW** @depth 5: [7,0.626405], n=(61,22)
**PRUNE** @depth 5: [7,0.626405]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(52,17,55,51,27,19,61,19,40,83,85,40,14,39,13,63,30,20,24)
**GROW** @depth 2: [1,0.252778], n=(12,16)
**GROW** @depth 3: [3,0.504167], n=(41,12)
**PRUNE** @depth 4: [3,0.504167]
**GROW** @depth 4: [8,0.54722], n=(18,36)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(53,17,55,19,35,12,16,17,59,19,39,80,86,41,13,42,13,63,28,20,25)
**GROW** @depth 4: [3,0.684375], n=(53,34)
**GROW** @depth 5: [2,0.443958], n=(31,49)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(52,17,56,19,35,12,16,16,60,19,39,31,49,54,35,41,12,41,13,62,25,20,28)
**PRUNE** @depth 3: [8,0.546337]
**GROW** @depth 7: [3,0.682292], n=(34,29)
**GROW** @depth 7: [3,0.658333], n=(22,20)
**GROW** @depth 5: [5,0.0761606], n=(37,15)
**GROW** @depth 4: [8,0.496028], n=(30,26)
**PRUNE** @depth 7: [3,0.779167]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(36,16,17,30,25,54,12,16,19,58,19,37,34,49,53,34,41,13,21,33,31,31,28,20,25)
**GROW** @depth 3: [3,0.5], n=(43,12)
**PRUNE** @depth 3: [3,0.501042]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(37,16,16,29,25,58,12,18,17,57,20,36,33,47,53,35,40,14,20,33,32,31,28,20,25)
Grow: 9.855%, Prune: 2.95%, Change: 78.33%, Swap: 18.83%

[1] "Fri Feb 09 14:42:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:42:54 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:42:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.4109 	 alpha= 1.8551 	 beta= 37675.81 
[1] "Fri Feb 09 14:42:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:42:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [4,0.0243537], n=(18,734)
**PRUNE** @depth 0: [4,0.0224803]
**GROW** @depth 0: [8,0.423654], n=(112,640)
**GROW** @depth 1: [3,0.529167], n=(211,429)
r=1000 d=[0] [0] [0]; n=(112,212,428)
r=2000 d=[0] [0] [0]; n=(112,212,428)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0]; mh=3 n=(112,212,428)
**GROW** @depth 2: [9,0.45858], n=(87,341)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
r=3000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
**GROW** @depth 2: [4,0.341701], n=(192,20)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
Grow: 1.42%, Prune: 0.277%, Change: 17.16%, Swap: 0%

[1] "Fri Feb 09 14:43:05 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:43:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:43:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:43:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:43:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:43:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:43:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:43:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:43:14 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0435; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0439; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.0645; time: 0.1 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0391; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0251; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.055; time: 0.0 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0535; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0417; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.046; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0621; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.0777; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0418; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.373; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0544; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0368; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.056; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0466; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0251
[1] "Fri Feb 09 14:43:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:43:39 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0678; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0226; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0256; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0315; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0428; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0149; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0844; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0555; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0334; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.483; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0143; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0272; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.106; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0143
[1] "Fri Feb 09 14:43:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:43:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========================                                              |  34%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:43:56 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:43:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |============================                                          |  40%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:44:06 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.898 11.884 1.92 5.338 27.442 26.858 10.498 2.266 10.14 
  - best initial criterion value(s) :  3328.615 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3328.6  |proj g|=       7.8814
At iterate     1  f =      -3340.1  |proj g|=        6.7544
At iterate     2  f =      -3398.2  |proj g|=        7.8537
At iterate     3  f =      -3435.6  |proj g|=        4.7662
At iterate     4  f =      -3460.8  |proj g|=        4.4083
At iterate     5  f =      -3467.3  |proj g|=        4.1755
At iterate     6  f =      -3470.3  |proj g|=        4.1391
At iterate     7  f =      -3481.4  |proj g|=        4.2088
At iterate     8  f =      -3486.9  |proj g|=        4.2328
At iterate     9  f =      -3488.9  |proj g|=        4.1909
At iterate    10  f =      -3494.8  |proj g|=        4.1931
At iterate    11  f =      -3496.4  |proj g|=        4.1637
At iterate    12  f =      -3498.1  |proj g|=        4.0611
At iterate    13  f =      -3500.2  |proj g|=        3.9477
At iterate    14  f =      -3504.3  |proj g|=        2.1749
At iterate    15  f =      -3508.6  |proj g|=        2.8156
At iterate    16  f =      -3509.3  |proj g|=        3.1922
At iterate    17  f =      -3509.4  |proj g|=        3.1865
At iterate    18  f =      -3509.9  |proj g|=        4.3269
At iterate    19  f =        -3510  |proj g|=        4.0227
At iterate    20  f =      -3510.2  |proj g|=        2.6501
At iterate    21  f =      -3511.7  |proj g|=        2.3562
At iterate    22  f =      -3512.6  |proj g|=        3.2352
At iterate    23  f =      -3513.1  |proj g|=        3.0041
At iterate    24  f =      -3514.1  |proj g|=        3.0241
At iterate    25  f =      -3521.7  |proj g|=        7.0185
At iterate    26  f =      -3525.8  |proj g|=        3.9145
At iterate    27  f =      -3527.5  |proj g|=        3.4709
At iterate    28  f =      -3528.2  |proj g|=        3.4542
At iterate    29  f =        -3529  |proj g|=        3.4147
At iterate    30  f =      -3529.2  |proj g|=        3.4077
At iterate    31  f =      -3531.5  |proj g|=        3.2684
At iterate    32  f =      -3538.3  |proj g|=         7.811
At iterate    33  f =      -3541.1  |proj g|=        2.8562
At iterate    34  f =      -3541.4  |proj g|=         3.201
At iterate    35  f =      -3541.5  |proj g|=        3.5902
At iterate    36  f =      -3541.7  |proj g|=        3.5338
At iterate    37  f =      -3542.2  |proj g|=        2.9428
At iterate    38  f =      -3542.6  |proj g|=        2.1265
At iterate    39  f =      -3542.7  |proj g|=        1.5167
At iterate    40  f =      -3542.9  |proj g|=        1.2604
At iterate    41  f =      -3542.9  |proj g|=        1.1795
At iterate    42  f =      -3542.9  |proj g|=        1.4852
At iterate    43  f =        -3543  |proj g|=        1.4883
At iterate    44  f =        -3543  |proj g|=        1.4913
At iterate    45  f =      -3543.1  |proj g|=        1.4924
At iterate    46  f =      -3543.3  |proj g|=        2.2509
At iterate    47  f =      -3543.5  |proj g|=        2.4122
At iterate    48  f =      -3543.6  |proj g|=        2.4062
At iterate    49  f =      -3543.8  |proj g|=        2.2959
At iterate    50  f =        -3544  |proj g|=        2.7622
At iterate    51  f =      -3544.2  |proj g|=        1.5037
At iterate    52  f =      -3544.2  |proj g|=        1.5299
At iterate    53  f =      -3544.3  |proj g|=        1.5253
At iterate    54  f =      -3544.3  |proj g|=        1.5126
At iterate    55  f =      -3544.3  |proj g|=       0.80832
At iterate    56  f =      -3544.3  |proj g|=        1.0986
At iterate    57  f =      -3544.4  |proj g|=       0.28595
At iterate    58  f =      -3544.4  |proj g|=       0.67442
At iterate    59  f =      -3544.4  |proj g|=       0.78913
At iterate    60  f =      -3544.4  |proj g|=        1.3157
At iterate    61  f =      -3544.4  |proj g|=       0.77041
At iterate    62  f =      -3544.4  |proj g|=         1.488
At iterate    63  f =      -3544.5  |proj g|=        1.4839
At iterate    64  f =      -3544.5  |proj g|=       0.92549
At iterate    65  f =      -3544.5  |proj g|=        1.3703
At iterate    66  f =      -3544.5  |proj g|=       0.78486
At iterate    67  f =      -3544.5  |proj g|=       0.45217
At iterate    68  f =      -3544.5  |proj g|=       0.42495
At iterate    69  f =      -3544.6  |proj g|=       0.33318
At iterate    70  f =      -3544.6  |proj g|=        1.4839
At iterate    71  f =      -3544.6  |proj g|=        1.1859
At iterate    72  f =      -3544.6  |proj g|=       0.55789
At iterate    73  f =      -3544.6  |proj g|=        0.6882
At iterate    74  f =      -3544.6  |proj g|=       0.69684
At iterate    75  f =      -3544.6  |proj g|=       0.56766
At iterate    76  f =      -3544.6  |proj g|=         1.482
At iterate    77  f =      -3544.7  |proj g|=        1.1465
At iterate    78  f =      -3544.7  |proj g|=       0.44315
At iterate    79  f =      -3544.7  |proj g|=       0.58944
At iterate    80  f =      -3544.7  |proj g|=       0.23528
At iterate    81  f =      -3544.7  |proj g|=       0.23733
At iterate    82  f =      -3544.7  |proj g|=       0.59329
At iterate    83  f =      -3544.7  |proj g|=       0.81767
At iterate    84  f =      -3544.7  |proj g|=       0.85629
At iterate    85  f =      -3544.7  |proj g|=       0.93306
At iterate    86  f =      -3544.8  |proj g|=       0.63527
At iterate    87  f =      -3544.8  |proj g|=       0.70299
At iterate    88  f =      -3544.8  |proj g|=        1.4776
At iterate    89  f =      -3544.8  |proj g|=       0.75748
At iterate    90  f =      -3544.8  |proj g|=        0.6528
At iterate    91  f =      -3544.8  |proj g|=       0.55715
At iterate    92  f =      -3544.8  |proj g|=       0.31567
At iterate    93  f =      -3544.8  |proj g|=       0.19976
At iterate    94  f =      -3544.8  |proj g|=        1.4375
At iterate    95  f =      -3544.8  |proj g|=        1.0246
At iterate    96  f =      -3544.8  |proj g|=       0.48397
At iterate    97  f =      -3544.8  |proj g|=       0.52294
At iterate    98  f =      -3544.8  |proj g|=       0.52864
At iterate    99  f =      -3544.9  |proj g|=       0.44264
At iterate   100  f =      -3544.9  |proj g|=        1.4798
At iterate   101  f =      -3544.9  |proj g|=        1.4476
final  value -3544.880687 
stopped after 101 iterations
[1] "Fri Feb 09 14:48:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 81.6381, its = 11
i = 2 (of 248), d = 50.4835, its = 11
i = 3 (of 248), d = 53.5942, its = 11
i = 4 (of 248), d = 122.859, its = 12
i = 5 (of 248), d = 85.1172, its = 12
i = 6 (of 248), d = 40.8853, its = 11
i = 7 (of 248), d = 88.4914, its = 11
i = 8 (of 248), d = 100.562, its = 11
i = 9 (of 248), d = 28.7216, its = 9
i = 10 (of 248), d = 107.053, its = 11
i = 11 (of 248), d = 37.9501, its = 10
i = 12 (of 248), d = 42.2289, its = 10
i = 13 (of 248), d = 63.3826, its = 11
i = 14 (of 248), d = 211.263, its = 12
i = 15 (of 248), d = 49.6758, its = 10
i = 16 (of 248), d = 60.4139, its = 11
i = 17 (of 248), d = 43.0261, its = 10
i = 18 (of 248), d = 76.0365, its = 11
i = 19 (of 248), d = 20.3274, its = 9
i = 20 (of 248), d = 67.6309, its = 11
i = 21 (of 248), d = 95.7672, its = 11
i = 22 (of 248), d = 29.5729, its = 10
i = 23 (of 248), d = 74.2623, its = 12
i = 24 (of 248), d = 153.261, its = 12
i = 25 (of 248), d = 107.241, its = 12
i = 26 (of 248), d = 126.005, its = 11
i = 27 (of 248), d = 61.6181, its = 10
i = 28 (of 248), d = 31.829, its = 10
i = 29 (of 248), d = 153.888, its = 13
i = 30 (of 248), d = 78.6486, its = 11
i = 31 (of 248), d = 84.493, its = 11
i = 32 (of 248), d = 70.9572, its = 11
i = 33 (of 248), d = 124.829, its = 12
i = 34 (of 248), d = 27.2667, its = 10
i = 35 (of 248), d = 126.943, its = 12
i = 36 (of 248), d = 123.006, its = 11
i = 37 (of 248), d = 86.5268, its = 11
i = 38 (of 248), d = 93.8513, its = 11
i = 39 (of 248), d = 34.6424, its = 10
i = 40 (of 248), d = 32.4118, its = 9
i = 41 (of 248), d = 59.4162, its = 11
i = 42 (of 248), d = 22.2584, its = 9
i = 43 (of 248), d = 57.314, its = 10
i = 44 (of 248), d = 44.9622, its = 10
i = 45 (of 248), d = 89.8757, its = 11
i = 46 (of 248), d = 49.8736, its = 10
i = 47 (of 248), d = 38.2174, its = 10
i = 48 (of 248), d = 19.8057, its = 9
i = 49 (of 248), d = 49.6311, its = 10
i = 50 (of 248), d = 33.8076, its = 10
i = 51 (of 248), d = 156.372, its = 12
i = 52 (of 248), d = 57.5233, its = 11
i = 53 (of 248), d = 121.299, its = 11
i = 54 (of 248), d = 46.6802, its = 10
i = 55 (of 248), d = 120.731, its = 12
i = 56 (of 248), d = 21.4573, its = 9
i = 57 (of 248), d = 137.217, its = 13
i = 58 (of 248), d = 90.9739, its = 12
i = 59 (of 248), d = 68.8682, its = 11
i = 60 (of 248), d = 109.753, its = 12
i = 61 (of 248), d = 143.46, its = 13
i = 62 (of 248), d = 94.4793, its = 11
i = 63 (of 248), d = 113.162, its = 11
i = 64 (of 248), d = 83.9198, its = 11
i = 65 (of 248), d = 26.2431, its = 9
i = 66 (of 248), d = 125.068, its = 11
i = 67 (of 248), d = 19.5886, its = 9
i = 68 (of 248), d = 68.4551, its = 11
i = 69 (of 248), d = 32.363, its = 9
i = 70 (of 248), d = 17.375, its = 9
i = 71 (of 248), d = 156.267, its = 13
i = 72 (of 248), d = 96.4112, its = 11
i = 73 (of 248), d = 41.77, its = 10
i = 74 (of 248), d = 85.3034, its = 11
i = 75 (of 248), d = 42.2964, its = 10
i = 76 (of 248), d = 115.922, its = 12
i = 77 (of 248), d = 45.0567, its = 10
i = 78 (of 248), d = 74.683, its = 11
i = 79 (of 248), d = 69.6093, its = 11
i = 80 (of 248), d = 83.3347, its = 11
i = 81 (of 248), d = 35.6791, its = 10
i = 82 (of 248), d = 32.7981, its = 10
i = 83 (of 248), d = 73.8158, its = 11
i = 84 (of 248), d = 96.1281, its = 11
i = 85 (of 248), d = 71.8901, its = 11
i = 86 (of 248), d = 28.1677, its = 9
i = 87 (of 248), d = 55.9643, its = 11
i = 88 (of 248), d = 48.9644, its = 11
i = 89 (of 248), d = 53.4303, its = 10
i = 90 (of 248), d = 92.1895, its = 11
i = 91 (of 248), d = 114.369, its = 12
i = 92 (of 248), d = 54.2139, its = 11
i = 93 (of 248), d = 50.4993, its = 10
i = 94 (of 248), d = 108.794, its = 11
i = 95 (of 248), d = 74.9373, its = 11
i = 96 (of 248), d = 17.3664, its = 9
i = 97 (of 248), d = 53.5812, its = 10
i = 98 (of 248), d = 34.5303, its = 10
i = 99 (of 248), d = 41.3775, its = 11
i = 100 (of 248), d = 25.2923, its = 9
i = 101 (of 248), d = 21.1352, its = 9
i = 102 (of 248), d = 45.4291, its = 10
i = 103 (of 248), d = 19.1583, its = 9
i = 104 (of 248), d = 43.337, its = 11
i = 105 (of 248), d = 79.7952, its = 11
i = 106 (of 248), d = 75.669, its = 11
i = 107 (of 248), d = 105.899, its = 11
i = 108 (of 248), d = 40.2151, its = 10
i = 109 (of 248), d = 124.589, its = 12
i = 110 (of 248), d = 26.967, its = 9
i = 111 (of 248), d = 24.8384, its = 9
i = 112 (of 248), d = 144.982, its = 12
i = 113 (of 248), d = 64.592, its = 11
i = 114 (of 248), d = 34.2722, its = 10
i = 115 (of 248), d = 20.8262, its = 10
i = 116 (of 248), d = 95.4175, its = 12
i = 117 (of 248), d = 42.7777, its = 10
i = 118 (of 248), d = 67.4093, its = 10
i = 119 (of 248), d = 160.209, its = 13
i = 120 (of 248), d = 70.5525, its = 11
i = 121 (of 248), d = 46.5869, its = 11
i = 122 (of 248), d = 130.863, its = 11
i = 123 (of 248), d = 67.4338, its = 11
i = 124 (of 248), d = 89.1165, its = 11
i = 125 (of 248), d = 158.681, its = 13
i = 126 (of 248), d = 54.6375, its = 11
i = 127 (of 248), d = 39.9938, its = 10
i = 128 (of 248), d = 25.499, its = 10
i = 129 (of 248), d = 84.9714, its = 11
i = 130 (of 248), d = 181.942, its = 12
i = 131 (of 248), d = 103.082, its = 11
i = 132 (of 248), d = 69.6513, its = 11
i = 133 (of 248), d = 118.543, its = 11
i = 134 (of 248), d = 23.4147, its = 9
i = 135 (of 248), d = 51.1434, its = 10
i = 136 (of 248), d = 119.697, its = 11
i = 137 (of 248), d = 107.513, its = 11
i = 138 (of 248), d = 71.958, its = 11
i = 139 (of 248), d = 70.734, its = 11
i = 140 (of 248), d = 118.219, its = 11
i = 141 (of 248), d = 109.166, its = 12
i = 142 (of 248), d = 131.509, its = 11
i = 143 (of 248), d = 152.703, its = 13
i = 144 (of 248), d = 111.201, its = 11
i = 145 (of 248), d = 35.9442, its = 10
i = 146 (of 248), d = 73.7117, its = 11
i = 147 (of 248), d = 67.7912, its = 11
i = 148 (of 248), d = 144.357, its = 12
i = 149 (of 248), d = 67.5248, its = 11
i = 150 (of 248), d = 119.162, its = 13
i = 151 (of 248), d = 31.7838, its = 9
i = 152 (of 248), d = 119.782, its = 11
i = 153 (of 248), d = 100.789, its = 11
i = 154 (of 248), d = 53.3399, its = 11
i = 155 (of 248), d = 92.1972, its = 12
i = 156 (of 248), d = 96.9205, its = 11
i = 157 (of 248), d = 62.7226, its = 11
i = 158 (of 248), d = 96.3754, its = 11
i = 159 (of 248), d = 17.0282, its = 9
i = 160 (of 248), d = 50.6264, its = 10
i = 161 (of 248), d = 111.173, its = 12
i = 162 (of 248), d = 40.6053, its = 10
i = 163 (of 248), d = 137.811, its = 12
i = 164 (of 248), d = 95.1407, its = 12
i = 165 (of 248), d = 64.3601, its = 11
i = 166 (of 248), d = 39.709, its = 10
i = 167 (of 248), d = 68.5079, its = 11
i = 168 (of 248), d = 41.163, its = 10
i = 169 (of 248), d = 28.7441, its = 10
i = 170 (of 248), d = 137.169, its = 12
i = 171 (of 248), d = 65.1753, its = 11
i = 172 (of 248), d = 110.921, its = 13
i = 173 (of 248), d = 247.727, its = 15
i = 174 (of 248), d = 44.7524, its = 10
i = 175 (of 248), d = 141.238, its = 12
i = 176 (of 248), d = 141.581, its = 12
i = 177 (of 248), d = 58.5192, its = 11
i = 178 (of 248), d = 32.7251, its = 9
i = 179 (of 248), d = 45.1026, its = 11
i = 180 (of 248), d = 88.0771, its = 11
i = 181 (of 248), d = 70.4445, its = 11
i = 182 (of 248), d = 41.9674, its = 10
i = 183 (of 248), d = 167.955, its = 13
i = 184 (of 248), d = 94.3134, its = 12
i = 185 (of 248), d = 62.4957, its = 11
i = 186 (of 248), d = 36.2491, its = 10
i = 187 (of 248), d = 122.821, its = 12
i = 188 (of 248), d = 92.4164, its = 12
i = 189 (of 248), d = 49.2627, its = 11
i = 190 (of 248), d = 130.051, its = 12
i = 191 (of 248), d = 22.8844, its = 9
i = 192 (of 248), d = 35.5348, its = 10
i = 193 (of 248), d = 36.829, its = 10
i = 194 (of 248), d = 92.8097, its = 12
i = 195 (of 248), d = 21.9829, its = 9
i = 196 (of 248), d = 37.3593, its = 11
i = 197 (of 248), d = 32.4234, its = 10
i = 198 (of 248), d = 97.5475, its = 11
i = 199 (of 248), d = 63.2311, its = 11
i = 200 (of 248), d = 65.9531, its = 11
i = 201 (of 248), d = 21.3636, its = 9
i = 202 (of 248), d = 99.1268, its = 12
i = 203 (of 248), d = 63.3029, its = 10
i = 204 (of 248), d = 25.7021, its = 10
i = 205 (of 248), d = 84.1015, its = 11
i = 206 (of 248), d = 25.0211, its = 9
i = 207 (of 248), d = 111.012, its = 12
i = 208 (of 248), d = 41.877, its = 10
i = 209 (of 248), d = 136.782, its = 12
i = 210 (of 248), d = 111.164, its = 12
i = 211 (of 248), d = 23.004, its = 9
i = 212 (of 248), d = 44.3854, its = 10
i = 213 (of 248), d = 106.141, its = 11
i = 214 (of 248), d = 45.9094, its = 10
i = 215 (of 248), d = 61.8428, its = 11
i = 216 (of 248), d = 35.7859, its = 10
i = 217 (of 248), d = 28.4618, its = 9
i = 218 (of 248), d = 79.8788, its = 12
i = 219 (of 248), d = 16.6131, its = 8
i = 220 (of 248), d = 69.3035, its = 11
i = 221 (of 248), d = 55.3285, its = 10
i = 222 (of 248), d = 118.965, its = 11
i = 223 (of 248), d = 102.234, its = 11
i = 224 (of 248), d = 81.7287, its = 11
i = 225 (of 248), d = 94.0626, its = 11
i = 226 (of 248), d = 41.4353, its = 10
i = 227 (of 248), d = 138.562, its = 11
i = 228 (of 248), d = 103.526, its = 12
i = 229 (of 248), d = 127.687, its = 11
i = 230 (of 248), d = 18.2559, its = 9
i = 231 (of 248), d = 87.0673, its = 11
i = 232 (of 248), d = 29.3141, its = 10
i = 233 (of 248), d = 128.234, its = 11
i = 234 (of 248), d = 70.6027, its = 11
i = 235 (of 248), d = 19.0193, its = 9
i = 236 (of 248), d = 59.1825, its = 11
i = 237 (of 248), d = 36.401, its = 10
i = 238 (of 248), d = 56.2294, its = 11
i = 239 (of 248), d = 50.2586, its = 11
i = 240 (of 248), d = 92.1659, its = 11
i = 241 (of 248), d = 40.8583, its = 10
i = 242 (of 248), d = 68.093, its = 11
i = 243 (of 248), d = 69.1133, its = 11
i = 244 (of 248), d = 77.5975, its = 11
i = 245 (of 248), d = 38.4981, its = 10
i = 246 (of 248), d = 188.831, its = 12
i = 247 (of 248), d = 57.716, its = 11
i = 248 (of 248), d = 84.6372, its = 11
[1] "Fri Feb 09 14:50:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 14:50:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 14:50:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 14:50:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 14:50:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 14:50:17 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 2543.484541 
iter  10 value 12.067619
iter  20 value 1.449677
iter  30 value 0.458507
iter  40 value 0.258877
iter  50 value 0.189027
iter  60 value 0.140282
iter  70 value 0.118401
iter  80 value 0.109604
iter  90 value 0.105634
iter 100 value 0.102051
final  value 0.102051 
stopped after 100 iterations
# weights:  210
initial  value 813.324229 
iter  10 value 6.713533
iter  20 value 1.393011
iter  30 value 0.530417
iter  40 value 0.295328
iter  50 value 0.209775
iter  60 value 0.157097
iter  70 value 0.126951
iter  80 value 0.118305
iter  90 value 0.112587
iter 100 value 0.109138
final  value 0.109138 
stopped after 100 iterations
# weights:  210
initial  value 553.331027 
iter  10 value 13.211015
iter  20 value 1.195954
iter  30 value 0.438350
iter  40 value 0.309172
iter  50 value 0.242162
iter  60 value 0.195939
iter  70 value 0.176528
iter  80 value 0.166150
iter  90 value 0.156925
iter 100 value 0.149163
final  value 0.149163 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00463; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 308.452632 
iter  10 value 9.296977
iter  20 value 1.673860
iter  30 value 0.222488
iter  40 value 0.060094
iter  50 value 0.029802
iter  60 value 0.022999
iter  70 value 0.018192
iter  80 value 0.016663
iter  90 value 0.015708
iter 100 value 0.014424
final  value 0.014424 
stopped after 100 iterations
# weights:  144
initial  value 6910.333001 
iter  10 value 36.852088
iter  20 value 5.655185
iter  30 value 0.480044
iter  40 value 0.139952
iter  50 value 0.072200
iter  60 value 0.045339
iter  70 value 0.036111
iter  80 value 0.028733
iter  90 value 0.025161
iter 100 value 0.021826
final  value 0.021826 
stopped after 100 iterations
# weights:  144
initial  value 1747.404566 
iter  10 value 81.933792
iter  20 value 20.771334
iter  30 value 3.260150
iter  40 value 0.572280
iter  50 value 0.264238
iter  60 value 0.175164
iter  70 value 0.139502
iter  80 value 0.120616
iter  90 value 0.102525
iter 100 value 0.089978
final  value 0.089978 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00497; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 640.123230 
iter  10 value 20.793671
iter  20 value 3.277362
iter  30 value 0.635231
iter  40 value 0.122760
iter  50 value 0.043086
iter  60 value 0.029573
iter  70 value 0.022930
iter  80 value 0.020675
iter  90 value 0.017852
iter 100 value 0.016722
final  value 0.016722 
stopped after 100 iterations
# weights:  133
initial  value 6722.932607 
iter  10 value 46.862906
iter  20 value 7.280622
iter  30 value 0.444243
iter  40 value 0.157630
iter  50 value 0.054426
iter  60 value 0.035143
iter  70 value 0.025247
iter  80 value 0.021686
iter  90 value 0.018049
iter 100 value 0.016688
final  value 0.016688 
stopped after 100 iterations
# weights:  133
initial  value 1886.239490 
iter  10 value 63.443629
iter  20 value 28.006690
iter  30 value 3.689073
iter  40 value 0.792477
iter  50 value 0.302340
iter  60 value 0.141450
iter  70 value 0.073584
iter  80 value 0.054391
iter  90 value 0.046808
iter 100 value 0.040990
final  value 0.040990 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00309; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 598.899214 
iter  10 value 25.194309
iter  20 value 1.730931
iter  30 value 0.413256
iter  40 value 0.201173
iter  50 value 0.165744
iter  60 value 0.142449
iter  70 value 0.109599
iter  80 value 0.088646
iter  90 value 0.080979
iter 100 value 0.080279
final  value 0.080279 
stopped after 100 iterations
# weights:  23
initial  value 830.918577 
iter  10 value 137.258589
iter  20 value 45.256428
iter  30 value 24.199743
iter  40 value 6.394196
iter  50 value 2.856434
iter  60 value 0.972731
iter  70 value 0.383890
iter  80 value 0.257693
iter  90 value 0.225113
iter 100 value 0.218960
final  value 0.218960 
stopped after 100 iterations
# weights:  23
initial  value 1865.663082 
iter  10 value 57.332816
iter  20 value 3.960219
iter  30 value 0.700403
iter  40 value 0.399667
iter  50 value 0.286195
iter  60 value 0.220176
iter  70 value 0.175747
iter  80 value 0.152174
iter  90 value 0.139471
iter 100 value 0.136224
final  value 0.136224 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.00751; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 293.403010 
iter  10 value 23.081515
iter  20 value 3.382767
iter  30 value 1.025342
iter  40 value 0.602894
iter  50 value 0.426807
iter  60 value 0.361710
iter  70 value 0.327529
iter  80 value 0.298512
iter  90 value 0.272365
iter 100 value 0.257786
final  value 0.257786 
stopped after 100 iterations
# weights:  210
initial  value 2086.177955 
iter  10 value 16.645414
iter  20 value 4.658501
iter  30 value 1.487783
iter  40 value 0.764106
iter  50 value 0.496057
iter  60 value 0.370216
iter  70 value 0.333305
iter  80 value 0.308826
iter  90 value 0.292443
iter 100 value 0.279662
final  value 0.279662 
stopped after 100 iterations
# weights:  210
initial  value 672.883463 
iter  10 value 11.027668
iter  20 value 2.648454
iter  30 value 0.845734
iter  40 value 0.543418
iter  50 value 0.371458
iter  60 value 0.325069
iter  70 value 0.302650
iter  80 value 0.280175
iter  90 value 0.262494
iter 100 value 0.252190
final  value 0.252190 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00686; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 5233.867158 
iter  10 value 51.945830
iter  20 value 18.306633
iter  30 value 13.758725
iter  40 value 12.160336
iter  50 value 10.815236
iter  60 value 9.841766
iter  70 value 9.462043
iter  80 value 9.142112
iter  90 value 8.975765
iter 100 value 8.937460
final  value 8.937460 
stopped after 100 iterations
# weights:  199
initial  value 818.326742 
iter  10 value 59.354831
iter  20 value 23.911641
iter  30 value 17.925420
iter  40 value 14.644672
iter  50 value 11.787656
iter  60 value 10.470600
iter  70 value 9.922330
iter  80 value 9.672140
iter  90 value 9.510570
iter 100 value 9.370019
final  value 9.370019 
stopped after 100 iterations
# weights:  199
initial  value 2108.146636 
iter  10 value 206.948506
iter  20 value 155.752447
iter  30 value 134.470903
iter  40 value 100.619564
iter  50 value 54.288637
iter  60 value 27.354347
iter  70 value 20.780023
iter  80 value 14.455907
iter  90 value 12.432157
iter 100 value 11.036605
final  value 11.036605 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0663; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 1872.350783 
iter  10 value 20.589154
iter  20 value 1.352590
iter  30 value 0.471146
iter  40 value 0.355103
iter  50 value 0.317765
iter  60 value 0.300779
iter  70 value 0.287336
iter  80 value 0.274283
iter  90 value 0.264631
iter 100 value 0.244282
final  value 0.244282 
stopped after 100 iterations
# weights:  144
initial  value 1483.905007 
iter  10 value 55.705682
iter  20 value 13.913587
iter  30 value 2.425611
iter  40 value 0.783706
iter  50 value 0.407474
iter  60 value 0.329849
iter  70 value 0.272490
iter  80 value 0.237060
iter  90 value 0.205862
iter 100 value 0.173622
final  value 0.173622 
stopped after 100 iterations
# weights:  144
initial  value 2142.134178 
iter  10 value 21.271809
iter  20 value 3.420112
iter  30 value 0.633158
iter  40 value 0.135450
iter  50 value 0.069217
iter  60 value 0.055148
iter  70 value 0.046325
iter  80 value 0.040256
iter  90 value 0.036037
iter 100 value 0.033314
final  value 0.033314 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00734; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 2998.010327 
iter  10 value 41.102744
iter  20 value 14.372036
iter  30 value 2.811739
iter  40 value 0.581095
iter  50 value 0.320959
iter  60 value 0.188371
iter  70 value 0.148823
iter  80 value 0.125617
iter  90 value 0.101837
iter 100 value 0.093969
final  value 0.093969 
stopped after 100 iterations
# weights:  89
initial  value 1397.102741 
iter  10 value 66.357744
iter  20 value 9.551179
iter  30 value 1.510497
iter  40 value 0.355115
iter  50 value 0.187488
iter  60 value 0.122849
iter  70 value 0.085334
iter  80 value 0.070674
iter  90 value 0.065933
iter 100 value 0.061935
final  value 0.061935 
stopped after 100 iterations
# weights:  89
initial  value 3045.598422 
iter  10 value 18.282584
iter  20 value 3.611159
iter  30 value 0.628419
iter  40 value 0.171803
iter  50 value 0.109853
iter  60 value 0.088855
iter  70 value 0.068648
iter  80 value 0.060009
iter  90 value 0.053994
iter 100 value 0.049491
final  value 0.049491 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00387; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 2443.200252 
iter  10 value 56.720549
iter  20 value 4.522461
iter  30 value 0.873640
iter  40 value 0.547720
iter  50 value 0.440033
iter  60 value 0.373587
iter  70 value 0.301960
iter  80 value 0.244984
iter  90 value 0.218735
iter 100 value 0.185924
final  value 0.185924 
stopped after 100 iterations
# weights:  210
initial  value 3118.848635 
iter  10 value 25.551786
iter  20 value 2.784568
iter  30 value 0.519225
iter  40 value 0.188712
iter  50 value 0.140307
iter  60 value 0.108623
iter  70 value 0.086591
iter  80 value 0.072857
iter  90 value 0.068458
iter 100 value 0.063999
final  value 0.063999 
stopped after 100 iterations
# weights:  210
initial  value 310.687596 
iter  10 value 14.670294
iter  20 value 2.939559
iter  30 value 0.423727
iter  40 value 0.160553
iter  50 value 0.101801
iter  60 value 0.082294
iter  70 value 0.064415
iter  80 value 0.057339
iter  90 value 0.051564
iter 100 value 0.047403
final  value 0.047403 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00466; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 3137.631093 
iter  10 value 29.361503
iter  20 value 4.223758
iter  30 value 0.366907
iter  40 value 0.076931
iter  50 value 0.053183
iter  60 value 0.037220
iter  70 value 0.028120
iter  80 value 0.019975
iter  90 value 0.014731
iter 100 value 0.012252
final  value 0.012252 
stopped after 100 iterations
# weights:  56
initial  value 2397.342722 
iter  10 value 61.333094
iter  20 value 9.919273
iter  30 value 2.056787
iter  40 value 0.572479
iter  50 value 0.183901
iter  60 value 0.082299
iter  70 value 0.047447
iter  80 value 0.028811
iter  90 value 0.022789
iter 100 value 0.021172
final  value 0.021172 
stopped after 100 iterations
# weights:  56
initial  value 2188.483715 
iter  10 value 30.048571
iter  20 value 2.726216
iter  30 value 0.583973
iter  40 value 0.168426
iter  50 value 0.063196
iter  60 value 0.041094
iter  70 value 0.026028
iter  80 value 0.017760
iter  90 value 0.013409
iter 100 value 0.008569
final  value 0.008569 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00491; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 2856.050265 
iter  10 value 114.444159
iter  20 value 55.787513
iter  30 value 33.464858
iter  40 value 24.665426
iter  50 value 21.826794
iter  60 value 18.989135
iter  70 value 14.747343
iter  80 value 11.972069
iter  90 value 9.505293
iter 100 value 8.869169
final  value 8.869169 
stopped after 100 iterations
# weights:  199
initial  value 1774.976045 
iter  10 value 54.725727
iter  20 value 23.656840
iter  30 value 12.687424
iter  40 value 10.293064
iter  50 value 8.750562
iter  60 value 8.287927
iter  70 value 7.998771
iter  80 value 7.839674
iter  90 value 7.721450
iter 100 value 7.627767
final  value 7.627767 
stopped after 100 iterations
# weights:  199
initial  value 280.522469 
iter  10 value 42.994491
iter  20 value 16.708404
iter  30 value 12.206404
iter  40 value 10.210419
iter  50 value 9.233679
iter  60 value 8.538285
iter  70 value 8.122363
iter  80 value 7.730598
iter  90 value 7.572702
iter 100 value 7.499406
final  value 7.499406 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.062; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 637.027601 
iter  10 value 46.170123
iter  20 value 21.330540
iter  30 value 13.177239
iter  40 value 11.403893
iter  50 value 10.436990
iter  60 value 9.918260
iter  70 value 9.545151
iter  80 value 9.215486
iter  90 value 8.808993
iter 100 value 8.620048
final  value 8.620048 
stopped after 100 iterations
# weights:  122
initial  value 1343.075721 
iter  10 value 41.023207
iter  20 value 20.670698
iter  30 value 15.723159
iter  40 value 12.839412
iter  50 value 11.764107
iter  60 value 10.851768
iter  70 value 9.996229
iter  80 value 9.195876
iter  90 value 8.531951
iter 100 value 8.209945
final  value 8.209945 
stopped after 100 iterations
# weights:  122
initial  value 5329.328466 
iter  10 value 291.574058
iter  20 value 70.919911
iter  30 value 26.964556
iter  40 value 19.502556
iter  50 value 16.475684
iter  60 value 14.961146
iter  70 value 13.569156
iter  80 value 12.285466
iter  90 value 11.125163
iter 100 value 10.656649
final  value 10.656649 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0643; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 3149.302140 
iter  10 value 34.628464
iter  20 value 10.068225
iter  30 value 2.821098
iter  40 value 2.036383
iter  50 value 1.615466
iter  60 value 1.359294
iter  70 value 1.049959
iter  80 value 0.844009
iter  90 value 0.741874
iter 100 value 0.683613
final  value 0.683613 
stopped after 100 iterations
# weights:  56
initial  value 601.709563 
iter  10 value 95.397082
iter  20 value 18.439859
iter  30 value 12.265603
iter  40 value 8.824025
iter  50 value 4.800147
iter  60 value 2.950865
iter  70 value 1.637809
iter  80 value 1.094973
iter  90 value 0.869752
iter 100 value 0.709190
final  value 0.709190 
stopped after 100 iterations
# weights:  56
initial  value 1241.892560 
iter  10 value 58.315950
iter  20 value 19.483580
iter  30 value 6.031265
iter  40 value 3.374827
iter  50 value 2.333127
iter  60 value 1.831011
iter  70 value 1.358034
iter  80 value 1.031012
iter  90 value 0.918938
iter 100 value 0.876674
final  value 0.876674 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0124; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 369.979848 
iter  10 value 10.451916
iter  20 value 2.223334
iter  30 value 0.662553
iter  40 value 0.371023
iter  50 value 0.273608
iter  60 value 0.201645
iter  70 value 0.163375
iter  80 value 0.144422
iter  90 value 0.133567
iter 100 value 0.128020
final  value 0.128020 
stopped after 100 iterations
# weights:  144
initial  value 1350.820036 
iter  10 value 27.560982
iter  20 value 2.727714
iter  30 value 0.647322
iter  40 value 0.451568
iter  50 value 0.354496
iter  60 value 0.315140
iter  70 value 0.279887
iter  80 value 0.255064
iter  90 value 0.240329
iter 100 value 0.225107
final  value 0.225107 
stopped after 100 iterations
# weights:  144
initial  value 1807.350673 
iter  10 value 41.844261
iter  20 value 10.726076
iter  30 value 1.491593
iter  40 value 0.503768
iter  50 value 0.284786
iter  60 value 0.219878
iter  70 value 0.169013
iter  80 value 0.135866
iter  90 value 0.121085
iter 100 value 0.112582
final  value 0.112582 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00556; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 1878.480939 
iter  10 value 79.959950
iter  20 value 31.803180
iter  30 value 9.645332
iter  40 value 3.418266
iter  50 value 1.992778
iter  60 value 1.585467
iter  70 value 1.366546
iter  80 value 1.179967
iter  90 value 1.083423
iter 100 value 1.027514
final  value 1.027514 
stopped after 100 iterations
# weights:  122
initial  value 1616.809533 
iter  10 value 27.065885
iter  20 value 5.080728
iter  30 value 2.588934
iter  40 value 2.064002
iter  50 value 1.829511
iter  60 value 1.671918
iter  70 value 1.441309
iter  80 value 1.292627
iter  90 value 1.186532
iter 100 value 1.132477
final  value 1.132477 
stopped after 100 iterations
# weights:  122
initial  value 188.029690 
iter  10 value 12.824519
iter  20 value 3.187587
iter  30 value 1.725936
iter  40 value 1.349429
iter  50 value 1.165037
iter  60 value 1.078348
iter  70 value 1.039255
iter  80 value 1.018260
iter  90 value 1.005968
iter 100 value 0.995528
final  value 0.995528 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.015; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 2232.231629 
iter  10 value 65.416617
iter  20 value 17.479683
iter  30 value 3.181380
iter  40 value 0.672082
iter  50 value 0.450487
iter  60 value 0.326297
iter  70 value 0.262990
iter  80 value 0.225356
iter  90 value 0.212973
iter 100 value 0.201596
final  value 0.201596 
stopped after 100 iterations
# weights:  100
initial  value 1639.148156 
iter  10 value 17.790156
iter  20 value 5.636315
iter  30 value 0.843515
iter  40 value 0.158557
iter  50 value 0.106178
iter  60 value 0.084400
iter  70 value 0.072228
iter  80 value 0.063063
iter  90 value 0.059522
iter 100 value 0.057951
final  value 0.057951 
stopped after 100 iterations
# weights:  100
initial  value 269.569061 
iter  10 value 12.739542
iter  20 value 1.138631
iter  30 value 0.166514
iter  40 value 0.088689
iter  50 value 0.068531
iter  60 value 0.060090
iter  70 value 0.054517
iter  80 value 0.050716
iter  90 value 0.049306
iter 100 value 0.047647
final  value 0.047647 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0043; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 1380.409632 
iter  10 value 72.739213
iter  20 value 25.693224
iter  30 value 18.624003
iter  40 value 15.484713
iter  50 value 13.483779
iter  60 value 12.441081
iter  70 value 11.894107
iter  80 value 11.505200
iter  90 value 11.229781
iter 100 value 11.057624
final  value 11.057624 
stopped after 100 iterations
# weights:  56
initial  value 1587.024979 
iter  10 value 72.153790
iter  20 value 40.939361
iter  30 value 17.921107
iter  40 value 14.002948
iter  50 value 13.225989
iter  60 value 12.983609
iter  70 value 12.909839
iter  80 value 12.882775
iter  90 value 12.869110
iter 100 value 12.866588
final  value 12.866588 
stopped after 100 iterations
# weights:  56
initial  value 2044.760813 
iter  10 value 83.701804
iter  20 value 22.375939
iter  30 value 15.830842
iter  40 value 13.455801
iter  50 value 12.159397
iter  60 value 11.642866
iter  70 value 11.412208
iter  80 value 11.160191
iter  90 value 10.919193
iter 100 value 10.824881
final  value 10.824881 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0666; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 248.681698 
iter  10 value 22.435792
iter  20 value 5.905874
iter  30 value 0.254590
iter  40 value 0.045682
iter  50 value 0.014007
iter  60 value 0.006663
iter  70 value 0.003859
iter  80 value 0.002894
iter  90 value 0.002152
iter 100 value 0.001739
final  value 0.001739 
stopped after 100 iterations
# weights:  155
initial  value 990.257493 
iter  10 value 15.415218
iter  20 value 1.501850
iter  30 value 0.246928
iter  40 value 0.041726
iter  50 value 0.010799
iter  60 value 0.004120
iter  70 value 0.001859
iter  80 value 0.001543
iter  90 value 0.001348
iter 100 value 0.001205
final  value 0.001205 
stopped after 100 iterations
# weights:  155
initial  value 4475.394005 
iter  10 value 36.762674
iter  20 value 3.167285
iter  30 value 0.688563
iter  40 value 0.232469
iter  50 value 0.082358
iter  60 value 0.039666
iter  70 value 0.015391
iter  80 value 0.007772
iter  90 value 0.005253
iter 100 value 0.004145
final  value 0.004145 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00231; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 1240.023062 
iter  10 value 76.248610
iter  20 value 19.927452
iter  30 value 2.612708
iter  40 value 0.811507
iter  50 value 0.442594
iter  60 value 0.254143
iter  70 value 0.142040
iter  80 value 0.087912
iter  90 value 0.073837
iter 100 value 0.070460
final  value 0.070460 
stopped after 100 iterations
# weights:  45
initial  value 1085.842345 
iter  10 value 113.024968
iter  20 value 9.554805
iter  30 value 1.036887
iter  40 value 0.620567
iter  50 value 0.568695
iter  60 value 0.495058
iter  70 value 0.452938
iter  80 value 0.423418
iter  90 value 0.401759
iter 100 value 0.393114
final  value 0.393114 
stopped after 100 iterations
# weights:  45
initial  value 895.075212 
iter  10 value 34.799518
iter  20 value 12.596052
iter  30 value 1.586428
iter  40 value 0.339488
iter  50 value 0.163879
iter  60 value 0.118840
iter  70 value 0.108335
iter  80 value 0.101610
iter  90 value 0.097646
iter 100 value 0.094721
final  value 0.094721 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00789; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 1229.781946 
iter  10 value 28.833807
iter  20 value 3.504737
iter  30 value 1.458393
iter  40 value 1.179932
iter  50 value 1.050020
iter  60 value 0.993691
iter  70 value 0.944567
iter  80 value 0.854683
iter  90 value 0.789433
iter 100 value 0.695964
final  value 0.695964 
stopped after 100 iterations
# weights:  78
initial  value 149.767690 
iter  10 value 30.118197
iter  20 value 6.171456
iter  30 value 1.673832
iter  40 value 0.935809
iter  50 value 0.785754
iter  60 value 0.715077
iter  70 value 0.646547
iter  80 value 0.607520
iter  90 value 0.585360
iter 100 value 0.568652
final  value 0.568652 
stopped after 100 iterations
# weights:  78
initial  value 3381.543146 
iter  10 value 55.109505
iter  20 value 19.192290
iter  30 value 4.443704
iter  40 value 2.116215
iter  50 value 1.375393
iter  60 value 0.972903
iter  70 value 0.851891
iter  80 value 0.743101
iter  90 value 0.657276
iter 100 value 0.618499
final  value 0.618499 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0128; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00231
# weights:  155
initial  value 5541.523714 
iter  10 value 30.923686
iter  20 value 2.873398
iter  30 value 0.297169
iter  40 value 0.076277
iter  50 value 0.023881
iter  60 value 0.010634
iter  70 value 0.005223
iter  80 value 0.003559
iter  90 value 0.002281
iter 100 value 0.001841
final  value 0.001841 
stopped after 100 iterations
[1] "Fri Feb 09 14:50:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 550 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 342
 number of selected nodes                               : 79 
[1] "Fri Feb 09 14:50:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 14:50:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 14:51:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 14:51:07 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0407; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0357; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0354; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0534; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0405; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0432; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0356; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0345; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0397; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0371; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0433; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0384; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0389; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0362; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0412; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0359; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0343
[1] "Fri Feb 09 14:53:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 14:53:05 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.178; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.148; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.354; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.132
[1] "Fri Feb 09 14:53:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 14:53:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:53:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.252129 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1          1.857 
    Y 2          1.808 
    Y 3         0.6813 
    Y 4          3.133 
    Y 5          2.161 
[1] "Fri Feb 09 14:53:41 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.154; time: 0.8 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.148; time: 0.5 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.16; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.171; time: 0.8 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.559; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.233; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.198; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.173; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.268; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.226; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.155; time: 0.0 min
[Tune] Result: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683 : rmse.test.rmse=0.143
[1] "Fri Feb 09 14:56:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 14:56:18 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [9,0.500986], n=(372,380)
**GROW** @depth 1: [3,0.5], n=(199,172)
**GROW** @depth 1: [9,0.677318], n=(199,182)
**GROW** @depth 2: [9,0.573767], n=(79,122)
**GROW** @depth 3: [7,0.306048], n=(63,116)
**GROW** @depth 3: [4,0.50253], n=(102,19)
**GROW** @depth 2: [3,0.672165], n=(54,27)
**GROW** @depth 3: [9,0.393097], n=(165,29)
**GROW** @depth 2: [9,0.248915], n=(12,161)
**GROW** @depth 3: [9,0.836884], n=(79,36)
**GROW** @depth 4: [4,0.198521], n=(35,126)
**GROW** @depth 3: [2,0.630644], n=(24,38)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(168,32,12,35,126,54,24,105,19,24,38,74,41)
**GROW** @depth 5: [4,0.491242], n=(13,28)
**GROW** @depth 3: [8,0.456769], n=(91,75)
**PRUNE** @depth 3: [4,0.493967]
**GROW** @depth 4: [9,0.609467], n=(29,45)
**GROW** @depth 4: [9,0.373767], n=(42,84)
**GROW** @depth 5: [8,0.644541], n=(19,22)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(93,71,33,13,38,19,23,81,56,30,44,74,25,37,49,38,28)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 5: [8,0.644541]
**GROW** @depth 4: [9,0.371992], n=(14,23)
**PRUNE** @depth 5: [9,0.368442]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(94,68,31,12,59,23,80,60,32,41,74,25,37,50,36,30)
**GROW** @depth 3: [3,0.8], n=(58,16)
**GROW** @depth 4: [9,0.312032], n=(24,32)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(94,72,33,12,23,32,21,79,60,32,42,58,17,25,37,50,35,30)
**PRUNE** @depth 4: [9,0.314398]
**GROW** @depth 5: [3,0.203093], n=(20,74)
**PRUNE** @depth 5: [3,0.203093]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(95,73,36,12,52,21,78,58,37,39,52,22,25,37,49,36,30)
**GROW** @depth 4: [9,0.213807], n=(53,41)
**GROW** @depth 4: [9,0.277515], n=(38,36)
**PRUNE** @depth 4: [8,0.458515]
**GROW** @depth 4: [9,0.0944773], n=(23,46)
**GROW** @depth 3: [9,0.439448], n=(23,13)
**GROW** @depth 4: [9,0.303748], n=(16,47)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(24,44,37,16,46,24,13,13,51,21,77,54,44,37,53,22,24,38,49,35,30)
**GROW** @depth 5: [4,0.6703], n=(65,13)
**GROW** @depth 6: [9,0.404931], n=(14,51)
**PRUNE** @depth 5: [9,0.404734]
**GROW** @depth 4: [4,0.500973], n=(37,15)
**PRUNE** @depth 5: [4,0.198132]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(26,40,38,16,47,24,13,12,37,15,82,13,61,40,38,52,22,24,38,49,35,30)
Grow: 7.775%, Prune: 2.21%, Change: 76.51%, Swap: 21.89%

[1] "Fri Feb 09 14:56:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 14:56:27 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 14:56:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.9075 	 alpha= 1.7943 	 beta= 42824.58 
[1] "Fri Feb 09 14:56:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 14:56:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [8,0.481223], n=(161,591)
**GROW** @depth 1: [3,0.5], n=(87,504)
r=1000 d=[0] [0] [0]; n=(161,87,504)
**GROW** @depth 2: [9,0.500986], n=(163,341)
r=2000 d=[0] [0] [0] [0]; n=(159,84,168,341)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [1,0.505527], n=(36,305)
r=1000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=2000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=3000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
Grow: 1.146%, Prune: 0%, Change: 5.689%, Swap: 0%

[1] "Fri Feb 09 14:56:38 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 14:56:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 14:56:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 14:56:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 14:56:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 14:56:42 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 14:56:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 14:56:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 14:56:46 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0617; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0466; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0611; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.0531; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0724; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0556; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0459; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.0564; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0829; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0509; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0577; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0573; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0514; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0462; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0501; time: 0.0 min
[Tune] Result: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9 : rmse.test.rmse=0.0459
[1] "Fri Feb 09 14:57:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 14:57:00 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0554; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.082; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.07; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0388; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0257; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.642; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0236; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0199; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.0157
[1] "Fri Feb 09 14:57:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:57:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=================                                                     |  24%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:57:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:57:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==========                                                            |  14%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 14:57:25 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.59 11.826 1.94 5.138 28.498 27.69 10.946 2.29 10.14 
  - best initial criterion value(s) :  3469.405 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3469.4  |proj g|=       4.2341
At iterate     1  f =      -3498.1  |proj g|=        4.6682
At iterate     2  f =      -3506.4  |proj g|=        5.1236
At iterate     3  f =      -3516.3  |proj g|=        4.8528
At iterate     4  f =      -3519.9  |proj g|=         6.425
At iterate     5  f =      -3520.6  |proj g|=         4.413
At iterate     6  f =      -3520.8  |proj g|=        4.3889
At iterate     7  f =      -3520.9  |proj g|=        4.1675
At iterate     8  f =      -3521.5  |proj g|=        3.8063
At iterate     9  f =        -3524  |proj g|=        3.8493
At iterate    10  f =      -3525.7  |proj g|=        2.6873
At iterate    11  f =      -3526.3  |proj g|=        2.4203
At iterate    12  f =      -3526.5  |proj g|=        2.2814
At iterate    13  f =      -3526.6  |proj g|=        2.1223
At iterate    14  f =      -3526.7  |proj g|=        2.0808
At iterate    15  f =      -3526.8  |proj g|=        1.9743
At iterate    16  f =        -3527  |proj g|=        1.8276
At iterate    17  f =      -3527.6  |proj g|=        1.6117
At iterate    18  f =      -3528.2  |proj g|=        1.7447
At iterate    19  f =      -3528.4  |proj g|=         2.093
At iterate    20  f =      -3528.5  |proj g|=         1.981
At iterate    21  f =        -3529  |proj g|=        2.0703
At iterate    22  f =        -3529  |proj g|=        1.9565
At iterate    23  f =        -3529  |proj g|=        2.0203
At iterate    24  f =      -3529.1  |proj g|=        2.1176
At iterate    25  f =      -3529.3  |proj g|=         2.263
At iterate    26  f =        -3530  |proj g|=        2.7002
At iterate    27  f =      -3531.1  |proj g|=        2.6416
At iterate    28  f =      -3532.1  |proj g|=        3.0961
At iterate    29  f =      -3532.2  |proj g|=        3.3331
At iterate    30  f =      -3532.9  |proj g|=         3.623
At iterate    31  f =      -3533.4  |proj g|=        2.7651
At iterate    32  f =      -3533.5  |proj g|=        2.7141
At iterate    33  f =      -3533.5  |proj g|=        2.7166
At iterate    34  f =      -3533.9  |proj g|=        2.4864
At iterate    35  f =      -3534.4  |proj g|=        2.0998
At iterate    36  f =      -3535.5  |proj g|=        1.3731
At iterate    37  f =      -3536.3  |proj g|=        2.3182
At iterate    38  f =      -3536.9  |proj g|=        2.3769
At iterate    39  f =      -3537.3  |proj g|=        2.5307
At iterate    40  f =      -3537.5  |proj g|=        1.8144
At iterate    41  f =      -3537.6  |proj g|=        1.1428
At iterate    42  f =      -3537.7  |proj g|=       0.96344
At iterate    43  f =      -3537.8  |proj g|=        1.8291
At iterate    44  f =      -3537.8  |proj g|=        1.8307
At iterate    45  f =        -3538  |proj g|=        1.8277
At iterate    46  f =      -3538.1  |proj g|=        2.2608
At iterate    47  f =      -3538.2  |proj g|=        1.0875
At iterate    48  f =      -3538.3  |proj g|=       0.95746
At iterate    49  f =      -3538.3  |proj g|=        1.0798
At iterate    50  f =      -3538.3  |proj g|=        1.6881
At iterate    51  f =      -3538.4  |proj g|=        1.8311
At iterate    52  f =      -3538.4  |proj g|=        1.2553
At iterate    53  f =      -3538.4  |proj g|=        1.8296
At iterate    54  f =      -3538.5  |proj g|=       0.73298
At iterate    55  f =      -3538.5  |proj g|=       0.60371
At iterate    56  f =      -3538.5  |proj g|=       0.54439
At iterate    57  f =      -3538.5  |proj g|=       0.54322
At iterate    58  f =      -3538.5  |proj g|=        1.4066
At iterate    59  f =      -3538.5  |proj g|=       0.54518
At iterate    60  f =      -3538.5  |proj g|=       0.54491
At iterate    61  f =      -3538.5  |proj g|=       0.62885
At iterate    62  f =      -3538.6  |proj g|=       0.71125
At iterate    63  f =      -3538.6  |proj g|=       0.80463
At iterate    64  f =      -3538.6  |proj g|=        1.3181
At iterate    65  f =      -3538.7  |proj g|=        1.8327
At iterate    66  f =      -3538.8  |proj g|=        1.8301
At iterate    67  f =      -3538.8  |proj g|=       0.65424
At iterate    68  f =      -3538.8  |proj g|=       0.63675
At iterate    69  f =      -3538.8  |proj g|=        0.6487
At iterate    70  f =      -3538.9  |proj g|=         1.834
At iterate    71  f =      -3538.9  |proj g|=        1.8527
At iterate    72  f =      -3539.1  |proj g|=        1.8311
At iterate    73  f =      -3539.3  |proj g|=        1.8166
At iterate    74  f =      -3539.8  |proj g|=        1.4271
At iterate    75  f =      -3540.3  |proj g|=       0.94408
At iterate    76  f =      -3540.4  |proj g|=        1.8385
At iterate    77  f =      -3540.6  |proj g|=        1.2009
At iterate    78  f =      -3540.7  |proj g|=        1.0522
At iterate    79  f =      -3540.7  |proj g|=       0.54574
At iterate    80  f =      -3540.7  |proj g|=       0.96606
At iterate    81  f =      -3540.7  |proj g|=       0.52287
At iterate    82  f =      -3540.7  |proj g|=       0.44395
At iterate    83  f =      -3540.7  |proj g|=        1.8379
At iterate    84  f =      -3540.7  |proj g|=       0.58613
At iterate    85  f =      -3540.8  |proj g|=        0.6167
At iterate    86  f =      -3540.8  |proj g|=       0.53993
At iterate    87  f =      -3540.8  |proj g|=       0.49111
At iterate    88  f =      -3540.8  |proj g|=       0.25601
At iterate    89  f =      -3540.8  |proj g|=       0.34693
At iterate    90  f =      -3540.8  |proj g|=       0.24649
At iterate    91  f =      -3540.8  |proj g|=       0.40303
At iterate    92  f =      -3540.8  |proj g|=       0.16723
At iterate    93  f =      -3540.8  |proj g|=       0.10758
At iterate    94  f =      -3540.8  |proj g|=       0.10117
At iterate    95  f =      -3540.8  |proj g|=       0.49123
At iterate    96  f =      -3540.8  |proj g|=       0.40476
At iterate    97  f =      -3540.8  |proj g|=       0.35135
At iterate    98  f =      -3540.8  |proj g|=       0.19974
At iterate    99  f =      -3540.8  |proj g|=       0.14438
At iterate   100  f =      -3540.8  |proj g|=       0.21026
At iterate   101  f =      -3540.8  |proj g|=       0.31687
final  value -3540.780764 
stopped after 101 iterations
[1] "Fri Feb 09 15:02:26 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 84.104, its = 11
i = 2 (of 248), d = 122.377, its = 12
i = 3 (of 248), d = 120.223, its = 11
i = 4 (of 248), d = 247.864, its = 13
i = 5 (of 248), d = 100.807, its = 12
i = 6 (of 248), d = 72.2713, its = 11
i = 7 (of 248), d = 88.9088, its = 11
i = 8 (of 248), d = 55.8078, its = 11
i = 9 (of 248), d = 18.8109, its = 9
i = 10 (of 248), d = 63.3732, its = 11
i = 11 (of 248), d = 42.5769, its = 10
i = 12 (of 248), d = 41.855, its = 10
i = 13 (of 248), d = 100.353, its = 11
i = 14 (of 248), d = 18.2793, its = 9
i = 15 (of 248), d = 45.6186, its = 10
i = 16 (of 248), d = 43.0061, its = 10
i = 17 (of 248), d = 34.5651, its = 10
i = 18 (of 248), d = 100.506, its = 12
i = 19 (of 248), d = 38.9449, its = 10
i = 20 (of 248), d = 51.8952, its = 10
i = 21 (of 248), d = 54.2861, its = 11
i = 22 (of 248), d = 75.0682, its = 11
i = 23 (of 248), d = 67.6005, its = 11
i = 24 (of 248), d = 67.4791, its = 11
i = 25 (of 248), d = 70.2667, its = 11
i = 26 (of 248), d = 26.6728, its = 9
i = 27 (of 248), d = 186.318, its = 12
i = 28 (of 248), d = 116.462, its = 11
i = 29 (of 248), d = 18.902, its = 9
i = 30 (of 248), d = 144.345, its = 13
i = 31 (of 248), d = 136.129, its = 12
i = 32 (of 248), d = 113.363, its = 12
i = 33 (of 248), d = 16.357, its = 8
i = 34 (of 248), d = 130.211, its = 12
i = 35 (of 248), d = 62.976, its = 11
i = 36 (of 248), d = 51.2249, its = 10
i = 37 (of 248), d = 101.098, its = 11
i = 38 (of 248), d = 80.3592, its = 11
i = 39 (of 248), d = 25.7046, its = 9
i = 40 (of 248), d = 81.3557, its = 11
i = 41 (of 248), d = 89.0848, its = 12
i = 42 (of 248), d = 30.5218, its = 10
i = 43 (of 248), d = 99.4758, its = 11
i = 44 (of 248), d = 60.3076, its = 11
i = 45 (of 248), d = 51.8147, its = 11
i = 46 (of 248), d = 117.492, its = 11
i = 47 (of 248), d = 144.325, its = 12
i = 48 (of 248), d = 116.21, its = 12
i = 49 (of 248), d = 22.3144, its = 9
i = 50 (of 248), d = 102.861, its = 11
i = 51 (of 248), d = 63.8859, its = 11
i = 52 (of 248), d = 85.3755, its = 11
i = 53 (of 248), d = 130.181, its = 12
i = 54 (of 248), d = 41.72, its = 11
i = 55 (of 248), d = 32.4172, its = 10
i = 56 (of 248), d = 102.842, its = 11
i = 57 (of 248), d = 69.5534, its = 11
i = 58 (of 248), d = 84.4106, its = 12
i = 59 (of 248), d = 66.448, its = 11
i = 60 (of 248), d = 131.614, its = 11
i = 61 (of 248), d = 53.8962, its = 11
i = 62 (of 248), d = 104.837, its = 11
i = 63 (of 248), d = 139.678, its = 12
i = 64 (of 248), d = 36.9965, its = 10
i = 65 (of 248), d = 41.0991, its = 11
i = 66 (of 248), d = 78.5869, its = 11
i = 67 (of 248), d = 91.3337, its = 11
i = 68 (of 248), d = 53.9066, its = 10
i = 69 (of 248), d = 42.7163, its = 10
i = 70 (of 248), d = 63.0968, its = 11
i = 71 (of 248), d = 87.0422, its = 11
i = 72 (of 248), d = 34.4941, its = 10
i = 73 (of 248), d = 86.99, its = 11
i = 74 (of 248), d = 117.729, its = 11
i = 75 (of 248), d = 92.1188, its = 11
i = 76 (of 248), d = 92.9841, its = 11
i = 77 (of 248), d = 90.3848, its = 11
i = 78 (of 248), d = 30.6116, its = 9
i = 79 (of 248), d = 211.251, its = 12
i = 80 (of 248), d = 69.5717, its = 11
i = 81 (of 248), d = 105.187, its = 11
i = 82 (of 248), d = 80.1597, its = 12
i = 83 (of 248), d = 80.2726, its = 11
i = 84 (of 248), d = 35.5304, its = 10
i = 85 (of 248), d = 31.353, its = 9
i = 86 (of 248), d = 18.6061, its = 9
i = 87 (of 248), d = 67.9279, its = 11
i = 88 (of 248), d = 71.7992, its = 11
i = 89 (of 248), d = 52.488, its = 11
i = 90 (of 248), d = 107.894, its = 11
i = 91 (of 248), d = 41.1745, its = 10
i = 92 (of 248), d = 49.4877, its = 11
i = 93 (of 248), d = 29.4545, its = 10
i = 94 (of 248), d = 42.5415, its = 10
i = 95 (of 248), d = 54.4042, its = 10
i = 96 (of 248), d = 40.9334, its = 11
i = 97 (of 248), d = 96.0904, its = 11
i = 98 (of 248), d = 83.6, its = 11
i = 99 (of 248), d = 67.842, its = 11
i = 100 (of 248), d = 55.4431, its = 11
i = 101 (of 248), d = 111.965, its = 12
i = 102 (of 248), d = 115.055, its = 12
i = 103 (of 248), d = 75.3956, its = 12
i = 104 (of 248), d = 108.092, its = 11
i = 105 (of 248), d = 28.0496, its = 9
i = 106 (of 248), d = 59.0125, its = 10
i = 107 (of 248), d = 79.677, its = 11
i = 108 (of 248), d = 67.1615, its = 11
i = 109 (of 248), d = 22.0527, its = 9
i = 110 (of 248), d = 25.219, its = 10
i = 111 (of 248), d = 70.5911, its = 11
i = 112 (of 248), d = 24.0757, its = 9
i = 113 (of 248), d = 112.782, its = 12
i = 114 (of 248), d = 27.4041, its = 9
i = 115 (of 248), d = 107.858, its = 11
i = 116 (of 248), d = 24.3282, its = 10
i = 117 (of 248), d = 130.212, its = 11
i = 118 (of 248), d = 123.909, its = 12
i = 119 (of 248), d = 53.7783, its = 11
i = 120 (of 248), d = 40.4588, its = 10
i = 121 (of 248), d = 26.6957, its = 10
i = 122 (of 248), d = 80.1324, its = 11
i = 123 (of 248), d = 106.998, its = 11
i = 124 (of 248), d = 65.3706, its = 11
i = 125 (of 248), d = 28.4075, its = 10
i = 126 (of 248), d = 105.858, its = 12
i = 127 (of 248), d = 71.297, its = 11
i = 128 (of 248), d = 137.906, its = 12
i = 129 (of 248), d = 34.4663, its = 10
i = 130 (of 248), d = 25.3857, its = 9
i = 131 (of 248), d = 24.8732, its = 9
i = 132 (of 248), d = 79.2349, its = 11
i = 133 (of 248), d = 120.15, its = 11
i = 134 (of 248), d = 68.6, its = 11
i = 135 (of 248), d = 20.0734, its = 9
i = 136 (of 248), d = 97.4592, its = 12
i = 137 (of 248), d = 52.7073, its = 11
i = 138 (of 248), d = 56.8751, its = 11
i = 139 (of 248), d = 120.807, its = 11
i = 140 (of 248), d = 136.682, its = 12
i = 141 (of 248), d = 62.9474, its = 11
i = 142 (of 248), d = 86.1129, its = 11
i = 143 (of 248), d = 92.4817, its = 11
i = 144 (of 248), d = 76.5097, its = 11
i = 145 (of 248), d = 31.6971, its = 10
i = 146 (of 248), d = 52.1789, its = 11
i = 147 (of 248), d = 132.147, its = 12
i = 148 (of 248), d = 71.3747, its = 11
i = 149 (of 248), d = 107.122, its = 12
i = 150 (of 248), d = 97.4228, its = 11
i = 151 (of 248), d = 24.8457, its = 9
i = 152 (of 248), d = 46.4404, its = 10
i = 153 (of 248), d = 111.387, its = 11
i = 154 (of 248), d = 113.421, its = 12
i = 155 (of 248), d = 199.149, its = 12
i = 156 (of 248), d = 60.5404, its = 11
i = 157 (of 248), d = 63.8253, its = 11
i = 158 (of 248), d = 73.7908, its = 11
i = 159 (of 248), d = 120.958, its = 11
i = 160 (of 248), d = 83.8209, its = 11
i = 161 (of 248), d = 26.0174, its = 9
i = 162 (of 248), d = 147.677, its = 12
i = 163 (of 248), d = 119.065, its = 12
i = 164 (of 248), d = 25.7508, its = 9
i = 165 (of 248), d = 65.4453, its = 11
i = 166 (of 248), d = 98.5605, its = 11
i = 167 (of 248), d = 28.8312, its = 10
i = 168 (of 248), d = 26.7932, its = 10
i = 169 (of 248), d = 87.2949, its = 11
i = 170 (of 248), d = 70.8195, its = 11
i = 171 (of 248), d = 83.8967, its = 12
i = 172 (of 248), d = 53.4034, its = 10
i = 173 (of 248), d = 126.555, its = 12
i = 174 (of 248), d = 95.5742, its = 11
i = 175 (of 248), d = 174.109, its = 15
i = 176 (of 248), d = 145.689, its = 12
i = 177 (of 248), d = 65.2836, its = 11
i = 178 (of 248), d = 41.4739, its = 10
i = 179 (of 248), d = 46.4535, its = 11
i = 180 (of 248), d = 95.5674, its = 11
i = 181 (of 248), d = 87.1681, its = 11
i = 182 (of 248), d = 88.1437, its = 11
i = 183 (of 248), d = 73.4157, its = 11
i = 184 (of 248), d = 31.3504, its = 10
i = 185 (of 248), d = 135.232, its = 12
i = 186 (of 248), d = 38.2027, its = 10
i = 187 (of 248), d = 42.5121, its = 10
i = 188 (of 248), d = 46.5841, its = 10
i = 189 (of 248), d = 126.632, its = 11
i = 190 (of 248), d = 19.8721, its = 9
i = 191 (of 248), d = 53.8209, its = 11
i = 192 (of 248), d = 95.2676, its = 12
i = 193 (of 248), d = 45.7526, its = 10
i = 194 (of 248), d = 28.3362, its = 10
i = 195 (of 248), d = 20.2027, its = 9
i = 196 (of 248), d = 61.7015, its = 11
i = 197 (of 248), d = 22.1422, its = 9
i = 198 (of 248), d = 81.6979, its = 11
i = 199 (of 248), d = 49.4313, its = 10
i = 200 (of 248), d = 68.8008, its = 11
i = 201 (of 248), d = 131.338, its = 13
i = 202 (of 248), d = 33.4307, its = 10
i = 203 (of 248), d = 103.383, its = 11
i = 204 (of 248), d = 78.8732, its = 11
i = 205 (of 248), d = 39.9014, its = 10
i = 206 (of 248), d = 66.6039, its = 11
i = 207 (of 248), d = 80.2143, its = 11
i = 208 (of 248), d = 63.3766, its = 11
i = 209 (of 248), d = 82.3508, its = 11
i = 210 (of 248), d = 121.586, its = 11
i = 211 (of 248), d = 12.6609, its = 8
i = 212 (of 248), d = 79.6616, its = 11
i = 213 (of 248), d = 151.958, its = 12
i = 214 (of 248), d = 124.09, its = 11
i = 215 (of 248), d = 65.1176, its = 11
i = 216 (of 248), d = 120.715, its = 11
i = 217 (of 248), d = 94.2498, its = 11
i = 218 (of 248), d = 76.3032, its = 11
i = 219 (of 248), d = 76.254, its = 11
i = 220 (of 248), d = 44.1877, its = 11
i = 221 (of 248), d = 77.5188, its = 11
i = 222 (of 248), d = 64.7545, its = 11
i = 223 (of 248), d = 38.7528, its = 10
i = 224 (of 248), d = 35.2067, its = 10
i = 225 (of 248), d = 97.5529, its = 11
i = 226 (of 248), d = 61.2856, its = 11
i = 227 (of 248), d = 280.626, its = 14
i = 228 (of 248), d = 122.016, its = 11
i = 229 (of 248), d = 102.433, its = 11
i = 230 (of 248), d = 51.6697, its = 11
i = 231 (of 248), d = 27.0003, its = 10
i = 232 (of 248), d = 28.2024, its = 10
i = 233 (of 248), d = 127.559, its = 12
i = 234 (of 248), d = 17.5082, its = 9
i = 235 (of 248), d = 86.5413, its = 12
i = 236 (of 248), d = 35.3908, its = 10
i = 237 (of 248), d = 103.854, its = 12
i = 238 (of 248), d = 40.67, its = 11
i = 239 (of 248), d = 82.4445, its = 11
i = 240 (of 248), d = 36.6512, its = 10
i = 241 (of 248), d = 64.7304, its = 11
i = 242 (of 248), d = 83.5979, its = 11
i = 243 (of 248), d = 88.6875, its = 11
i = 244 (of 248), d = 57.8284, its = 11
i = 245 (of 248), d = 92.3226, its = 11
i = 246 (of 248), d = 63.8166, its = 11
i = 247 (of 248), d = 288.791, its = 13
i = 248 (of 248), d = 52.2815, its = 11
[1] "Fri Feb 09 15:03:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 15:03:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 15:03:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 15:03:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 15:03:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 15:03:42 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  12
initial  value 1886.242049 
iter  10 value 211.731237
iter  20 value 168.554128
iter  30 value 41.798980
iter  40 value 8.567724
iter  50 value 5.264310
iter  60 value 1.286582
iter  70 value 1.025578
iter  80 value 1.021273
final  value 1.021255 
converged
# weights:  12
initial  value 1326.804522 
iter  10 value 147.615201
iter  20 value 11.999866
iter  30 value 8.055937
iter  40 value 3.327668
iter  50 value 2.298144
iter  60 value 1.632888
iter  70 value 1.060418
iter  80 value 1.026781
iter  90 value 1.019457
final  value 1.019451 
converged
# weights:  12
initial  value 1089.118282 
iter  10 value 16.303153
iter  20 value 4.081621
iter  30 value 1.612496
iter  40 value 1.359023
iter  50 value 1.355977
final  value 1.355974 
converged
[Tune-y] 1: rmse.test.rmse=0.0266; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  155
initial  value 980.215333 
iter  10 value 27.650909
iter  20 value 4.673707
iter  30 value 0.523498
iter  40 value 0.164857
iter  50 value 0.118946
iter  60 value 0.091236
iter  70 value 0.075838
iter  80 value 0.065893
iter  90 value 0.060386
iter 100 value 0.056723
final  value 0.056723 
stopped after 100 iterations
# weights:  155
initial  value 672.353809 
iter  10 value 10.897790
iter  20 value 2.697914
iter  30 value 0.667226
iter  40 value 0.229733
iter  50 value 0.115548
iter  60 value 0.093960
iter  70 value 0.077523
iter  80 value 0.067077
iter  90 value 0.061920
iter 100 value 0.058244
final  value 0.058244 
stopped after 100 iterations
# weights:  155
initial  value 4624.619451 
iter  10 value 24.060521
iter  20 value 2.336666
iter  30 value 0.461275
iter  40 value 0.214818
iter  50 value 0.151784
iter  60 value 0.124342
iter  70 value 0.102772
iter  80 value 0.087552
iter  90 value 0.079772
iter 100 value 0.075302
final  value 0.075302 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00374; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  111
initial  value 2403.042539 
iter  10 value 25.612080
iter  20 value 3.269293
iter  30 value 1.159937
iter  40 value 0.732488
iter  50 value 0.548876
iter  60 value 0.473375
iter  70 value 0.438354
iter  80 value 0.411076
iter  90 value 0.385268
iter 100 value 0.361146
final  value 0.361146 
stopped after 100 iterations
# weights:  111
initial  value 2424.432077 
iter  10 value 100.090345
iter  20 value 9.660215
iter  30 value 2.503552
iter  40 value 1.427902
iter  50 value 1.039047
iter  60 value 0.869229
iter  70 value 0.741049
iter  80 value 0.658382
iter  90 value 0.606251
iter 100 value 0.575381
final  value 0.575381 
stopped after 100 iterations
# weights:  111
initial  value 1657.844963 
iter  10 value 39.647861
iter  20 value 6.706783
iter  30 value 1.536084
iter  40 value 1.067724
iter  50 value 0.820323
iter  60 value 0.702420
iter  70 value 0.642620
iter  80 value 0.570518
iter  90 value 0.536106
iter 100 value 0.508298
final  value 0.508298 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0102; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  12
initial  value 1406.975957 
iter  10 value 210.053071
iter  20 value 210.043265
iter  30 value 210.034313
iter  40 value 209.217832
iter  50 value 82.736485
iter  60 value 5.276622
iter  70 value 0.859639
iter  80 value 0.329150
iter  90 value 0.193829
iter 100 value 0.180712
final  value 0.180712 
stopped after 100 iterations
# weights:  12
initial  value 2522.552103 
iter  10 value 290.806541
iter  20 value 184.113179
iter  30 value 76.686920
iter  40 value 39.546947
iter  50 value 14.804012
iter  60 value 4.243488
iter  70 value 0.980642
iter  80 value 0.661454
iter  90 value 0.232614
iter 100 value 0.164970
final  value 0.164970 
stopped after 100 iterations
# weights:  12
initial  value 394.887820 
iter  10 value 18.186076
iter  20 value 0.860543
iter  30 value 0.357367
iter  40 value 0.168133
iter  50 value 0.133537
iter  60 value 0.132944
iter  70 value 0.132750
final  value 0.132748 
converged
[Tune-y] 4: rmse.test.rmse=0.0126; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  166
initial  value 218.878569 
iter  10 value 17.275693
iter  20 value 2.441191
iter  30 value 0.705841
iter  40 value 0.131318
iter  50 value 0.069055
iter  60 value 0.047485
iter  70 value 0.041354
iter  80 value 0.038403
iter  90 value 0.036433
iter 100 value 0.033975
final  value 0.033975 
stopped after 100 iterations
# weights:  166
initial  value 281.718427 
iter  10 value 16.710042
iter  20 value 4.111126
iter  30 value 0.799542
iter  40 value 0.215765
iter  50 value 0.079175
iter  60 value 0.046809
iter  70 value 0.034851
iter  80 value 0.029749
iter  90 value 0.026579
iter 100 value 0.023269
final  value 0.023269 
stopped after 100 iterations
# weights:  166
initial  value 9835.836218 
iter  10 value 44.910862
iter  20 value 4.145114
iter  30 value 0.472708
iter  40 value 0.159193
iter  50 value 0.102939
iter  60 value 0.082937
iter  70 value 0.070918
iter  80 value 0.065718
iter  90 value 0.059556
iter 100 value 0.055758
final  value 0.055758 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0036; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  155
initial  value 183.414280 
iter  10 value 15.423064
iter  20 value 2.052259
iter  30 value 0.891886
iter  40 value 0.585942
iter  50 value 0.498764
iter  60 value 0.461761
iter  70 value 0.425323
iter  80 value 0.383391
iter  90 value 0.361287
iter 100 value 0.339500
final  value 0.339500 
stopped after 100 iterations
# weights:  155
initial  value 1648.707440 
iter  10 value 27.704426
iter  20 value 4.448410
iter  30 value 1.608952
iter  40 value 0.896766
iter  50 value 0.572562
iter  60 value 0.479499
iter  70 value 0.440133
iter  80 value 0.402749
iter  90 value 0.378910
iter 100 value 0.363374
final  value 0.363374 
stopped after 100 iterations
# weights:  155
initial  value 956.298730 
iter  10 value 30.892636
iter  20 value 11.644700
iter  30 value 2.204284
iter  40 value 1.157768
iter  50 value 0.627930
iter  60 value 0.523221
iter  70 value 0.470840
iter  80 value 0.415100
iter  90 value 0.378878
iter 100 value 0.355484
final  value 0.355484 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.00902; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  34
initial  value 3344.076100 
iter  10 value 275.115244
iter  20 value 172.493820
iter  30 value 68.118713
iter  40 value 21.088922
iter  50 value 5.240108
iter  60 value 3.788131
iter  70 value 3.669869
iter  80 value 3.642968
iter  90 value 3.530476
iter 100 value 3.424745
final  value 3.424745 
stopped after 100 iterations
# weights:  34
initial  value 3568.760480 
iter  10 value 88.791540
iter  20 value 21.381156
iter  30 value 5.265524
iter  40 value 3.030152
iter  50 value 2.605552
iter  60 value 2.336706
iter  70 value 2.267469
iter  80 value 2.254334
iter  90 value 2.188575
iter 100 value 2.142689
final  value 2.142689 
stopped after 100 iterations
# weights:  34
initial  value 2459.128824 
iter  10 value 212.508390
iter  20 value 185.490023
iter  30 value 14.147998
iter  40 value 5.640086
iter  50 value 2.933798
iter  60 value 2.603301
iter  70 value 2.357742
iter  80 value 2.183205
iter  90 value 2.104180
iter 100 value 1.992609
final  value 1.992609 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0269; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  199
initial  value 1361.253429 
iter  10 value 7.111818
iter  20 value 1.260749
iter  30 value 0.689526
iter  40 value 0.437425
iter  50 value 0.313377
iter  60 value 0.275742
iter  70 value 0.257767
iter  80 value 0.245163
iter  90 value 0.236039
iter 100 value 0.230234
final  value 0.230234 
stopped after 100 iterations
# weights:  199
initial  value 2096.486724 
iter  10 value 15.951633
iter  20 value 2.796066
iter  30 value 1.318932
iter  40 value 0.804989
iter  50 value 0.590395
iter  60 value 0.526502
iter  70 value 0.473508
iter  80 value 0.438887
iter  90 value 0.411515
iter 100 value 0.379691
final  value 0.379691 
stopped after 100 iterations
# weights:  199
initial  value 874.241019 
iter  10 value 25.635958
iter  20 value 3.629667
iter  30 value 1.613376
iter  40 value 0.902028
iter  50 value 0.622744
iter  60 value 0.517189
iter  70 value 0.471424
iter  80 value 0.444203
iter  90 value 0.421954
iter 100 value 0.399725
final  value 0.399725 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0092; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  155
initial  value 4409.667596 
iter  10 value 43.039680
iter  20 value 3.717967
iter  30 value 1.122682
iter  40 value 0.673748
iter  50 value 0.528213
iter  60 value 0.476791
iter  70 value 0.436923
iter  80 value 0.376667
iter  90 value 0.332057
iter 100 value 0.292372
final  value 0.292372 
stopped after 100 iterations
# weights:  155
initial  value 508.895616 
iter  10 value 18.438871
iter  20 value 2.384866
iter  30 value 0.623126
iter  40 value 0.368948
iter  50 value 0.288726
iter  60 value 0.231791
iter  70 value 0.210105
iter  80 value 0.193008
iter  90 value 0.182878
iter 100 value 0.174357
final  value 0.174357 
stopped after 100 iterations
# weights:  155
initial  value 3198.694981 
iter  10 value 25.286803
iter  20 value 2.555893
iter  30 value 0.637575
iter  40 value 0.343791
iter  50 value 0.250724
iter  60 value 0.192697
iter  70 value 0.172522
iter  80 value 0.157632
iter  90 value 0.148900
iter 100 value 0.140001
final  value 0.140001 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00489; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  12
initial  value 901.435276 
iter  10 value 91.443044
iter  20 value 11.987149
iter  30 value 7.380577
iter  40 value 1.531114
iter  50 value 0.430933
iter  60 value 0.213318
iter  70 value 0.069163
iter  80 value 0.043189
iter  90 value 0.032389
iter 100 value 0.019212
final  value 0.019212 
stopped after 100 iterations
# weights:  12
initial  value 1067.483943 
iter  10 value 20.213512
iter  20 value 4.873918
iter  30 value 1.510440
iter  40 value 0.304874
iter  50 value 0.107059
iter  60 value 0.076875
iter  70 value 0.043234
iter  80 value 0.026061
iter  90 value 0.022342
iter 100 value 0.017154
final  value 0.017154 
stopped after 100 iterations
# weights:  12
initial  value 1862.117342 
iter  10 value 180.942503
iter  20 value 96.995223
iter  30 value 16.837305
iter  40 value 2.198386
iter  50 value 0.644103
iter  60 value 0.164697
iter  70 value 0.071361
iter  80 value 0.057785
iter  90 value 0.031005
iter 100 value 0.021911
final  value 0.021911 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00613; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  89
initial  value 687.736836 
iter  10 value 36.507411
iter  20 value 6.151767
iter  30 value 0.925844
iter  40 value 0.420072
iter  50 value 0.304184
iter  60 value 0.232471
iter  70 value 0.181458
iter  80 value 0.153690
iter  90 value 0.133839
iter 100 value 0.114882
final  value 0.114882 
stopped after 100 iterations
# weights:  89
initial  value 2000.986776 
iter  10 value 40.674524
iter  20 value 7.492982
iter  30 value 0.993921
iter  40 value 0.427956
iter  50 value 0.282319
iter  60 value 0.228322
iter  70 value 0.191017
iter  80 value 0.180242
iter  90 value 0.166160
iter 100 value 0.157397
final  value 0.157397 
stopped after 100 iterations
# weights:  89
initial  value 692.836160 
iter  10 value 72.523844
iter  20 value 20.841971
iter  30 value 5.980273
iter  40 value 1.718963
iter  50 value 0.841956
iter  60 value 0.484122
iter  70 value 0.400589
iter  80 value 0.335934
iter  90 value 0.310644
iter 100 value 0.282859
final  value 0.282859 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00658; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  45
initial  value 327.457985 
iter  10 value 37.519860
iter  20 value 6.806175
iter  30 value 3.753452
iter  40 value 3.069126
iter  50 value 2.706513
iter  60 value 2.472377
iter  70 value 2.252218
iter  80 value 2.223711
iter  90 value 2.199986
iter 100 value 2.183097
final  value 2.183097 
stopped after 100 iterations
# weights:  45
initial  value 3417.308362 
iter  10 value 120.456471
iter  20 value 32.231007
iter  30 value 9.982389
iter  40 value 5.046339
iter  50 value 4.159813
iter  60 value 3.568230
iter  70 value 2.675184
iter  80 value 2.515633
iter  90 value 2.355122
iter 100 value 2.290566
final  value 2.290566 
stopped after 100 iterations
# weights:  45
initial  value 2619.374906 
iter  10 value 41.344606
iter  20 value 12.663747
iter  30 value 4.446267
iter  40 value 2.517793
iter  50 value 2.322932
iter  60 value 2.244054
iter  70 value 2.201155
iter  80 value 2.130772
iter  90 value 2.100922
iter 100 value 2.092253
final  value 2.092253 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0218; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  34
initial  value 694.337195 
iter  10 value 36.763377
iter  20 value 10.158035
iter  30 value 1.063722
iter  40 value 0.357771
iter  50 value 0.226181
iter  60 value 0.214847
iter  70 value 0.203702
iter  80 value 0.199082
iter  90 value 0.181054
iter 100 value 0.160229
final  value 0.160229 
stopped after 100 iterations
# weights:  34
initial  value 1426.861813 
iter  10 value 161.564770
iter  20 value 78.317219
iter  30 value 21.716582
iter  40 value 3.958148
iter  50 value 0.901439
iter  60 value 0.220010
iter  70 value 0.123931
iter  80 value 0.063890
iter  90 value 0.047302
iter 100 value 0.041515
final  value 0.041515 
stopped after 100 iterations
# weights:  34
initial  value 2336.194381 
iter  10 value 21.504678
iter  20 value 1.742141
iter  30 value 0.335613
iter  40 value 0.141238
iter  50 value 0.047969
iter  60 value 0.031135
iter  70 value 0.020679
iter  80 value 0.016221
iter  90 value 0.010140
iter 100 value 0.007275
final  value 0.007275 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00528; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  67
initial  value 1910.754006 
iter  10 value 51.809264
iter  20 value 7.686705
iter  30 value 1.671676
iter  40 value 0.509129
iter  50 value 0.211504
iter  60 value 0.148632
iter  70 value 0.112554
iter  80 value 0.095735
iter  90 value 0.086555
iter 100 value 0.076056
final  value 0.076056 
stopped after 100 iterations
# weights:  67
initial  value 1194.793812 
iter  10 value 16.114933
iter  20 value 0.752548
iter  30 value 0.149308
iter  40 value 0.054335
iter  50 value 0.039247
iter  60 value 0.033738
iter  70 value 0.030144
iter  80 value 0.028170
iter  90 value 0.025956
iter 100 value 0.024008
final  value 0.024008 
stopped after 100 iterations
# weights:  67
initial  value 2801.292638 
iter  10 value 53.314208
iter  20 value 3.531660
iter  30 value 0.689122
iter  40 value 0.258865
iter  50 value 0.187506
iter  60 value 0.169768
iter  70 value 0.158181
iter  80 value 0.140609
iter  90 value 0.128647
iter 100 value 0.120992
final  value 0.120992 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00588; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  67
initial  value 368.751643 
iter  10 value 16.670471
iter  20 value 3.166658
iter  30 value 0.637786
iter  40 value 0.196828
iter  50 value 0.109807
iter  60 value 0.090969
iter  70 value 0.068643
iter  80 value 0.058647
iter  90 value 0.052419
iter 100 value 0.049542
final  value 0.049542 
stopped after 100 iterations
# weights:  67
initial  value 206.724342 
iter  10 value 31.952716
iter  20 value 3.244149
iter  30 value 0.387792
iter  40 value 0.128300
iter  50 value 0.081410
iter  60 value 0.066478
iter  70 value 0.059503
iter  80 value 0.053338
iter  90 value 0.048994
iter 100 value 0.043630
final  value 0.043630 
stopped after 100 iterations
# weights:  67
initial  value 2523.915154 
iter  10 value 69.346118
iter  20 value 4.176635
iter  30 value 0.632670
iter  40 value 0.197757
iter  50 value 0.175701
iter  60 value 0.164044
iter  70 value 0.148410
iter  80 value 0.127027
iter  90 value 0.112107
iter 100 value 0.090822
final  value 0.090822 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00585; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  177
initial  value 2833.997746 
iter  10 value 175.365254
iter  20 value 136.587241
iter  30 value 104.297983
iter  40 value 80.632681
iter  50 value 58.874827
iter  60 value 44.661450
iter  70 value 41.196494
iter  80 value 39.560712
iter  90 value 39.023799
iter 100 value 38.749265
final  value 38.749265 
stopped after 100 iterations
# weights:  177
initial  value 1086.882930 
iter  10 value 218.654123
iter  20 value 146.453237
iter  30 value 59.427409
iter  40 value 47.993328
iter  50 value 43.412657
iter  60 value 41.831744
iter  70 value 40.967355
iter  80 value 40.331746
iter  90 value 40.026396
iter 100 value 39.894762
final  value 39.894762 
stopped after 100 iterations
# weights:  177
initial  value 3168.984283 
iter  10 value 212.174766
iter  20 value 58.846779
iter  30 value 47.000139
iter  40 value 42.099090
iter  50 value 40.675245
iter  60 value 39.560410
iter  70 value 39.086903
iter  80 value 38.899374
iter  90 value 38.746619
iter 100 value 38.626215
final  value 38.626215 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  78
initial  value 532.620753 
iter  10 value 30.916762
iter  20 value 1.654775
iter  30 value 0.283721
iter  40 value 0.104834
iter  50 value 0.067265
iter  60 value 0.052933
iter  70 value 0.045300
iter  80 value 0.039974
iter  90 value 0.035449
iter 100 value 0.033837
final  value 0.033837 
stopped after 100 iterations
# weights:  78
initial  value 2515.839811 
iter  10 value 111.491716
iter  20 value 13.458005
iter  30 value 1.579797
iter  40 value 0.172416
iter  50 value 0.061179
iter  60 value 0.036782
iter  70 value 0.028085
iter  80 value 0.023008
iter  90 value 0.020598
iter 100 value 0.019103
final  value 0.019103 
stopped after 100 iterations
# weights:  78
initial  value 3454.895747 
iter  10 value 33.305176
iter  20 value 5.497034
iter  30 value 0.394783
iter  40 value 0.105933
iter  50 value 0.087541
iter  60 value 0.071584
iter  70 value 0.064642
iter  80 value 0.058192
iter  90 value 0.054208
iter 100 value 0.050900
final  value 0.050900 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00398; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  56
initial  value 1739.456349 
iter  10 value 35.955216
iter  20 value 4.087289
iter  30 value 2.907407
iter  40 value 2.458695
iter  50 value 1.905039
iter  60 value 1.377567
iter  70 value 1.162804
iter  80 value 1.122729
iter  90 value 1.099044
iter 100 value 1.077034
final  value 1.077034 
stopped after 100 iterations
# weights:  56
initial  value 1781.642472 
iter  10 value 20.578340
iter  20 value 4.022921
iter  30 value 1.735687
iter  40 value 1.205379
iter  50 value 1.034850
iter  60 value 0.945909
iter  70 value 0.919913
iter  80 value 0.904924
iter  90 value 0.898591
iter 100 value 0.895513
final  value 0.895513 
stopped after 100 iterations
# weights:  56
initial  value 757.897236 
iter  10 value 22.049365
iter  20 value 5.251267
iter  30 value 2.723952
iter  40 value 2.210905
iter  50 value 1.759985
iter  60 value 1.478490
iter  70 value 1.314492
iter  80 value 1.205977
iter  90 value 1.134039
iter 100 value 1.084744
final  value 1.084744 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0172; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  177
initial  value 683.612408 
iter  10 value 10.125634
iter  20 value 3.729177
iter  30 value 1.565639
iter  40 value 0.839596
iter  50 value 0.558351
iter  60 value 0.425306
iter  70 value 0.369287
iter  80 value 0.349182
iter  90 value 0.336680
iter 100 value 0.327441
final  value 0.327441 
stopped after 100 iterations
# weights:  177
initial  value 932.984095 
iter  10 value 38.613722
iter  20 value 7.255049
iter  30 value 1.979077
iter  40 value 1.234693
iter  50 value 0.923929
iter  60 value 0.805525
iter  70 value 0.713036
iter  80 value 0.630037
iter  90 value 0.574346
iter 100 value 0.507216
final  value 0.507216 
stopped after 100 iterations
# weights:  177
initial  value 880.516959 
iter  10 value 15.249037
iter  20 value 3.844261
iter  30 value 1.105801
iter  40 value 0.639579
iter  50 value 0.426166
iter  60 value 0.380207
iter  70 value 0.348550
iter  80 value 0.329504
iter  90 value 0.316260
iter 100 value 0.308259
final  value 0.308259 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00873; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  188
initial  value 2267.883145 
iter  10 value 10.917860
iter  20 value 1.897624
iter  30 value 0.252481
iter  40 value 0.039932
iter  50 value 0.022104
iter  60 value 0.014545
iter  70 value 0.011651
iter  80 value 0.010278
iter  90 value 0.009163
iter 100 value 0.008504
final  value 0.008504 
stopped after 100 iterations
# weights:  188
initial  value 5271.794211 
iter  10 value 42.050836
iter  20 value 9.707525
iter  30 value 3.069790
iter  40 value 0.352457
iter  50 value 0.095266
iter  60 value 0.033826
iter  70 value 0.023946
iter  80 value 0.019412
iter  90 value 0.016494
iter 100 value 0.013812
final  value 0.013812 
stopped after 100 iterations
# weights:  188
initial  value 1328.648812 
iter  10 value 13.219153
iter  20 value 2.121086
iter  30 value 0.345438
iter  40 value 0.152004
iter  50 value 0.099230
iter  60 value 0.074263
iter  70 value 0.065320
iter  80 value 0.062446
iter  90 value 0.060591
iter 100 value 0.059007
final  value 0.059007 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00289; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.00289
# weights:  188
initial  value 6312.354747 
iter  10 value 26.264903
iter  20 value 5.902366
iter  30 value 1.468389
iter  40 value 0.307202
iter  50 value 0.071146
iter  60 value 0.031992
iter  70 value 0.018857
iter  80 value 0.015470
iter  90 value 0.013818
iter 100 value 0.012888
final  value 0.012888 
stopped after 100 iterations
[1] "Fri Feb 09 15:04:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 601 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 365
 number of selected nodes                               : 90 
[1] "Fri Feb 09 15:04:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 15:04:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 15:04:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 15:04:35 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0332; time: 0.1 min
[Tune-x] 3: mtry=5; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0338; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0485; time: 0.0 min
[Tune-x] 5: mtry=7; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.036; time: 0.1 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0518; time: 0.0 min
[Tune-x] 8: mtry=8; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 9: mtry=7; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0452; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.033; time: 0.1 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0465; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.0328; time: 0.1 min
[Tune-x] 15: mtry=3; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0334; time: 0.1 min
[Tune-x] 16: mtry=8; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0408; time: 0.1 min
[Tune-x] 17: mtry=4; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0324; time: 0.1 min
[Tune-x] 18: mtry=3; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0353; time: 0.1 min
[Tune-x] 19: mtry=8; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0373; time: 0.1 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0357; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=3 : rmse.test.rmse=0.0324
[1] "Fri Feb 09 15:06:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 15:06:20 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.327; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.327; time: 0.0 min
[Tune] Result: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46 : rmse.test.rmse=0.119
[1] "Fri Feb 09 15:06:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 15:06:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:06:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.115121 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2308 
    Y 1          1.855 
    Y 2          3.131 
    Y 3          2.043 
    Y 4           1.88 
    Y 5          1.641 
[1] "Fri Feb 09 15:06:53 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
[Tune-y] 1: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
[Tune-y] 3: rmse.test.rmse=0.192; time: 0.5 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
[Tune-y] 4: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
[Tune-y] 5: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.2 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
[Tune-y] 8: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.262; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
[Tune-y] 10: rmse.test.rmse=0.959; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.196; time: 0.3 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
[Tune-y] 15: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.158; time: 0.8 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
[Tune-y] 18: rmse.test.rmse=0.212; time: 1.2 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
[Tune-y] 19: rmse.test.rmse=0.0758; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
[Tune-y] 20: rmse.test.rmse=0.387; time: 0.0 min
[Tune] Result: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386 : rmse.test.rmse=0.0758
[1] "Fri Feb 09 15:10:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 15:10:27 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.594666], n=(505,247)
**GROW** @depth 1: [4,0.550019], n=(179,66)
**GROW** @depth 1: [6,0.489538], n=(447,59)
**PRUNE** @depth 1: [6,0.492889]
**GROW** @depth 1: [1,0.316428], n=(205,299)
**GROW** @depth 1: [6,0.432497], n=(182,24)
**PRUNE** @depth 1: [6,0.432199]
**GROW** @depth 2: [3,0.4875], n=(79,215)
**GROW** @depth 3: [5,0.255448], n=(37,145)
**GROW** @depth 2: [4,0.380667], n=(179,28)
**GROW** @depth 3: [2,0.498822], n=(133,82)
**GROW** @depth 4: [3,0.554167], n=(19,126)
**GROW** @depth 2: [5,0.222214], n=(132,47)
**GROW** @depth 3: [6,0.294512], n=(68,16)
**PRUNE** @depth 3: [6,0.29682]
**GROW** @depth 4: [2,0.397173], n=(50,81)
**GROW** @depth 2: [1,0.79309], n=(38,28)
**GROW** @depth 3: [6,0.154442], n=(15,69)
**GROW** @depth 4: [2,0.262538], n=(31,16)
**GROW** @depth 5: [7,0.469042], n=(61,20)
**PRUNE** @depth 5: [7,0.465994]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(131,32,14,29,17,68,49,82,84,40,16,142,20,28)
**GROW** @depth 4: [1,0.678723], n=(54,88)
**GROW** @depth 4: [2,0.227701], n=(100,29)
**GROW** @depth 5: [1,0.149525], n=(52,48)
**GROW** @depth 5: [3,0.779167], n=(67,20)
**GROW** @depth 4: [9,0.507495], n=(56,16)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(51,49,22,30,24,27,18,57,17,44,83,86,39,15,53,62,29,20,26)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 4: [2,0.260855]
**GROW** @depth 5: [3,0.779167], n=(38,13)
**GROW** @depth 5: [7,0.626405], n=(61,22)
**PRUNE** @depth 5: [7,0.626405]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(52,17,55,51,27,19,61,19,40,83,85,40,14,39,13,63,30,20,24)
**GROW** @depth 2: [1,0.252778], n=(12,16)
**GROW** @depth 3: [3,0.504167], n=(41,12)
**PRUNE** @depth 4: [3,0.504167]
**GROW** @depth 4: [8,0.54722], n=(18,36)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(53,17,55,19,35,12,16,17,59,19,39,80,86,41,13,42,13,63,28,20,25)
**GROW** @depth 4: [3,0.684375], n=(53,34)
**GROW** @depth 5: [2,0.443958], n=(31,49)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(52,17,56,19,35,12,16,16,60,19,39,31,49,54,35,41,12,41,13,62,25,20,28)
**PRUNE** @depth 3: [8,0.546337]
**GROW** @depth 7: [3,0.682292], n=(34,29)
**GROW** @depth 7: [3,0.658333], n=(22,20)
**GROW** @depth 5: [5,0.0761606], n=(37,15)
**GROW** @depth 4: [8,0.496028], n=(30,26)
**PRUNE** @depth 7: [3,0.779167]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(36,16,17,30,25,54,12,16,19,58,19,37,34,49,53,34,41,13,21,33,31,31,28,20,25)
**GROW** @depth 3: [3,0.5], n=(43,12)
**PRUNE** @depth 3: [3,0.501042]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(37,16,16,29,25,58,12,18,17,57,20,36,33,47,53,35,40,14,20,33,32,31,28,20,25)
Grow: 9.855%, Prune: 2.95%, Change: 78.33%, Swap: 18.83%

[1] "Fri Feb 09 15:10:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 15:10:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 15:10:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.4109 	 alpha= 1.8551 	 beta= 37675.81 
[1] "Fri Feb 09 15:10:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 15:10:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [4,0.0243537], n=(18,734)
**PRUNE** @depth 0: [4,0.0224803]
**GROW** @depth 0: [8,0.423654], n=(112,640)
**GROW** @depth 1: [3,0.529167], n=(211,429)
r=1000 d=[0] [0] [0]; n=(112,212,428)
r=2000 d=[0] [0] [0]; n=(112,212,428)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0]; mh=3 n=(112,212,428)
**GROW** @depth 2: [9,0.45858], n=(87,341)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
r=3000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
**GROW** @depth 2: [4,0.341701], n=(192,20)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
Grow: 1.42%, Prune: 0.277%, Change: 17.16%, Swap: 0%

[1] "Fri Feb 09 15:10:46 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 15:10:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 15:10:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 15:10:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 15:10:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 15:10:50 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 15:10:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 15:10:51 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:10:54 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0435; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0439; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.0645; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0391; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0251; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.055; time: 0.1 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0535; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0417; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.046; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0621; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.0777; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0418; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.373; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0544; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0368; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.056; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0466; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0251
[1] "Fri Feb 09 15:11:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 15:11:18 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0678; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0226; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0256; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0315; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0428; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0149; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0844; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0555; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0334; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.483; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0143; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0272; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.106; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0143
[1] "Fri Feb 09 15:11:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:11:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=============                                                         |  18%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:11:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:11:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:11:41 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.898 11.884 1.92 5.338 27.442 26.858 10.498 2.266 10.14 
  - best initial criterion value(s) :  3328.615 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3328.6  |proj g|=       7.8814
At iterate     1  f =      -3340.1  |proj g|=        6.7544
At iterate     2  f =      -3398.2  |proj g|=        7.8537
At iterate     3  f =      -3435.6  |proj g|=        4.7662
At iterate     4  f =      -3460.8  |proj g|=        4.4083
At iterate     5  f =      -3467.3  |proj g|=        4.1755
At iterate     6  f =      -3470.3  |proj g|=        4.1391
At iterate     7  f =      -3481.4  |proj g|=        4.2088
At iterate     8  f =      -3486.9  |proj g|=        4.2328
At iterate     9  f =      -3488.9  |proj g|=        4.1909
At iterate    10  f =      -3494.8  |proj g|=        4.1931
At iterate    11  f =      -3496.4  |proj g|=        4.1637
At iterate    12  f =      -3498.1  |proj g|=        4.0611
At iterate    13  f =      -3500.2  |proj g|=        3.9477
At iterate    14  f =      -3504.3  |proj g|=        2.1749
At iterate    15  f =      -3508.6  |proj g|=        2.8156
At iterate    16  f =      -3509.3  |proj g|=        3.1922
At iterate    17  f =      -3509.4  |proj g|=        3.1865
At iterate    18  f =      -3509.9  |proj g|=        4.3269
At iterate    19  f =        -3510  |proj g|=        4.0227
At iterate    20  f =      -3510.2  |proj g|=        2.6501
At iterate    21  f =      -3511.7  |proj g|=        2.3562
At iterate    22  f =      -3512.6  |proj g|=        3.2352
At iterate    23  f =      -3513.1  |proj g|=        3.0041
At iterate    24  f =      -3514.1  |proj g|=        3.0241
At iterate    25  f =      -3521.7  |proj g|=        7.0185
At iterate    26  f =      -3525.8  |proj g|=        3.9145
At iterate    27  f =      -3527.5  |proj g|=        3.4709
At iterate    28  f =      -3528.2  |proj g|=        3.4542
At iterate    29  f =        -3529  |proj g|=        3.4147
At iterate    30  f =      -3529.2  |proj g|=        3.4077
At iterate    31  f =      -3531.5  |proj g|=        3.2684
At iterate    32  f =      -3538.3  |proj g|=         7.811
At iterate    33  f =      -3541.1  |proj g|=        2.8562
At iterate    34  f =      -3541.4  |proj g|=         3.201
At iterate    35  f =      -3541.5  |proj g|=        3.5902
At iterate    36  f =      -3541.7  |proj g|=        3.5338
At iterate    37  f =      -3542.2  |proj g|=        2.9428
At iterate    38  f =      -3542.6  |proj g|=        2.1265
At iterate    39  f =      -3542.7  |proj g|=        1.5167
At iterate    40  f =      -3542.9  |proj g|=        1.2604
At iterate    41  f =      -3542.9  |proj g|=        1.1795
At iterate    42  f =      -3542.9  |proj g|=        1.4852
At iterate    43  f =        -3543  |proj g|=        1.4883
At iterate    44  f =        -3543  |proj g|=        1.4913
At iterate    45  f =      -3543.1  |proj g|=        1.4924
At iterate    46  f =      -3543.3  |proj g|=        2.2509
At iterate    47  f =      -3543.5  |proj g|=        2.4122
At iterate    48  f =      -3543.6  |proj g|=        2.4062
At iterate    49  f =      -3543.8  |proj g|=        2.2959
At iterate    50  f =        -3544  |proj g|=        2.7622
At iterate    51  f =      -3544.2  |proj g|=        1.5037
At iterate    52  f =      -3544.2  |proj g|=        1.5299
At iterate    53  f =      -3544.3  |proj g|=        1.5253
At iterate    54  f =      -3544.3  |proj g|=        1.5126
At iterate    55  f =      -3544.3  |proj g|=       0.80832
At iterate    56  f =      -3544.3  |proj g|=        1.0986
At iterate    57  f =      -3544.4  |proj g|=       0.28595
At iterate    58  f =      -3544.4  |proj g|=       0.67442
At iterate    59  f =      -3544.4  |proj g|=       0.78913
At iterate    60  f =      -3544.4  |proj g|=        1.3157
At iterate    61  f =      -3544.4  |proj g|=       0.77041
At iterate    62  f =      -3544.4  |proj g|=         1.488
At iterate    63  f =      -3544.5  |proj g|=        1.4839
At iterate    64  f =      -3544.5  |proj g|=       0.92549
At iterate    65  f =      -3544.5  |proj g|=        1.3703
At iterate    66  f =      -3544.5  |proj g|=       0.78486
At iterate    67  f =      -3544.5  |proj g|=       0.45217
At iterate    68  f =      -3544.5  |proj g|=       0.42495
At iterate    69  f =      -3544.6  |proj g|=       0.33318
At iterate    70  f =      -3544.6  |proj g|=        1.4839
At iterate    71  f =      -3544.6  |proj g|=        1.1859
At iterate    72  f =      -3544.6  |proj g|=       0.55789
At iterate    73  f =      -3544.6  |proj g|=        0.6882
At iterate    74  f =      -3544.6  |proj g|=       0.69684
At iterate    75  f =      -3544.6  |proj g|=       0.56766
At iterate    76  f =      -3544.6  |proj g|=         1.482
At iterate    77  f =      -3544.7  |proj g|=        1.1465
At iterate    78  f =      -3544.7  |proj g|=       0.44315
At iterate    79  f =      -3544.7  |proj g|=       0.58944
At iterate    80  f =      -3544.7  |proj g|=       0.23528
At iterate    81  f =      -3544.7  |proj g|=       0.23733
At iterate    82  f =      -3544.7  |proj g|=       0.59329
At iterate    83  f =      -3544.7  |proj g|=       0.81767
At iterate    84  f =      -3544.7  |proj g|=       0.85629
At iterate    85  f =      -3544.7  |proj g|=       0.93306
At iterate    86  f =      -3544.8  |proj g|=       0.63527
At iterate    87  f =      -3544.8  |proj g|=       0.70299
At iterate    88  f =      -3544.8  |proj g|=        1.4776
At iterate    89  f =      -3544.8  |proj g|=       0.75748
At iterate    90  f =      -3544.8  |proj g|=        0.6528
At iterate    91  f =      -3544.8  |proj g|=       0.55715
At iterate    92  f =      -3544.8  |proj g|=       0.31567
At iterate    93  f =      -3544.8  |proj g|=       0.19976
At iterate    94  f =      -3544.8  |proj g|=        1.4375
At iterate    95  f =      -3544.8  |proj g|=        1.0246
At iterate    96  f =      -3544.8  |proj g|=       0.48397
At iterate    97  f =      -3544.8  |proj g|=       0.52294
At iterate    98  f =      -3544.8  |proj g|=       0.52864
At iterate    99  f =      -3544.9  |proj g|=       0.44264
At iterate   100  f =      -3544.9  |proj g|=        1.4798
At iterate   101  f =      -3544.9  |proj g|=        1.4476
final  value -3544.880687 
stopped after 101 iterations
[1] "Fri Feb 09 15:16:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 81.6381, its = 11
i = 2 (of 248), d = 50.4835, its = 11
i = 3 (of 248), d = 53.5942, its = 11
i = 4 (of 248), d = 122.859, its = 12
i = 5 (of 248), d = 85.1172, its = 12
i = 6 (of 248), d = 40.8853, its = 11
i = 7 (of 248), d = 88.4914, its = 11
i = 8 (of 248), d = 100.562, its = 11
i = 9 (of 248), d = 28.7216, its = 9
i = 10 (of 248), d = 107.053, its = 11
i = 11 (of 248), d = 37.9501, its = 10
i = 12 (of 248), d = 42.2289, its = 10
i = 13 (of 248), d = 63.3826, its = 11
i = 14 (of 248), d = 211.263, its = 12
i = 15 (of 248), d = 49.6758, its = 10
i = 16 (of 248), d = 60.4139, its = 11
i = 17 (of 248), d = 43.0261, its = 10
i = 18 (of 248), d = 76.0365, its = 11
i = 19 (of 248), d = 20.3274, its = 9
i = 20 (of 248), d = 67.6309, its = 11
i = 21 (of 248), d = 95.7672, its = 11
i = 22 (of 248), d = 29.5729, its = 10
i = 23 (of 248), d = 74.2623, its = 12
i = 24 (of 248), d = 153.261, its = 12
i = 25 (of 248), d = 107.241, its = 12
i = 26 (of 248), d = 126.005, its = 11
i = 27 (of 248), d = 61.6181, its = 10
i = 28 (of 248), d = 31.829, its = 10
i = 29 (of 248), d = 153.888, its = 13
i = 30 (of 248), d = 78.6486, its = 11
i = 31 (of 248), d = 84.493, its = 11
i = 32 (of 248), d = 70.9572, its = 11
i = 33 (of 248), d = 124.829, its = 12
i = 34 (of 248), d = 27.2667, its = 10
i = 35 (of 248), d = 126.943, its = 12
i = 36 (of 248), d = 123.006, its = 11
i = 37 (of 248), d = 86.5268, its = 11
i = 38 (of 248), d = 93.8513, its = 11
i = 39 (of 248), d = 34.6424, its = 10
i = 40 (of 248), d = 32.4118, its = 9
i = 41 (of 248), d = 59.4162, its = 11
i = 42 (of 248), d = 22.2584, its = 9
i = 43 (of 248), d = 57.314, its = 10
i = 44 (of 248), d = 44.9622, its = 10
i = 45 (of 248), d = 89.8757, its = 11
i = 46 (of 248), d = 49.8736, its = 10
i = 47 (of 248), d = 38.2174, its = 10
i = 48 (of 248), d = 19.8057, its = 9
i = 49 (of 248), d = 49.6311, its = 10
i = 50 (of 248), d = 33.8076, its = 10
i = 51 (of 248), d = 156.372, its = 12
i = 52 (of 248), d = 57.5233, its = 11
i = 53 (of 248), d = 121.299, its = 11
i = 54 (of 248), d = 46.6802, its = 10
i = 55 (of 248), d = 120.731, its = 12
i = 56 (of 248), d = 21.4573, its = 9
i = 57 (of 248), d = 137.217, its = 13
i = 58 (of 248), d = 90.9739, its = 12
i = 59 (of 248), d = 68.8682, its = 11
i = 60 (of 248), d = 109.753, its = 12
i = 61 (of 248), d = 143.46, its = 13
i = 62 (of 248), d = 94.4793, its = 11
i = 63 (of 248), d = 113.162, its = 11
i = 64 (of 248), d = 83.9198, its = 11
i = 65 (of 248), d = 26.2431, its = 9
i = 66 (of 248), d = 125.068, its = 11
i = 67 (of 248), d = 19.5886, its = 9
i = 68 (of 248), d = 68.4551, its = 11
i = 69 (of 248), d = 32.363, its = 9
i = 70 (of 248), d = 17.375, its = 9
i = 71 (of 248), d = 156.267, its = 13
i = 72 (of 248), d = 96.4112, its = 11
i = 73 (of 248), d = 41.77, its = 10
i = 74 (of 248), d = 85.3034, its = 11
i = 75 (of 248), d = 42.2964, its = 10
i = 76 (of 248), d = 115.922, its = 12
i = 77 (of 248), d = 45.0567, its = 10
i = 78 (of 248), d = 74.683, its = 11
i = 79 (of 248), d = 69.6093, its = 11
i = 80 (of 248), d = 83.3347, its = 11
i = 81 (of 248), d = 35.6791, its = 10
i = 82 (of 248), d = 32.7981, its = 10
i = 83 (of 248), d = 73.8158, its = 11
i = 84 (of 248), d = 96.1281, its = 11
i = 85 (of 248), d = 71.8901, its = 11
i = 86 (of 248), d = 28.1677, its = 9
i = 87 (of 248), d = 55.9643, its = 11
i = 88 (of 248), d = 48.9644, its = 11
i = 89 (of 248), d = 53.4303, its = 10
i = 90 (of 248), d = 92.1895, its = 11
i = 91 (of 248), d = 114.369, its = 12
i = 92 (of 248), d = 54.2139, its = 11
i = 93 (of 248), d = 50.4993, its = 10
i = 94 (of 248), d = 108.794, its = 11
i = 95 (of 248), d = 74.9373, its = 11
i = 96 (of 248), d = 17.3664, its = 9
i = 97 (of 248), d = 53.5812, its = 10
i = 98 (of 248), d = 34.5303, its = 10
i = 99 (of 248), d = 41.3775, its = 11
i = 100 (of 248), d = 25.2923, its = 9
i = 101 (of 248), d = 21.1352, its = 9
i = 102 (of 248), d = 45.4291, its = 10
i = 103 (of 248), d = 19.1583, its = 9
i = 104 (of 248), d = 43.337, its = 11
i = 105 (of 248), d = 79.7952, its = 11
i = 106 (of 248), d = 75.669, its = 11
i = 107 (of 248), d = 105.899, its = 11
i = 108 (of 248), d = 40.2151, its = 10
i = 109 (of 248), d = 124.589, its = 12
i = 110 (of 248), d = 26.967, its = 9
i = 111 (of 248), d = 24.8384, its = 9
i = 112 (of 248), d = 144.982, its = 12
i = 113 (of 248), d = 64.592, its = 11
i = 114 (of 248), d = 34.2722, its = 10
i = 115 (of 248), d = 20.8262, its = 10
i = 116 (of 248), d = 95.4175, its = 12
i = 117 (of 248), d = 42.7777, its = 10
i = 118 (of 248), d = 67.4093, its = 10
i = 119 (of 248), d = 160.209, its = 13
i = 120 (of 248), d = 70.5525, its = 11
i = 121 (of 248), d = 46.5869, its = 11
i = 122 (of 248), d = 130.863, its = 11
i = 123 (of 248), d = 67.4338, its = 11
i = 124 (of 248), d = 89.1165, its = 11
i = 125 (of 248), d = 158.681, its = 13
i = 126 (of 248), d = 54.6375, its = 11
i = 127 (of 248), d = 39.9938, its = 10
i = 128 (of 248), d = 25.499, its = 10
i = 129 (of 248), d = 84.9714, its = 11
i = 130 (of 248), d = 181.942, its = 12
i = 131 (of 248), d = 103.082, its = 11
i = 132 (of 248), d = 69.6513, its = 11
i = 133 (of 248), d = 118.543, its = 11
i = 134 (of 248), d = 23.4147, its = 9
i = 135 (of 248), d = 51.1434, its = 10
i = 136 (of 248), d = 119.697, its = 11
i = 137 (of 248), d = 107.513, its = 11
i = 138 (of 248), d = 71.958, its = 11
i = 139 (of 248), d = 70.734, its = 11
i = 140 (of 248), d = 118.219, its = 11
i = 141 (of 248), d = 109.166, its = 12
i = 142 (of 248), d = 131.509, its = 11
i = 143 (of 248), d = 152.703, its = 13
i = 144 (of 248), d = 111.201, its = 11
i = 145 (of 248), d = 35.9442, its = 10
i = 146 (of 248), d = 73.7117, its = 11
i = 147 (of 248), d = 67.7912, its = 11
i = 148 (of 248), d = 144.357, its = 12
i = 149 (of 248), d = 67.5248, its = 11
i = 150 (of 248), d = 119.162, its = 13
i = 151 (of 248), d = 31.7838, its = 9
i = 152 (of 248), d = 119.782, its = 11
i = 153 (of 248), d = 100.789, its = 11
i = 154 (of 248), d = 53.3399, its = 11
i = 155 (of 248), d = 92.1972, its = 12
i = 156 (of 248), d = 96.9205, its = 11
i = 157 (of 248), d = 62.7226, its = 11
i = 158 (of 248), d = 96.3754, its = 11
i = 159 (of 248), d = 17.0282, its = 9
i = 160 (of 248), d = 50.6264, its = 10
i = 161 (of 248), d = 111.173, its = 12
i = 162 (of 248), d = 40.6053, its = 10
i = 163 (of 248), d = 137.811, its = 12
i = 164 (of 248), d = 95.1407, its = 12
i = 165 (of 248), d = 64.3601, its = 11
i = 166 (of 248), d = 39.709, its = 10
i = 167 (of 248), d = 68.5079, its = 11
i = 168 (of 248), d = 41.163, its = 10
i = 169 (of 248), d = 28.7441, its = 10
i = 170 (of 248), d = 137.169, its = 12
i = 171 (of 248), d = 65.1753, its = 11
i = 172 (of 248), d = 110.921, its = 13
i = 173 (of 248), d = 247.727, its = 15
i = 174 (of 248), d = 44.7524, its = 10
i = 175 (of 248), d = 141.238, its = 12
i = 176 (of 248), d = 141.581, its = 12
i = 177 (of 248), d = 58.5192, its = 11
i = 178 (of 248), d = 32.7251, its = 9
i = 179 (of 248), d = 45.1026, its = 11
i = 180 (of 248), d = 88.0771, its = 11
i = 181 (of 248), d = 70.4445, its = 11
i = 182 (of 248), d = 41.9674, its = 10
i = 183 (of 248), d = 167.955, its = 13
i = 184 (of 248), d = 94.3134, its = 12
i = 185 (of 248), d = 62.4957, its = 11
i = 186 (of 248), d = 36.2491, its = 10
i = 187 (of 248), d = 122.821, its = 12
i = 188 (of 248), d = 92.4164, its = 12
i = 189 (of 248), d = 49.2627, its = 11
i = 190 (of 248), d = 130.051, its = 12
i = 191 (of 248), d = 22.8844, its = 9
i = 192 (of 248), d = 35.5348, its = 10
i = 193 (of 248), d = 36.829, its = 10
i = 194 (of 248), d = 92.8097, its = 12
i = 195 (of 248), d = 21.9829, its = 9
i = 196 (of 248), d = 37.3593, its = 11
i = 197 (of 248), d = 32.4234, its = 10
i = 198 (of 248), d = 97.5475, its = 11
i = 199 (of 248), d = 63.2311, its = 11
i = 200 (of 248), d = 65.9531, its = 11
i = 201 (of 248), d = 21.3636, its = 9
i = 202 (of 248), d = 99.1268, its = 12
i = 203 (of 248), d = 63.3029, its = 10
i = 204 (of 248), d = 25.7021, its = 10
i = 205 (of 248), d = 84.1015, its = 11
i = 206 (of 248), d = 25.0211, its = 9
i = 207 (of 248), d = 111.012, its = 12
i = 208 (of 248), d = 41.877, its = 10
i = 209 (of 248), d = 136.782, its = 12
i = 210 (of 248), d = 111.164, its = 12
i = 211 (of 248), d = 23.004, its = 9
i = 212 (of 248), d = 44.3854, its = 10
i = 213 (of 248), d = 106.141, its = 11
i = 214 (of 248), d = 45.9094, its = 10
i = 215 (of 248), d = 61.8428, its = 11
i = 216 (of 248), d = 35.7859, its = 10
i = 217 (of 248), d = 28.4618, its = 9
i = 218 (of 248), d = 79.8788, its = 12
i = 219 (of 248), d = 16.6131, its = 8
i = 220 (of 248), d = 69.3035, its = 11
i = 221 (of 248), d = 55.3285, its = 10
i = 222 (of 248), d = 118.965, its = 11
i = 223 (of 248), d = 102.234, its = 11
i = 224 (of 248), d = 81.7287, its = 11
i = 225 (of 248), d = 94.0626, its = 11
i = 226 (of 248), d = 41.4353, its = 10
i = 227 (of 248), d = 138.562, its = 11
i = 228 (of 248), d = 103.526, its = 12
i = 229 (of 248), d = 127.687, its = 11
i = 230 (of 248), d = 18.2559, its = 9
i = 231 (of 248), d = 87.0673, its = 11
i = 232 (of 248), d = 29.3141, its = 10
i = 233 (of 248), d = 128.234, its = 11
i = 234 (of 248), d = 70.6027, its = 11
i = 235 (of 248), d = 19.0193, its = 9
i = 236 (of 248), d = 59.1825, its = 11
i = 237 (of 248), d = 36.401, its = 10
i = 238 (of 248), d = 56.2294, its = 11
i = 239 (of 248), d = 50.2586, its = 11
i = 240 (of 248), d = 92.1659, its = 11
i = 241 (of 248), d = 40.8583, its = 10
i = 242 (of 248), d = 68.093, its = 11
i = 243 (of 248), d = 69.1133, its = 11
i = 244 (of 248), d = 77.5975, its = 11
i = 245 (of 248), d = 38.4981, its = 10
i = 246 (of 248), d = 188.831, its = 12
i = 247 (of 248), d = 57.716, its = 11
i = 248 (of 248), d = 84.6372, its = 11
[1] "Fri Feb 09 15:17:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 15:17:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 15:17:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 15:17:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 15:17:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 15:17:41 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 2543.484541 
iter  10 value 12.067619
iter  20 value 1.449677
iter  30 value 0.458507
iter  40 value 0.258877
iter  50 value 0.189027
iter  60 value 0.140282
iter  70 value 0.118401
iter  80 value 0.109604
iter  90 value 0.105634
iter 100 value 0.102051
final  value 0.102051 
stopped after 100 iterations
# weights:  210
initial  value 813.324229 
iter  10 value 6.713533
iter  20 value 1.393011
iter  30 value 0.530417
iter  40 value 0.295328
iter  50 value 0.209775
iter  60 value 0.157097
iter  70 value 0.126951
iter  80 value 0.118305
iter  90 value 0.112587
iter 100 value 0.109138
final  value 0.109138 
stopped after 100 iterations
# weights:  210
initial  value 553.331027 
iter  10 value 13.211015
iter  20 value 1.195954
iter  30 value 0.438350
iter  40 value 0.309172
iter  50 value 0.242162
iter  60 value 0.195939
iter  70 value 0.176528
iter  80 value 0.166150
iter  90 value 0.156925
iter 100 value 0.149163
final  value 0.149163 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00463; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 308.452632 
iter  10 value 9.296977
iter  20 value 1.673860
iter  30 value 0.222488
iter  40 value 0.060094
iter  50 value 0.029802
iter  60 value 0.022999
iter  70 value 0.018192
iter  80 value 0.016663
iter  90 value 0.015708
iter 100 value 0.014424
final  value 0.014424 
stopped after 100 iterations
# weights:  144
initial  value 6910.333001 
iter  10 value 36.852088
iter  20 value 5.655185
iter  30 value 0.480044
iter  40 value 0.139952
iter  50 value 0.072200
iter  60 value 0.045339
iter  70 value 0.036111
iter  80 value 0.028733
iter  90 value 0.025161
iter 100 value 0.021826
final  value 0.021826 
stopped after 100 iterations
# weights:  144
initial  value 1747.404566 
iter  10 value 81.933792
iter  20 value 20.771334
iter  30 value 3.260150
iter  40 value 0.572280
iter  50 value 0.264238
iter  60 value 0.175164
iter  70 value 0.139502
iter  80 value 0.120616
iter  90 value 0.102525
iter 100 value 0.089978
final  value 0.089978 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00497; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 640.123230 
iter  10 value 20.793671
iter  20 value 3.277362
iter  30 value 0.635231
iter  40 value 0.122760
iter  50 value 0.043086
iter  60 value 0.029573
iter  70 value 0.022930
iter  80 value 0.020675
iter  90 value 0.017852
iter 100 value 0.016722
final  value 0.016722 
stopped after 100 iterations
# weights:  133
initial  value 6722.932607 
iter  10 value 46.862906
iter  20 value 7.280622
iter  30 value 0.444243
iter  40 value 0.157630
iter  50 value 0.054426
iter  60 value 0.035143
iter  70 value 0.025247
iter  80 value 0.021686
iter  90 value 0.018049
iter 100 value 0.016688
final  value 0.016688 
stopped after 100 iterations
# weights:  133
initial  value 1886.239490 
iter  10 value 63.443629
iter  20 value 28.006690
iter  30 value 3.689073
iter  40 value 0.792477
iter  50 value 0.302340
iter  60 value 0.141450
iter  70 value 0.073584
iter  80 value 0.054391
iter  90 value 0.046808
iter 100 value 0.040990
final  value 0.040990 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00309; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 598.899214 
iter  10 value 25.194309
iter  20 value 1.730931
iter  30 value 0.413256
iter  40 value 0.201173
iter  50 value 0.165744
iter  60 value 0.142449
iter  70 value 0.109599
iter  80 value 0.088646
iter  90 value 0.080979
iter 100 value 0.080279
final  value 0.080279 
stopped after 100 iterations
# weights:  23
initial  value 830.918577 
iter  10 value 137.258589
iter  20 value 45.256428
iter  30 value 24.199743
iter  40 value 6.394196
iter  50 value 2.856434
iter  60 value 0.972731
iter  70 value 0.383890
iter  80 value 0.257693
iter  90 value 0.225113
iter 100 value 0.218960
final  value 0.218960 
stopped after 100 iterations
# weights:  23
initial  value 1865.663082 
iter  10 value 57.332816
iter  20 value 3.960219
iter  30 value 0.700403
iter  40 value 0.399667
iter  50 value 0.286195
iter  60 value 0.220176
iter  70 value 0.175747
iter  80 value 0.152174
iter  90 value 0.139471
iter 100 value 0.136224
final  value 0.136224 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.00751; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 293.403010 
iter  10 value 23.081515
iter  20 value 3.382767
iter  30 value 1.025342
iter  40 value 0.602894
iter  50 value 0.426807
iter  60 value 0.361710
iter  70 value 0.327529
iter  80 value 0.298512
iter  90 value 0.272365
iter 100 value 0.257786
final  value 0.257786 
stopped after 100 iterations
# weights:  210
initial  value 2086.177955 
iter  10 value 16.645414
iter  20 value 4.658501
iter  30 value 1.487783
iter  40 value 0.764106
iter  50 value 0.496057
iter  60 value 0.370216
iter  70 value 0.333305
iter  80 value 0.308826
iter  90 value 0.292443
iter 100 value 0.279662
final  value 0.279662 
stopped after 100 iterations
# weights:  210
initial  value 672.883463 
iter  10 value 11.027668
iter  20 value 2.648454
iter  30 value 0.845734
iter  40 value 0.543418
iter  50 value 0.371458
iter  60 value 0.325069
iter  70 value 0.302650
iter  80 value 0.280175
iter  90 value 0.262494
iter 100 value 0.252190
final  value 0.252190 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00686; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 5233.867158 
iter  10 value 51.945830
iter  20 value 18.306633
iter  30 value 13.758725
iter  40 value 12.160336
iter  50 value 10.815236
iter  60 value 9.841766
iter  70 value 9.462043
iter  80 value 9.142112
iter  90 value 8.975765
iter 100 value 8.937460
final  value 8.937460 
stopped after 100 iterations
# weights:  199
initial  value 818.326742 
iter  10 value 59.354831
iter  20 value 23.911641
iter  30 value 17.925420
iter  40 value 14.644672
iter  50 value 11.787656
iter  60 value 10.470600
iter  70 value 9.922330
iter  80 value 9.672140
iter  90 value 9.510570
iter 100 value 9.370019
final  value 9.370019 
stopped after 100 iterations
# weights:  199
initial  value 2108.146636 
iter  10 value 206.948506
iter  20 value 155.752447
iter  30 value 134.470903
iter  40 value 100.619564
iter  50 value 54.288637
iter  60 value 27.354347
iter  70 value 20.780023
iter  80 value 14.455907
iter  90 value 12.432157
iter 100 value 11.036605
final  value 11.036605 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0663; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 1872.350783 
iter  10 value 20.589154
iter  20 value 1.352590
iter  30 value 0.471146
iter  40 value 0.355103
iter  50 value 0.317765
iter  60 value 0.300779
iter  70 value 0.287336
iter  80 value 0.274283
iter  90 value 0.264631
iter 100 value 0.244282
final  value 0.244282 
stopped after 100 iterations
# weights:  144
initial  value 1483.905007 
iter  10 value 55.705682
iter  20 value 13.913587
iter  30 value 2.425611
iter  40 value 0.783706
iter  50 value 0.407474
iter  60 value 0.329849
iter  70 value 0.272490
iter  80 value 0.237060
iter  90 value 0.205862
iter 100 value 0.173622
final  value 0.173622 
stopped after 100 iterations
# weights:  144
initial  value 2142.134178 
iter  10 value 21.271809
iter  20 value 3.420112
iter  30 value 0.633158
iter  40 value 0.135450
iter  50 value 0.069217
iter  60 value 0.055148
iter  70 value 0.046325
iter  80 value 0.040256
iter  90 value 0.036037
iter 100 value 0.033314
final  value 0.033314 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00734; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 2998.010327 
iter  10 value 41.102744
iter  20 value 14.372036
iter  30 value 2.811739
iter  40 value 0.581095
iter  50 value 0.320959
iter  60 value 0.188371
iter  70 value 0.148823
iter  80 value 0.125617
iter  90 value 0.101837
iter 100 value 0.093969
final  value 0.093969 
stopped after 100 iterations
# weights:  89
initial  value 1397.102741 
iter  10 value 66.357744
iter  20 value 9.551179
iter  30 value 1.510497
iter  40 value 0.355115
iter  50 value 0.187488
iter  60 value 0.122849
iter  70 value 0.085334
iter  80 value 0.070674
iter  90 value 0.065933
iter 100 value 0.061935
final  value 0.061935 
stopped after 100 iterations
# weights:  89
initial  value 3045.598422 
iter  10 value 18.282584
iter  20 value 3.611159
iter  30 value 0.628419
iter  40 value 0.171803
iter  50 value 0.109853
iter  60 value 0.088855
iter  70 value 0.068648
iter  80 value 0.060009
iter  90 value 0.053994
iter 100 value 0.049491
final  value 0.049491 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00387; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 2443.200252 
iter  10 value 56.720549
iter  20 value 4.522461
iter  30 value 0.873640
iter  40 value 0.547720
iter  50 value 0.440033
iter  60 value 0.373587
iter  70 value 0.301960
iter  80 value 0.244984
iter  90 value 0.218735
iter 100 value 0.185924
final  value 0.185924 
stopped after 100 iterations
# weights:  210
initial  value 3118.848635 
iter  10 value 25.551786
iter  20 value 2.784568
iter  30 value 0.519225
iter  40 value 0.188712
iter  50 value 0.140307
iter  60 value 0.108623
iter  70 value 0.086591
iter  80 value 0.072857
iter  90 value 0.068458
iter 100 value 0.063999
final  value 0.063999 
stopped after 100 iterations
# weights:  210
initial  value 310.687596 
iter  10 value 14.670294
iter  20 value 2.939559
iter  30 value 0.423727
iter  40 value 0.160553
iter  50 value 0.101801
iter  60 value 0.082294
iter  70 value 0.064415
iter  80 value 0.057339
iter  90 value 0.051564
iter 100 value 0.047403
final  value 0.047403 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00466; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 3137.631093 
iter  10 value 29.361503
iter  20 value 4.223758
iter  30 value 0.366907
iter  40 value 0.076931
iter  50 value 0.053183
iter  60 value 0.037220
iter  70 value 0.028120
iter  80 value 0.019975
iter  90 value 0.014731
iter 100 value 0.012252
final  value 0.012252 
stopped after 100 iterations
# weights:  56
initial  value 2397.342722 
iter  10 value 61.333094
iter  20 value 9.919273
iter  30 value 2.056787
iter  40 value 0.572479
iter  50 value 0.183901
iter  60 value 0.082299
iter  70 value 0.047447
iter  80 value 0.028811
iter  90 value 0.022789
iter 100 value 0.021172
final  value 0.021172 
stopped after 100 iterations
# weights:  56
initial  value 2188.483715 
iter  10 value 30.048571
iter  20 value 2.726216
iter  30 value 0.583973
iter  40 value 0.168426
iter  50 value 0.063196
iter  60 value 0.041094
iter  70 value 0.026028
iter  80 value 0.017760
iter  90 value 0.013409
iter 100 value 0.008569
final  value 0.008569 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00491; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 2856.050265 
iter  10 value 114.444159
iter  20 value 55.787513
iter  30 value 33.464858
iter  40 value 24.665426
iter  50 value 21.826794
iter  60 value 18.989135
iter  70 value 14.747343
iter  80 value 11.972069
iter  90 value 9.505293
iter 100 value 8.869169
final  value 8.869169 
stopped after 100 iterations
# weights:  199
initial  value 1774.976045 
iter  10 value 54.725727
iter  20 value 23.656840
iter  30 value 12.687424
iter  40 value 10.293064
iter  50 value 8.750562
iter  60 value 8.287927
iter  70 value 7.998771
iter  80 value 7.839674
iter  90 value 7.721450
iter 100 value 7.627767
final  value 7.627767 
stopped after 100 iterations
# weights:  199
initial  value 280.522469 
iter  10 value 42.994491
iter  20 value 16.708404
iter  30 value 12.206404
iter  40 value 10.210419
iter  50 value 9.233679
iter  60 value 8.538285
iter  70 value 8.122363
iter  80 value 7.730598
iter  90 value 7.572702
iter 100 value 7.499406
final  value 7.499406 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.062; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 637.027601 
iter  10 value 46.170123
iter  20 value 21.330540
iter  30 value 13.177239
iter  40 value 11.403893
iter  50 value 10.436990
iter  60 value 9.918260
iter  70 value 9.545151
iter  80 value 9.215486
iter  90 value 8.808993
iter 100 value 8.620048
final  value 8.620048 
stopped after 100 iterations
# weights:  122
initial  value 1343.075721 
iter  10 value 41.023207
iter  20 value 20.670698
iter  30 value 15.723159
iter  40 value 12.839412
iter  50 value 11.764107
iter  60 value 10.851768
iter  70 value 9.996229
iter  80 value 9.195876
iter  90 value 8.531951
iter 100 value 8.209945
final  value 8.209945 
stopped after 100 iterations
# weights:  122
initial  value 5329.328466 
iter  10 value 291.574058
iter  20 value 70.919911
iter  30 value 26.964556
iter  40 value 19.502556
iter  50 value 16.475684
iter  60 value 14.961146
iter  70 value 13.569156
iter  80 value 12.285466
iter  90 value 11.125163
iter 100 value 10.656649
final  value 10.656649 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0643; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 3149.302140 
iter  10 value 34.628464
iter  20 value 10.068225
iter  30 value 2.821098
iter  40 value 2.036383
iter  50 value 1.615466
iter  60 value 1.359294
iter  70 value 1.049959
iter  80 value 0.844009
iter  90 value 0.741874
iter 100 value 0.683613
final  value 0.683613 
stopped after 100 iterations
# weights:  56
initial  value 601.709563 
iter  10 value 95.397082
iter  20 value 18.439859
iter  30 value 12.265603
iter  40 value 8.824025
iter  50 value 4.800147
iter  60 value 2.950865
iter  70 value 1.637809
iter  80 value 1.094973
iter  90 value 0.869752
iter 100 value 0.709190
final  value 0.709190 
stopped after 100 iterations
# weights:  56
initial  value 1241.892560 
iter  10 value 58.315950
iter  20 value 19.483580
iter  30 value 6.031265
iter  40 value 3.374827
iter  50 value 2.333127
iter  60 value 1.831011
iter  70 value 1.358034
iter  80 value 1.031012
iter  90 value 0.918938
iter 100 value 0.876674
final  value 0.876674 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0124; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 369.979848 
iter  10 value 10.451916
iter  20 value 2.223334
iter  30 value 0.662553
iter  40 value 0.371023
iter  50 value 0.273608
iter  60 value 0.201645
iter  70 value 0.163375
iter  80 value 0.144422
iter  90 value 0.133567
iter 100 value 0.128020
final  value 0.128020 
stopped after 100 iterations
# weights:  144
initial  value 1350.820036 
iter  10 value 27.560982
iter  20 value 2.727714
iter  30 value 0.647322
iter  40 value 0.451568
iter  50 value 0.354496
iter  60 value 0.315140
iter  70 value 0.279887
iter  80 value 0.255064
iter  90 value 0.240329
iter 100 value 0.225107
final  value 0.225107 
stopped after 100 iterations
# weights:  144
initial  value 1807.350673 
iter  10 value 41.844261
iter  20 value 10.726076
iter  30 value 1.491593
iter  40 value 0.503768
iter  50 value 0.284786
iter  60 value 0.219878
iter  70 value 0.169013
iter  80 value 0.135866
iter  90 value 0.121085
iter 100 value 0.112582
final  value 0.112582 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00556; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 1878.480939 
iter  10 value 79.959950
iter  20 value 31.803180
iter  30 value 9.645332
iter  40 value 3.418266
iter  50 value 1.992778
iter  60 value 1.585467
iter  70 value 1.366546
iter  80 value 1.179967
iter  90 value 1.083423
iter 100 value 1.027514
final  value 1.027514 
stopped after 100 iterations
# weights:  122
initial  value 1616.809533 
iter  10 value 27.065885
iter  20 value 5.080728
iter  30 value 2.588934
iter  40 value 2.064002
iter  50 value 1.829511
iter  60 value 1.671918
iter  70 value 1.441309
iter  80 value 1.292627
iter  90 value 1.186532
iter 100 value 1.132477
final  value 1.132477 
stopped after 100 iterations
# weights:  122
initial  value 188.029690 
iter  10 value 12.824519
iter  20 value 3.187587
iter  30 value 1.725936
iter  40 value 1.349429
iter  50 value 1.165037
iter  60 value 1.078348
iter  70 value 1.039255
iter  80 value 1.018260
iter  90 value 1.005968
iter 100 value 0.995528
final  value 0.995528 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.015; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 2232.231629 
iter  10 value 65.416617
iter  20 value 17.479683
iter  30 value 3.181380
iter  40 value 0.672082
iter  50 value 0.450487
iter  60 value 0.326297
iter  70 value 0.262990
iter  80 value 0.225356
iter  90 value 0.212973
iter 100 value 0.201596
final  value 0.201596 
stopped after 100 iterations
# weights:  100
initial  value 1639.148156 
iter  10 value 17.790156
iter  20 value 5.636315
iter  30 value 0.843515
iter  40 value 0.158557
iter  50 value 0.106178
iter  60 value 0.084400
iter  70 value 0.072228
iter  80 value 0.063063
iter  90 value 0.059522
iter 100 value 0.057951
final  value 0.057951 
stopped after 100 iterations
# weights:  100
initial  value 269.569061 
iter  10 value 12.739542
iter  20 value 1.138631
iter  30 value 0.166514
iter  40 value 0.088689
iter  50 value 0.068531
iter  60 value 0.060090
iter  70 value 0.054517
iter  80 value 0.050716
iter  90 value 0.049306
iter 100 value 0.047647
final  value 0.047647 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0043; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 1380.409632 
iter  10 value 72.739213
iter  20 value 25.693224
iter  30 value 18.624003
iter  40 value 15.484713
iter  50 value 13.483779
iter  60 value 12.441081
iter  70 value 11.894107
iter  80 value 11.505200
iter  90 value 11.229781
iter 100 value 11.057624
final  value 11.057624 
stopped after 100 iterations
# weights:  56
initial  value 1587.024979 
iter  10 value 72.153790
iter  20 value 40.939361
iter  30 value 17.921107
iter  40 value 14.002948
iter  50 value 13.225989
iter  60 value 12.983609
iter  70 value 12.909839
iter  80 value 12.882775
iter  90 value 12.869110
iter 100 value 12.866588
final  value 12.866588 
stopped after 100 iterations
# weights:  56
initial  value 2044.760813 
iter  10 value 83.701804
iter  20 value 22.375939
iter  30 value 15.830842
iter  40 value 13.455801
iter  50 value 12.159397
iter  60 value 11.642866
iter  70 value 11.412208
iter  80 value 11.160191
iter  90 value 10.919193
iter 100 value 10.824881
final  value 10.824881 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0666; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 248.681698 
iter  10 value 22.435792
iter  20 value 5.905874
iter  30 value 0.254590
iter  40 value 0.045682
iter  50 value 0.014007
iter  60 value 0.006663
iter  70 value 0.003859
iter  80 value 0.002894
iter  90 value 0.002152
iter 100 value 0.001739
final  value 0.001739 
stopped after 100 iterations
# weights:  155
initial  value 990.257493 
iter  10 value 15.415218
iter  20 value 1.501850
iter  30 value 0.246928
iter  40 value 0.041726
iter  50 value 0.010799
iter  60 value 0.004120
iter  70 value 0.001859
iter  80 value 0.001543
iter  90 value 0.001348
iter 100 value 0.001205
final  value 0.001205 
stopped after 100 iterations
# weights:  155
initial  value 4475.394005 
iter  10 value 36.762674
iter  20 value 3.167285
iter  30 value 0.688563
iter  40 value 0.232469
iter  50 value 0.082358
iter  60 value 0.039666
iter  70 value 0.015391
iter  80 value 0.007772
iter  90 value 0.005253
iter 100 value 0.004145
final  value 0.004145 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00231; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 1240.023062 
iter  10 value 76.248610
iter  20 value 19.927452
iter  30 value 2.612708
iter  40 value 0.811507
iter  50 value 0.442594
iter  60 value 0.254143
iter  70 value 0.142040
iter  80 value 0.087912
iter  90 value 0.073837
iter 100 value 0.070460
final  value 0.070460 
stopped after 100 iterations
# weights:  45
initial  value 1085.842345 
iter  10 value 113.024968
iter  20 value 9.554805
iter  30 value 1.036887
iter  40 value 0.620567
iter  50 value 0.568695
iter  60 value 0.495058
iter  70 value 0.452938
iter  80 value 0.423418
iter  90 value 0.401759
iter 100 value 0.393114
final  value 0.393114 
stopped after 100 iterations
# weights:  45
initial  value 895.075212 
iter  10 value 34.799518
iter  20 value 12.596052
iter  30 value 1.586428
iter  40 value 0.339488
iter  50 value 0.163879
iter  60 value 0.118840
iter  70 value 0.108335
iter  80 value 0.101610
iter  90 value 0.097646
iter 100 value 0.094721
final  value 0.094721 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00789; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 1229.781946 
iter  10 value 28.833807
iter  20 value 3.504737
iter  30 value 1.458393
iter  40 value 1.179932
iter  50 value 1.050020
iter  60 value 0.993691
iter  70 value 0.944567
iter  80 value 0.854683
iter  90 value 0.789433
iter 100 value 0.695964
final  value 0.695964 
stopped after 100 iterations
# weights:  78
initial  value 149.767690 
iter  10 value 30.118197
iter  20 value 6.171456
iter  30 value 1.673832
iter  40 value 0.935809
iter  50 value 0.785754
iter  60 value 0.715077
iter  70 value 0.646547
iter  80 value 0.607520
iter  90 value 0.585360
iter 100 value 0.568652
final  value 0.568652 
stopped after 100 iterations
# weights:  78
initial  value 3381.543146 
iter  10 value 55.109505
iter  20 value 19.192290
iter  30 value 4.443704
iter  40 value 2.116215
iter  50 value 1.375393
iter  60 value 0.972903
iter  70 value 0.851891
iter  80 value 0.743101
iter  90 value 0.657276
iter 100 value 0.618499
final  value 0.618499 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0128; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00231
# weights:  155
initial  value 5541.523714 
iter  10 value 30.923686
iter  20 value 2.873398
iter  30 value 0.297169
iter  40 value 0.076277
iter  50 value 0.023881
iter  60 value 0.010634
iter  70 value 0.005223
iter  80 value 0.003559
iter  90 value 0.002281
iter 100 value 0.001841
final  value 0.001841 
stopped after 100 iterations
[1] "Fri Feb 09 15:18:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 550 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 342
 number of selected nodes                               : 79 
[1] "Fri Feb 09 15:18:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 15:18:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 15:18:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 15:18:33 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0407; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0357; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0354; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0534; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0405; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0432; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0356; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0345; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0397; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0371; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0433; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0384; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0389; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0362; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0412; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0359; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0343
[1] "Fri Feb 09 15:20:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 15:20:34 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.178; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.148; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.354; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.132
[1] "Fri Feb 09 15:20:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 15:20:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:21:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.253129 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1          1.857 
    Y 2          1.808 
    Y 3         0.6813 
    Y 4          3.133 
    Y 5          2.161 
[1] "Fri Feb 09 15:21:11 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.154; time: 0.7 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.148; time: 0.5 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.16; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.171; time: 0.8 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.559; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.233; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.198; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.173; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.268; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.226; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.155; time: 0.0 min
[Tune] Result: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683 : rmse.test.rmse=0.143
[1] "Fri Feb 09 15:23:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 15:23:46 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [9,0.500986], n=(372,380)
**GROW** @depth 1: [3,0.5], n=(199,172)
**GROW** @depth 1: [9,0.677318], n=(199,182)
**GROW** @depth 2: [9,0.573767], n=(79,122)
**GROW** @depth 3: [7,0.306048], n=(63,116)
**GROW** @depth 3: [4,0.50253], n=(102,19)
**GROW** @depth 2: [3,0.672165], n=(54,27)
**GROW** @depth 3: [9,0.393097], n=(165,29)
**GROW** @depth 2: [9,0.248915], n=(12,161)
**GROW** @depth 3: [9,0.836884], n=(79,36)
**GROW** @depth 4: [4,0.198521], n=(35,126)
**GROW** @depth 3: [2,0.630644], n=(24,38)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(168,32,12,35,126,54,24,105,19,24,38,74,41)
**GROW** @depth 5: [4,0.491242], n=(13,28)
**GROW** @depth 3: [8,0.456769], n=(91,75)
**PRUNE** @depth 3: [4,0.493967]
**GROW** @depth 4: [9,0.609467], n=(29,45)
**GROW** @depth 4: [9,0.373767], n=(42,84)
**GROW** @depth 5: [8,0.644541], n=(19,22)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(93,71,33,13,38,19,23,81,56,30,44,74,25,37,49,38,28)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 5: [8,0.644541]
**GROW** @depth 4: [9,0.371992], n=(14,23)
**PRUNE** @depth 5: [9,0.368442]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(94,68,31,12,59,23,80,60,32,41,74,25,37,50,36,30)
**GROW** @depth 3: [3,0.8], n=(58,16)
**GROW** @depth 4: [9,0.312032], n=(24,32)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(94,72,33,12,23,32,21,79,60,32,42,58,17,25,37,50,35,30)
**PRUNE** @depth 4: [9,0.314398]
**GROW** @depth 5: [3,0.203093], n=(20,74)
**PRUNE** @depth 5: [3,0.203093]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(95,73,36,12,52,21,78,58,37,39,52,22,25,37,49,36,30)
**GROW** @depth 4: [9,0.213807], n=(53,41)
**GROW** @depth 4: [9,0.277515], n=(38,36)
**PRUNE** @depth 4: [8,0.458515]
**GROW** @depth 4: [9,0.0944773], n=(23,46)
**GROW** @depth 3: [9,0.439448], n=(23,13)
**GROW** @depth 4: [9,0.303748], n=(16,47)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(24,44,37,16,46,24,13,13,51,21,77,54,44,37,53,22,24,38,49,35,30)
**GROW** @depth 5: [4,0.6703], n=(65,13)
**GROW** @depth 6: [9,0.404931], n=(14,51)
**PRUNE** @depth 5: [9,0.404734]
**GROW** @depth 4: [4,0.500973], n=(37,15)
**PRUNE** @depth 5: [4,0.198132]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(26,40,38,16,47,24,13,12,37,15,82,13,61,40,38,52,22,24,38,49,35,30)
Grow: 7.775%, Prune: 2.21%, Change: 76.51%, Swap: 21.89%

[1] "Fri Feb 09 15:23:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 15:23:54 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 15:23:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.9075 	 alpha= 1.7943 	 beta= 42824.58 
[1] "Fri Feb 09 15:23:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 15:23:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [8,0.481223], n=(161,591)
**GROW** @depth 1: [3,0.5], n=(87,504)
r=1000 d=[0] [0] [0]; n=(161,87,504)
**GROW** @depth 2: [9,0.500986], n=(163,341)
r=2000 d=[0] [0] [0] [0]; n=(159,84,168,341)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [1,0.505527], n=(36,305)
r=1000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=2000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=3000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(159,84,169,35,305)
Grow: 1.146%, Prune: 0%, Change: 5.689%, Swap: 0%

[1] "Fri Feb 09 15:24:05 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 15:24:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 15:24:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 15:24:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 15:24:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 15:24:10 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 15:24:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 15:24:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:24:14 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0617; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0466; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0611; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.0531; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0724; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0556; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0459; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.0564; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0829; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0509; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0577; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0573; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0514; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0462; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0501; time: 0.0 min
[Tune] Result: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9 : rmse.test.rmse=0.0459
[1] "Fri Feb 09 15:24:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 15:24:28 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0554; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.082; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.07; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0388; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0257; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.642; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0236; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0199; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.0157
[1] "Fri Feb 09 15:24:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===================================                                   |  50%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:24:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====================                                                  |  28%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:24:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:24:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==========                                                            |  14%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:24:54 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.59 11.826 1.94 5.138 28.498 27.69 10.946 2.29 10.14 
  - best initial criterion value(s) :  3469.405 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3469.4  |proj g|=       4.2341
At iterate     1  f =      -3498.1  |proj g|=        4.6682
At iterate     2  f =      -3506.4  |proj g|=        5.1236
At iterate     3  f =      -3516.3  |proj g|=        4.8528
At iterate     4  f =      -3519.9  |proj g|=         6.425
At iterate     5  f =      -3520.6  |proj g|=         4.413
At iterate     6  f =      -3520.8  |proj g|=        4.3889
At iterate     7  f =      -3520.9  |proj g|=        4.1675
At iterate     8  f =      -3521.5  |proj g|=        3.8063
At iterate     9  f =        -3524  |proj g|=        3.8493
At iterate    10  f =      -3525.7  |proj g|=        2.6873
At iterate    11  f =      -3526.3  |proj g|=        2.4203
At iterate    12  f =      -3526.5  |proj g|=        2.2814
At iterate    13  f =      -3526.6  |proj g|=        2.1223
At iterate    14  f =      -3526.7  |proj g|=        2.0808
At iterate    15  f =      -3526.8  |proj g|=        1.9743
At iterate    16  f =        -3527  |proj g|=        1.8276
At iterate    17  f =      -3527.6  |proj g|=        1.6117
At iterate    18  f =      -3528.2  |proj g|=        1.7447
At iterate    19  f =      -3528.4  |proj g|=         2.093
At iterate    20  f =      -3528.5  |proj g|=         1.981
At iterate    21  f =        -3529  |proj g|=        2.0703
At iterate    22  f =        -3529  |proj g|=        1.9565
At iterate    23  f =        -3529  |proj g|=        2.0203
At iterate    24  f =      -3529.1  |proj g|=        2.1176
At iterate    25  f =      -3529.3  |proj g|=         2.263
At iterate    26  f =        -3530  |proj g|=        2.7002
At iterate    27  f =      -3531.1  |proj g|=        2.6416
At iterate    28  f =      -3532.1  |proj g|=        3.0961
At iterate    29  f =      -3532.2  |proj g|=        3.3331
At iterate    30  f =      -3532.9  |proj g|=         3.623
At iterate    31  f =      -3533.4  |proj g|=        2.7651
At iterate    32  f =      -3533.5  |proj g|=        2.7141
At iterate    33  f =      -3533.5  |proj g|=        2.7166
At iterate    34  f =      -3533.9  |proj g|=        2.4864
At iterate    35  f =      -3534.4  |proj g|=        2.0998
At iterate    36  f =      -3535.5  |proj g|=        1.3731
At iterate    37  f =      -3536.3  |proj g|=        2.3182
At iterate    38  f =      -3536.9  |proj g|=        2.3769
At iterate    39  f =      -3537.3  |proj g|=        2.5307
At iterate    40  f =      -3537.5  |proj g|=        1.8144
At iterate    41  f =      -3537.6  |proj g|=        1.1428
At iterate    42  f =      -3537.7  |proj g|=       0.96344
At iterate    43  f =      -3537.8  |proj g|=        1.8291
At iterate    44  f =      -3537.8  |proj g|=        1.8307
At iterate    45  f =        -3538  |proj g|=        1.8277
At iterate    46  f =      -3538.1  |proj g|=        2.2608
At iterate    47  f =      -3538.2  |proj g|=        1.0875
At iterate    48  f =      -3538.3  |proj g|=       0.95746
At iterate    49  f =      -3538.3  |proj g|=        1.0798
At iterate    50  f =      -3538.3  |proj g|=        1.6881
At iterate    51  f =      -3538.4  |proj g|=        1.8311
At iterate    52  f =      -3538.4  |proj g|=        1.2553
At iterate    53  f =      -3538.4  |proj g|=        1.8296
At iterate    54  f =      -3538.5  |proj g|=       0.73298
At iterate    55  f =      -3538.5  |proj g|=       0.60371
At iterate    56  f =      -3538.5  |proj g|=       0.54439
At iterate    57  f =      -3538.5  |proj g|=       0.54322
At iterate    58  f =      -3538.5  |proj g|=        1.4066
At iterate    59  f =      -3538.5  |proj g|=       0.54518
At iterate    60  f =      -3538.5  |proj g|=       0.54491
At iterate    61  f =      -3538.5  |proj g|=       0.62885
At iterate    62  f =      -3538.6  |proj g|=       0.71125
At iterate    63  f =      -3538.6  |proj g|=       0.80463
At iterate    64  f =      -3538.6  |proj g|=        1.3181
At iterate    65  f =      -3538.7  |proj g|=        1.8327
At iterate    66  f =      -3538.8  |proj g|=        1.8301
At iterate    67  f =      -3538.8  |proj g|=       0.65424
At iterate    68  f =      -3538.8  |proj g|=       0.63675
At iterate    69  f =      -3538.8  |proj g|=        0.6487
At iterate    70  f =      -3538.9  |proj g|=         1.834
At iterate    71  f =      -3538.9  |proj g|=        1.8527
At iterate    72  f =      -3539.1  |proj g|=        1.8311
At iterate    73  f =      -3539.3  |proj g|=        1.8166
At iterate    74  f =      -3539.8  |proj g|=        1.4271
At iterate    75  f =      -3540.3  |proj g|=       0.94408
At iterate    76  f =      -3540.4  |proj g|=        1.8385
At iterate    77  f =      -3540.6  |proj g|=        1.2009
At iterate    78  f =      -3540.7  |proj g|=        1.0522
At iterate    79  f =      -3540.7  |proj g|=       0.54574
At iterate    80  f =      -3540.7  |proj g|=       0.96606
At iterate    81  f =      -3540.7  |proj g|=       0.52287
At iterate    82  f =      -3540.7  |proj g|=       0.44395
At iterate    83  f =      -3540.7  |proj g|=        1.8379
At iterate    84  f =      -3540.7  |proj g|=       0.58613
At iterate    85  f =      -3540.8  |proj g|=        0.6167
At iterate    86  f =      -3540.8  |proj g|=       0.53993
At iterate    87  f =      -3540.8  |proj g|=       0.49111
At iterate    88  f =      -3540.8  |proj g|=       0.25601
At iterate    89  f =      -3540.8  |proj g|=       0.34693
At iterate    90  f =      -3540.8  |proj g|=       0.24649
At iterate    91  f =      -3540.8  |proj g|=       0.40303
At iterate    92  f =      -3540.8  |proj g|=       0.16723
At iterate    93  f =      -3540.8  |proj g|=       0.10758
At iterate    94  f =      -3540.8  |proj g|=       0.10117
At iterate    95  f =      -3540.8  |proj g|=       0.49123
At iterate    96  f =      -3540.8  |proj g|=       0.40476
At iterate    97  f =      -3540.8  |proj g|=       0.35135
At iterate    98  f =      -3540.8  |proj g|=       0.19974
At iterate    99  f =      -3540.8  |proj g|=       0.14438
At iterate   100  f =      -3540.8  |proj g|=       0.21026
At iterate   101  f =      -3540.8  |proj g|=       0.31687
final  value -3540.780764 
stopped after 101 iterations
[1] "Fri Feb 09 15:30:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 84.104, its = 11
i = 2 (of 248), d = 122.377, its = 12
i = 3 (of 248), d = 120.223, its = 11
i = 4 (of 248), d = 247.864, its = 13
i = 5 (of 248), d = 100.807, its = 12
i = 6 (of 248), d = 72.2713, its = 11
i = 7 (of 248), d = 88.9088, its = 11
i = 8 (of 248), d = 55.8078, its = 11
i = 9 (of 248), d = 18.8109, its = 9
i = 10 (of 248), d = 63.3732, its = 11
i = 11 (of 248), d = 42.5769, its = 10
i = 12 (of 248), d = 41.855, its = 10
i = 13 (of 248), d = 100.353, its = 11
i = 14 (of 248), d = 18.2793, its = 9
i = 15 (of 248), d = 45.6186, its = 10
i = 16 (of 248), d = 43.0061, its = 10
i = 17 (of 248), d = 34.5651, its = 10
i = 18 (of 248), d = 100.506, its = 12
i = 19 (of 248), d = 38.9449, its = 10
i = 20 (of 248), d = 51.8952, its = 10
i = 21 (of 248), d = 54.2861, its = 11
i = 22 (of 248), d = 75.0682, its = 11
i = 23 (of 248), d = 67.6005, its = 11
i = 24 (of 248), d = 67.4791, its = 11
i = 25 (of 248), d = 70.2667, its = 11
i = 26 (of 248), d = 26.6728, its = 9
i = 27 (of 248), d = 186.318, its = 12
i = 28 (of 248), d = 116.462, its = 11
i = 29 (of 248), d = 18.902, its = 9
i = 30 (of 248), d = 144.345, its = 13
i = 31 (of 248), d = 136.129, its = 12
i = 32 (of 248), d = 113.363, its = 12
i = 33 (of 248), d = 16.357, its = 8
i = 34 (of 248), d = 130.211, its = 12
i = 35 (of 248), d = 62.976, its = 11
i = 36 (of 248), d = 51.2249, its = 10
i = 37 (of 248), d = 101.098, its = 11
i = 38 (of 248), d = 80.3592, its = 11
i = 39 (of 248), d = 25.7046, its = 9
i = 40 (of 248), d = 81.3557, its = 11
i = 41 (of 248), d = 89.0848, its = 12
i = 42 (of 248), d = 30.5218, its = 10
i = 43 (of 248), d = 99.4758, its = 11
i = 44 (of 248), d = 60.3076, its = 11
i = 45 (of 248), d = 51.8147, its = 11
i = 46 (of 248), d = 117.492, its = 11
i = 47 (of 248), d = 144.325, its = 12
i = 48 (of 248), d = 116.21, its = 12
i = 49 (of 248), d = 22.3144, its = 9
i = 50 (of 248), d = 102.861, its = 11
i = 51 (of 248), d = 63.8859, its = 11
i = 52 (of 248), d = 85.3755, its = 11
i = 53 (of 248), d = 130.181, its = 12
i = 54 (of 248), d = 41.72, its = 11
i = 55 (of 248), d = 32.4172, its = 10
i = 56 (of 248), d = 102.842, its = 11
i = 57 (of 248), d = 69.5534, its = 11
i = 58 (of 248), d = 84.4106, its = 12
i = 59 (of 248), d = 66.448, its = 11
i = 60 (of 248), d = 131.614, its = 11
i = 61 (of 248), d = 53.8962, its = 11
i = 62 (of 248), d = 104.837, its = 11
i = 63 (of 248), d = 139.678, its = 12
i = 64 (of 248), d = 36.9965, its = 10
i = 65 (of 248), d = 41.0991, its = 11
i = 66 (of 248), d = 78.5869, its = 11
i = 67 (of 248), d = 91.3337, its = 11
i = 68 (of 248), d = 53.9066, its = 10
i = 69 (of 248), d = 42.7163, its = 10
i = 70 (of 248), d = 63.0968, its = 11
i = 71 (of 248), d = 87.0422, its = 11
i = 72 (of 248), d = 34.4941, its = 10
i = 73 (of 248), d = 86.99, its = 11
i = 74 (of 248), d = 117.729, its = 11
i = 75 (of 248), d = 92.1188, its = 11
i = 76 (of 248), d = 92.9841, its = 11
i = 77 (of 248), d = 90.3848, its = 11
i = 78 (of 248), d = 30.6116, its = 9
i = 79 (of 248), d = 211.251, its = 12
i = 80 (of 248), d = 69.5717, its = 11
i = 81 (of 248), d = 105.187, its = 11
i = 82 (of 248), d = 80.1597, its = 12
i = 83 (of 248), d = 80.2726, its = 11
i = 84 (of 248), d = 35.5304, its = 10
i = 85 (of 248), d = 31.353, its = 9
i = 86 (of 248), d = 18.6061, its = 9
i = 87 (of 248), d = 67.9279, its = 11
i = 88 (of 248), d = 71.7992, its = 11
i = 89 (of 248), d = 52.488, its = 11
i = 90 (of 248), d = 107.894, its = 11
i = 91 (of 248), d = 41.1745, its = 10
i = 92 (of 248), d = 49.4877, its = 11
i = 93 (of 248), d = 29.4545, its = 10
i = 94 (of 248), d = 42.5415, its = 10
i = 95 (of 248), d = 54.4042, its = 10
i = 96 (of 248), d = 40.9334, its = 11
i = 97 (of 248), d = 96.0904, its = 11
i = 98 (of 248), d = 83.6, its = 11
i = 99 (of 248), d = 67.842, its = 11
i = 100 (of 248), d = 55.4431, its = 11
i = 101 (of 248), d = 111.965, its = 12
i = 102 (of 248), d = 115.055, its = 12
i = 103 (of 248), d = 75.3956, its = 12
i = 104 (of 248), d = 108.092, its = 11
i = 105 (of 248), d = 28.0496, its = 9
i = 106 (of 248), d = 59.0125, its = 10
i = 107 (of 248), d = 79.677, its = 11
i = 108 (of 248), d = 67.1615, its = 11
i = 109 (of 248), d = 22.0527, its = 9
i = 110 (of 248), d = 25.219, its = 10
i = 111 (of 248), d = 70.5911, its = 11
i = 112 (of 248), d = 24.0757, its = 9
i = 113 (of 248), d = 112.782, its = 12
i = 114 (of 248), d = 27.4041, its = 9
i = 115 (of 248), d = 107.858, its = 11
i = 116 (of 248), d = 24.3282, its = 10
i = 117 (of 248), d = 130.212, its = 11
i = 118 (of 248), d = 123.909, its = 12
i = 119 (of 248), d = 53.7783, its = 11
i = 120 (of 248), d = 40.4588, its = 10
i = 121 (of 248), d = 26.6957, its = 10
i = 122 (of 248), d = 80.1324, its = 11
i = 123 (of 248), d = 106.998, its = 11
i = 124 (of 248), d = 65.3706, its = 11
i = 125 (of 248), d = 28.4075, its = 10
i = 126 (of 248), d = 105.858, its = 12
i = 127 (of 248), d = 71.297, its = 11
i = 128 (of 248), d = 137.906, its = 12
i = 129 (of 248), d = 34.4663, its = 10
i = 130 (of 248), d = 25.3857, its = 9
i = 131 (of 248), d = 24.8732, its = 9
i = 132 (of 248), d = 79.2349, its = 11
i = 133 (of 248), d = 120.15, its = 11
i = 134 (of 248), d = 68.6, its = 11
i = 135 (of 248), d = 20.0734, its = 9
i = 136 (of 248), d = 97.4592, its = 12
i = 137 (of 248), d = 52.7073, its = 11
i = 138 (of 248), d = 56.8751, its = 11
i = 139 (of 248), d = 120.807, its = 11
i = 140 (of 248), d = 136.682, its = 12
i = 141 (of 248), d = 62.9474, its = 11
i = 142 (of 248), d = 86.1129, its = 11
i = 143 (of 248), d = 92.4817, its = 11
i = 144 (of 248), d = 76.5097, its = 11
i = 145 (of 248), d = 31.6971, its = 10
i = 146 (of 248), d = 52.1789, its = 11
i = 147 (of 248), d = 132.147, its = 12
i = 148 (of 248), d = 71.3747, its = 11
i = 149 (of 248), d = 107.122, its = 12
i = 150 (of 248), d = 97.4228, its = 11
i = 151 (of 248), d = 24.8457, its = 9
i = 152 (of 248), d = 46.4404, its = 10
i = 153 (of 248), d = 111.387, its = 11
i = 154 (of 248), d = 113.421, its = 12
i = 155 (of 248), d = 199.149, its = 12
i = 156 (of 248), d = 60.5404, its = 11
i = 157 (of 248), d = 63.8253, its = 11
i = 158 (of 248), d = 73.7908, its = 11
i = 159 (of 248), d = 120.958, its = 11
i = 160 (of 248), d = 83.8209, its = 11
i = 161 (of 248), d = 26.0174, its = 9
i = 162 (of 248), d = 147.677, its = 12
i = 163 (of 248), d = 119.065, its = 12
i = 164 (of 248), d = 25.7508, its = 9
i = 165 (of 248), d = 65.4453, its = 11
i = 166 (of 248), d = 98.5605, its = 11
i = 167 (of 248), d = 28.8312, its = 10
i = 168 (of 248), d = 26.7932, its = 10
i = 169 (of 248), d = 87.2949, its = 11
i = 170 (of 248), d = 70.8195, its = 11
i = 171 (of 248), d = 83.8967, its = 12
i = 172 (of 248), d = 53.4034, its = 10
i = 173 (of 248), d = 126.555, its = 12
i = 174 (of 248), d = 95.5742, its = 11
i = 175 (of 248), d = 174.109, its = 15
i = 176 (of 248), d = 145.689, its = 12
i = 177 (of 248), d = 65.2836, its = 11
i = 178 (of 248), d = 41.4739, its = 10
i = 179 (of 248), d = 46.4535, its = 11
i = 180 (of 248), d = 95.5674, its = 11
i = 181 (of 248), d = 87.1681, its = 11
i = 182 (of 248), d = 88.1437, its = 11
i = 183 (of 248), d = 73.4157, its = 11
i = 184 (of 248), d = 31.3504, its = 10
i = 185 (of 248), d = 135.232, its = 12
i = 186 (of 248), d = 38.2027, its = 10
i = 187 (of 248), d = 42.5121, its = 10
i = 188 (of 248), d = 46.5841, its = 10
i = 189 (of 248), d = 126.632, its = 11
i = 190 (of 248), d = 19.8721, its = 9
i = 191 (of 248), d = 53.8209, its = 11
i = 192 (of 248), d = 95.2676, its = 12
i = 193 (of 248), d = 45.7526, its = 10
i = 194 (of 248), d = 28.3362, its = 10
i = 195 (of 248), d = 20.2027, its = 9
i = 196 (of 248), d = 61.7015, its = 11
i = 197 (of 248), d = 22.1422, its = 9
i = 198 (of 248), d = 81.6979, its = 11
i = 199 (of 248), d = 49.4313, its = 10
i = 200 (of 248), d = 68.8008, its = 11
i = 201 (of 248), d = 131.338, its = 13
i = 202 (of 248), d = 33.4307, its = 10
i = 203 (of 248), d = 103.383, its = 11
i = 204 (of 248), d = 78.8732, its = 11
i = 205 (of 248), d = 39.9014, its = 10
i = 206 (of 248), d = 66.6039, its = 11
i = 207 (of 248), d = 80.2143, its = 11
i = 208 (of 248), d = 63.3766, its = 11
i = 209 (of 248), d = 82.3508, its = 11
i = 210 (of 248), d = 121.586, its = 11
i = 211 (of 248), d = 12.6609, its = 8
i = 212 (of 248), d = 79.6616, its = 11
i = 213 (of 248), d = 151.958, its = 12
i = 214 (of 248), d = 124.09, its = 11
i = 215 (of 248), d = 65.1176, its = 11
i = 216 (of 248), d = 120.715, its = 11
i = 217 (of 248), d = 94.2498, its = 11
i = 218 (of 248), d = 76.3032, its = 11
i = 219 (of 248), d = 76.254, its = 11
i = 220 (of 248), d = 44.1877, its = 11
i = 221 (of 248), d = 77.5188, its = 11
i = 222 (of 248), d = 64.7545, its = 11
i = 223 (of 248), d = 38.7528, its = 10
i = 224 (of 248), d = 35.2067, its = 10
i = 225 (of 248), d = 97.5529, its = 11
i = 226 (of 248), d = 61.2856, its = 11
i = 227 (of 248), d = 280.626, its = 14
i = 228 (of 248), d = 122.016, its = 11
i = 229 (of 248), d = 102.433, its = 11
i = 230 (of 248), d = 51.6697, its = 11
i = 231 (of 248), d = 27.0003, its = 10
i = 232 (of 248), d = 28.2024, its = 10
i = 233 (of 248), d = 127.559, its = 12
i = 234 (of 248), d = 17.5082, its = 9
i = 235 (of 248), d = 86.5413, its = 12
i = 236 (of 248), d = 35.3908, its = 10
i = 237 (of 248), d = 103.854, its = 12
i = 238 (of 248), d = 40.67, its = 11
i = 239 (of 248), d = 82.4445, its = 11
i = 240 (of 248), d = 36.6512, its = 10
i = 241 (of 248), d = 64.7304, its = 11
i = 242 (of 248), d = 83.5979, its = 11
i = 243 (of 248), d = 88.6875, its = 11
i = 244 (of 248), d = 57.8284, its = 11
i = 245 (of 248), d = 92.3226, its = 11
i = 246 (of 248), d = 63.8166, its = 11
i = 247 (of 248), d = 288.791, its = 13
i = 248 (of 248), d = 52.2815, its = 11
[1] "Fri Feb 09 15:31:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 15:31:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 15:31:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 15:31:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 15:31:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 15:31:47 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  12
initial  value 1886.242049 
iter  10 value 211.731237
iter  20 value 168.554128
iter  30 value 41.798980
iter  40 value 8.567724
iter  50 value 5.264310
iter  60 value 1.286582
iter  70 value 1.025578
iter  80 value 1.021273
final  value 1.021255 
converged
# weights:  12
initial  value 1326.804522 
iter  10 value 147.615201
iter  20 value 11.999866
iter  30 value 8.055937
iter  40 value 3.327668
iter  50 value 2.298144
iter  60 value 1.632888
iter  70 value 1.060418
iter  80 value 1.026781
iter  90 value 1.019457
final  value 1.019451 
converged
# weights:  12
initial  value 1089.118282 
iter  10 value 16.303153
iter  20 value 4.081621
iter  30 value 1.612496
iter  40 value 1.359023
iter  50 value 1.355977
final  value 1.355974 
converged
[Tune-y] 1: rmse.test.rmse=0.0266; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  155
initial  value 980.215333 
iter  10 value 27.650909
iter  20 value 4.673707
iter  30 value 0.523498
iter  40 value 0.164857
iter  50 value 0.118946
iter  60 value 0.091236
iter  70 value 0.075838
iter  80 value 0.065893
iter  90 value 0.060386
iter 100 value 0.056723
final  value 0.056723 
stopped after 100 iterations
# weights:  155
initial  value 672.353809 
iter  10 value 10.897790
iter  20 value 2.697914
iter  30 value 0.667226
iter  40 value 0.229733
iter  50 value 0.115548
iter  60 value 0.093960
iter  70 value 0.077523
iter  80 value 0.067077
iter  90 value 0.061920
iter 100 value 0.058244
final  value 0.058244 
stopped after 100 iterations
# weights:  155
initial  value 4624.619451 
iter  10 value 24.060521
iter  20 value 2.336666
iter  30 value 0.461275
iter  40 value 0.214818
iter  50 value 0.151784
iter  60 value 0.124342
iter  70 value 0.102772
iter  80 value 0.087552
iter  90 value 0.079772
iter 100 value 0.075302
final  value 0.075302 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00374; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  111
initial  value 2403.042539 
iter  10 value 25.612080
iter  20 value 3.269293
iter  30 value 1.159937
iter  40 value 0.732488
iter  50 value 0.548876
iter  60 value 0.473375
iter  70 value 0.438354
iter  80 value 0.411076
iter  90 value 0.385268
iter 100 value 0.361146
final  value 0.361146 
stopped after 100 iterations
# weights:  111
initial  value 2424.432077 
iter  10 value 100.090345
iter  20 value 9.660215
iter  30 value 2.503552
iter  40 value 1.427902
iter  50 value 1.039047
iter  60 value 0.869229
iter  70 value 0.741049
iter  80 value 0.658382
iter  90 value 0.606251
iter 100 value 0.575381
final  value 0.575381 
stopped after 100 iterations
# weights:  111
initial  value 1657.844963 
iter  10 value 39.647861
iter  20 value 6.706783
iter  30 value 1.536084
iter  40 value 1.067724
iter  50 value 0.820323
iter  60 value 0.702420
iter  70 value 0.642620
iter  80 value 0.570518
iter  90 value 0.536106
iter 100 value 0.508298
final  value 0.508298 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0102; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  12
initial  value 1406.975957 
iter  10 value 210.053071
iter  20 value 210.043265
iter  30 value 210.034313
iter  40 value 209.217832
iter  50 value 82.736485
iter  60 value 5.276622
iter  70 value 0.859639
iter  80 value 0.329150
iter  90 value 0.193829
iter 100 value 0.180712
final  value 0.180712 
stopped after 100 iterations
# weights:  12
initial  value 2522.552103 
iter  10 value 290.806541
iter  20 value 184.113179
iter  30 value 76.686920
iter  40 value 39.546947
iter  50 value 14.804012
iter  60 value 4.243488
iter  70 value 0.980642
iter  80 value 0.661454
iter  90 value 0.232614
iter 100 value 0.164970
final  value 0.164970 
stopped after 100 iterations
# weights:  12
initial  value 394.887820 
iter  10 value 18.186076
iter  20 value 0.860543
iter  30 value 0.357367
iter  40 value 0.168133
iter  50 value 0.133537
iter  60 value 0.132944
iter  70 value 0.132750
final  value 0.132748 
converged
[Tune-y] 4: rmse.test.rmse=0.0126; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  166
initial  value 218.878569 
iter  10 value 17.275693
iter  20 value 2.441191
iter  30 value 0.705841
iter  40 value 0.131318
iter  50 value 0.069055
iter  60 value 0.047485
iter  70 value 0.041354
iter  80 value 0.038403
iter  90 value 0.036433
iter 100 value 0.033975
final  value 0.033975 
stopped after 100 iterations
# weights:  166
initial  value 281.718427 
iter  10 value 16.710042
iter  20 value 4.111126
iter  30 value 0.799542
iter  40 value 0.215765
iter  50 value 0.079175
iter  60 value 0.046809
iter  70 value 0.034851
iter  80 value 0.029749
iter  90 value 0.026579
iter 100 value 0.023269
final  value 0.023269 
stopped after 100 iterations
# weights:  166
initial  value 9835.836218 
iter  10 value 44.910862
iter  20 value 4.145114
iter  30 value 0.472708
iter  40 value 0.159193
iter  50 value 0.102939
iter  60 value 0.082937
iter  70 value 0.070918
iter  80 value 0.065718
iter  90 value 0.059556
iter 100 value 0.055758
final  value 0.055758 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.0036; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  155
initial  value 183.414280 
iter  10 value 15.423064
iter  20 value 2.052259
iter  30 value 0.891886
iter  40 value 0.585942
iter  50 value 0.498764
iter  60 value 0.461761
iter  70 value 0.425323
iter  80 value 0.383391
iter  90 value 0.361287
iter 100 value 0.339500
final  value 0.339500 
stopped after 100 iterations
# weights:  155
initial  value 1648.707440 
iter  10 value 27.704426
iter  20 value 4.448410
iter  30 value 1.608952
iter  40 value 0.896766
iter  50 value 0.572562
iter  60 value 0.479499
iter  70 value 0.440133
iter  80 value 0.402749
iter  90 value 0.378910
iter 100 value 0.363374
final  value 0.363374 
stopped after 100 iterations
# weights:  155
initial  value 956.298730 
iter  10 value 30.892636
iter  20 value 11.644700
iter  30 value 2.204284
iter  40 value 1.157768
iter  50 value 0.627930
iter  60 value 0.523221
iter  70 value 0.470840
iter  80 value 0.415100
iter  90 value 0.378878
iter 100 value 0.355484
final  value 0.355484 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.00902; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  34
initial  value 3344.076100 
iter  10 value 275.115244
iter  20 value 172.493820
iter  30 value 68.118713
iter  40 value 21.088922
iter  50 value 5.240108
iter  60 value 3.788131
iter  70 value 3.669869
iter  80 value 3.642968
iter  90 value 3.530476
iter 100 value 3.424745
final  value 3.424745 
stopped after 100 iterations
# weights:  34
initial  value 3568.760480 
iter  10 value 88.791540
iter  20 value 21.381156
iter  30 value 5.265524
iter  40 value 3.030152
iter  50 value 2.605552
iter  60 value 2.336706
iter  70 value 2.267469
iter  80 value 2.254334
iter  90 value 2.188575
iter 100 value 2.142689
final  value 2.142689 
stopped after 100 iterations
# weights:  34
initial  value 2459.128824 
iter  10 value 212.508390
iter  20 value 185.490023
iter  30 value 14.147998
iter  40 value 5.640086
iter  50 value 2.933798
iter  60 value 2.603301
iter  70 value 2.357742
iter  80 value 2.183205
iter  90 value 2.104180
iter 100 value 1.992609
final  value 1.992609 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0269; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  199
initial  value 1361.253429 
iter  10 value 7.111818
iter  20 value 1.260749
iter  30 value 0.689526
iter  40 value 0.437425
iter  50 value 0.313377
iter  60 value 0.275742
iter  70 value 0.257767
iter  80 value 0.245163
iter  90 value 0.236039
iter 100 value 0.230234
final  value 0.230234 
stopped after 100 iterations
# weights:  199
initial  value 2096.486724 
iter  10 value 15.951633
iter  20 value 2.796066
iter  30 value 1.318932
iter  40 value 0.804989
iter  50 value 0.590395
iter  60 value 0.526502
iter  70 value 0.473508
iter  80 value 0.438887
iter  90 value 0.411515
iter 100 value 0.379691
final  value 0.379691 
stopped after 100 iterations
# weights:  199
initial  value 874.241019 
iter  10 value 25.635958
iter  20 value 3.629667
iter  30 value 1.613376
iter  40 value 0.902028
iter  50 value 0.622744
iter  60 value 0.517189
iter  70 value 0.471424
iter  80 value 0.444203
iter  90 value 0.421954
iter 100 value 0.399725
final  value 0.399725 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0092; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  155
initial  value 4409.667596 
iter  10 value 43.039680
iter  20 value 3.717967
iter  30 value 1.122682
iter  40 value 0.673748
iter  50 value 0.528213
iter  60 value 0.476791
iter  70 value 0.436923
iter  80 value 0.376667
iter  90 value 0.332057
iter 100 value 0.292372
final  value 0.292372 
stopped after 100 iterations
# weights:  155
initial  value 508.895616 
iter  10 value 18.438871
iter  20 value 2.384866
iter  30 value 0.623126
iter  40 value 0.368948
iter  50 value 0.288726
iter  60 value 0.231791
iter  70 value 0.210105
iter  80 value 0.193008
iter  90 value 0.182878
iter 100 value 0.174357
final  value 0.174357 
stopped after 100 iterations
# weights:  155
initial  value 3198.694981 
iter  10 value 25.286803
iter  20 value 2.555893
iter  30 value 0.637575
iter  40 value 0.343791
iter  50 value 0.250724
iter  60 value 0.192697
iter  70 value 0.172522
iter  80 value 0.157632
iter  90 value 0.148900
iter 100 value 0.140001
final  value 0.140001 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00489; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  12
initial  value 901.435276 
iter  10 value 91.443044
iter  20 value 11.987149
iter  30 value 7.380577
iter  40 value 1.531114
iter  50 value 0.430933
iter  60 value 0.213318
iter  70 value 0.069163
iter  80 value 0.043189
iter  90 value 0.032389
iter 100 value 0.019212
final  value 0.019212 
stopped after 100 iterations
# weights:  12
initial  value 1067.483943 
iter  10 value 20.213512
iter  20 value 4.873918
iter  30 value 1.510440
iter  40 value 0.304874
iter  50 value 0.107059
iter  60 value 0.076875
iter  70 value 0.043234
iter  80 value 0.026061
iter  90 value 0.022342
iter 100 value 0.017154
final  value 0.017154 
stopped after 100 iterations
# weights:  12
initial  value 1862.117342 
iter  10 value 180.942503
iter  20 value 96.995223
iter  30 value 16.837305
iter  40 value 2.198386
iter  50 value 0.644103
iter  60 value 0.164697
iter  70 value 0.071361
iter  80 value 0.057785
iter  90 value 0.031005
iter 100 value 0.021911
final  value 0.021911 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00613; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  89
initial  value 687.736836 
iter  10 value 36.507411
iter  20 value 6.151767
iter  30 value 0.925844
iter  40 value 0.420072
iter  50 value 0.304184
iter  60 value 0.232471
iter  70 value 0.181458
iter  80 value 0.153690
iter  90 value 0.133839
iter 100 value 0.114882
final  value 0.114882 
stopped after 100 iterations
# weights:  89
initial  value 2000.986776 
iter  10 value 40.674524
iter  20 value 7.492982
iter  30 value 0.993921
iter  40 value 0.427956
iter  50 value 0.282319
iter  60 value 0.228322
iter  70 value 0.191017
iter  80 value 0.180242
iter  90 value 0.166160
iter 100 value 0.157397
final  value 0.157397 
stopped after 100 iterations
# weights:  89
initial  value 692.836160 
iter  10 value 72.523844
iter  20 value 20.841971
iter  30 value 5.980273
iter  40 value 1.718963
iter  50 value 0.841956
iter  60 value 0.484122
iter  70 value 0.400589
iter  80 value 0.335934
iter  90 value 0.310644
iter 100 value 0.282859
final  value 0.282859 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00658; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  45
initial  value 327.457985 
iter  10 value 37.519860
iter  20 value 6.806175
iter  30 value 3.753452
iter  40 value 3.069126
iter  50 value 2.706513
iter  60 value 2.472377
iter  70 value 2.252218
iter  80 value 2.223711
iter  90 value 2.199986
iter 100 value 2.183097
final  value 2.183097 
stopped after 100 iterations
# weights:  45
initial  value 3417.308362 
iter  10 value 120.456471
iter  20 value 32.231007
iter  30 value 9.982389
iter  40 value 5.046339
iter  50 value 4.159813
iter  60 value 3.568230
iter  70 value 2.675184
iter  80 value 2.515633
iter  90 value 2.355122
iter 100 value 2.290566
final  value 2.290566 
stopped after 100 iterations
# weights:  45
initial  value 2619.374906 
iter  10 value 41.344606
iter  20 value 12.663747
iter  30 value 4.446267
iter  40 value 2.517793
iter  50 value 2.322932
iter  60 value 2.244054
iter  70 value 2.201155
iter  80 value 2.130772
iter  90 value 2.100922
iter 100 value 2.092253
final  value 2.092253 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0218; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  34
initial  value 694.337195 
iter  10 value 36.763377
iter  20 value 10.158035
iter  30 value 1.063722
iter  40 value 0.357771
iter  50 value 0.226181
iter  60 value 0.214847
iter  70 value 0.203702
iter  80 value 0.199082
iter  90 value 0.181054
iter 100 value 0.160229
final  value 0.160229 
stopped after 100 iterations
# weights:  34
initial  value 1426.861813 
iter  10 value 161.564770
iter  20 value 78.317219
iter  30 value 21.716582
iter  40 value 3.958148
iter  50 value 0.901439
iter  60 value 0.220010
iter  70 value 0.123931
iter  80 value 0.063890
iter  90 value 0.047302
iter 100 value 0.041515
final  value 0.041515 
stopped after 100 iterations
# weights:  34
initial  value 2336.194381 
iter  10 value 21.504678
iter  20 value 1.742141
iter  30 value 0.335613
iter  40 value 0.141238
iter  50 value 0.047969
iter  60 value 0.031135
iter  70 value 0.020679
iter  80 value 0.016221
iter  90 value 0.010140
iter 100 value 0.007275
final  value 0.007275 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00528; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  67
initial  value 1910.754006 
iter  10 value 51.809264
iter  20 value 7.686705
iter  30 value 1.671676
iter  40 value 0.509129
iter  50 value 0.211504
iter  60 value 0.148632
iter  70 value 0.112554
iter  80 value 0.095735
iter  90 value 0.086555
iter 100 value 0.076056
final  value 0.076056 
stopped after 100 iterations
# weights:  67
initial  value 1194.793812 
iter  10 value 16.114933
iter  20 value 0.752548
iter  30 value 0.149308
iter  40 value 0.054335
iter  50 value 0.039247
iter  60 value 0.033738
iter  70 value 0.030144
iter  80 value 0.028170
iter  90 value 0.025956
iter 100 value 0.024008
final  value 0.024008 
stopped after 100 iterations
# weights:  67
initial  value 2801.292638 
iter  10 value 53.314208
iter  20 value 3.531660
iter  30 value 0.689122
iter  40 value 0.258865
iter  50 value 0.187506
iter  60 value 0.169768
iter  70 value 0.158181
iter  80 value 0.140609
iter  90 value 0.128647
iter 100 value 0.120992
final  value 0.120992 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00588; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  67
initial  value 368.751643 
iter  10 value 16.670471
iter  20 value 3.166658
iter  30 value 0.637786
iter  40 value 0.196828
iter  50 value 0.109807
iter  60 value 0.090969
iter  70 value 0.068643
iter  80 value 0.058647
iter  90 value 0.052419
iter 100 value 0.049542
final  value 0.049542 
stopped after 100 iterations
# weights:  67
initial  value 206.724342 
iter  10 value 31.952716
iter  20 value 3.244149
iter  30 value 0.387792
iter  40 value 0.128300
iter  50 value 0.081410
iter  60 value 0.066478
iter  70 value 0.059503
iter  80 value 0.053338
iter  90 value 0.048994
iter 100 value 0.043630
final  value 0.043630 
stopped after 100 iterations
# weights:  67
initial  value 2523.915154 
iter  10 value 69.346118
iter  20 value 4.176635
iter  30 value 0.632670
iter  40 value 0.197757
iter  50 value 0.175701
iter  60 value 0.164044
iter  70 value 0.148410
iter  80 value 0.127027
iter  90 value 0.112107
iter 100 value 0.090822
final  value 0.090822 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00585; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  177
initial  value 2833.997746 
iter  10 value 175.365254
iter  20 value 136.587241
iter  30 value 104.297983
iter  40 value 80.632681
iter  50 value 58.874827
iter  60 value 44.661450
iter  70 value 41.196494
iter  80 value 39.560712
iter  90 value 39.023799
iter 100 value 38.749265
final  value 38.749265 
stopped after 100 iterations
# weights:  177
initial  value 1086.882930 
iter  10 value 218.654123
iter  20 value 146.453237
iter  30 value 59.427409
iter  40 value 47.993328
iter  50 value 43.412657
iter  60 value 41.831744
iter  70 value 40.967355
iter  80 value 40.331746
iter  90 value 40.026396
iter 100 value 39.894762
final  value 39.894762 
stopped after 100 iterations
# weights:  177
initial  value 3168.984283 
iter  10 value 212.174766
iter  20 value 58.846779
iter  30 value 47.000139
iter  40 value 42.099090
iter  50 value 40.675245
iter  60 value 39.560410
iter  70 value 39.086903
iter  80 value 38.899374
iter  90 value 38.746619
iter 100 value 38.626215
final  value 38.626215 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  78
initial  value 532.620753 
iter  10 value 30.916762
iter  20 value 1.654775
iter  30 value 0.283721
iter  40 value 0.104834
iter  50 value 0.067265
iter  60 value 0.052933
iter  70 value 0.045300
iter  80 value 0.039974
iter  90 value 0.035449
iter 100 value 0.033837
final  value 0.033837 
stopped after 100 iterations
# weights:  78
initial  value 2515.839811 
iter  10 value 111.491716
iter  20 value 13.458005
iter  30 value 1.579797
iter  40 value 0.172416
iter  50 value 0.061179
iter  60 value 0.036782
iter  70 value 0.028085
iter  80 value 0.023008
iter  90 value 0.020598
iter 100 value 0.019103
final  value 0.019103 
stopped after 100 iterations
# weights:  78
initial  value 3454.895747 
iter  10 value 33.305176
iter  20 value 5.497034
iter  30 value 0.394783
iter  40 value 0.105933
iter  50 value 0.087541
iter  60 value 0.071584
iter  70 value 0.064642
iter  80 value 0.058192
iter  90 value 0.054208
iter 100 value 0.050900
final  value 0.050900 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00398; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  56
initial  value 1739.456349 
iter  10 value 35.955216
iter  20 value 4.087289
iter  30 value 2.907407
iter  40 value 2.458695
iter  50 value 1.905039
iter  60 value 1.377567
iter  70 value 1.162804
iter  80 value 1.122729
iter  90 value 1.099044
iter 100 value 1.077034
final  value 1.077034 
stopped after 100 iterations
# weights:  56
initial  value 1781.642472 
iter  10 value 20.578340
iter  20 value 4.022921
iter  30 value 1.735687
iter  40 value 1.205379
iter  50 value 1.034850
iter  60 value 0.945909
iter  70 value 0.919913
iter  80 value 0.904924
iter  90 value 0.898591
iter 100 value 0.895513
final  value 0.895513 
stopped after 100 iterations
# weights:  56
initial  value 757.897236 
iter  10 value 22.049365
iter  20 value 5.251267
iter  30 value 2.723952
iter  40 value 2.210905
iter  50 value 1.759985
iter  60 value 1.478490
iter  70 value 1.314492
iter  80 value 1.205977
iter  90 value 1.134039
iter 100 value 1.084744
final  value 1.084744 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0172; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  177
initial  value 683.612408 
iter  10 value 10.125634
iter  20 value 3.729177
iter  30 value 1.565639
iter  40 value 0.839596
iter  50 value 0.558351
iter  60 value 0.425306
iter  70 value 0.369287
iter  80 value 0.349182
iter  90 value 0.336680
iter 100 value 0.327441
final  value 0.327441 
stopped after 100 iterations
# weights:  177
initial  value 932.984095 
iter  10 value 38.613722
iter  20 value 7.255049
iter  30 value 1.979077
iter  40 value 1.234693
iter  50 value 0.923929
iter  60 value 0.805525
iter  70 value 0.713036
iter  80 value 0.630037
iter  90 value 0.574346
iter 100 value 0.507216
final  value 0.507216 
stopped after 100 iterations
# weights:  177
initial  value 880.516959 
iter  10 value 15.249037
iter  20 value 3.844261
iter  30 value 1.105801
iter  40 value 0.639579
iter  50 value 0.426166
iter  60 value 0.380207
iter  70 value 0.348550
iter  80 value 0.329504
iter  90 value 0.316260
iter 100 value 0.308259
final  value 0.308259 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00873; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  188
initial  value 2267.883145 
iter  10 value 10.917860
iter  20 value 1.897624
iter  30 value 0.252481
iter  40 value 0.039932
iter  50 value 0.022104
iter  60 value 0.014545
iter  70 value 0.011651
iter  80 value 0.010278
iter  90 value 0.009163
iter 100 value 0.008504
final  value 0.008504 
stopped after 100 iterations
# weights:  188
initial  value 5271.794211 
iter  10 value 42.050836
iter  20 value 9.707525
iter  30 value 3.069790
iter  40 value 0.352457
iter  50 value 0.095266
iter  60 value 0.033826
iter  70 value 0.023946
iter  80 value 0.019412
iter  90 value 0.016494
iter 100 value 0.013812
final  value 0.013812 
stopped after 100 iterations
# weights:  188
initial  value 1328.648812 
iter  10 value 13.219153
iter  20 value 2.121086
iter  30 value 0.345438
iter  40 value 0.152004
iter  50 value 0.099230
iter  60 value 0.074263
iter  70 value 0.065320
iter  80 value 0.062446
iter  90 value 0.060591
iter 100 value 0.059007
final  value 0.059007 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00289; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.00289
# weights:  188
initial  value 6312.354747 
iter  10 value 26.264903
iter  20 value 5.902366
iter  30 value 1.468389
iter  40 value 0.307202
iter  50 value 0.071146
iter  60 value 0.031992
iter  70 value 0.018857
iter  80 value 0.015470
iter  90 value 0.013818
iter 100 value 0.012888
final  value 0.012888 
stopped after 100 iterations
[1] "Fri Feb 09 15:32:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 601 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 365
 number of selected nodes                               : 90 
[1] "Fri Feb 09 15:32:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 15:32:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 15:32:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 15:32:36 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0332; time: 0.1 min
[Tune-x] 3: mtry=5; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0338; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0485; time: 0.0 min
[Tune-x] 5: mtry=7; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.036; time: 0.1 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0518; time: 0.0 min
[Tune-x] 8: mtry=8; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 9: mtry=7; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0452; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.033; time: 0.1 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0465; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.0328; time: 0.1 min
[Tune-x] 15: mtry=3; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0334; time: 0.1 min
[Tune-x] 16: mtry=8; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0408; time: 0.1 min
[Tune-x] 17: mtry=4; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0324; time: 0.1 min
[Tune-x] 18: mtry=3; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0353; time: 0.1 min
[Tune-x] 19: mtry=8; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0373; time: 0.1 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0357; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=3 : rmse.test.rmse=0.0324
[1] "Fri Feb 09 15:34:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 15:34:21 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.327; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.327; time: 0.0 min
[Tune] Result: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46 : rmse.test.rmse=0.119
[1] "Fri Feb 09 15:34:24 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 15:34:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:34:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.248128 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2308 
    Y 1          1.855 
    Y 2          3.131 
    Y 3          2.043 
    Y 4           1.88 
    Y 5          1.641 
[1] "Fri Feb 09 15:34:57 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
[Tune-y] 1: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
[Tune-y] 3: rmse.test.rmse=0.192; time: 0.5 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
[Tune-y] 4: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
[Tune-y] 5: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.2 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
[Tune-y] 8: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.262; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
[Tune-y] 10: rmse.test.rmse=0.959; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.196; time: 0.3 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
[Tune-y] 15: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.158; time: 0.8 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
[Tune-y] 18: rmse.test.rmse=0.212; time: 1.2 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
[Tune-y] 19: rmse.test.rmse=0.0758; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
[Tune-y] 20: rmse.test.rmse=0.387; time: 0.0 min
[Tune] Result: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386 : rmse.test.rmse=0.0758
[1] "Fri Feb 09 15:38:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 15:38:29 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.594666], n=(505,247)
**GROW** @depth 1: [4,0.550019], n=(179,66)
**GROW** @depth 1: [6,0.489538], n=(447,59)
**PRUNE** @depth 1: [6,0.492889]
**GROW** @depth 1: [1,0.316428], n=(205,299)
**GROW** @depth 1: [6,0.432497], n=(182,24)
**PRUNE** @depth 1: [6,0.432199]
**GROW** @depth 2: [3,0.485417], n=(77,217)
**GROW** @depth 3: [5,0.255448], n=(37,145)
**GROW** @depth 3: [5,0.278114], n=(31,114)
**PRUNE** @depth 3: [5,0.278114]
**GROW** @depth 3: [5,0.17608], n=(21,195)
**GROW** @depth 4: [9,0.621499], n=(174,21)
**GROW** @depth 3: [3,0.5], n=(45,18)
**GROW** @depth 3: [1,0.792079], n=(125,22)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(210,45,20,42,169,21,33,125,22,65)
**GROW** @depth 5: [4,0.499813], n=(120,49)
**GROW** @depth 6: [1,0.452819], n=(26,23)
**GROW** @depth 7: [1,0.517074], n=(30,44)
**GROW** @depth 2: [1,0.777531], n=(37,30)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(207,46,20,45,73,26,46,24,23,32,122,20,38,30)

Sampling @ nn=0 pred locs:
**GROW** @depth 2: [6,0.261226], n=(143,64)
**GROW** @depth 4: [9,0.463116], n=(20,30)
**GROW** @depth 3: [8,0.48632], n=(15,18)
**GROW** @depth 8: [4,0.269764], n=(27,19)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(143,64,45,21,20,28,66,29,27,19,25,23,15,17,124,19,37,30)
**PRUNE** @depth 4: [8,0.48632]
**GROW** @depth 3: [1,0.161245], n=(25,39)
**GROW** @depth 4: [3,0.505208], n=(20,19)
**GROW** @depth 4: [8,0.499559], n=(40,41)
**GROW** @depth 3: [1,0.699131], n=(19,17)
**GROW** @depth 3: [8,0.501324], n=(55,34)
**GROW** @depth 3: [3,0.536458], n=(16,17)
**GROW** @depth 4: [1,0.0697111], n=(23,31)
**GROW** @depth 4: [1,0.717115], n=(75,51)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(23,31,34,37,39,27,20,44,21,19,29,64,28,27,19,25,23,16,14,74,52,21,20,16,29)
**GROW** @depth 5: [4,0.183215], n=(13,38)
**GROW** @depth 5: [1,0.711053], n=(17,16)
**GROW** @depth 6: [4,0.583365], n=(45,19)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(22,34,34,35,39,27,20,44,21,19,29,45,19,28,26,19,25,24,15,15,17,16,55,38,21,20,16,29)
**PRUNE** @depth 4: [5,0.252022]
**GROW** @depth 7: [3,0.661458], n=(23,32)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(22,34,34,35,40,26,20,44,21,19,29,44,19,29,27,19,25,23,30,17,16,23,32,38,21,20,16,29)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(22,35,34,34,41,25,20,44,19,19,35,44,19,27,25,19,25,23,30,18,15,23,31,38,20,21,17,29)
Grow: 8.791%, Prune: 1.543%, Change: 74.83%, Swap: 5.231%

[1] "Fri Feb 09 15:38:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 15:38:38 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 15:38:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.4109 	 alpha= 1.8551 	 beta= 37675.81 
[1] "Fri Feb 09 15:38:42 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 15:38:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [3,0.501042], n=(278,474)
**GROW** @depth 1: [9,0.499014], n=(130,344)
**GROW** @depth 1: [8,0.517211], n=(165,113)
r=1000 d=[0] [0] [0] [0]; n=(165,113,130,344)
r=2000 d=[0] [0] [0] [0]; n=(165,113,130,344)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=3 n=(165,113,130,344)
**GROW** @depth 2: [8,0.719329], n=(98,32)
**GROW** @depth 2: [1,0.577692], n=(107,237)
r=2000 d=[0] [0] [0] [0] [0] [0]; mh=4 n=(165,113,98,32,107,237)
**GROW** @depth 2: [9,0.427811], n=(130,35)
**PRUNE** @depth 2: [9,0.427811]
r=3000 d=[0] [0] [0] [0] [0] [0]; mh=4 n=(165,113,98,32,106,238)
r=4000 d=[0] [0] [0] [0] [0] [0]; mh=4 n=(165,113,98,32,106,238)
r=5000 d=[0] [0] [0] [0] [0] [0]; mh=4 n=(165,113,96,32,108,238)
Grow: 1.63%, Prune: 0.2762%, Change: 18.98%, Swap: 0%

[1] "Fri Feb 09 15:38:49 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 15:38:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 15:38:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 15:38:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 15:38:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 15:38:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 15:38:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 15:38:55 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:38:57 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0694; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0683; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.102; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0616; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0392; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.0844; time: 0.1 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0828; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0636; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.0727; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0969; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0672; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.201; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.581; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=0.623; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0856; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.057; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.0884; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0588; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0692; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0392
[1] "Fri Feb 09 15:39:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 15:39:22 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0771; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0315; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0309; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0344; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.128; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0368; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0414; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0537; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0221; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0947; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0636; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.227; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0431; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.557; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0208; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0359; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.115; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0208
[1] "Fri Feb 09 15:39:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:39:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:39:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:39:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======                                                                |   8%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:39:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  8.763898 9.50903 11.37251 9.001752 10.86051 10.96777 8.918264 10.28504 9.203891 
  - best initial criterion value(s) :  2994.783 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -2994.8  |proj g|=       7.5228
At iterate     1  f =      -3009.4  |proj g|=        6.6916
At iterate     2  f =      -3092.9  |proj g|=        6.9566
At iterate     3  f =      -3112.2  |proj g|=        8.9747
At iterate     4  f =        -3148  |proj g|=        8.6869
At iterate     5  f =      -3155.8  |proj g|=        9.4853
At iterate     6  f =      -3171.2  |proj g|=        3.2469
At iterate     7  f =      -3174.8  |proj g|=        2.9888
At iterate     8  f =        -3180  |proj g|=        7.8484
At iterate     9  f =      -3182.4  |proj g|=        3.9714
At iterate    10  f =      -3185.1  |proj g|=        2.0881
At iterate    11  f =      -3187.8  |proj g|=        2.3172
At iterate    12  f =      -3188.9  |proj g|=        3.0082
At iterate    13  f =      -3194.2  |proj g|=        8.8656
At iterate    14  f =      -3196.3  |proj g|=        8.0432
At iterate    15  f =      -3201.8  |proj g|=        3.8261
At iterate    16  f =      -3207.1  |proj g|=        2.8417
At iterate    17  f =      -3208.8  |proj g|=         2.763
At iterate    18  f =      -3209.7  |proj g|=        1.9696
At iterate    19  f =        -3210  |proj g|=         1.999
At iterate    20  f =      -3210.1  |proj g|=        1.1291
At iterate    21  f =      -3210.4  |proj g|=        1.1538
At iterate    22  f =      -3210.8  |proj g|=        1.6633
At iterate    23  f =      -3210.9  |proj g|=        2.0578
At iterate    24  f =        -3211  |proj g|=       0.82897
At iterate    25  f =        -3211  |proj g|=       0.37027
At iterate    26  f =      -3211.1  |proj g|=       0.69829
At iterate    27  f =      -3211.1  |proj g|=         1.016
At iterate    28  f =      -3211.2  |proj g|=        1.4101
At iterate    29  f =      -3211.2  |proj g|=        1.0083
At iterate    30  f =      -3211.2  |proj g|=       0.52638
At iterate    31  f =      -3211.3  |proj g|=       0.72581
At iterate    32  f =      -3211.4  |proj g|=       0.97103
At iterate    33  f =      -3211.5  |proj g|=        1.7282
At iterate    34  f =      -3211.6  |proj g|=       0.80652
At iterate    35  f =      -3211.8  |proj g|=       0.92948
At iterate    36  f =        -3212  |proj g|=        1.7316
At iterate    37  f =      -3212.1  |proj g|=        1.7787
At iterate    38  f =      -3212.1  |proj g|=       0.67123
At iterate    39  f =      -3212.2  |proj g|=       0.61352
At iterate    40  f =      -3212.3  |proj g|=        1.3675
At iterate    41  f =      -3212.4  |proj g|=        1.0211
At iterate    42  f =      -3212.4  |proj g|=       0.55867
At iterate    43  f =      -3212.4  |proj g|=       0.25714
At iterate    44  f =      -3212.4  |proj g|=       0.16407
At iterate    45  f =      -3212.4  |proj g|=      0.035642
At iterate    46  f =      -3212.4  |proj g|=       0.05357
At iterate    47  f =      -3212.4  |proj g|=      0.048586
At iterate    48  f =      -3212.4  |proj g|=     0.0073233
Bad direction in the line search;
   refresh the lbfgs memory and restart the iteration.
At iterate    49  f =      -3212.4  |proj g|=     0.0080512

iterations 49
function evaluations 80
segments explored during Cauchy searches 55
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.0080512
final function value -3212.45

F = -3212.45
final  value -3212.445270 
converged
[1] "Fri Feb 09 15:43:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 72.8093, its = 10
i = 2 (of 248), d = 32.018, its = 10
i = 3 (of 248), d = 75.8415, its = 11
i = 4 (of 248), d = 125.4, its = 12
i = 5 (of 248), d = 97.7447, its = 32
i = 6 (of 248), d = 68.0374, its = 12
i = 7 (of 248), d = 92.7357, its = 11
i = 8 (of 248), d = 114.697, its = 11
i = 9 (of 248), d = 33.8708, its = 10
i = 10 (of 248), d = 91.4939, its = 11
i = 11 (of 248), d = 42.4474, its = 10
i = 12 (of 248), d = 31.1299, its = 10
i = 13 (of 248), d = 40.7211, its = 11
i = 14 (of 248), d = 52.9752, its = 11
i = 15 (of 248), d = 37.6419, its = 10
i = 16 (of 248), d = 49.9188, its = 11
i = 17 (of 248), d = 28.2831, its = 10
i = 18 (of 248), d = 42.6023, its = 10
i = 19 (of 248), d = 59.3223, its = 11
i = 20 (of 248), d = 45.8726, its = 11
i = 21 (of 248), d = 111.498, its = 11
i = 22 (of 248), d = 47.4157, its = 10
i = 23 (of 248), d = 47.4523, its = 10
i = 24 (of 248), d = 52.0173, its = 12
i = 25 (of 248), d = 49.2937, its = 11
i = 26 (of 248), d = 103.66, its = 11
i = 27 (of 248), d = 25.7558, its = 10
i = 28 (of 248), d = 57.7672, its = 10
i = 29 (of 248), d = 48.9173, its = 10
i = 30 (of 248), d = 36.6674, its = 10
i = 31 (of 248), d = 39.3441, its = 11
i = 32 (of 248), d = 87.2748, its = 11
i = 33 (of 248), d = 49.6202, its = 11
i = 34 (of 248), d = 40.2278, its = 11
i = 35 (of 248), d = 128.218, its = 11
i = 36 (of 248), d = 97.6011, its = 12
i = 37 (of 248), d = 50.4157, its = 11
i = 38 (of 248), d = 77.7966, its = 11
i = 39 (of 248), d = 25.9692, its = 10
i = 40 (of 248), d = 65.6758, its = 11
i = 41 (of 248), d = 35.868, its = 11
i = 42 (of 248), d = 49.3986, its = 11
i = 43 (of 248), d = 88.0436, its = 12
i = 44 (of 248), d = 29.9823, its = 10
i = 45 (of 248), d = 44.8844, its = 10
i = 46 (of 248), d = 42.6509, its = 10
i = 47 (of 248), d = 33.1453, its = 10
i = 48 (of 248), d = 61.9913, its = 11
i = 49 (of 248), d = 26.546, its = 10
i = 50 (of 248), d = 46.8212, its = 10
i = 51 (of 248), d = 63.9103, its = 11
i = 52 (of 248), d = 37.7835, its = 10
i = 53 (of 248), d = 109.559, its = 11
i = 54 (of 248), d = 54.0555, its = 12
i = 55 (of 248), d = 51.0629, its = 11
i = 56 (of 248), d = 55.6551, its = 11
i = 57 (of 248), d = 51.6086, its = 11
i = 58 (of 248), d = 153.207, its = 30
i = 59 (of 248), d = 73.0006, its = 11
i = 60 (of 248), d = 106.655, its = 10
i = 61 (of 248), d = 65.8808, its = 11
i = 62 (of 248), d = 67.2031, its = 11
i = 63 (of 248), d = 97.5268, its = 12
i = 64 (of 248), d = 50.9966, its = 11
i = 65 (of 248), d = 39.9557, its = 10
i = 66 (of 248), d = 118.78, its = 11
i = 67 (of 248), d = 50.4523, its = 11
i = 68 (of 248), d = 44.4111, its = 10
i = 69 (of 248), d = 45.3789, its = 10
i = 70 (of 248), d = 72.4158, its = 11
i = 71 (of 248), d = 89.8102, its = 11
i = 72 (of 248), d = 100.2, its = 10
i = 73 (of 248), d = 31.741, its = 10
i = 74 (of 248), d = 119.779, its = 11
i = 75 (of 248), d = 36.3928, its = 10
i = 76 (of 248), d = 51.4237, its = 11
i = 77 (of 248), d = 34.9814, its = 10
i = 78 (of 248), d = 65.1622, its = 11
i = 79 (of 248), d = 39.6982, its = 11
i = 80 (of 248), d = 90.3146, its = 33
i = 81 (of 248), d = 38.8921, its = 10
i = 82 (of 248), d = 30.1662, its = 9
i = 83 (of 248), d = 62.6282, its = 11
i = 84 (of 248), d = 113.197, its = 11
i = 85 (of 248), d = 104.244, its = 13
i = 86 (of 248), d = 35.0245, its = 10
i = 87 (of 248), d = 28.9018, its = 10
i = 88 (of 248), d = 31.6816, its = 10
i = 89 (of 248), d = 37.9867, its = 10
i = 90 (of 248), d = 43.0147, its = 12
i = 91 (of 248), d = 119.841, its = 11
i = 92 (of 248), d = 62.8674, its = 10
i = 93 (of 248), d = 30.7536, its = 10
i = 94 (of 248), d = 111.173, its = 11
i = 95 (of 248), d = 62.696, its = 10
i = 96 (of 248), d = 42.1757, its = 11
i = 97 (of 248), d = 33.3821, its = 10
i = 98 (of 248), d = 62.2231, its = 11
i = 99 (of 248), d = 34.942, its = 10
i = 100 (of 248), d = 49.4714, its = 10
i = 101 (of 248), d = 58.0157, its = 11
i = 102 (of 248), d = 48.6589, its = 11
i = 103 (of 248), d = 50.2954, its = 10
i = 104 (of 248), d = 33.8553, its = 10
i = 105 (of 248), d = 76.807, its = 11
i = 106 (of 248), d = 74.9311, its = 11
i = 107 (of 248), d = 124.239, its = 12
i = 108 (of 248), d = 45.1761, its = 10
i = 109 (of 248), d = 74.8121, its = 11
i = 110 (of 248), d = 32.3766, its = 10
i = 111 (of 248), d = 30.5721, its = 10
i = 112 (of 248), d = 117.705, its = 11
i = 113 (of 248), d = 59.7887, its = 11
i = 114 (of 248), d = 63.7239, its = 11
i = 115 (of 248), d = 54.6595, its = 10
i = 116 (of 248), d = 77.5688, its = 11
i = 117 (of 248), d = 31.9399, its = 10
i = 118 (of 248), d = 62.7423, its = 12
i = 119 (of 248), d = 113.721, its = 12
i = 120 (of 248), d = 68.7854, its = 13
i = 121 (of 248), d = 32.9517, its = 10
i = 122 (of 248), d = 129.909, its = 11
i = 123 (of 248), d = 42.3887, its = 11
i = 124 (of 248), d = 45.9668, its = 11
i = 125 (of 248), d = 92.4552, its = 12
i = 126 (of 248), d = 78.3492, its = 11
i = 127 (of 248), d = 35.0151, its = 10
i = 128 (of 248), d = 44.2673, its = 10
i = 129 (of 248), d = 83.9531, its = 11
i = 130 (of 248), d = 134.522, its = 24
i = 131 (of 248), d = 112.325, its = 11
i = 132 (of 248), d = 42.5039, its = 11
i = 133 (of 248), d = 90.0018, its = 12
i = 134 (of 248), d = 39.0435, its = 10
i = 135 (of 248), d = 32.6393, its = 10
i = 136 (of 248), d = 119.991, its = 11
i = 137 (of 248), d = 95.501, its = 12
i = 138 (of 248), d = 61.3916, its = 11
i = 139 (of 248), d = 42.2675, its = 10
i = 140 (of 248), d = 90.9074, its = 11
i = 141 (of 248), d = 77.6004, its = 11
i = 142 (of 248), d = 68.816, its = 12
i = 143 (of 248), d = 124.607, its = 14
i = 144 (of 248), d = 79.4686, its = 12
i = 145 (of 248), d = 50.2693, its = 10
i = 146 (of 248), d = 54.6512, its = 11
i = 147 (of 248), d = 52.8796, its = 10
i = 148 (of 248), d = 83.5776, its = 12
i = 149 (of 248), d = 52.9211, its = 11
i = 150 (of 248), d = 48.5901, its = 11
i = 151 (of 248), d = 40.3655, its = 10
i = 152 (of 248), d = 64.6414, its = 12
i = 153 (of 248), d = 108.694, its = 10
i = 154 (of 248), d = 38.9248, its = 10
i = 155 (of 248), d = 99.3563, its = 11
i = 156 (of 248), d = 50.0042, its = 11
i = 157 (of 248), d = 35.8584, its = 11
i = 158 (of 248), d = 106.239, its = 10
i = 159 (of 248), d = 56.3945, its = 11
i = 160 (of 248), d = 67.6008, its = 12
i = 161 (of 248), d = 92.5343, its = 11
i = 162 (of 248), d = 39.0082, its = 10
i = 163 (of 248), d = 56.6721, its = 11
i = 164 (of 248), d = 57.3891, its = 11
i = 165 (of 248), d = 106.486, its = 12
i = 166 (of 248), d = 37.6229, its = 11
i = 167 (of 248), d = 74.9388, its = 10
i = 168 (of 248), d = 27.0799, its = 10
i = 169 (of 248), d = 31.1007, its = 9
i = 170 (of 248), d = 70.0572, its = 13
i = 171 (of 248), d = 47.3584, its = 10
i = 172 (of 248), d = 55.2687, its = 11
i = 173 (of 248), d = 63.4567, its = 10
i = 174 (of 248), d = 34.1021, its = 10
i = 175 (of 248), d = 94.4041, its = 12
i = 176 (of 248), d = 60.6417, its = 11
i = 177 (of 248), d = 69.9228, its = 11
i = 178 (of 248), d = 56.6087, its = 10
i = 179 (of 248), d = 34.4396, its = 10
i = 180 (of 248), d = 51.4561, its = 11
i = 181 (of 248), d = 59.3231, its = 11
i = 182 (of 248), d = 34.1839, its = 10
i = 183 (of 248), d = 79.5304, its = 12
i = 184 (of 248), d = 57.7419, its = 11
i = 185 (of 248), d = 95.2554, its = 12
i = 186 (of 248), d = 30.994, its = 9
i = 187 (of 248), d = 107.44, its = 12
i = 188 (of 248), d = 74.796, its = 15
i = 189 (of 248), d = 34.3078, its = 10
i = 190 (of 248), d = 110.373, its = 12
i = 191 (of 248), d = 61.2018, its = 10
i = 192 (of 248), d = 62.7306, its = 11
i = 193 (of 248), d = 39.0048, its = 10
i = 194 (of 248), d = 75.4055, its = 11
i = 195 (of 248), d = 47.451, its = 10
i = 196 (of 248), d = 57.2201, its = 11
i = 197 (of 248), d = 39.7771, its = 10
i = 198 (of 248), d = 62.3144, its = 11
i = 199 (of 248), d = 59.1452, its = 11
i = 200 (of 248), d = 77.5087, its = 11
i = 201 (of 248), d = 59.5161, its = 10
i = 202 (of 248), d = 128.49, its = 11
i = 203 (of 248), d = 29.2466, its = 10
i = 204 (of 248), d = 30.8235, its = 10
i = 205 (of 248), d = 50.0146, its = 10
i = 206 (of 248), d = 58.3047, its = 11
i = 207 (of 248), d = 89.2876, its = 11
i = 208 (of 248), d = 32.2985, its = 10
i = 209 (of 248), d = 119.556, its = 11
i = 210 (of 248), d = 115.599, its = 11
i = 211 (of 248), d = 51.0713, its = 11
i = 212 (of 248), d = 32.0436, its = 10
i = 213 (of 248), d = 57.8382, its = 10
i = 214 (of 248), d = 31.6263, its = 10
i = 215 (of 248), d = 76.8631, its = 29
i = 216 (of 248), d = 28.7421, its = 10
i = 217 (of 248), d = 43.2933, its = 11
i = 218 (of 248), d = 62.2039, its = 11
i = 219 (of 248), d = 51.0789, its = 10
i = 220 (of 248), d = 35.0254, its = 10
i = 221 (of 248), d = 42.4316, its = 13
i = 222 (of 248), d = 118.497, its = 11
i = 223 (of 248), d = 85.0847, its = 12
i = 224 (of 248), d = 52.9186, its = 12
i = 225 (of 248), d = 70.8908, its = 12
i = 226 (of 248), d = 43.5079, its = 10
i = 227 (of 248), d = 133.769, its = 11
i = 228 (of 248), d = 129.024, its = 11
i = 229 (of 248), d = 79.7992, its = 12
i = 230 (of 248), d = 60.762, its = 11
i = 231 (of 248), d = 59.5966, its = 10
i = 232 (of 248), d = 67.9006, its = 11
i = 233 (of 248), d = 63.6086, its = 11
i = 234 (of 248), d = 62.9335, its = 10
i = 235 (of 248), d = 55.9403, its = 11
i = 236 (of 248), d = 47.2089, its = 11
i = 237 (of 248), d = 42.2251, its = 10
i = 238 (of 248), d = 60.9724, its = 10
i = 239 (of 248), d = 32.7881, its = 10
i = 240 (of 248), d = 49.2592, its = 11
i = 241 (of 248), d = 42.5327, its = 10
i = 242 (of 248), d = 28.0271, its = 10
i = 243 (of 248), d = 38.965, its = 10
i = 244 (of 248), d = 106.132, its = 10
i = 245 (of 248), d = 23.298, its = 9
i = 246 (of 248), d = 72.5075, its = 11
i = 247 (of 248), d = 37.0172, its = 9
i = 248 (of 248), d = 44.2874, its = 10
[1] "Fri Feb 09 15:44:07 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 15:44:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 15:44:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 15:44:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 15:44:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 15:44:19 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 550.094405 
iter  10 value 6.955642
iter  20 value 0.540458
iter  30 value 0.284922
iter  40 value 0.214393
iter  50 value 0.161075
iter  60 value 0.134479
iter  70 value 0.118971
iter  80 value 0.112530
iter  90 value 0.105757
iter 100 value 0.100787
final  value 0.100787 
stopped after 100 iterations
# weights:  210
initial  value 614.773194 
iter  10 value 7.448417
iter  20 value 1.037662
iter  30 value 0.395836
iter  40 value 0.279960
iter  50 value 0.209929
iter  60 value 0.152656
iter  70 value 0.130921
iter  80 value 0.117752
iter  90 value 0.108983
iter 100 value 0.101330
final  value 0.101330 
stopped after 100 iterations
# weights:  210
initial  value 1138.870357 
iter  10 value 4.166227
iter  20 value 0.565238
iter  30 value 0.246944
iter  40 value 0.178076
iter  50 value 0.132951
iter  60 value 0.107732
iter  70 value 0.098271
iter  80 value 0.091660
iter  90 value 0.088170
iter 100 value 0.085375
final  value 0.085375 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.0047; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 962.073831 
iter  10 value 6.222509
iter  20 value 0.887297
iter  30 value 0.144796
iter  40 value 0.051442
iter  50 value 0.029421
iter  60 value 0.020977
iter  70 value 0.017818
iter  80 value 0.016222
iter  90 value 0.015201
iter 100 value 0.014191
final  value 0.014191 
stopped after 100 iterations
# weights:  144
initial  value 1822.373564 
iter  10 value 3.745113
iter  20 value 0.637793
iter  30 value 0.098189
iter  40 value 0.041006
iter  50 value 0.021075
iter  60 value 0.016321
iter  70 value 0.014764
iter  80 value 0.013599
iter  90 value 0.012478
iter 100 value 0.011630
final  value 0.011630 
stopped after 100 iterations
# weights:  144
initial  value 335.355487 
iter  10 value 4.684490
iter  20 value 0.436370
iter  30 value 0.110770
iter  40 value 0.049332
iter  50 value 0.030426
iter  60 value 0.021096
iter  70 value 0.016590
iter  80 value 0.014950
iter  90 value 0.013783
iter 100 value 0.013132
final  value 0.013132 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00231; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 691.346131 
iter  10 value 6.730827
iter  20 value 1.474789
iter  30 value 0.371515
iter  40 value 0.140823
iter  50 value 0.064264
iter  60 value 0.041831
iter  70 value 0.029197
iter  80 value 0.021905
iter  90 value 0.018519
iter 100 value 0.016753
final  value 0.016753 
stopped after 100 iterations
# weights:  133
initial  value 1797.301924 
iter  10 value 9.092620
iter  20 value 1.663985
iter  30 value 0.417856
iter  40 value 0.083112
iter  50 value 0.040447
iter  60 value 0.025783
iter  70 value 0.018878
iter  80 value 0.016516
iter  90 value 0.014996
iter 100 value 0.013986
final  value 0.013986 
stopped after 100 iterations
# weights:  133
initial  value 601.112225 
iter  10 value 13.236161
iter  20 value 1.751320
iter  30 value 0.459900
iter  40 value 0.181090
iter  50 value 0.093088
iter  60 value 0.048963
iter  70 value 0.039565
iter  80 value 0.034486
iter  90 value 0.032332
iter 100 value 0.030072
final  value 0.030072 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00299; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 610.532446 
iter  10 value 66.684351
iter  20 value 9.011633
iter  30 value 1.389569
iter  40 value 0.506488
iter  50 value 0.439078
iter  60 value 0.402626
iter  70 value 0.328874
iter  80 value 0.318702
iter  90 value 0.315425
iter 100 value 0.314632
final  value 0.314632 
stopped after 100 iterations
# weights:  23
initial  value 339.725153 
iter  10 value 24.352672
iter  20 value 4.044966
iter  30 value 0.877745
iter  40 value 0.500828
iter  50 value 0.429092
iter  60 value 0.409762
iter  70 value 0.373733
iter  80 value 0.360727
iter  90 value 0.347072
iter 100 value 0.338510
final  value 0.338510 
stopped after 100 iterations
# weights:  23
initial  value 610.810229 
iter  10 value 45.967642
iter  20 value 6.054407
iter  30 value 2.146200
iter  40 value 0.366264
iter  50 value 0.268391
iter  60 value 0.218208
iter  70 value 0.157400
iter  80 value 0.144198
iter  90 value 0.127935
iter 100 value 0.125288
final  value 0.125288 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0132; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 660.333969 
iter  10 value 10.072935
iter  20 value 1.243036
iter  30 value 0.751040
iter  40 value 0.490978
iter  50 value 0.368579
iter  60 value 0.318255
iter  70 value 0.291348
iter  80 value 0.266857
iter  90 value 0.241020
iter 100 value 0.218217
final  value 0.218217 
stopped after 100 iterations
# weights:  210
initial  value 985.004307 
iter  10 value 5.238539
iter  20 value 1.190150
iter  30 value 0.642599
iter  40 value 0.430686
iter  50 value 0.328045
iter  60 value 0.303165
iter  70 value 0.278142
iter  80 value 0.259243
iter  90 value 0.242685
iter 100 value 0.229205
final  value 0.229205 
stopped after 100 iterations
# weights:  210
initial  value 609.619632 
iter  10 value 9.734888
iter  20 value 1.324273
iter  30 value 0.751147
iter  40 value 0.468823
iter  50 value 0.334267
iter  60 value 0.288871
iter  70 value 0.261733
iter  80 value 0.247269
iter  90 value 0.234686
iter 100 value 0.226230
final  value 0.226230 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00688; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 951.950168 
iter  10 value 55.208714
iter  20 value 17.464986
iter  30 value 10.210316
iter  40 value 9.042612
iter  50 value 8.607160
iter  60 value 8.514623
iter  70 value 8.480848
iter  80 value 8.458880
iter  90 value 8.446081
iter 100 value 8.442531
final  value 8.442531 
stopped after 100 iterations
# weights:  199
initial  value 1291.186077 
iter  10 value 56.436492
iter  20 value 13.435660
iter  30 value 9.422494
iter  40 value 8.724386
iter  50 value 8.442909
iter  60 value 8.302558
iter  70 value 8.243817
iter  80 value 8.189765
iter  90 value 8.136962
iter 100 value 8.109577
final  value 8.109577 
stopped after 100 iterations
# weights:  199
initial  value 935.715763 
iter  10 value 34.890754
iter  20 value 15.351198
iter  30 value 10.853242
iter  40 value 9.134635
iter  50 value 8.664202
iter  60 value 8.504854
iter  70 value 8.412681
iter  80 value 8.359123
iter  90 value 8.316371
iter 100 value 8.278824
final  value 8.278824 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0504; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 381.489052 
iter  10 value 5.045966
iter  20 value 0.527234
iter  30 value 0.128364
iter  40 value 0.074703
iter  50 value 0.053190
iter  60 value 0.043207
iter  70 value 0.037384
iter  80 value 0.030757
iter  90 value 0.028313
iter 100 value 0.025679
final  value 0.025679 
stopped after 100 iterations
# weights:  144
initial  value 781.757813 
iter  10 value 8.105225
iter  20 value 0.604100
iter  30 value 0.142299
iter  40 value 0.057922
iter  50 value 0.040386
iter  60 value 0.033030
iter  70 value 0.030293
iter  80 value 0.027886
iter  90 value 0.025163
iter 100 value 0.023213
final  value 0.023213 
stopped after 100 iterations
# weights:  144
initial  value 624.262611 
iter  10 value 2.511930
iter  20 value 0.373177
iter  30 value 0.122072
iter  40 value 0.050500
iter  50 value 0.039327
iter  60 value 0.033566
iter  70 value 0.029351
iter  80 value 0.026132
iter  90 value 0.023544
iter 100 value 0.022468
final  value 0.022468 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00236; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 933.200781 
iter  10 value 14.765978
iter  20 value 3.260241
iter  30 value 0.991292
iter  40 value 0.393124
iter  50 value 0.306367
iter  60 value 0.253336
iter  70 value 0.230590
iter  80 value 0.217482
iter  90 value 0.206239
iter 100 value 0.196611
final  value 0.196611 
stopped after 100 iterations
# weights:  89
initial  value 703.018671 
iter  10 value 16.509383
iter  20 value 2.689568
iter  30 value 0.685294
iter  40 value 0.235414
iter  50 value 0.158090
iter  60 value 0.133242
iter  70 value 0.117419
iter  80 value 0.106903
iter  90 value 0.100123
iter 100 value 0.094721
final  value 0.094721 
stopped after 100 iterations
# weights:  89
initial  value 965.384970 
iter  10 value 5.965769
iter  20 value 1.099349
iter  30 value 0.424126
iter  40 value 0.191195
iter  50 value 0.129466
iter  60 value 0.109640
iter  70 value 0.101166
iter  80 value 0.095704
iter  90 value 0.091077
iter 100 value 0.087245
final  value 0.087245 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00619; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 1149.417661 
iter  10 value 8.653918
iter  20 value 0.454324
iter  30 value 0.191968
iter  40 value 0.122352
iter  50 value 0.102216
iter  60 value 0.087409
iter  70 value 0.072161
iter  80 value 0.062192
iter  90 value 0.056167
iter 100 value 0.052360
final  value 0.052360 
stopped after 100 iterations
# weights:  210
initial  value 1182.228893 
iter  10 value 8.608352
iter  20 value 0.798891
iter  30 value 0.275428
iter  40 value 0.146547
iter  50 value 0.108591
iter  60 value 0.094841
iter  70 value 0.082978
iter  80 value 0.070286
iter  90 value 0.062888
iter 100 value 0.057821
final  value 0.057821 
stopped after 100 iterations
# weights:  210
initial  value 744.171423 
iter  10 value 9.598052
iter  20 value 1.234250
iter  30 value 0.366438
iter  40 value 0.221965
iter  50 value 0.178544
iter  60 value 0.159976
iter  70 value 0.146132
iter  80 value 0.132082
iter  90 value 0.118160
iter 100 value 0.109626
final  value 0.109626 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00381; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 861.416253 
iter  10 value 6.774215
iter  20 value 1.084381
iter  30 value 0.190831
iter  40 value 0.035737
iter  50 value 0.014647
iter  60 value 0.007650
iter  70 value 0.005405
iter  80 value 0.003872
iter  90 value 0.003077
iter 100 value 0.002773
final  value 0.002773 
stopped after 100 iterations
# weights:  56
initial  value 637.824089 
iter  10 value 14.464180
iter  20 value 3.494457
iter  30 value 0.694515
iter  40 value 0.197883
iter  50 value 0.088084
iter  60 value 0.030100
iter  70 value 0.017204
iter  80 value 0.012991
iter  90 value 0.008441
iter 100 value 0.006295
final  value 0.006295 
stopped after 100 iterations
# weights:  56
initial  value 631.978151 
iter  10 value 13.078195
iter  20 value 3.632065
iter  30 value 0.796786
iter  40 value 0.262152
iter  50 value 0.147671
iter  60 value 0.055122
iter  70 value 0.011497
iter  80 value 0.007290
iter  90 value 0.004325
iter 100 value 0.003425
final  value 0.003425 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00306; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 986.844444 
iter  10 value 62.477433
iter  20 value 26.496069
iter  30 value 17.344176
iter  40 value 12.471344
iter  50 value 9.880572
iter  60 value 8.711968
iter  70 value 7.881198
iter  80 value 7.645911
iter  90 value 7.384354
iter 100 value 7.085455
final  value 7.085455 
stopped after 100 iterations
# weights:  199
initial  value 1996.198450 
iter  10 value 33.671510
iter  20 value 12.816749
iter  30 value 11.137418
iter  40 value 9.814166
iter  50 value 8.707707
iter  60 value 7.677344
iter  70 value 7.230523
iter  80 value 6.993375
iter  90 value 6.858602
iter 100 value 6.753227
final  value 6.753227 
stopped after 100 iterations
# weights:  199
initial  value 1677.126616 
iter  10 value 54.646689
iter  20 value 29.844256
iter  30 value 19.322831
iter  40 value 11.704036
iter  50 value 9.397774
iter  60 value 8.690259
iter  70 value 8.233557
iter  80 value 7.780032
iter  90 value 7.479911
iter 100 value 7.211507
final  value 7.211507 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0492; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 347.050811 
iter  10 value 30.090138
iter  20 value 15.242970
iter  30 value 10.216536
iter  40 value 8.938645
iter  50 value 8.029141
iter  60 value 7.345006
iter  70 value 7.127949
iter  80 value 6.949788
iter  90 value 6.855314
iter 100 value 6.795874
final  value 6.795874 
stopped after 100 iterations
# weights:  122
initial  value 506.653150 
iter  10 value 56.697740
iter  20 value 24.482947
iter  30 value 15.599026
iter  40 value 9.553126
iter  50 value 7.974611
iter  60 value 7.224650
iter  70 value 7.001314
iter  80 value 6.915152
iter  90 value 6.860270
iter 100 value 6.815002
final  value 6.815002 
stopped after 100 iterations
# weights:  122
initial  value 1226.582691 
iter  10 value 20.175756
iter  20 value 9.404451
iter  30 value 8.503197
iter  40 value 8.001157
iter  50 value 7.363546
iter  60 value 7.182191
iter  70 value 7.031579
iter  80 value 6.958482
iter  90 value 6.949681
iter 100 value 6.946396
final  value 6.946396 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.057; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 891.494916 
iter  10 value 23.381027
iter  20 value 8.181486
iter  30 value 4.071453
iter  40 value 2.548671
iter  50 value 1.954662
iter  60 value 1.692810
iter  70 value 1.525844
iter  80 value 1.285765
iter  90 value 1.115941
iter 100 value 1.022383
final  value 1.022383 
stopped after 100 iterations
# weights:  56
initial  value 1384.755320 
iter  10 value 14.295178
iter  20 value 2.637539
iter  30 value 1.089760
iter  40 value 0.861390
iter  50 value 0.780954
iter  60 value 0.731292
iter  70 value 0.701592
iter  80 value 0.684872
iter  90 value 0.669370
iter 100 value 0.660447
final  value 0.660447 
stopped after 100 iterations
# weights:  56
initial  value 628.215295 
iter  10 value 26.251976
iter  20 value 8.382659
iter  30 value 4.085118
iter  40 value 2.866361
iter  50 value 2.414400
iter  60 value 2.170755
iter  70 value 1.970830
iter  80 value 1.760380
iter  90 value 1.581354
iter 100 value 1.512317
final  value 1.512317 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0204; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 613.908121 
iter  10 value 10.654897
iter  20 value 1.342439
iter  30 value 0.517510
iter  40 value 0.319876
iter  50 value 0.250146
iter  60 value 0.215741
iter  70 value 0.190409
iter  80 value 0.175583
iter  90 value 0.167472
iter 100 value 0.158325
final  value 0.158325 
stopped after 100 iterations
# weights:  144
initial  value 1198.810492 
iter  10 value 11.899980
iter  20 value 2.200889
iter  30 value 0.945485
iter  40 value 0.495627
iter  50 value 0.399544
iter  60 value 0.351712
iter  70 value 0.327327
iter  80 value 0.309340
iter  90 value 0.293975
iter 100 value 0.277766
final  value 0.277766 
stopped after 100 iterations
# weights:  144
initial  value 625.145424 
iter  10 value 6.582764
iter  20 value 1.061337
iter  30 value 0.398930
iter  40 value 0.227274
iter  50 value 0.188907
iter  60 value 0.138763
iter  70 value 0.122015
iter  80 value 0.110785
iter  90 value 0.105026
iter 100 value 0.100156
final  value 0.100156 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00541; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 566.478886 
iter  10 value 14.452694
iter  20 value 4.414901
iter  30 value 3.068351
iter  40 value 2.286122
iter  50 value 1.815200
iter  60 value 1.626643
iter  70 value 1.513253
iter  80 value 1.399405
iter  90 value 1.298907
iter 100 value 1.227203
final  value 1.227203 
stopped after 100 iterations
# weights:  122
initial  value 655.802897 
iter  10 value 7.094567
iter  20 value 2.466064
iter  30 value 1.310462
iter  40 value 1.067346
iter  50 value 0.978957
iter  60 value 0.925153
iter  70 value 0.905333
iter  80 value 0.887896
iter  90 value 0.881472
iter 100 value 0.876794
final  value 0.876794 
stopped after 100 iterations
# weights:  122
initial  value 546.555143 
iter  10 value 4.871843
iter  20 value 2.002975
iter  30 value 1.272972
iter  40 value 1.044597
iter  50 value 0.982674
iter  60 value 0.948287
iter  70 value 0.930477
iter  80 value 0.922702
iter  90 value 0.915100
iter 100 value 0.908683
final  value 0.908683 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0174; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 699.586495 
iter  10 value 13.399815
iter  20 value 1.848382
iter  30 value 0.812281
iter  40 value 0.408502
iter  50 value 0.280561
iter  60 value 0.230671
iter  70 value 0.207405
iter  80 value 0.194031
iter  90 value 0.180131
iter 100 value 0.174162
final  value 0.174162 
stopped after 100 iterations
# weights:  100
initial  value 1007.066924 
iter  10 value 8.249474
iter  20 value 1.101967
iter  30 value 0.387331
iter  40 value 0.143818
iter  50 value 0.104213
iter  60 value 0.086773
iter  70 value 0.077246
iter  80 value 0.070651
iter  90 value 0.067842
iter 100 value 0.066217
final  value 0.066217 
stopped after 100 iterations
# weights:  100
initial  value 959.660378 
iter  10 value 7.023806
iter  20 value 1.219729
iter  30 value 0.316072
iter  40 value 0.130066
iter  50 value 0.099674
iter  60 value 0.080177
iter  70 value 0.068506
iter  80 value 0.061836
iter  90 value 0.058609
iter 100 value 0.056678
final  value 0.056678 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0047; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 559.074995 
iter  10 value 46.570438
iter  20 value 24.877420
iter  30 value 15.678298
iter  40 value 12.306475
iter  50 value 10.998185
iter  60 value 10.130726
iter  70 value 9.911979
iter  80 value 9.810834
iter  90 value 9.752104
iter 100 value 9.742963
final  value 9.742963 
stopped after 100 iterations
# weights:  56
initial  value 478.994912 
iter  10 value 22.142564
iter  20 value 12.922128
iter  30 value 11.372083
iter  40 value 10.300163
iter  50 value 9.876537
iter  60 value 9.703516
iter  70 value 9.667088
iter  80 value 9.655248
iter  90 value 9.654781
iter 100 value 9.654441
final  value 9.654441 
stopped after 100 iterations
# weights:  56
initial  value 535.453064 
iter  10 value 32.252509
iter  20 value 23.133301
iter  30 value 17.206400
iter  40 value 13.549420
iter  50 value 12.182542
iter  60 value 11.143360
iter  70 value 10.911523
iter  80 value 10.882741
iter  90 value 10.876255
iter 100 value 10.874751
final  value 10.874751 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.075; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 822.702935 
iter  10 value 9.713194
iter  20 value 1.133096
iter  30 value 0.229758
iter  40 value 0.045123
iter  50 value 0.011883
iter  60 value 0.006486
iter  70 value 0.003125
iter  80 value 0.001920
iter  90 value 0.001440
iter 100 value 0.001204
final  value 0.001204 
stopped after 100 iterations
# weights:  155
initial  value 410.788127 
iter  10 value 3.393272
iter  20 value 0.489831
iter  30 value 0.073814
iter  40 value 0.014603
iter  50 value 0.005500
iter  60 value 0.002527
iter  70 value 0.001702
iter  80 value 0.001340
iter  90 value 0.001082
iter 100 value 0.000956
final  value 0.000956 
stopped after 100 iterations
# weights:  155
initial  value 1185.270651 
iter  10 value 12.945174
iter  20 value 1.546718
iter  30 value 0.246491
iter  40 value 0.062544
iter  50 value 0.023553
iter  60 value 0.009044
iter  70 value 0.005742
iter  80 value 0.003767
iter  90 value 0.002736
iter 100 value 0.002309
final  value 0.002309 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00181; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 396.809929 
iter  10 value 22.759352
iter  20 value 5.503186
iter  30 value 0.999364
iter  40 value 0.424604
iter  50 value 0.212503
iter  60 value 0.148336
iter  70 value 0.117465
iter  80 value 0.095390
iter  90 value 0.083340
iter 100 value 0.080554
final  value 0.080554 
stopped after 100 iterations
# weights:  45
initial  value 817.195383 
iter  10 value 20.266200
iter  20 value 3.058911
iter  30 value 1.033785
iter  40 value 0.383334
iter  50 value 0.256503
iter  60 value 0.178100
iter  70 value 0.148026
iter  80 value 0.137064
iter  90 value 0.126922
iter 100 value 0.123305
final  value 0.123305 
stopped after 100 iterations
# weights:  45
initial  value 298.281041 
iter  10 value 15.598044
iter  20 value 3.909394
iter  30 value 2.197665
iter  40 value 0.796810
iter  50 value 0.415157
iter  60 value 0.306582
iter  70 value 0.219654
iter  80 value 0.166097
iter  90 value 0.131363
iter 100 value 0.128656
final  value 0.128656 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00897; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 528.525594 
iter  10 value 21.091167
iter  20 value 4.490367
iter  30 value 2.244624
iter  40 value 1.627894
iter  50 value 1.264701
iter  60 value 1.025422
iter  70 value 0.949927
iter  80 value 0.860800
iter  90 value 0.783129
iter 100 value 0.740091
final  value 0.740091 
stopped after 100 iterations
# weights:  78
initial  value 855.480269 
iter  10 value 9.056975
iter  20 value 3.198585
iter  30 value 1.616555
iter  40 value 1.078052
iter  50 value 0.907346
iter  60 value 0.823728
iter  70 value 0.777430
iter  80 value 0.740599
iter  90 value 0.719370
iter 100 value 0.704563
final  value 0.704563 
stopped after 100 iterations
# weights:  78
initial  value 956.596857 
iter  10 value 11.424540
iter  20 value 3.221139
iter  30 value 1.537097
iter  40 value 1.064052
iter  50 value 0.901989
iter  60 value 0.821446
iter  70 value 0.778338
iter  80 value 0.749807
iter  90 value 0.716433
iter 100 value 0.694357
final  value 0.694357 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0127; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00181
# weights:  155
initial  value 1865.713129 
iter  10 value 18.239964
iter  20 value 1.647311
iter  30 value 0.300324
iter  40 value 0.096561
iter  50 value 0.043067
iter  60 value 0.023292
iter  70 value 0.012333
iter  80 value 0.005299
iter  90 value 0.002865
iter 100 value 0.002153
final  value 0.002153 
stopped after 100 iterations
[1] "Fri Feb 09 15:44:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 603 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 375
 number of selected nodes                               : 76 
[1] "Fri Feb 09 15:44:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 15:44:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 15:45:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 15:45:14 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0635; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0559; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0553; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0832; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0633; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0674; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0556; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0537; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0618; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0578; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0675; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0598; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0607; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0565; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0569; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0535; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0643; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0567; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0589; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0559; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0535
[1] "Fri Feb 09 15:47:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 15:47:13 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.339; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.574; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.339; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.574; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.574; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.237; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.339; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.236; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.206; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.278; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.992; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.574; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.244; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.339; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.231; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.277; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.574; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.552; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.206
[1] "Fri Feb 09 15:47:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 15:47:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:47:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 3.192183 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1         0.4606 
    Y 2         0.3855 
    Y 3         -1.369 
    Y 4           2.45 
    Y 5         0.9348 
[1] "Fri Feb 09 15:47:54 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.196; time: 0.7 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.26; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.195; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.174; time: 0.4 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.199; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.224; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.231; time: 0.8 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.262; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.525; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.282; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.272; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.201; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.301; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.301; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.232; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.282; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.25; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.179; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.199; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.174
[1] "Fri Feb 09 15:50:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 15:50:41 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  These variables have zero variances: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [9,0.500986], n=(372,380)
**GROW** @depth 1: [3,0.5], n=(199,172)
**GROW** @depth 1: [9,0.827416], n=(333,48)
**GROW** @depth 2: [9,0.692702], n=(211,118)
**GROW** @depth 2: [3,0.245361], n=(28,164)
**GROW** @depth 2: [4,0.500973], n=(16,29)
**GROW** @depth 3: [2,0.492812], n=(61,153)
**GROW** @depth 3: [4,0.493967], n=(92,36)
**GROW** @depth 3: [9,0.246154], n=(66,102)
**GROW** @depth 4: [9,0.703156], n=(14,74)
**GROW** @depth 4: [7,0.511968], n=(19,16)
**GROW** @depth 2: [3,0.738144], n=(156,13)
**PRUNE** @depth 4: [7,0.511968]
**GROW** @depth 3: [6,0.281762], n=(100,55)
**GROW** @depth 3: [4,0.554691], n=(117,36)
**PRUNE** @depth 3: [6,0.281762]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(28,66,104,153,13,64,118,36,12,75,35,18,30)
**GROW** @depth 4: [9,0.353846], n=(50,51)
**GROW** @depth 4: [9,0.140828], n=(27,40)
**PRUNE** @depth 4: [9,0.702367]
**GROW** @depth 3: [4,0.500973], n=(127,27)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(27,27,39,55,50,125,28,12,65,118,34,89,35,16,32)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 4: [4,0.500973]
**GROW** @depth 6: [9,0.423471], n=(36,14)
**GROW** @depth 5: [9,0.192308], n=(23,18)
**GROW** @depth 4: [4,0.50253], n=(125,26)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(28,25,22,19,55,37,13,123,27,14,65,119,34,87,34,17,33)
**GROW** @depth 4: [4,0.251849], n=(42,45)
**PRUNE** @depth 4: [4,0.250681]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(37,12,24,19,61,40,14,113,27,15,66,117,34,89,34,17,33)
**GROW** @depth 4: [3,0.621649], n=(82,30)
**PRUNE** @depth 4: [3,0.621649]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(35,13,26,19,61,41,13,111,28,14,67,116,34,90,34,17,33)
**GROW** @depth 4: [9,0.388166], n=(54,55)
**GROW** @depth 4: [9,0.585404], n=(21,95)
**GROW** @depth 4: [4,0.710782], n=(20,13)
**GROW** @depth 3: [9,0.0625247], n=(12,23)
**PRUNE** @depth 4: [9,0.585404]
**PRUNE** @depth 4: [4,0.710782]
**GROW** @depth 6: [9,0.30217], n=(31,28)
**GROW** @depth 7: [9,0.396252], n=(27,16)
**PRUNE** @depth 7: [9,0.396252]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(12,23,14,27,19,31,28,43,13,53,58,28,14,65,116,34,90,34,17,33)
**GROW** @depth 4: [3,0.581443], n=(30,34)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=10 n=(12,23,12,26,21,36,31,47,17,44,54,28,13,32,32,116,35,89,34,17,33)
Grow: 7.902%, Prune: 2.5%, Change: 70.72%, Swap: 31.23%

[1] "Fri Feb 09 15:50:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 15:50:50 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 15:50:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.9075 	 alpha= 1.7943 	 beta= 42824.58 
[1] "Fri Feb 09 15:50:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 15:50:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [8,0.481223], n=(161,591)
**GROW** @depth 1: [9,0.494477], n=(132,30)
r=1000 d=[0] [0] [0]; n=(132,30,590)
**GROW** @depth 1: [2,0.497886], n=(285,305)
**PRUNE** @depth 1: [2,0.497886]
**GROW** @depth 1: [2,0.473871], n=(258,332)
r=2000 d=[0] [0] [0] [0]; n=(132,30,257,333)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=3 n=(132,30,257,333)
r=2000 d=[0] [0] [0] [0]; mh=3 n=(132,30,257,333)
r=3000 d=[0] [0] [0] [0]; mh=3 n=(132,30,257,333)
r=4000 d=[0] [0] [0] [0]; mh=3 n=(132,30,257,333)
**GROW** @depth 2: [4,0.506033], n=(230,27)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(132,30,230,27,333)
Grow: 1.471%, Prune: 0.277%, Change: 33.68%, Swap: 0%

[1] "Fri Feb 09 15:51:00 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 15:51:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 15:51:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 15:51:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 15:51:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 15:51:04 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 15:51:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 15:51:06 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:51:08 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0609; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0457; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0591; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.053; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0729; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0561; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0459; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.0564; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0828; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0506; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0578; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0583; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0506; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0454; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0536; time: 0.0 min
[Tune] Result: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12 : rmse.test.rmse=0.0454
[1] "Fri Feb 09 15:51:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 15:51:22 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0554; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.082; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.07; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0388; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0257; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.642; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0236; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0199; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.0157
[1] "Fri Feb 09 15:51:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:51:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:51:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:51:41 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 15:51:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  8.491189 9.462622 11.49097 8.664481 11.27844 11.30753 9.298849 10.39397 9.203891 
  - best initial criterion value(s) :  3469.405 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3469.4  |proj g|=       4.6669
At iterate     1  f =      -3497.1  |proj g|=        4.0177
At iterate     2  f =      -3514.8  |proj g|=        4.1545
At iterate     3  f =      -3522.1  |proj g|=        9.5374
At iterate     4  f =      -3527.3  |proj g|=        4.6809
At iterate     5  f =      -3529.9  |proj g|=        4.3023
At iterate     6  f =      -3533.4  |proj g|=        6.0301
At iterate     7  f =      -3535.8  |proj g|=        2.4061
At iterate     8  f =      -3536.7  |proj g|=       0.92022
At iterate     9  f =      -3536.9  |proj g|=       0.86569
At iterate    10  f =      -3537.2  |proj g|=        1.3441
At iterate    11  f =      -3537.4  |proj g|=        2.4055
At iterate    12  f =      -3537.5  |proj g|=        1.8509
At iterate    13  f =        -3538  |proj g|=       0.72247
At iterate    14  f =        -3538  |proj g|=        0.4761
At iterate    15  f =      -3538.1  |proj g|=       0.49422
At iterate    16  f =      -3538.4  |proj g|=        0.9141
At iterate    17  f =      -3538.5  |proj g|=        1.3317
At iterate    18  f =      -3538.5  |proj g|=       0.68465
At iterate    19  f =      -3538.6  |proj g|=       0.24369
At iterate    20  f =      -3538.6  |proj g|=       0.19199
At iterate    21  f =      -3538.6  |proj g|=       0.16367
At iterate    22  f =      -3538.7  |proj g|=       0.11709
At iterate    23  f =      -3538.7  |proj g|=       0.21298
At iterate    24  f =      -3538.7  |proj g|=      0.086648
At iterate    25  f =      -3538.7  |proj g|=      0.084396
At iterate    26  f =      -3538.7  |proj g|=      0.070057
At iterate    27  f =      -3538.7  |proj g|=       0.06798
At iterate    28  f =      -3538.7  |proj g|=       0.14816
At iterate    29  f =      -3538.7  |proj g|=      0.063231
At iterate    30  f =      -3538.7  |proj g|=      0.046377
At iterate    31  f =      -3538.7  |proj g|=     0.0085348
At iterate    32  f =      -3538.7  |proj g|=     0.0051372
At iterate    33  f =      -3538.7  |proj g|=     0.0054688

iterations 33
function evaluations 42
segments explored during Cauchy searches 37
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.0054688
final function value -3538.67

F = -3538.67
final  value -3538.666163 
converged
[1] "Fri Feb 09 15:53:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 111.658, its = 11
i = 2 (of 248), d = 157.908, its = 12
i = 3 (of 248), d = 159.247, its = 11
i = 4 (of 248), d = 163.054, its = 59
i = 5 (of 248), d = 136.46, its = 12
i = 6 (of 248), d = 87.2695, its = 11
i = 7 (of 248), d = 163.054, its = 60
i = 8 (of 248), d = 72.6348, its = 12
i = 9 (of 248), d = 49.3058, its = 11
i = 10 (of 248), d = 111.116, its = 12
i = 11 (of 248), d = 58.2906, its = 11
i = 12 (of 248), d = 54.8381, its = 11
i = 13 (of 248), d = 147.448, its = 12
i = 14 (of 248), d = 32.3386, its = 10
i = 15 (of 248), d = 95.0981, its = 11
i = 16 (of 248), d = 67.54, its = 11
i = 17 (of 248), d = 73.7696, its = 11
i = 18 (of 248), d = 148.525, its = 12
i = 19 (of 248), d = 54.7524, its = 11
i = 20 (of 248), d = 111.342, its = 11
i = 21 (of 248), d = 89.6229, its = 11
i = 22 (of 248), d = 115.66, its = 12
i = 23 (of 248), d = 128.034, its = 12
i = 24 (of 248), d = 90.2576, its = 11
i = 25 (of 248), d = 68.3818, its = 11
i = 26 (of 248), d = 54.0698, its = 10
i = 27 (of 248), d = 163.054, its = 64
i = 28 (of 248), d = 133.416, its = 12
i = 29 (of 248), d = 28.9372, its = 10
i = 30 (of 248), d = 131.185, its = 13
i = 31 (of 248), d = 130.773, its = 11
i = 32 (of 248), d = 163.054, its = 60
i = 33 (of 248), d = 35.1598, its = 10
i = 34 (of 248), d = 147.953, its = 12
i = 35 (of 248), d = 127.136, its = 11
i = 36 (of 248), d = 72.7633, its = 11
i = 37 (of 248), d = 116.503, its = 12
i = 38 (of 248), d = 83.7861, its = 11
i = 39 (of 248), d = 50.8531, its = 10
i = 40 (of 248), d = 110.534, its = 11
i = 41 (of 248), d = 163.054, its = 60
i = 42 (of 248), d = 60.1156, its = 11
i = 43 (of 248), d = 133.251, its = 12
i = 44 (of 248), d = 90.079, its = 12
i = 45 (of 248), d = 89.9236, its = 11
i = 46 (of 248), d = 153.885, its = 12
i = 47 (of 248), d = 154.45, its = 12
i = 48 (of 248), d = 154.479, its = 12
i = 49 (of 248), d = 45.5012, its = 11
i = 50 (of 248), d = 150.514, its = 12
i = 51 (of 248), d = 78.6131, its = 11
i = 52 (of 248), d = 115.9, its = 12
i = 53 (of 248), d = 158.724, its = 12
i = 54 (of 248), d = 58.8087, its = 11
i = 55 (of 248), d = 63.0344, its = 11
i = 56 (of 248), d = 133.344, its = 12
i = 57 (of 248), d = 89.0204, its = 11
i = 58 (of 248), d = 115.428, its = 11
i = 59 (of 248), d = 85.1584, its = 11
i = 60 (of 248), d = 142.245, its = 12
i = 61 (of 248), d = 68.2397, its = 11
i = 62 (of 248), d = 110.684, its = 12
i = 63 (of 248), d = 163.054, its = 55
i = 64 (of 248), d = 60.2774, its = 11
i = 65 (of 248), d = 75.6193, its = 11
i = 66 (of 248), d = 114.361, its = 12
i = 67 (of 248), d = 79.4885, its = 11
i = 68 (of 248), d = 85.3043, its = 11
i = 69 (of 248), d = 75.2839, its = 11
i = 70 (of 248), d = 107.167, its = 11
i = 71 (of 248), d = 94.7798, its = 12
i = 72 (of 248), d = 59.0247, its = 11
i = 73 (of 248), d = 124.887, its = 12
i = 74 (of 248), d = 139.176, its = 12
i = 75 (of 248), d = 111.095, its = 12
i = 76 (of 248), d = 121.898, its = 11
i = 77 (of 248), d = 130.633, its = 11
i = 78 (of 248), d = 55.5543, its = 10
i = 79 (of 248), d = 163.054, its = 65
i = 80 (of 248), d = 133.494, its = 12
i = 81 (of 248), d = 113.045, its = 11
i = 82 (of 248), d = 134.026, its = 11
i = 83 (of 248), d = 94.871, its = 12
i = 84 (of 248), d = 64.8662, its = 11
i = 85 (of 248), d = 59.9685, its = 11
i = 86 (of 248), d = 44.0341, its = 10
i = 87 (of 248), d = 77.8744, its = 11
i = 88 (of 248), d = 69.2148, its = 11
i = 89 (of 248), d = 122.932, its = 11
i = 90 (of 248), d = 140.646, its = 12
i = 91 (of 248), d = 66.042, its = 12
i = 92 (of 248), d = 96.4653, its = 11
i = 93 (of 248), d = 52.1978, its = 11
i = 94 (of 248), d = 101.26, its = 11
i = 95 (of 248), d = 89.8277, its = 11
i = 96 (of 248), d = 100.664, its = 11
i = 97 (of 248), d = 124.481, its = 12
i = 98 (of 248), d = 119.864, its = 12
i = 99 (of 248), d = 121.226, its = 11
i = 100 (of 248), d = 108.259, its = 12
i = 101 (of 248), d = 160.072, its = 12
i = 102 (of 248), d = 163.054, its = 59
i = 103 (of 248), d = 86.8554, its = 12
i = 104 (of 248), d = 109.946, its = 11
i = 105 (of 248), d = 54.7075, its = 11
i = 106 (of 248), d = 98.9553, its = 12
i = 107 (of 248), d = 93.8493, its = 12
i = 108 (of 248), d = 82.3049, its = 12
i = 109 (of 248), d = 40.1708, its = 10
i = 110 (of 248), d = 58.9266, its = 10
i = 111 (of 248), d = 137.313, its = 12
i = 112 (of 248), d = 43.813, its = 10
i = 113 (of 248), d = 163.054, its = 57
i = 114 (of 248), d = 51.2179, its = 11
i = 115 (of 248), d = 146.584, its = 12
i = 116 (of 248), d = 45.9567, its = 10
i = 117 (of 248), d = 155.601, its = 12
i = 118 (of 248), d = 163.054, its = 58
i = 119 (of 248), d = 71.6051, its = 11
i = 120 (of 248), d = 38.7065, its = 10
i = 121 (of 248), d = 52.7655, its = 11
i = 122 (of 248), d = 133.194, its = 12
i = 123 (of 248), d = 130.342, its = 11
i = 124 (of 248), d = 104.188, its = 11
i = 125 (of 248), d = 56.6244, its = 11
i = 126 (of 248), d = 163.054, its = 57
i = 127 (of 248), d = 75.6539, its = 12
i = 128 (of 248), d = 137.074, its = 12
i = 129 (of 248), d = 55.8681, its = 11
i = 130 (of 248), d = 41.916, its = 10
i = 131 (of 248), d = 55.3004, its = 10
i = 132 (of 248), d = 123.574, its = 11
i = 133 (of 248), d = 122.489, its = 13
i = 134 (of 248), d = 120.058, its = 12
i = 135 (of 248), d = 44.9502, its = 10
i = 136 (of 248), d = 134.175, its = 12
i = 137 (of 248), d = 86.1019, its = 11
i = 138 (of 248), d = 112.084, its = 11
i = 139 (of 248), d = 163.054, its = 63
i = 140 (of 248), d = 132.967, its = 11
i = 141 (of 248), d = 96.9233, its = 11
i = 142 (of 248), d = 100.637, its = 12
i = 143 (of 248), d = 133.237, its = 12
i = 144 (of 248), d = 126.995, its = 12
i = 145 (of 248), d = 57.3258, its = 12
i = 146 (of 248), d = 81.8226, its = 11
i = 147 (of 248), d = 163.054, its = 56
i = 148 (of 248), d = 76.1607, its = 11
i = 149 (of 248), d = 163.054, its = 60
i = 150 (of 248), d = 144.732, its = 12
i = 151 (of 248), d = 51.8975, its = 11
i = 152 (of 248), d = 61.6707, its = 11
i = 153 (of 248), d = 139.009, its = 12
i = 154 (of 248), d = 120.155, its = 13
i = 155 (of 248), d = 163.054, its = 58
i = 156 (of 248), d = 69.7248, its = 12
i = 157 (of 248), d = 110.961, its = 11
i = 158 (of 248), d = 85.3633, its = 12
i = 159 (of 248), d = 163.054, its = 60
i = 160 (of 248), d = 157.804, its = 12
i = 161 (of 248), d = 55.8157, its = 10
i = 162 (of 248), d = 151.711, its = 12
i = 163 (of 248), d = 155.083, its = 12
i = 164 (of 248), d = 43.7949, its = 12
i = 165 (of 248), d = 111.754, its = 12
i = 166 (of 248), d = 107.449, its = 11
i = 167 (of 248), d = 55.4039, its = 11
i = 168 (of 248), d = 41.4845, its = 11
i = 169 (of 248), d = 105.456, its = 11
i = 170 (of 248), d = 125.766, its = 11
i = 171 (of 248), d = 82.6233, its = 11
i = 172 (of 248), d = 126.614, its = 12
i = 173 (of 248), d = 143.243, its = 12
i = 174 (of 248), d = 148.538, its = 12
i = 175 (of 248), d = 118.621, its = 13
i = 176 (of 248), d = 142.691, its = 12
i = 177 (of 248), d = 91.6156, its = 12
i = 178 (of 248), d = 62.7344, its = 11
i = 179 (of 248), d = 72.91, its = 11
i = 180 (of 248), d = 157.219, its = 12
i = 181 (of 248), d = 146.925, its = 12
i = 182 (of 248), d = 96.1542, its = 12
i = 183 (of 248), d = 88.8954, its = 12
i = 184 (of 248), d = 58.7666, its = 11
i = 185 (of 248), d = 158.936, its = 12
i = 186 (of 248), d = 63.0748, its = 11
i = 187 (of 248), d = 62.0207, its = 11
i = 188 (of 248), d = 95.4419, its = 12
i = 189 (of 248), d = 130.861, its = 12
i = 190 (of 248), d = 45.0972, its = 10
i = 191 (of 248), d = 74.0925, its = 11
i = 192 (of 248), d = 140.96, its = 12
i = 193 (of 248), d = 64.8741, its = 11
i = 194 (of 248), d = 61.8416, its = 10
i = 195 (of 248), d = 40.9645, its = 10
i = 196 (of 248), d = 109.171, its = 11
i = 197 (of 248), d = 51.7962, its = 11
i = 198 (of 248), d = 90.486, its = 11
i = 199 (of 248), d = 78.2068, its = 11
i = 200 (of 248), d = 141.671, its = 12
i = 201 (of 248), d = 140.595, its = 12
i = 202 (of 248), d = 56.6668, its = 11
i = 203 (of 248), d = 133.131, its = 11
i = 204 (of 248), d = 130.133, its = 12
i = 205 (of 248), d = 68.2493, its = 12
i = 206 (of 248), d = 150.308, its = 12
i = 207 (of 248), d = 131.778, its = 11
i = 208 (of 248), d = 136.922, its = 12
i = 209 (of 248), d = 81.6894, its = 12
i = 210 (of 248), d = 114.924, its = 12
i = 211 (of 248), d = 32.7738, its = 10
i = 212 (of 248), d = 109.186, its = 12
i = 213 (of 248), d = 139.906, its = 12
i = 214 (of 248), d = 154.136, its = 12
i = 215 (of 248), d = 107.455, its = 12
i = 216 (of 248), d = 154.716, its = 12
i = 217 (of 248), d = 127.867, its = 12
i = 218 (of 248), d = 140.022, its = 12
i = 219 (of 248), d = 127.641, its = 12
i = 220 (of 248), d = 69.5378, its = 11
i = 221 (of 248), d = 112.511, its = 12
i = 222 (of 248), d = 97.3465, its = 11
i = 223 (of 248), d = 82.4939, its = 11
i = 224 (of 248), d = 74.3805, its = 11
i = 225 (of 248), d = 106.715, its = 12
i = 226 (of 248), d = 105.258, its = 11
i = 227 (of 248), d = 136.299, its = 12
i = 228 (of 248), d = 160.052, its = 12
i = 229 (of 248), d = 137.228, its = 12
i = 230 (of 248), d = 116.693, its = 12
i = 231 (of 248), d = 42.7659, its = 11
i = 232 (of 248), d = 43.7099, its = 10
i = 233 (of 248), d = 143.031, its = 12
i = 234 (of 248), d = 45.1366, its = 11
i = 235 (of 248), d = 122.121, its = 12
i = 236 (of 248), d = 83.4146, its = 11
i = 237 (of 248), d = 163.054, its = 61
i = 238 (of 248), d = 83.5213, its = 12
i = 239 (of 248), d = 124.069, its = 12
i = 240 (of 248), d = 74.1298, its = 11
i = 241 (of 248), d = 136.091, its = 11
i = 242 (of 248), d = 143.121, its = 12
i = 243 (of 248), d = 103.68, its = 12
i = 244 (of 248), d = 77.5051, its = 11
i = 245 (of 248), d = 160.049, its = 12
i = 246 (of 248), d = 117.198, its = 11
i = 247 (of 248), d = 127.986, its = 12
i = 248 (of 248), d = 94.691, its = 12
[1] "Fri Feb 09 15:54:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 15:54:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 15:54:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 15:54:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 15:54:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 15:55:04 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  12
initial  value 1939.024560 
iter  10 value 246.846503
iter  20 value 226.312445
iter  30 value 176.852520
iter  40 value 71.267179
iter  50 value 14.294680
iter  60 value 2.145603
iter  70 value 1.084361
iter  80 value 0.965167
iter  90 value 0.955930
final  value 0.955928 
converged
# weights:  12
initial  value 1267.942174 
iter  10 value 30.062653
iter  20 value 3.013162
iter  30 value 1.337578
iter  40 value 0.982470
iter  50 value 0.956009
final  value 0.956005 
converged
# weights:  12
initial  value 1221.736154 
iter  10 value 63.120024
iter  20 value 4.039282
iter  30 value 1.821613
iter  40 value 1.289761
iter  50 value 1.285253
final  value 1.285250 
converged
[Tune-y] 1: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  155
initial  value 450.302215 
iter  10 value 10.392714
iter  20 value 0.347878
iter  30 value 0.130128
iter  40 value 0.084826
iter  50 value 0.065596
iter  60 value 0.053947
iter  70 value 0.042501
iter  80 value 0.037279
iter  90 value 0.033278
iter 100 value 0.031711
final  value 0.031711 
stopped after 100 iterations
# weights:  155
initial  value 642.600628 
iter  10 value 2.197764
iter  20 value 0.270161
iter  30 value 0.112371
iter  40 value 0.071277
iter  50 value 0.055343
iter  60 value 0.043172
iter  70 value 0.033991
iter  80 value 0.028707
iter  90 value 0.024249
iter 100 value 0.022859
final  value 0.022859 
stopped after 100 iterations
# weights:  155
initial  value 4107.485930 
iter  10 value 8.072925
iter  20 value 0.880520
iter  30 value 0.323115
iter  40 value 0.236416
iter  50 value 0.216755
iter  60 value 0.196438
iter  70 value 0.183877
iter  80 value 0.175880
iter  90 value 0.167706
iter 100 value 0.160454
final  value 0.160454 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00308; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  111
initial  value 2397.049459 
iter  10 value 1.449301
iter  20 value 0.425525
iter  30 value 0.244676
iter  40 value 0.184048
iter  50 value 0.160435
iter  60 value 0.150395
iter  70 value 0.144373
iter  80 value 0.138631
iter  90 value 0.130165
iter 100 value 0.124593
final  value 0.124593 
stopped after 100 iterations
# weights:  111
initial  value 2646.702291 
iter  10 value 1.745602
iter  20 value 0.465764
iter  30 value 0.261801
iter  40 value 0.188415
iter  50 value 0.164401
iter  60 value 0.151822
iter  70 value 0.145890
iter  80 value 0.141510
iter  90 value 0.137092
iter 100 value 0.134092
final  value 0.134092 
stopped after 100 iterations
# weights:  111
initial  value 1456.357763 
iter  10 value 1.803169
iter  20 value 0.461864
iter  30 value 0.258113
iter  40 value 0.184670
iter  50 value 0.164747
iter  60 value 0.153381
iter  70 value 0.146667
iter  80 value 0.144275
iter  90 value 0.141958
iter 100 value 0.139982
final  value 0.139982 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00501; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  12
initial  value 1158.924270 
iter  10 value 126.706061
iter  20 value 30.161105
iter  30 value 4.712553
iter  40 value 1.496524
iter  50 value 0.469678
iter  60 value 0.198006
iter  70 value 0.165013
iter  80 value 0.140212
iter  90 value 0.139625
iter 100 value 0.139606
final  value 0.139606 
stopped after 100 iterations
# weights:  12
initial  value 2114.696468 
iter  10 value 90.850259
iter  20 value 34.285125
iter  30 value 14.569541
iter  40 value 3.136005
iter  50 value 0.842645
iter  60 value 0.643950
iter  70 value 0.222251
iter  80 value 0.155633
iter  90 value 0.141713
iter 100 value 0.139874
final  value 0.139874 
stopped after 100 iterations
# weights:  12
initial  value 448.474982 
iter  10 value 57.665500
iter  20 value 51.619299
iter  30 value 26.694368
iter  40 value 3.188643
iter  50 value 1.559771
iter  60 value 0.341641
iter  70 value 0.154322
iter  80 value 0.140602
iter  90 value 0.131668
iter 100 value 0.131591
final  value 0.131591 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.0104; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  166
initial  value 393.814788 
iter  10 value 2.553531
iter  20 value 0.188902
iter  30 value 0.056264
iter  40 value 0.024780
iter  50 value 0.020426
iter  60 value 0.018419
iter  70 value 0.015252
iter  80 value 0.013401
iter  90 value 0.012018
iter 100 value 0.010837
final  value 0.010837 
stopped after 100 iterations
# weights:  166
initial  value 834.041003 
iter  10 value 11.567883
iter  20 value 0.733310
iter  30 value 0.105831
iter  40 value 0.045243
iter  50 value 0.033636
iter  60 value 0.029184
iter  70 value 0.026646
iter  80 value 0.024706
iter  90 value 0.023037
iter 100 value 0.021463
final  value 0.021463 
stopped after 100 iterations
# weights:  166
initial  value 6674.913416 
iter  10 value 24.275724
iter  20 value 1.275632
iter  30 value 0.179251
iter  40 value 0.087876
iter  50 value 0.074598
iter  60 value 0.069789
iter  70 value 0.067755
iter  80 value 0.065895
iter  90 value 0.063751
iter 100 value 0.062134
final  value 0.062134 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00182; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  155
initial  value 159.467641 
iter  10 value 3.923255
iter  20 value 0.692528
iter  30 value 0.372064
iter  40 value 0.227717
iter  50 value 0.198259
iter  60 value 0.179746
iter  70 value 0.168819
iter  80 value 0.162311
iter  90 value 0.156973
iter 100 value 0.153215
final  value 0.153215 
stopped after 100 iterations
# weights:  155
initial  value 2220.186429 
iter  10 value 7.310211
iter  20 value 1.384386
iter  30 value 0.969593
iter  40 value 0.710747
iter  50 value 0.605622
iter  60 value 0.489625
iter  70 value 0.392798
iter  80 value 0.314618
iter  90 value 0.261950
iter 100 value 0.232764
final  value 0.232764 
stopped after 100 iterations
# weights:  155
initial  value 1020.978375 
iter  10 value 4.433199
iter  20 value 0.834581
iter  30 value 0.508791
iter  40 value 0.336669
iter  50 value 0.268448
iter  60 value 0.229678
iter  70 value 0.198266
iter  80 value 0.177638
iter  90 value 0.165611
iter 100 value 0.159634
final  value 0.159634 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0052; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  34
initial  value 3708.474974 
iter  10 value 18.924000
iter  20 value 7.044265
iter  30 value 2.783288
iter  40 value 1.869619
iter  50 value 1.673533
iter  60 value 1.593410
iter  70 value 1.438442
iter  80 value 1.280586
iter  90 value 1.192681
iter 100 value 1.174171
final  value 1.174171 
stopped after 100 iterations
# weights:  34
initial  value 2510.224928 
iter  10 value 14.478410
iter  20 value 4.630190
iter  30 value 3.184126
iter  40 value 2.456156
iter  50 value 2.173978
iter  60 value 1.908379
iter  70 value 1.809425
iter  80 value 1.682427
iter  90 value 1.593943
iter 100 value 1.566837
final  value 1.566837 
stopped after 100 iterations
# weights:  34
initial  value 2470.682907 
iter  10 value 13.469933
iter  20 value 4.837060
iter  30 value 2.994331
iter  40 value 2.074503
iter  50 value 1.885180
iter  60 value 1.828018
iter  70 value 1.719531
iter  80 value 1.502682
iter  90 value 1.445684
iter 100 value 1.434165
final  value 1.434165 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.025; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  199
initial  value 1629.399742 
iter  10 value 7.503479
iter  20 value 1.566903
iter  30 value 1.146975
iter  40 value 0.885441
iter  50 value 0.750471
iter  60 value 0.617673
iter  70 value 0.425154
iter  80 value 0.266693
iter  90 value 0.215248
iter 100 value 0.196238
final  value 0.196238 
stopped after 100 iterations
# weights:  199
initial  value 1210.950141 
iter  10 value 6.652399
iter  20 value 0.882532
iter  30 value 0.531708
iter  40 value 0.335225
iter  50 value 0.274886
iter  60 value 0.252290
iter  70 value 0.227518
iter  80 value 0.208136
iter  90 value 0.197565
iter 100 value 0.192668
final  value 0.192668 
stopped after 100 iterations
# weights:  199
initial  value 1935.211745 
iter  10 value 57.849741
iter  20 value 7.630384
iter  30 value 4.825364
iter  40 value 3.807464
iter  50 value 2.600090
iter  60 value 1.855704
iter  70 value 1.046138
iter  80 value 0.794925
iter  90 value 0.627990
iter 100 value 0.550598
final  value 0.550598 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00488; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  155
initial  value 3677.737549 
iter  10 value 112.439551
iter  20 value 7.265596
iter  30 value 2.537129
iter  40 value 1.809763
iter  50 value 1.604478
iter  60 value 1.364645
iter  70 value 1.225846
iter  80 value 1.120293
iter  90 value 1.007510
iter 100 value 0.919488
final  value 0.919488 
stopped after 100 iterations
# weights:  155
initial  value 380.993635 
iter  10 value 3.961527
iter  20 value 0.444895
iter  30 value 0.216517
iter  40 value 0.149975
iter  50 value 0.105352
iter  60 value 0.088987
iter  70 value 0.081359
iter  80 value 0.075057
iter  90 value 0.071643
iter 100 value 0.068785
final  value 0.068785 
stopped after 100 iterations
# weights:  155
initial  value 2879.901027 
iter  10 value 4.985014
iter  20 value 0.733811
iter  30 value 0.449608
iter  40 value 0.360546
iter  50 value 0.312378
iter  60 value 0.284157
iter  70 value 0.253704
iter  80 value 0.230611
iter  90 value 0.193976
iter 100 value 0.177050
final  value 0.177050 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00585; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  12
initial  value 1301.854936 
final  value 209.924393 
converged
# weights:  12
initial  value 990.013081 
iter  10 value 172.698082
iter  20 value 45.971816
iter  30 value 4.327402
iter  40 value 0.821294
iter  50 value 0.253619
iter  60 value 0.166484
iter  70 value 0.064978
iter  80 value 0.037923
iter  90 value 0.031179
iter 100 value 0.022776
final  value 0.022776 
stopped after 100 iterations
# weights:  12
initial  value 1599.002762 
iter  10 value 54.084708
iter  20 value 6.155021
iter  30 value 0.876754
iter  40 value 0.316370
iter  50 value 0.088519
iter  60 value 0.067113
iter  70 value 0.040363
iter  80 value 0.024029
iter  90 value 0.023097
iter 100 value 0.017575
final  value 0.017575 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.363; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  89
initial  value 1052.450143 
iter  10 value 2.706307
iter  20 value 0.613156
iter  30 value 0.233283
iter  40 value 0.160477
iter  50 value 0.126834
iter  60 value 0.108928
iter  70 value 0.096931
iter  80 value 0.089675
iter  90 value 0.083166
iter 100 value 0.078499
final  value 0.078499 
stopped after 100 iterations
# weights:  89
initial  value 1918.961842 
iter  10 value 3.659495
iter  20 value 0.575881
iter  30 value 0.156748
iter  40 value 0.111090
iter  50 value 0.085019
iter  60 value 0.070787
iter  70 value 0.065295
iter  80 value 0.062010
iter  90 value 0.059571
iter 100 value 0.057400
final  value 0.057400 
stopped after 100 iterations
# weights:  89
initial  value 911.733676 
iter  10 value 9.510540
iter  20 value 1.290869
iter  30 value 0.381287
iter  40 value 0.248959
iter  50 value 0.190323
iter  60 value 0.159617
iter  70 value 0.131698
iter  80 value 0.122769
iter  90 value 0.116236
iter 100 value 0.108610
final  value 0.108610 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00401; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  45
initial  value 416.039117 
iter  10 value 8.094623
iter  20 value 2.909514
iter  30 value 1.674607
iter  40 value 1.371306
iter  50 value 1.258189
iter  60 value 1.202803
iter  70 value 1.178958
iter  80 value 1.163697
iter  90 value 1.156069
iter 100 value 1.153822
final  value 1.153822 
stopped after 100 iterations
# weights:  45
initial  value 2257.072337 
iter  10 value 10.596486
iter  20 value 6.566003
iter  30 value 4.901735
iter  40 value 4.045378
iter  50 value 3.813430
iter  60 value 3.537885
iter  70 value 3.144611
iter  80 value 2.573617
iter  90 value 2.114247
iter 100 value 1.975183
final  value 1.975183 
stopped after 100 iterations
# weights:  45
initial  value 2043.593222 
iter  10 value 13.714432
iter  20 value 4.748175
iter  30 value 1.908869
iter  40 value 1.503057
iter  50 value 1.360949
iter  60 value 1.306318
iter  70 value 1.289782
iter  80 value 1.287101
iter  90 value 1.286783
iter 100 value 1.286275
final  value 1.286275 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0238; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  34
initial  value 724.974114 
iter  10 value 21.462809
iter  20 value 1.365206
iter  30 value 0.418291
iter  40 value 0.128307
iter  50 value 0.056203
iter  60 value 0.025956
iter  70 value 0.016629
iter  80 value 0.015182
iter  90 value 0.010723
iter 100 value 0.007774
final  value 0.007774 
stopped after 100 iterations
# weights:  34
initial  value 1547.884370 
iter  10 value 19.783241
iter  20 value 6.143488
iter  30 value 1.582671
iter  40 value 0.479239
iter  50 value 0.132909
iter  60 value 0.088442
iter  70 value 0.066142
iter  80 value 0.060668
iter  90 value 0.060091
iter 100 value 0.052056
final  value 0.052056 
stopped after 100 iterations
# weights:  34
initial  value 1947.987202 
iter  10 value 21.420639
iter  20 value 5.151118
iter  30 value 1.482975
iter  40 value 0.310875
iter  50 value 0.148822
iter  60 value 0.090831
iter  70 value 0.056916
iter  80 value 0.049997
iter  90 value 0.032276
iter 100 value 0.023943
final  value 0.023943 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0051; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  67
initial  value 1891.178446 
iter  10 value 3.744275
iter  20 value 0.637918
iter  30 value 0.059598
iter  40 value 0.032116
iter  50 value 0.021236
iter  60 value 0.018082
iter  70 value 0.016286
iter  80 value 0.014475
iter  90 value 0.013411
iter 100 value 0.012704
final  value 0.012704 
stopped after 100 iterations
# weights:  67
initial  value 867.245208 
iter  10 value 1.629206
iter  20 value 0.301607
iter  30 value 0.095443
iter  40 value 0.044058
iter  50 value 0.030084
iter  60 value 0.021011
iter  70 value 0.016046
iter  80 value 0.012842
iter  90 value 0.011681
iter 100 value 0.010843
final  value 0.010843 
stopped after 100 iterations
# weights:  67
initial  value 2456.762832 
iter  10 value 10.830907
iter  20 value 1.955439
iter  30 value 0.628501
iter  40 value 0.289281
iter  50 value 0.169383
iter  60 value 0.117119
iter  70 value 0.091941
iter  80 value 0.079214
iter  90 value 0.070016
iter 100 value 0.066456
final  value 0.066456 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00512; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  67
initial  value 465.035200 
iter  10 value 5.768928
iter  20 value 0.259611
iter  30 value 0.096270
iter  40 value 0.043986
iter  50 value 0.030417
iter  60 value 0.025982
iter  70 value 0.024624
iter  80 value 0.023928
iter  90 value 0.023261
iter 100 value 0.023018
final  value 0.023018 
stopped after 100 iterations
# weights:  67
initial  value 371.830267 
iter  10 value 3.576214
iter  20 value 1.053834
iter  30 value 0.252562
iter  40 value 0.115311
iter  50 value 0.067898
iter  60 value 0.050123
iter  70 value 0.043490
iter  80 value 0.040299
iter  90 value 0.038099
iter 100 value 0.035651
final  value 0.035651 
stopped after 100 iterations
# weights:  67
initial  value 2969.082850 
iter  10 value 2.544278
iter  20 value 0.354368
iter  30 value 0.098080
iter  40 value 0.054713
iter  50 value 0.031951
iter  60 value 0.026949
iter  70 value 0.023292
iter  80 value 0.021190
iter  90 value 0.020064
iter 100 value 0.019403
final  value 0.019403 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00307; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  177
initial  value 3447.634420 
iter  10 value 400.403086
iter  20 value 177.977532
iter  30 value 63.769689
iter  40 value 46.230348
iter  50 value 36.196104
iter  60 value 29.992155
iter  70 value 26.495700
iter  80 value 25.309512
iter  90 value 24.764739
iter 100 value 24.549277
final  value 24.549277 
stopped after 100 iterations
# weights:  177
initial  value 2379.577245 
iter  10 value 289.988788
iter  20 value 97.271685
iter  30 value 51.535330
iter  40 value 34.982783
iter  50 value 29.216152
iter  60 value 26.872918
iter  70 value 25.592096
iter  80 value 24.824879
iter  90 value 24.525857
iter 100 value 24.382037
final  value 24.382037 
stopped after 100 iterations
# weights:  177
initial  value 1892.574794 
iter  10 value 115.028977
iter  20 value 68.354912
iter  30 value 52.257976
iter  40 value 46.890104
iter  50 value 41.514509
iter  60 value 37.602928
iter  70 value 34.299931
iter  80 value 32.277905
iter  90 value 29.544628
iter 100 value 27.238588
final  value 27.238588 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0523; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  78
initial  value 468.873627 
iter  10 value 2.246112
iter  20 value 0.306028
iter  30 value 0.084369
iter  40 value 0.027514
iter  50 value 0.019308
iter  60 value 0.015415
iter  70 value 0.012963
iter  80 value 0.011978
iter  90 value 0.010725
iter 100 value 0.010177
final  value 0.010177 
stopped after 100 iterations
# weights:  78
initial  value 1839.285550 
iter  10 value 9.915580
iter  20 value 2.143921
iter  30 value 0.301098
iter  40 value 0.113144
iter  50 value 0.070912
iter  60 value 0.059002
iter  70 value 0.052169
iter  80 value 0.049061
iter  90 value 0.047270
iter 100 value 0.044361
final  value 0.044361 
stopped after 100 iterations
# weights:  78
initial  value 3944.529187 
iter  10 value 1.181282
iter  20 value 0.146113
iter  30 value 0.077680
iter  40 value 0.037326
iter  50 value 0.025658
iter  60 value 0.020776
iter  70 value 0.016998
iter  80 value 0.013801
iter  90 value 0.011868
iter 100 value 0.010532
final  value 0.010532 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00289; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  56
initial  value 1362.910912 
iter  10 value 26.659036
iter  20 value 5.286335
iter  30 value 3.366745
iter  40 value 2.626901
iter  50 value 2.049713
iter  60 value 1.604184
iter  70 value 1.310830
iter  80 value 1.135237
iter  90 value 0.990554
iter 100 value 0.877727
final  value 0.877727 
stopped after 100 iterations
# weights:  56
initial  value 1575.689598 
iter  10 value 2.817560
iter  20 value 1.191749
iter  30 value 0.852637
iter  40 value 0.759365
iter  50 value 0.651323
iter  60 value 0.625947
iter  70 value 0.595285
iter  80 value 0.563367
iter  90 value 0.540370
iter 100 value 0.527359
final  value 0.527359 
stopped after 100 iterations
# weights:  56
initial  value 623.379011 
iter  10 value 3.012076
iter  20 value 1.316743
iter  30 value 0.949784
iter  40 value 0.784983
iter  50 value 0.682241
iter  60 value 0.599924
iter  70 value 0.567172
iter  80 value 0.553295
iter  90 value 0.546746
iter 100 value 0.537855
final  value 0.537855 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0195; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  177
initial  value 799.971641 
iter  10 value 3.227304
iter  20 value 1.117103
iter  30 value 0.651265
iter  40 value 0.464780
iter  50 value 0.381094
iter  60 value 0.327650
iter  70 value 0.291627
iter  80 value 0.275090
iter  90 value 0.261353
iter 100 value 0.243022
final  value 0.243022 
stopped after 100 iterations
# weights:  177
initial  value 708.582013 
iter  10 value 4.104872
iter  20 value 0.895564
iter  30 value 0.524116
iter  40 value 0.328489
iter  50 value 0.284513
iter  60 value 0.263446
iter  70 value 0.246071
iter  80 value 0.227654
iter  90 value 0.213547
iter 100 value 0.204496
final  value 0.204496 
stopped after 100 iterations
# weights:  177
initial  value 1296.556542 
iter  10 value 11.613825
iter  20 value 1.173563
iter  30 value 0.714880
iter  40 value 0.418766
iter  50 value 0.359437
iter  60 value 0.318829
iter  70 value 0.280229
iter  80 value 0.245146
iter  90 value 0.216208
iter 100 value 0.196947
final  value 0.196947 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0059; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  188
initial  value 2910.819428 
iter  10 value 4.116538
iter  20 value 0.203268
iter  30 value 0.029681
iter  40 value 0.015552
iter  50 value 0.010538
iter  60 value 0.009658
iter  70 value 0.009004
iter  80 value 0.008473
iter  90 value 0.007885
iter 100 value 0.007521
final  value 0.007521 
stopped after 100 iterations
# weights:  188
initial  value 3874.136486 
iter  10 value 3.171850
iter  20 value 0.229895
iter  30 value 0.038264
iter  40 value 0.020755
iter  50 value 0.014138
iter  60 value 0.012065
iter  70 value 0.011055
iter  80 value 0.010391
iter  90 value 0.009822
iter 100 value 0.009206
final  value 0.009206 
stopped after 100 iterations
# weights:  188
initial  value 1552.549403 
iter  10 value 34.219327
iter  20 value 0.745064
iter  30 value 0.122522
iter  40 value 0.041847
iter  50 value 0.023779
iter  60 value 0.018186
iter  70 value 0.016231
iter  80 value 0.015103
iter  90 value 0.014112
iter 100 value 0.013299
final  value 0.013299 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00135; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.00135
# weights:  188
initial  value 2763.699403 
iter  10 value 14.872951
iter  20 value 0.497494
iter  30 value 0.080691
iter  40 value 0.032312
iter  50 value 0.017771
iter  60 value 0.013963
iter  70 value 0.012406
iter  80 value 0.011619
iter  90 value 0.011119
iter 100 value 0.010457
final  value 0.010457 
stopped after 100 iterations
[1] "Fri Feb 09 15:55:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 645 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 387
 number of selected nodes                               : 89 
[1] "Fri Feb 09 15:55:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 15:55:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 15:55:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 15:55:58 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0332; time: 0.1 min
[Tune-x] 3: mtry=5; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0338; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0485; time: 0.0 min
[Tune-x] 5: mtry=7; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0342; time: 0.1 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.036; time: 0.1 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0518; time: 0.0 min
[Tune-x] 8: mtry=8; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 9: mtry=7; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0452; time: 0.1 min
[Tune-x] 11: mtry=4; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.033; time: 0.1 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0465; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.0328; time: 0.1 min
[Tune-x] 15: mtry=3; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0333; time: 0.1 min
[Tune-x] 16: mtry=8; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0408; time: 0.1 min
[Tune-x] 17: mtry=4; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0324; time: 0.1 min
[Tune-x] 18: mtry=3; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0353; time: 0.1 min
[Tune-x] 19: mtry=8; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0357; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=3 : rmse.test.rmse=0.0324
[1] "Fri Feb 09 15:57:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 15:57:44 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.327; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.327; time: 0.0 min
[Tune] Result: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46 : rmse.test.rmse=0.119
[1] "Fri Feb 09 15:57:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 15:57:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 15:58:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.27513 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2308 
    Y 1          1.855 
    Y 2          3.131 
    Y 3          2.043 
    Y 4           1.88 
    Y 5          1.641 
[1] "Fri Feb 09 15:58:16 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
[Tune-y] 1: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
[Tune-y] 3: rmse.test.rmse=0.192; time: 0.5 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
[Tune-y] 4: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
[Tune-y] 5: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
[Tune-y] 6: rmse.test.rmse=0.223; time: 0.2 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
[Tune-y] 8: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.262; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
[Tune-y] 10: rmse.test.rmse=0.959; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.196; time: 0.3 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
[Tune-y] 15: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.158; time: 0.8 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
[Tune-y] 18: rmse.test.rmse=0.212; time: 1.2 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
[Tune-y] 19: rmse.test.rmse=0.0774; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
[Tune-y] 20: rmse.test.rmse=0.387; time: 0.0 min
[Tune] Result: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386 : rmse.test.rmse=0.0774
[1] "Fri Feb 09 16:01:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 16:01:50 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.405552], n=(309,443)
**GROW** @depth 1: [2,0.499677], n=(121,322)
**GROW** @depth 1: [6,0.501598], n=(228,81)
**GROW** @depth 2: [1,0.203473], n=(90,138)
**GROW** @depth 2: [8,0.454385], n=(66,55)
**GROW** @depth 2: [8,0.356779], n=(28,292)
**GROW** @depth 3: [7,0.453675], n=(74,17)
**GROW** @depth 4: [1,0.713256], n=(184,109)
**GROW** @depth 5: [1,0.556943], n=(52,130)
**GROW** @depth 6: [5,0.496557], n=(41,89)
**GROW** @depth 2: [4,0.50031], n=(108,74)
**GROW** @depth 4: [9,0.500424], n=(57,15)
**GROW** @depth 3: [4,0.75973], n=(50,24)
**GROW** @depth 4: [6,0.516495], n=(21,88)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(73,17,30,109,50,25,57,16,54,28,54,41,89,21,88)
**GROW** @depth 5: [8,0.6864], n=(22,66)
**GROW** @depth 3: [3,0.501387], n=(17,38)
**GROW** @depth 6: [1,0.807573], n=(38,28)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(78,18,30,103,50,23,57,16,16,40,28,47,48,89,21,23,37,28)

Sampling @ nn=0 pred locs:
**GROW** @depth 6: [3,0.612162], n=(51,38)
**PRUNE** @depth 3: [8,0.467194]
**GROW** @depth 4: [3,0.625737], n=(29,25)
**GROW** @depth 3: [4,0.200732], n=(45,58)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(78,17,31,45,57,50,23,55,21,55,28,29,24,73,19,39,17,26,40,25)
**PRUNE** @depth 4: [4,0.200732]
**GROW** @depth 4: [1,0.0818701], n=(28,48)
**GROW** @depth 5: [1,0.151025], n=(27,20)
**PRUNE** @depth 5: [1,0.151417]
**GROW** @depth 4: [8,0.471179], n=(12,39)
**CPRUNE** @depth 3: var=4, val=0.489744->0.490251, n=(105,50)
**GROW** @depth 6: [6,0.49143], n=(20,53)
**GROW** @depth 4: [7,0.147303], n=(31,74)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(28,48,17,31,31,74,50,23,57,17,55,29,29,25,19,53,19,38,16,28,41,24)
**GROW** @depth 5: [4,0.216677], n=(30,19)
**GROW** @depth 4: [1,0.288666], n=(19,32)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(28,30,20,17,31,32,71,17,33,23,57,16,56,28,29,25,20,53,19,38,17,27,41,24)
**GROW** @depth 4: [1,0.287277], n=(14,18)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(27,31,20,18,31,14,18,71,17,33,23,55,18,55,28,30,25,20,53,18,38,16,28,42,23)
**PRUNE** @depth 5: [1,0.293231]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(27,30,20,16,32,32,72,17,33,23,56,18,55,28,29,26,20,52,18,37,16,31,41,23)
Grow: 7.821%, Prune: 1.124%, Change: 84.64%, Swap: 16.13%

[1] "Fri Feb 09 16:01:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 16:01:59 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 16:02:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 21.2175 	 alpha= 2.7588 	 beta= 49945.33 
[1] "Fri Feb 09 16:02:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 16:02:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [3,0.503522], n=(369,383)
**GROW** @depth 1: [9,0.483248], n=(84,297)
**GROW** @depth 1: [8,0.49966], n=(307,64)
**GROW** @depth 2: [1,0.53948], n=(258,49)
r=1000 d=[0] [0] [0] [0] [0]; n=(257,44,72,78,301)
**GROW** @depth 3: [3,0.246715], n=(47,209)
**GROW** @depth 4: [5,0.4989], n=(179,28)
**PRUNE** @depth 4: [5,0.4989]
r=2000 d=[0] [0] [0] [0] [0] [0]; n=(50,206,48,69,78,301)

Sampling @ nn=0 pred locs:
**GROW** @depth 4: [7,0.492851], n=(157,48)
**GROW** @depth 2: [7,0.549589], n=(127,174)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(52,157,47,48,69,78,128,173)
**GROW** @depth 3: [1,0.550442], n=(68,105)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(50,159,47,48,69,78,128,68,105)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(50,159,47,48,69,78,128,68,105)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(50,159,47,48,69,78,128,68,105)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(50,159,47,43,74,78,128,68,105)
Grow: 2.594%, Prune: 0.3003%, Change: 47.45%, Swap: 1.639%

[1] "Fri Feb 09 16:02:11 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 16:02:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 16:02:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 16:02:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 16:02:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 16:02:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 16:02:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 16:02:17 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:02:19 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0453; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0436; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.066; time: 0.1 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.04; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0248; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.0559; time: 0.1 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0537; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0414; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.0472; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0627; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.0814; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0423; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=0.401; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0563; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0363; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.0539; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0383; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0461; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0248
[1] "Fri Feb 09 16:02:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 16:02:45 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0661; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0207; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.02; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0287; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.121; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.639; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0271; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0323; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0401; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0113; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.639; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.639; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0853; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.055; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0341; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.481; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0105; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0303; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.106; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0105
[1] "Fri Feb 09 16:02:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:02:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=========================                                             |  36%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:03:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:03:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:03:09 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  10.36304 11.85911 3.16191 2.239088 12.25793 12.73945 5.615811 8.435478 11.07619 
  - best initial criterion value(s) :  3546.871 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3546.9  |proj g|=        4.783
At iterate     1  f =      -3557.7  |proj g|=        9.0272
At iterate     2  f =      -3562.8  |proj g|=        7.5086
At iterate     3  f =      -3566.3  |proj g|=        3.9291
At iterate     4  f =      -3566.6  |proj g|=        3.7609
At iterate     5  f =      -3568.3  |proj g|=        2.3708
At iterate     6  f =      -3569.6  |proj g|=        4.4465
At iterate     7  f =      -3571.9  |proj g|=         2.251
At iterate     8  f =      -3572.5  |proj g|=         2.438
At iterate     9  f =      -3572.8  |proj g|=        1.9169
At iterate    10  f =      -3573.2  |proj g|=        1.7742
At iterate    11  f =      -3573.5  |proj g|=        1.6744
At iterate    12  f =      -3574.2  |proj g|=        2.7891
At iterate    13  f =      -3574.6  |proj g|=        4.5254
At iterate    14  f =      -3575.3  |proj g|=        4.5029
At iterate    15  f =      -3575.9  |proj g|=        1.5358
At iterate    16  f =        -3576  |proj g|=        1.1517
At iterate    17  f =        -3576  |proj g|=       0.45988
At iterate    18  f =        -3576  |proj g|=       0.45805
At iterate    19  f =      -3576.1  |proj g|=        1.1481
At iterate    20  f =      -3576.2  |proj g|=        1.2737
At iterate    21  f =      -3576.3  |proj g|=       0.78218
At iterate    22  f =      -3576.3  |proj g|=       0.44585
At iterate    23  f =      -3576.4  |proj g|=        1.1662
At iterate    24  f =      -3576.5  |proj g|=        1.2234
At iterate    25  f =        -3577  |proj g|=         2.167
At iterate    26  f =      -3579.6  |proj g|=        5.6979
ys=-3.307e+00  -gs= 1.367e+00, BFGS update SKIPPED
At iterate    27  f =        -3581  |proj g|=        5.5963
At iterate    28  f =      -3582.6  |proj g|=        4.5486
At iterate    29  f =      -3594.1  |proj g|=        6.3445
At iterate    30  f =      -3595.2  |proj g|=        6.2222
At iterate    31  f =        -3601  |proj g|=        5.9363
At iterate    32  f =      -3613.1  |proj g|=        5.5929
At iterate    33  f =        -3618  |proj g|=        4.1849
At iterate    34  f =      -3619.9  |proj g|=         4.642
At iterate    35  f =      -3620.2  |proj g|=        4.4476
At iterate    36  f =      -3621.8  |proj g|=        4.2393
At iterate    37  f =      -3623.9  |proj g|=        5.1582
At iterate    38  f =      -3625.9  |proj g|=         4.375
At iterate    39  f =      -3629.2  |proj g|=        3.8526
At iterate    40  f =        -3630  |proj g|=        2.8808
At iterate    41  f =      -3630.7  |proj g|=        2.2655
At iterate    42  f =      -3631.4  |proj g|=        2.1392
At iterate    43  f =      -3631.4  |proj g|=        2.1475
At iterate    44  f =      -3631.5  |proj g|=         2.159
At iterate    45  f =      -3631.9  |proj g|=        2.7076
At iterate    46  f =      -3632.1  |proj g|=         2.269
At iterate    47  f =      -3632.3  |proj g|=        2.1304
At iterate    48  f =      -3632.3  |proj g|=        2.1533
At iterate    49  f =      -3632.5  |proj g|=        2.1891
At iterate    50  f =      -3632.5  |proj g|=        2.1493
At iterate    51  f =      -3632.9  |proj g|=        2.0426
At iterate    52  f =      -3634.3  |proj g|=        2.1761
At iterate    53  f =      -3634.6  |proj g|=        1.5564
At iterate    54  f =      -3634.8  |proj g|=        2.1657
At iterate    55  f =      -3634.8  |proj g|=        1.1678
At iterate    56  f =      -3634.8  |proj g|=        1.1225
At iterate    57  f =      -3634.9  |proj g|=        1.0176
At iterate    58  f =      -3634.9  |proj g|=        1.0161
At iterate    59  f =      -3634.9  |proj g|=       0.76037
At iterate    60  f =      -3634.9  |proj g|=       0.23831
At iterate    61  f =        -3635  |proj g|=       0.22925
At iterate    62  f =        -3635  |proj g|=       0.29621
At iterate    63  f =        -3635  |proj g|=        1.5494
At iterate    64  f =        -3635  |proj g|=       0.92797
At iterate    65  f =        -3635  |proj g|=       0.12817
At iterate    66  f =        -3635  |proj g|=      0.067273
At iterate    67  f =        -3635  |proj g|=      0.068925
At iterate    68  f =        -3635  |proj g|=      0.049798
At iterate    69  f =        -3635  |proj g|=       0.22685
At iterate    70  f =        -3635  |proj g|=      0.021166
At iterate    71  f =        -3635  |proj g|=      0.014483
Bad direction in the line search;
   refresh the lbfgs memory and restart the iteration.
Line search cannot locate an adequate point after 20 function
and gradient evaluations
final  value -3634.978621 
stopped after 71 iterations
[1] "Fri Feb 09 16:07:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 69.9191, its = 12
i = 2 (of 248), d = 48.2062, its = 11
i = 3 (of 248), d = 49.2015, its = 11
i = 4 (of 248), d = 94.4399, its = 12
i = 5 (of 248), d = 74.1974, its = 12
i = 6 (of 248), d = 43.0889, its = 11
i = 7 (of 248), d = 78.4862, its = 11
i = 8 (of 248), d = 63.3781, its = 11
i = 9 (of 248), d = 29.8477, its = 10
i = 10 (of 248), d = 82.577, its = 12
i = 11 (of 248), d = 33.7577, its = 11
i = 12 (of 248), d = 54.2711, its = 12
i = 13 (of 248), d = 61.306, its = 11
i = 14 (of 248), d = 79.277, its = 12
i = 15 (of 248), d = 45.8031, its = 11
i = 16 (of 248), d = 41.4414, its = 11
i = 17 (of 248), d = 50.7041, its = 11
i = 18 (of 248), d = 49.7544, its = 11
i = 19 (of 248), d = 23.1943, its = 10
i = 20 (of 248), d = 50.625, its = 11
i = 21 (of 248), d = 80.9206, its = 12
i = 22 (of 248), d = 34.8262, its = 10
i = 23 (of 248), d = 57.0732, its = 11
i = 24 (of 248), d = 61.289, its = 11
i = 25 (of 248), d = 68.4469, its = 11
i = 26 (of 248), d = 77.2695, its = 12
i = 27 (of 248), d = 44.4367, its = 11
i = 28 (of 248), d = 33.1485, its = 10
i = 29 (of 248), d = 66.9334, its = 12
i = 30 (of 248), d = 84.4664, its = 12
i = 31 (of 248), d = 68.0095, its = 11
i = 32 (of 248), d = 64.3184, its = 12
i = 33 (of 248), d = 73.843, its = 12
i = 34 (of 248), d = 21.1264, its = 10
i = 35 (of 248), d = 80.139, its = 12
i = 36 (of 248), d = 91.852, its = 12
i = 37 (of 248), d = 87.2602, its = 12
i = 38 (of 248), d = 74.7941, its = 12
i = 39 (of 248), d = 41.4585, its = 11
i = 40 (of 248), d = 35.0621, its = 11
i = 41 (of 248), d = 55.2526, its = 11
i = 42 (of 248), d = 27.6715, its = 10
i = 43 (of 248), d = 50.3083, its = 11
i = 44 (of 248), d = 43.6515, its = 11
i = 45 (of 248), d = 54.0809, its = 11
i = 46 (of 248), d = 45.7193, its = 11
i = 47 (of 248), d = 36.187, its = 11
i = 48 (of 248), d = 24.0831, its = 10
i = 49 (of 248), d = 43.5724, its = 11
i = 50 (of 248), d = 29.4805, its = 10
i = 51 (of 248), d = 77.25, its = 12
i = 52 (of 248), d = 48.7416, its = 11
i = 53 (of 248), d = 100.974, its = 12
i = 54 (of 248), d = 39.5941, its = 11
i = 55 (of 248), d = 71.4676, its = 12
i = 56 (of 248), d = 27.4346, its = 10
i = 57 (of 248), d = 67.2855, its = 12
i = 58 (of 248), d = 56.7623, its = 11
i = 59 (of 248), d = 53.4059, its = 11
i = 60 (of 248), d = 70.6425, its = 11
i = 61 (of 248), d = 87.3836, its = 12
i = 62 (of 248), d = 65.7969, its = 11
i = 63 (of 248), d = 79.0548, its = 12
i = 64 (of 248), d = 70.6986, its = 12
i = 65 (of 248), d = 32.1653, its = 10
i = 66 (of 248), d = 96.018, its = 12
i = 67 (of 248), d = 18.1708, its = 9
i = 68 (of 248), d = 51.6374, its = 11
i = 69 (of 248), d = 33.7625, its = 10
i = 70 (of 248), d = 17.3666, its = 10
i = 71 (of 248), d = 69.5808, its = 12
i = 72 (of 248), d = 66.294, its = 11
i = 73 (of 248), d = 40.1089, its = 11
i = 74 (of 248), d = 95.2806, its = 12
i = 75 (of 248), d = 41.1363, its = 11
i = 76 (of 248), d = 60.5061, its = 11
i = 77 (of 248), d = 34.9603, its = 11
i = 78 (of 248), d = 77.3311, its = 12
i = 79 (of 248), d = 63.1678, its = 12
i = 80 (of 248), d = 63.1905, its = 11
i = 81 (of 248), d = 35.3571, its = 11
i = 82 (of 248), d = 28.8185, its = 10
i = 83 (of 248), d = 58.7823, its = 12
i = 84 (of 248), d = 51.432, its = 12
i = 85 (of 248), d = 58.4254, its = 11
i = 86 (of 248), d = 24.5552, its = 10
i = 87 (of 248), d = 66.4874, its = 11
i = 88 (of 248), d = 48.405, its = 11
i = 89 (of 248), d = 47.2522, its = 11
i = 90 (of 248), d = 64.5376, its = 11
i = 91 (of 248), d = 90.2458, its = 12
i = 92 (of 248), d = 59.0222, its = 11
i = 93 (of 248), d = 45.3094, its = 11
i = 94 (of 248), d = 86.0075, its = 12
i = 95 (of 248), d = 74.3467, its = 12
i = 96 (of 248), d = 24.5945, its = 10
i = 97 (of 248), d = 58.7393, its = 11
i = 98 (of 248), d = 38.0226, its = 11
i = 99 (of 248), d = 48.4393, its = 11
i = 100 (of 248), d = 23.4778, its = 10
i = 101 (of 248), d = 24.0005, its = 10
i = 102 (of 248), d = 55.7468, its = 11
i = 103 (of 248), d = 21.0443, its = 10
i = 104 (of 248), d = 42.183, its = 11
i = 105 (of 248), d = 47.7846, its = 12
i = 106 (of 248), d = 58.8897, its = 11
i = 107 (of 248), d = 65.4856, its = 12
i = 108 (of 248), d = 48.0329, its = 11
i = 109 (of 248), d = 104.446, its = 12
i = 110 (of 248), d = 28.0816, its = 10
i = 111 (of 248), d = 27.9099, its = 11
i = 112 (of 248), d = 45.1214, its = 11
i = 113 (of 248), d = 45.2868, its = 11
i = 114 (of 248), d = 32.5551, its = 10
i = 115 (of 248), d = 25.6134, its = 10
i = 116 (of 248), d = 83.3207, its = 12
i = 117 (of 248), d = 39.6919, its = 11
i = 118 (of 248), d = 54.7516, its = 12
i = 119 (of 248), d = 80.5839, its = 12
i = 120 (of 248), d = 58.8501, its = 12
i = 121 (of 248), d = 56.8981, its = 11
i = 122 (of 248), d = 94.5454, its = 12
i = 123 (of 248), d = 55.0511, its = 11
i = 124 (of 248), d = 61.7682, its = 12
i = 125 (of 248), d = 86.3208, its = 12
i = 126 (of 248), d = 55.9986, its = 11
i = 127 (of 248), d = 30.9203, its = 11
i = 128 (of 248), d = 27.704, its = 10
i = 129 (of 248), d = 63.4645, its = 11
i = 130 (of 248), d = 94.3751, its = 12
i = 131 (of 248), d = 83.6029, its = 12
i = 132 (of 248), d = 60.9601, its = 11
i = 133 (of 248), d = 88.2285, its = 12
i = 134 (of 248), d = 29.8084, its = 10
i = 135 (of 248), d = 38.1115, its = 11
i = 136 (of 248), d = 82.7769, its = 12
i = 137 (of 248), d = 93.7206, its = 12
i = 138 (of 248), d = 79.3848, its = 12
i = 139 (of 248), d = 62.2609, its = 11
i = 140 (of 248), d = 85.5744, its = 12
i = 141 (of 248), d = 64.145, its = 12
i = 142 (of 248), d = 77.5914, its = 12
i = 143 (of 248), d = 74.2803, its = 12
i = 144 (of 248), d = 73.9225, its = 12
i = 145 (of 248), d = 28.5123, its = 10
i = 146 (of 248), d = 51.5251, its = 12
i = 147 (of 248), d = 65.544, its = 11
i = 148 (of 248), d = 96.6782, its = 12
i = 149 (of 248), d = 50.3543, its = 11
i = 150 (of 248), d = 74.2405, its = 12
i = 151 (of 248), d = 24.8658, its = 10
i = 152 (of 248), d = 97.8989, its = 12
i = 153 (of 248), d = 62.0279, its = 11
i = 154 (of 248), d = 52.0094, its = 11
i = 155 (of 248), d = 58.3561, its = 11
i = 156 (of 248), d = 66.9735, its = 11
i = 157 (of 248), d = 47.6599, its = 11
i = 158 (of 248), d = 69.7841, its = 11
i = 159 (of 248), d = 23.7087, its = 10
i = 160 (of 248), d = 63.2988, its = 11
i = 161 (of 248), d = 96.4701, its = 12
i = 162 (of 248), d = 43.2487, its = 11
i = 163 (of 248), d = 60.0116, its = 11
i = 164 (of 248), d = 58.6383, its = 12
i = 165 (of 248), d = 60.1766, its = 12
i = 166 (of 248), d = 37.3673, its = 11
i = 167 (of 248), d = 61.0604, its = 11
i = 168 (of 248), d = 42.215, its = 11
i = 169 (of 248), d = 32.992, its = 11
i = 170 (of 248), d = 65.9152, its = 11
i = 171 (of 248), d = 57.2441, its = 11
i = 172 (of 248), d = 81.3391, its = 12
i = 173 (of 248), d = 61.114, its = 12
i = 174 (of 248), d = 49.0702, its = 11
i = 175 (of 248), d = 88.2745, its = 12
i = 176 (of 248), d = 96.9106, its = 12
i = 177 (of 248), d = 73.0777, its = 11
i = 178 (of 248), d = 34.5561, its = 11
i = 179 (of 248), d = 41.7198, its = 11
i = 180 (of 248), d = 70.3386, its = 12
i = 181 (of 248), d = 43.2585, its = 11
i = 182 (of 248), d = 42.6636, its = 11
i = 183 (of 248), d = 77.4715, its = 12
i = 184 (of 248), d = 49.5825, its = 11
i = 185 (of 248), d = 55.8896, its = 12
i = 186 (of 248), d = 22.938, its = 10
i = 187 (of 248), d = 93.2494, its = 12
i = 188 (of 248), d = 79.4158, its = 11
i = 189 (of 248), d = 39.8544, its = 10
i = 190 (of 248), d = 95.0846, its = 12
i = 191 (of 248), d = 26.0684, its = 10
i = 192 (of 248), d = 39.9369, its = 11
i = 193 (of 248), d = 35.4475, its = 11
i = 194 (of 248), d = 84.7227, its = 12
i = 195 (of 248), d = 23.076, its = 10
i = 196 (of 248), d = 35.8121, its = 10
i = 197 (of 248), d = 29.3929, its = 10
i = 198 (of 248), d = 60.901, its = 11
i = 199 (of 248), d = 52.5061, its = 11
i = 200 (of 248), d = 48.4795, its = 11
i = 201 (of 248), d = 27.9892, its = 10
i = 202 (of 248), d = 74.7596, its = 11
i = 203 (of 248), d = 50.2242, its = 12
i = 204 (of 248), d = 28.9227, its = 10
i = 205 (of 248), d = 79.4262, its = 12
i = 206 (of 248), d = 29.5569, its = 10
i = 207 (of 248), d = 63.0504, its = 11
i = 208 (of 248), d = 44.3951, its = 11
i = 209 (of 248), d = 71.9744, its = 11
i = 210 (of 248), d = 87.1041, its = 12
i = 211 (of 248), d = 21.6503, its = 10
i = 212 (of 248), d = 41.0012, its = 11
i = 213 (of 248), d = 56.8745, its = 11
i = 214 (of 248), d = 43.1753, its = 11
i = 215 (of 248), d = 66.3659, its = 12
i = 216 (of 248), d = 49.5656, its = 11
i = 217 (of 248), d = 25.4212, its = 10
i = 218 (of 248), d = 68.2691, its = 11
i = 219 (of 248), d = 19.5435, its = 9
i = 220 (of 248), d = 50.6706, its = 11
i = 221 (of 248), d = 51.1869, its = 11
i = 222 (of 248), d = 84.4228, its = 12
i = 223 (of 248), d = 84.5537, its = 12
i = 224 (of 248), d = 73.7298, its = 11
i = 225 (of 248), d = 71.0354, its = 11
i = 226 (of 248), d = 46.622, its = 11
i = 227 (of 248), d = 92.616, its = 12
i = 228 (of 248), d = 72.86, its = 12
i = 229 (of 248), d = 83.4088, its = 12
i = 230 (of 248), d = 18.3701, its = 9
i = 231 (of 248), d = 77.5503, its = 12
i = 232 (of 248), d = 30.1975, its = 10
i = 233 (of 248), d = 83.2484, its = 12
i = 234 (of 248), d = 76.6115, its = 12
i = 235 (of 248), d = 26.3281, its = 10
i = 236 (of 248), d = 49.3421, its = 11
i = 237 (of 248), d = 30.4769, its = 10
i = 238 (of 248), d = 42.265, its = 11
i = 239 (of 248), d = 58.3404, its = 12
i = 240 (of 248), d = 62.9414, its = 12
i = 241 (of 248), d = 32.181, its = 11
i = 242 (of 248), d = 48.6238, its = 12
i = 243 (of 248), d = 60.1652, its = 12
i = 244 (of 248), d = 70.7457, its = 11
i = 245 (of 248), d = 42.6647, its = 11
i = 246 (of 248), d = 56.376, its = 11
i = 247 (of 248), d = 51.3329, its = 11
i = 248 (of 248), d = 58.4704, its = 11
[1] "Fri Feb 09 16:08:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 16:09:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 16:09:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 16:09:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 16:09:17 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 1688.076967 
iter  10 value 4.909028
iter  20 value 0.496401
iter  30 value 0.264576
iter  40 value 0.191112
iter  50 value 0.126704
iter  60 value 0.104732
iter  70 value 0.095224
iter  80 value 0.090495
iter  90 value 0.087449
iter 100 value 0.085639
final  value 0.085639 
stopped after 100 iterations
# weights:  210
initial  value 1173.506125 
iter  10 value 11.694142
iter  20 value 1.832422
iter  30 value 0.474157
iter  40 value 0.295165
iter  50 value 0.216396
iter  60 value 0.171459
iter  70 value 0.157767
iter  80 value 0.147944
iter  90 value 0.141142
iter 100 value 0.131915
final  value 0.131915 
stopped after 100 iterations
# weights:  210
initial  value 595.920525 
iter  10 value 21.907282
iter  20 value 2.618699
iter  30 value 0.722326
iter  40 value 0.428176
iter  50 value 0.332392
iter  60 value 0.271989
iter  70 value 0.246381
iter  80 value 0.219296
iter  90 value 0.206291
iter 100 value 0.196180
final  value 0.196180 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00499; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 282.806357 
iter  10 value 6.885330
iter  20 value 0.757194
iter  30 value 0.131825
iter  40 value 0.047240
iter  50 value 0.030847
iter  60 value 0.023435
iter  70 value 0.020935
iter  80 value 0.019326
iter  90 value 0.018064
iter 100 value 0.016796
final  value 0.016796 
stopped after 100 iterations
# weights:  144
initial  value 6126.953569 
iter  10 value 14.503822
iter  20 value 1.389778
iter  30 value 0.275451
iter  40 value 0.080182
iter  50 value 0.049348
iter  60 value 0.044148
iter  70 value 0.041215
iter  80 value 0.038537
iter  90 value 0.034152
iter 100 value 0.033160
final  value 0.033160 
stopped after 100 iterations
# weights:  144
initial  value 1385.561817 
iter  10 value 30.026813
iter  20 value 8.359942
iter  30 value 1.155381
iter  40 value 0.227388
iter  50 value 0.066019
iter  60 value 0.044429
iter  70 value 0.037397
iter  80 value 0.035240
iter  90 value 0.033152
iter 100 value 0.031521
final  value 0.031521 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00253; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 674.565933 
iter  10 value 11.253823
iter  20 value 1.324212
iter  30 value 0.160515
iter  40 value 0.051178
iter  50 value 0.029944
iter  60 value 0.024772
iter  70 value 0.021890
iter  80 value 0.019679
iter  90 value 0.018008
iter 100 value 0.016676
final  value 0.016676 
stopped after 100 iterations
# weights:  133
initial  value 6832.006597 
iter  10 value 56.745918
iter  20 value 8.831297
iter  30 value 0.774858
iter  40 value 0.127350
iter  50 value 0.054970
iter  60 value 0.039287
iter  70 value 0.034797
iter  80 value 0.031782
iter  90 value 0.029701
iter 100 value 0.025946
final  value 0.025946 
stopped after 100 iterations
# weights:  133
initial  value 2119.817296 
iter  10 value 46.268190
iter  20 value 3.261794
iter  30 value 0.226094
iter  40 value 0.059754
iter  50 value 0.033790
iter  60 value 0.024095
iter  70 value 0.020691
iter  80 value 0.018290
iter  90 value 0.016727
iter 100 value 0.014443
final  value 0.014443 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00258; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 541.656746 
iter  10 value 33.692538
iter  20 value 4.198316
iter  30 value 1.490850
iter  40 value 0.557180
iter  50 value 0.334053
iter  60 value 0.254202
iter  70 value 0.189825
iter  80 value 0.164152
iter  90 value 0.158668
iter 100 value 0.156414
final  value 0.156414 
stopped after 100 iterations
# weights:  23
initial  value 773.882779 
iter  10 value 56.582061
iter  20 value 11.969194
iter  30 value 2.204525
iter  40 value 0.812275
iter  50 value 0.413605
iter  60 value 0.349649
iter  70 value 0.239507
iter  80 value 0.168759
iter  90 value 0.136671
iter 100 value 0.130324
final  value 0.130324 
stopped after 100 iterations
# weights:  23
initial  value 1949.548360 
iter  10 value 63.953291
iter  20 value 18.074689
iter  30 value 6.446039
iter  40 value 5.315878
iter  50 value 3.514957
iter  60 value 2.450906
iter  70 value 1.125153
iter  80 value 0.263262
iter  90 value 0.244322
iter 100 value 0.207784
final  value 0.207784 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.00977; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 311.768921 
iter  10 value 6.060716
iter  20 value 1.217718
iter  30 value 0.723800
iter  40 value 0.468119
iter  50 value 0.404316
iter  60 value 0.380081
iter  70 value 0.352931
iter  80 value 0.331215
iter  90 value 0.309461
iter 100 value 0.292513
final  value 0.292513 
stopped after 100 iterations
# weights:  210
initial  value 2092.174818 
iter  10 value 13.678048
iter  20 value 2.018722
iter  30 value 0.920854
iter  40 value 0.623903
iter  50 value 0.495899
iter  60 value 0.451413
iter  70 value 0.413862
iter  80 value 0.381646
iter  90 value 0.354989
iter 100 value 0.335719
final  value 0.335719 
stopped after 100 iterations
# weights:  210
initial  value 336.767012 
iter  10 value 6.854142
iter  20 value 1.567148
iter  30 value 0.730725
iter  40 value 0.434545
iter  50 value 0.299564
iter  60 value 0.258836
iter  70 value 0.238925
iter  80 value 0.220723
iter  90 value 0.212240
iter 100 value 0.206939
final  value 0.206939 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00882; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 3241.820330 
iter  10 value 65.825805
iter  20 value 24.746846
iter  30 value 17.351496
iter  40 value 13.709727
iter  50 value 11.929780
iter  60 value 10.996303
iter  70 value 9.865514
iter  80 value 9.432252
iter  90 value 9.205388
iter 100 value 9.070037
final  value 9.070037 
stopped after 100 iterations
# weights:  199
initial  value 1084.945256 
iter  10 value 66.411208
iter  20 value 29.837793
iter  30 value 20.182995
iter  40 value 16.851205
iter  50 value 11.710649
iter  60 value 10.099917
iter  70 value 9.400031
iter  80 value 9.076106
iter  90 value 8.947889
iter 100 value 8.912767
final  value 8.912767 
stopped after 100 iterations
# weights:  199
initial  value 1571.502075 
iter  10 value 125.162051
iter  20 value 87.028658
iter  30 value 65.947542
iter  40 value 51.495159
iter  50 value 26.843075
iter  60 value 17.857202
iter  70 value 13.637303
iter  80 value 11.934598
iter  90 value 10.899615
iter 100 value 10.276390
final  value 10.276390 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 1682.069848 
iter  10 value 9.814134
iter  20 value 1.666823
iter  30 value 0.300995
iter  40 value 0.140230
iter  50 value 0.089076
iter  60 value 0.069365
iter  70 value 0.060674
iter  80 value 0.056113
iter  90 value 0.051259
iter 100 value 0.049472
final  value 0.049472 
stopped after 100 iterations
# weights:  144
initial  value 1165.506804 
iter  10 value 144.618237
iter  20 value 32.422108
iter  30 value 1.421362
iter  40 value 0.665323
iter  50 value 0.476999
iter  60 value 0.266179
iter  70 value 0.167204
iter  80 value 0.111460
iter  90 value 0.089551
iter 100 value 0.074740
final  value 0.074740 
stopped after 100 iterations
# weights:  144
initial  value 2456.025586 
iter  10 value 74.232085
iter  20 value 19.893951
iter  30 value 2.081880
iter  40 value 0.350120
iter  50 value 0.217721
iter  60 value 0.179066
iter  70 value 0.151206
iter  80 value 0.136155
iter  90 value 0.127258
iter 100 value 0.120579
final  value 0.120579 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00492; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 3094.867400 
iter  10 value 12.647996
iter  20 value 1.004547
iter  30 value 0.308505
iter  40 value 0.182114
iter  50 value 0.144206
iter  60 value 0.123424
iter  70 value 0.118753
iter  80 value 0.114912
iter  90 value 0.111935
iter 100 value 0.108185
final  value 0.108185 
stopped after 100 iterations
# weights:  89
initial  value 1746.998542 
iter  10 value 21.316986
iter  20 value 3.097141
iter  30 value 0.382558
iter  40 value 0.159897
iter  50 value 0.124440
iter  60 value 0.108896
iter  70 value 0.100971
iter  80 value 0.096059
iter  90 value 0.092721
iter 100 value 0.089306
final  value 0.089306 
stopped after 100 iterations
# weights:  89
initial  value 3067.350309 
iter  10 value 20.936428
iter  20 value 3.151968
iter  30 value 0.660409
iter  40 value 0.239832
iter  50 value 0.153413
iter  60 value 0.132567
iter  70 value 0.117007
iter  80 value 0.106749
iter  90 value 0.098960
iter 100 value 0.092088
final  value 0.092088 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00586; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 2640.944556 
iter  10 value 32.975748
iter  20 value 7.965604
iter  30 value 1.574311
iter  40 value 0.444915
iter  50 value 0.256805
iter  60 value 0.195055
iter  70 value 0.134857
iter  80 value 0.109967
iter  90 value 0.093999
iter 100 value 0.084684
final  value 0.084684 
stopped after 100 iterations
# weights:  210
initial  value 3566.344870 
iter  10 value 21.526961
iter  20 value 6.662724
iter  30 value 1.336565
iter  40 value 0.394740
iter  50 value 0.312805
iter  60 value 0.247016
iter  70 value 0.219569
iter  80 value 0.200177
iter  90 value 0.189413
iter 100 value 0.181785
final  value 0.181785 
stopped after 100 iterations
# weights:  210
initial  value 234.223802 
iter  10 value 16.284328
iter  20 value 2.176770
iter  30 value 0.657106
iter  40 value 0.212479
iter  50 value 0.138169
iter  60 value 0.115933
iter  70 value 0.094726
iter  80 value 0.081139
iter  90 value 0.074968
iter 100 value 0.070733
final  value 0.070733 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00419; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 2592.852135 
iter  10 value 49.220281
iter  20 value 2.798392
iter  30 value 0.576842
iter  40 value 0.306926
iter  50 value 0.154444
iter  60 value 0.074242
iter  70 value 0.041502
iter  80 value 0.028743
iter  90 value 0.025554
iter 100 value 0.023669
final  value 0.023669 
stopped after 100 iterations
# weights:  56
initial  value 2371.520500 
iter  10 value 12.121749
iter  20 value 2.449484
iter  30 value 0.368644
iter  40 value 0.099907
iter  50 value 0.028898
iter  60 value 0.011318
iter  70 value 0.008569
iter  80 value 0.007413
iter  90 value 0.006754
iter 100 value 0.006179
final  value 0.006179 
stopped after 100 iterations
# weights:  56
initial  value 2104.789084 
iter  10 value 49.950772
iter  20 value 4.228171
iter  30 value 0.683757
iter  40 value 0.229599
iter  50 value 0.085917
iter  60 value 0.041106
iter  70 value 0.028249
iter  80 value 0.018635
iter  90 value 0.013527
iter 100 value 0.008489
final  value 0.008489 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00439; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 2985.284775 
iter  10 value 160.571638
iter  20 value 122.439511
iter  30 value 108.087225
iter  40 value 79.145184
iter  50 value 43.146679
iter  60 value 23.297203
iter  70 value 15.452315
iter  80 value 10.422863
iter  90 value 8.632519
iter 100 value 7.863284
final  value 7.863284 
stopped after 100 iterations
# weights:  199
initial  value 1890.269236 
iter  10 value 50.430637
iter  20 value 20.601190
iter  30 value 15.359547
iter  40 value 12.305486
iter  50 value 10.519770
iter  60 value 9.204941
iter  70 value 8.221643
iter  80 value 7.720237
iter  90 value 7.475313
iter 100 value 7.330763
final  value 7.330763 
stopped after 100 iterations
# weights:  199
initial  value 272.625096 
iter  10 value 47.646117
iter  20 value 19.238139
iter  30 value 13.762713
iter  40 value 10.653684
iter  50 value 8.523475
iter  60 value 7.867450
iter  70 value 7.571827
iter  80 value 7.423205
iter  90 value 7.315055
iter 100 value 7.244166
final  value 7.244166 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0468; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 841.478402 
iter  10 value 34.235667
iter  20 value 15.905378
iter  30 value 11.670818
iter  40 value 10.078372
iter  50 value 9.653492
iter  60 value 9.241249
iter  70 value 8.838180
iter  80 value 8.602245
iter  90 value 8.388324
iter 100 value 8.211822
final  value 8.211822 
stopped after 100 iterations
# weights:  122
initial  value 1464.722853 
iter  10 value 81.459327
iter  20 value 49.685597
iter  30 value 34.248222
iter  40 value 21.910600
iter  50 value 14.209783
iter  60 value 11.628986
iter  70 value 10.016230
iter  80 value 8.818272
iter  90 value 8.440548
iter 100 value 8.204491
final  value 8.204491 
stopped after 100 iterations
# weights:  122
initial  value 4834.872863 
iter  10 value 155.429567
iter  20 value 108.102125
iter  30 value 64.271861
iter  40 value 38.532090
iter  50 value 29.765092
iter  60 value 22.189585
iter  70 value 15.235156
iter  80 value 12.871783
iter  90 value 11.178649
iter 100 value 10.208269
final  value 10.208269 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0448; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 2990.779936 
iter  10 value 26.533177
iter  20 value 2.924374
iter  30 value 2.090730
iter  40 value 1.782716
iter  50 value 1.600869
iter  60 value 1.206085
iter  70 value 1.060928
iter  80 value 1.004350
iter  90 value 0.974365
iter 100 value 0.948846
final  value 0.948846 
stopped after 100 iterations
# weights:  56
initial  value 573.494979 
iter  10 value 79.193750
iter  20 value 18.474085
iter  30 value 2.168632
iter  40 value 0.961076
iter  50 value 0.739538
iter  60 value 0.663992
iter  70 value 0.628050
iter  80 value 0.585504
iter  90 value 0.557874
iter 100 value 0.542709
final  value 0.542709 
stopped after 100 iterations
# weights:  56
initial  value 1280.427983 
iter  10 value 33.864982
iter  20 value 3.584517
iter  30 value 0.965282
iter  40 value 0.802317
iter  50 value 0.751198
iter  60 value 0.710274
iter  70 value 0.640076
iter  80 value 0.600943
iter  90 value 0.585127
iter 100 value 0.566097
final  value 0.566097 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0152; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 378.603913 
iter  10 value 48.515559
iter  20 value 7.682880
iter  30 value 1.413608
iter  40 value 0.646980
iter  50 value 0.455028
iter  60 value 0.381236
iter  70 value 0.327720
iter  80 value 0.292888
iter  90 value 0.250731
iter 100 value 0.212346
final  value 0.212346 
stopped after 100 iterations
# weights:  144
initial  value 2127.164740 
iter  10 value 50.510148
iter  20 value 5.109257
iter  30 value 1.174230
iter  40 value 0.573188
iter  50 value 0.346948
iter  60 value 0.249824
iter  70 value 0.198629
iter  80 value 0.167814
iter  90 value 0.144324
iter 100 value 0.130550
final  value 0.130550 
stopped after 100 iterations
# weights:  144
initial  value 2000.005330 
iter  10 value 100.777115
iter  20 value 10.603301
iter  30 value 3.011529
iter  40 value 1.577571
iter  50 value 1.157699
iter  60 value 0.929263
iter  70 value 0.807486
iter  80 value 0.736725
iter  90 value 0.685453
iter 100 value 0.655396
final  value 0.655396 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00743; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 2436.784290 
iter  10 value 70.162005
iter  20 value 10.735937
iter  30 value 4.722828
iter  40 value 2.749012
iter  50 value 2.249212
iter  60 value 1.915756
iter  70 value 1.731176
iter  80 value 1.551134
iter  90 value 1.417249
iter 100 value 1.256406
final  value 1.256406 
stopped after 100 iterations
# weights:  122
initial  value 2078.524052 
iter  10 value 32.182422
iter  20 value 8.261500
iter  30 value 5.419604
iter  40 value 4.219287
iter  50 value 3.475478
iter  60 value 2.799503
iter  70 value 2.212608
iter  80 value 1.945374
iter  90 value 1.753049
iter 100 value 1.505403
final  value 1.505403 
stopped after 100 iterations
# weights:  122
initial  value 164.511024 
iter  10 value 19.717502
iter  20 value 4.536635
iter  30 value 2.120571
iter  40 value 1.676893
iter  50 value 1.413779
iter  60 value 1.255528
iter  70 value 1.117788
iter  80 value 1.064378
iter  90 value 1.024257
iter 100 value 0.979057
final  value 0.979057 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.0176; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 2478.851418 
iter  10 value 68.593285
iter  20 value 18.586576
iter  30 value 1.887399
iter  40 value 0.475441
iter  50 value 0.266874
iter  60 value 0.205136
iter  70 value 0.165118
iter  80 value 0.147034
iter  90 value 0.126783
iter 100 value 0.109623
final  value 0.109623 
stopped after 100 iterations
# weights:  100
initial  value 1672.109044 
iter  10 value 24.336688
iter  20 value 3.285094
iter  30 value 0.450242
iter  40 value 0.175376
iter  50 value 0.118545
iter  60 value 0.093604
iter  70 value 0.081388
iter  80 value 0.075347
iter  90 value 0.071125
iter 100 value 0.067244
final  value 0.067244 
stopped after 100 iterations
# weights:  100
initial  value 169.871640 
iter  10 value 6.908240
iter  20 value 1.072497
iter  30 value 0.279475
iter  40 value 0.116557
iter  50 value 0.090293
iter  60 value 0.069750
iter  70 value 0.064837
iter  80 value 0.061644
iter  90 value 0.059817
iter 100 value 0.057972
final  value 0.057972 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.00483; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 1856.995164 
iter  10 value 72.692478
iter  20 value 22.690483
iter  30 value 15.877948
iter  40 value 14.213954
iter  50 value 12.728502
iter  60 value 12.039675
iter  70 value 11.083764
iter  80 value 10.554347
iter  90 value 10.190712
iter 100 value 10.049919
final  value 10.049919 
stopped after 100 iterations
# weights:  56
initial  value 1545.391216 
iter  10 value 47.939097
iter  20 value 14.794892
iter  30 value 11.652911
iter  40 value 10.821315
iter  50 value 10.350734
iter  60 value 10.101042
iter  70 value 10.047540
iter  80 value 10.036191
iter  90 value 10.027258
iter 100 value 10.002841
final  value 10.002841 
stopped after 100 iterations
# weights:  56
initial  value 1841.769916 
iter  10 value 56.863405
iter  20 value 23.827568
iter  30 value 14.333473
iter  40 value 12.399906
iter  50 value 11.804547
iter  60 value 11.429755
iter  70 value 10.889714
iter  80 value 10.678321
iter  90 value 10.509414
iter 100 value 10.465415
final  value 10.465415 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.054; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 188.251665 
iter  10 value 8.063610
iter  20 value 2.115943
iter  30 value 0.332472
iter  40 value 0.047945
iter  50 value 0.018860
iter  60 value 0.007624
iter  70 value 0.003826
iter  80 value 0.002263
iter  90 value 0.001829
iter 100 value 0.001540
final  value 0.001540 
stopped after 100 iterations
# weights:  155
initial  value 1370.717695 
iter  10 value 12.648977
iter  20 value 1.256631
iter  30 value 0.113921
iter  40 value 0.033717
iter  50 value 0.012667
iter  60 value 0.005989
iter  70 value 0.004028
iter  80 value 0.003094
iter  90 value 0.002574
iter 100 value 0.002291
final  value 0.002291 
stopped after 100 iterations
# weights:  155
initial  value 4589.667276 
iter  10 value 19.142385
iter  20 value 1.387880
iter  30 value 0.103979
iter  40 value 0.028355
iter  50 value 0.012506
iter  60 value 0.005748
iter  70 value 0.002727
iter  80 value 0.001988
iter  90 value 0.001702
iter 100 value 0.001455
final  value 0.001455 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00196; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 1026.832571 
iter  10 value 89.894512
iter  20 value 24.564199
iter  30 value 8.931547
iter  40 value 0.699318
iter  50 value 0.173393
iter  60 value 0.126451
iter  70 value 0.112693
iter  80 value 0.107681
iter  90 value 0.098779
iter 100 value 0.090993
final  value 0.090993 
stopped after 100 iterations
# weights:  45
initial  value 1291.516857 
iter  10 value 83.963227
iter  20 value 7.712568
iter  30 value 1.844745
iter  40 value 0.509147
iter  50 value 0.275644
iter  60 value 0.164822
iter  70 value 0.132698
iter  80 value 0.125733
iter  90 value 0.115904
iter 100 value 0.111533
final  value 0.111533 
stopped after 100 iterations
# weights:  45
initial  value 803.655859 
iter  10 value 33.249016
iter  20 value 6.498077
iter  30 value 0.862795
iter  40 value 0.382635
iter  50 value 0.239312
iter  60 value 0.189823
iter  70 value 0.169479
iter  80 value 0.161358
iter  90 value 0.155726
iter 100 value 0.154543
final  value 0.154543 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00612; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 1266.192036 
iter  10 value 18.022237
iter  20 value 3.460305
iter  30 value 1.500855
iter  40 value 1.125462
iter  50 value 0.935654
iter  60 value 0.781652
iter  70 value 0.712145
iter  80 value 0.674907
iter  90 value 0.653234
iter 100 value 0.636880
final  value 0.636880 
stopped after 100 iterations
# weights:  78
initial  value 169.547901 
iter  10 value 40.379543
iter  20 value 5.390320
iter  30 value 2.367799
iter  40 value 1.636856
iter  50 value 1.334734
iter  60 value 1.107772
iter  70 value 0.946716
iter  80 value 0.853094
iter  90 value 0.783025
iter 100 value 0.749828
final  value 0.749828 
stopped after 100 iterations
# weights:  78
initial  value 3483.656821 
iter  10 value 74.917901
iter  20 value 8.774388
iter  30 value 3.384744
iter  40 value 1.996575
iter  50 value 1.474531
iter  60 value 1.215173
iter  70 value 1.072195
iter  80 value 0.997143
iter  90 value 0.900973
iter 100 value 0.787280
final  value 0.787280 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0138; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00196
# weights:  155
initial  value 5425.379079 
iter  10 value 17.650654
iter  20 value 4.630173
iter  30 value 0.364672
iter  40 value 0.064191
iter  50 value 0.027563
iter  60 value 0.010841
iter  70 value 0.005925
iter  80 value 0.003981
iter  90 value 0.003115
iter 100 value 0.002441
final  value 0.002441 
stopped after 100 iterations
[1] "Fri Feb 09 16:09:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 580 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 360
 number of selected nodes                               : 79 
[1] "Fri Feb 09 16:09:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 16:09:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 16:10:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 16:10:07 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0409; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0359; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0355; time: 0.2 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0536; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0408; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0434; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0358; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0346; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0399; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0435; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0386; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0391; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0363; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0366; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0344; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0413; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0366; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0379; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0359; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0344
[1] "Fri Feb 09 16:12:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 16:12:08 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.639; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.369; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.369; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.369; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.179; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.639; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.169; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.369; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.218; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.149; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.178; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.369; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.355; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.132
[1] "Fri Feb 09 16:12:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 16:12:12 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:12:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 7 
Runtime: 1.895108 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1          1.854 
    Y 2            1.8 
    Y 3           0.69 
    Y 4          3.161 
    Y 5          2.177 
[1] "Fri Feb 09 16:12:48 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.155; time: 0.7 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.241; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.145; time: 0.5 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.161; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.213; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.176; time: 0.8 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.231; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.561; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.199; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.174; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.265; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.277; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.183; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.193; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.245; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.227; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.153; time: 0.0 min
[Tune] Result: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683 : rmse.test.rmse=0.144
[1] "Fri Feb 09 16:15:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 16:15:24 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [9,0.500622], n=(397,355)
**GROW** @depth 1: [8,0.501956], n=(84,268)
**GROW** @depth 2: [6,0.517264], n=(51,214)
**GROW** @depth 3: [1,0.501725], n=(26,188)
**GROW** @depth 1: [1,0.46942], n=(369,23)
**GROW** @depth 4: [3,0.666167], n=(13,37)
**PRUNE** @depth 4: [6,0.514264]
**GROW** @depth 4: [1,0.74507], n=(79,12)
**GROW** @depth 4: [9,0.755758], n=(78,70)
**GROW** @depth 2: [3,0.448984], n=(33,76)
**GROW** @depth 3: [2,0.491468], n=(14,19)
**GROW** @depth 2: [6,0.501413], n=(32,66)
**GROW** @depth 5: [1,0.621717], n=(47,37)
**GROW** @depth 2: [8,0.28483], n=(113,163)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(113,163,14,19,75,32,66,26,47,37,61,26,73)
**GROW** @depth 3: [8,0.396268], n=(78,83)
**GROW** @depth 5: [1,0.624248], n=(21,40)
**PRUNE** @depth 3: [9,0.755758]
**GROW** @depth 3: [6,0.75354], n=(53,13)
**PRUNE** @depth 3: [3,0.445905]
**GROW** @depth 4: [3,0.495787], n=(65,14)
**GROW** @depth 4: [1,0.8738], n=(74,25)
**GROW** @depth 3: [8,0.402553], n=(41,30)
**PRUNE** @depth 4: [8,0.400752]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(115,114,45,39,41,30,32,53,13,24,48,36,21,42,72,27)

Sampling @ nn=0 pred locs:
**GROW** @depth 3: [3,0.500003], n=(22,19)
**PRUNE** @depth 3: [3,0.500003]
**PRUNE** @depth 3: [6,0.75273]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(119,109,44,41,41,33,33,64,24,46,34,22,43,72,27)
**PRUNE** @depth 6: [1,0.619609]
**GROW** @depth 6: [8,0.750851], n=(51,15)
**GROW** @depth 5: [7,0.463103], n=(26,45)
**GROW** @depth 3: [6,0.754694], n=(51,13)
**GROW** @depth 4: [8,0.428274], n=(73,35)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(120,73,35,47,42,39,35,31,50,13,26,44,32,50,16,25,45,29)
**GROW** @depth 4: [6,0.6873], n=(37,13)
**PRUNE** @depth 4: [6,0.688226]
**PRUNE** @depth 4: [8,0.42921]
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(122,106,47,45,41,35,28,51,13,24,44,31,51,16,23,45,30)
**PRUNE** @depth 5: [7,0.457261]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(125,100,46,45,42,33,30,52,13,25,45,31,51,15,71,28)
**PRUNE** @depth 5: [8,0.752206]
**GROW** @depth 3: [1,0.228632], n=(67,58)
**GROW** @depth 3: [1,0.108594], n=(34,33)
**GROW** @depth 4: [3,0.275743], n=(13,100)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(34,34,49,13,101,41,45,42,33,31,50,13,24,46,32,65,72,27)
Grow: 8.069%, Prune: 3.099%, Change: 80.43%, Swap: 20.17%

[1] "Fri Feb 09 16:15:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 16:15:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 16:15:38 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 20.6836 	 alpha= 1.9622 	 beta= 44356.41 
[1] "Fri Feb 09 16:15:39 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 16:15:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [8,0.488665], n=(362,390)
**GROW** @depth 1: [8,0.243666], n=(102,260)
**GROW** @depth 2: [8,0.753563], n=(309,81)
**GROW** @depth 2: [1,0.487125], n=(177,78)
**GROW** @depth 3: [8,0.3455], n=(67,115)
**GROW** @depth 2: [4,0.501152], n=(76,228)
r=1000 d=[0] [0] [0] [0] [0] [0] [0]; n=(107,69,113,130,142,110,81)
r=2000 d=[0] [0] [0] [0] [0] [0] [0]; n=(108,68,113,130,142,110,81)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0] [0] [0] [0]; mh=5 n=(108,68,113,130,142,110,81)
**GROW** @depth 3: [5,0.506838], n=(34,96)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0]; mh=5 n=(107,69,113,34,96,142,110,81)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0]; mh=5 n=(107,69,113,34,96,142,110,81)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0]; mh=5 n=(107,69,113,34,96,142,110,81)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0]; mh=5 n=(107,69,113,34,96,142,110,81)
Grow: 1.994%, Prune: 0%, Change: 20%, Swap: 19.05%

[1] "Fri Feb 09 16:15:47 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 16:15:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 16:15:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 16:15:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 16:15:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 16:15:52 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 16:15:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 16:15:53 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:15:57 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0609; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0456; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0595; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.0528; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0714; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0561; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0467; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.056; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0817; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0509; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0574; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0561; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0506; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0464; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0528; time: 0.0 min
[Tune] Result: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16 : rmse.test.rmse=0.0456
[1] "Fri Feb 09 16:16:09 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 16:16:11 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.059; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0373; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.0824; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0235; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0223; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.127; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.068; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.013; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0396; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.179; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0143; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0226; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0268; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.642; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0234; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.135; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0173; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.013
[1] "Fri Feb 09 16:16:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |===============================================================       |  90%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:16:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=========================                                             |  36%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:16:27 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:16:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:16:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  10.02659 11.80112 3.14847 2.212944 12.31193 12.79471 5.722394 8.474374 11.07619 
  - best initial criterion value(s) :  3596.524 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3596.5  |proj g|=       5.3648
At iterate     1  f =        -3620  |proj g|=        6.0826
At iterate     2  f =      -3629.1  |proj g|=        6.5176
At iterate     3  f =      -3636.9  |proj g|=        8.3185
At iterate     4  f =      -3641.3  |proj g|=        4.6944
At iterate     5  f =      -3644.8  |proj g|=        4.1824
At iterate     6  f =      -3647.4  |proj g|=        7.8562
At iterate     7  f =      -3651.8  |proj g|=        5.7868
At iterate     8  f =      -3653.3  |proj g|=        3.5103
At iterate     9  f =      -3653.9  |proj g|=        2.3657
At iterate    10  f =      -3654.7  |proj g|=        2.2117
At iterate    11  f =      -3655.1  |proj g|=         2.112
At iterate    12  f =        -3656  |proj g|=        2.5388
At iterate    13  f =      -3657.8  |proj g|=        2.2991
At iterate    14  f =      -3658.6  |proj g|=        3.6325
At iterate    15  f =      -3660.2  |proj g|=       0.78217
At iterate    16  f =      -3660.5  |proj g|=       0.95398
At iterate    17  f =      -3660.5  |proj g|=       0.88999
At iterate    18  f =      -3660.6  |proj g|=       0.74267
At iterate    19  f =      -3660.8  |proj g|=       0.88922
At iterate    20  f =      -3660.9  |proj g|=        1.0718
At iterate    21  f =        -3661  |proj g|=        1.1779
At iterate    22  f =      -3661.1  |proj g|=        0.4113
At iterate    23  f =      -3661.1  |proj g|=       0.34506
At iterate    24  f =      -3661.1  |proj g|=       0.41696
At iterate    25  f =      -3661.1  |proj g|=       0.39528
At iterate    26  f =      -3661.1  |proj g|=       0.38168
At iterate    27  f =      -3661.3  |proj g|=        1.0589
At iterate    28  f =      -3661.4  |proj g|=       0.84294
At iterate    29  f =      -3661.4  |proj g|=       0.29931
At iterate    30  f =      -3661.4  |proj g|=       0.19563
At iterate    31  f =      -3661.5  |proj g|=        0.1493
At iterate    32  f =      -3661.5  |proj g|=       0.10596
At iterate    33  f =      -3661.5  |proj g|=      0.071046
At iterate    34  f =      -3661.5  |proj g|=      0.044121
At iterate    35  f =      -3661.5  |proj g|=       0.03789
At iterate    36  f =      -3661.5  |proj g|=     0.0050864
At iterate    37  f =      -3661.5  |proj g|=     0.0050632

iterations 37
function evaluations 53
segments explored during Cauchy searches 40
BFGS updates skipped 0
active bounds at final generalized Cauchy point 2
norm of the final projected gradient 0.00506324
final function value -3661.45

F = -3661.45
final  value -3661.453077 
converged
[1] "Fri Feb 09 16:18:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 80.3764, its = 12
i = 2 (of 248), d = 88.3069, its = 12
i = 3 (of 248), d = 87.9028, its = 12
i = 4 (of 248), d = 75.8238, its = 11
i = 5 (of 248), d = 58.5462, its = 11
i = 6 (of 248), d = 62.1823, its = 11
i = 7 (of 248), d = 73.7911, its = 11
i = 8 (of 248), d = 56.2931, its = 11
i = 9 (of 248), d = 16.6206, its = 9
i = 10 (of 248), d = 60.8084, its = 12
i = 11 (of 248), d = 41.7497, its = 10
i = 12 (of 248), d = 38.2541, its = 11
i = 13 (of 248), d = 82.2022, its = 12
i = 14 (of 248), d = 18.7811, its = 9
i = 15 (of 248), d = 52.9801, its = 11
i = 16 (of 248), d = 39.2597, its = 11
i = 17 (of 248), d = 25.5622, its = 10
i = 18 (of 248), d = 61.9602, its = 12
i = 19 (of 248), d = 37.6501, its = 11
i = 20 (of 248), d = 46.4041, its = 11
i = 21 (of 248), d = 51.9755, its = 11
i = 22 (of 248), d = 75.793, its = 11
i = 23 (of 248), d = 47.1414, its = 11
i = 24 (of 248), d = 60.9864, its = 11
i = 25 (of 248), d = 48.1983, its = 11
i = 26 (of 248), d = 27.0268, its = 10
i = 27 (of 248), d = 88.6418, its = 12
i = 28 (of 248), d = 76.818, its = 12
i = 29 (of 248), d = 16.8582, its = 9
i = 30 (of 248), d = 56.85, its = 11
i = 31 (of 248), d = 68.8979, its = 11
i = 32 (of 248), d = 70.9685, its = 12
i = 33 (of 248), d = 20.0436, its = 10
i = 34 (of 248), d = 71.314, its = 11
i = 35 (of 248), d = 67.0135, its = 11
i = 36 (of 248), d = 47.9189, its = 11
i = 37 (of 248), d = 74.6695, its = 11
i = 38 (of 248), d = 61.8898, its = 11
i = 39 (of 248), d = 21.1416, its = 10
i = 40 (of 248), d = 68.7249, its = 11
i = 41 (of 248), d = 68.7748, its = 11
i = 42 (of 248), d = 34.23, its = 10
i = 43 (of 248), d = 80.2447, its = 12
i = 44 (of 248), d = 63.3034, its = 11
i = 45 (of 248), d = 62.3984, its = 11
i = 46 (of 248), d = 69.7171, its = 12
i = 47 (of 248), d = 84.8541, its = 12
i = 48 (of 248), d = 58.2245, its = 12
i = 49 (of 248), d = 24.2608, its = 10
i = 50 (of 248), d = 73.6214, its = 12
i = 51 (of 248), d = 49.9159, its = 11
i = 52 (of 248), d = 69.1268, its = 11
i = 53 (of 248), d = 97.9276, its = 12
i = 54 (of 248), d = 45.6875, its = 11
i = 55 (of 248), d = 31.7638, its = 10
i = 56 (of 248), d = 62.4996, its = 11
i = 57 (of 248), d = 53.1183, its = 11
i = 58 (of 248), d = 72.833, its = 12
i = 59 (of 248), d = 79.6925, its = 12
i = 60 (of 248), d = 74.2402, its = 12
i = 61 (of 248), d = 50.2223, its = 11
i = 62 (of 248), d = 56.2454, its = 11
i = 63 (of 248), d = 93.2636, its = 12
i = 64 (of 248), d = 31.9144, its = 11
i = 65 (of 248), d = 39.6805, its = 11
i = 66 (of 248), d = 68.9612, its = 11
i = 67 (of 248), d = 53.3237, its = 12
i = 68 (of 248), d = 45.9509, its = 11
i = 69 (of 248), d = 46.6434, its = 11
i = 70 (of 248), d = 48.4011, its = 11
i = 71 (of 248), d = 46.1113, its = 12
i = 72 (of 248), d = 26.9023, its = 10
i = 73 (of 248), d = 63.4058, its = 11
i = 74 (of 248), d = 81.8881, its = 12
i = 75 (of 248), d = 78.9713, its = 12
i = 76 (of 248), d = 49.9277, its = 11
i = 77 (of 248), d = 72.4235, its = 12
i = 78 (of 248), d = 39.2094, its = 11
i = 79 (of 248), d = 95.2924, its = 12
i = 80 (of 248), d = 47.7011, its = 11
i = 81 (of 248), d = 51.248, its = 12
i = 82 (of 248), d = 66.8481, its = 12
i = 83 (of 248), d = 48.7994, its = 11
i = 84 (of 248), d = 40.177, its = 11
i = 85 (of 248), d = 31.7944, its = 10
i = 86 (of 248), d = 18.1163, its = 9
i = 87 (of 248), d = 47.1272, its = 11
i = 88 (of 248), d = 52.3316, its = 11
i = 89 (of 248), d = 48.1268, its = 11
i = 90 (of 248), d = 60.573, its = 11
i = 91 (of 248), d = 40.084, its = 11
i = 92 (of 248), d = 40.078, its = 11
i = 93 (of 248), d = 23.8511, its = 10
i = 94 (of 248), d = 54.5212, its = 11
i = 95 (of 248), d = 52.7239, its = 11
i = 96 (of 248), d = 49.56, its = 11
i = 97 (of 248), d = 73.2105, its = 11
i = 98 (of 248), d = 71.1928, its = 12
i = 99 (of 248), d = 54.7599, its = 11
i = 100 (of 248), d = 60.5693, its = 11
i = 101 (of 248), d = 76.1514, its = 11
i = 102 (of 248), d = 60.6878, its = 11
i = 103 (of 248), d = 49.6065, its = 12
i = 104 (of 248), d = 76.1232, its = 11
i = 105 (of 248), d = 23.9788, its = 10
i = 106 (of 248), d = 60.9201, its = 12
i = 107 (of 248), d = 71.2591, its = 12
i = 108 (of 248), d = 57.2988, its = 11
i = 109 (of 248), d = 27.4899, its = 10
i = 110 (of 248), d = 29.5574, its = 11
i = 111 (of 248), d = 57.5314, its = 11
i = 112 (of 248), d = 18.7574, its = 10
i = 113 (of 248), d = 78.3037, its = 12
i = 114 (of 248), d = 30.2955, its = 10
i = 115 (of 248), d = 98.5362, its = 12
i = 116 (of 248), d = 28.192, its = 10
i = 117 (of 248), d = 57.5437, its = 11
i = 118 (of 248), d = 90.2429, its = 12
i = 119 (of 248), d = 61.2092, its = 11
i = 120 (of 248), d = 24.3266, its = 10
i = 121 (of 248), d = 32.4832, its = 11
i = 122 (of 248), d = 54.7729, its = 11
i = 123 (of 248), d = 55.0239, its = 11
i = 124 (of 248), d = 55.3385, its = 12
i = 125 (of 248), d = 31.6187, its = 10
i = 126 (of 248), d = 79.9025, its = 12
i = 127 (of 248), d = 58.4844, its = 11
i = 128 (of 248), d = 56.8116, its = 11
i = 129 (of 248), d = 39.3461, its = 11
i = 130 (of 248), d = 22.1076, its = 10
i = 131 (of 248), d = 20.9826, its = 10
i = 132 (of 248), d = 58.1807, its = 11
i = 133 (of 248), d = 81.1797, its = 12
i = 134 (of 248), d = 76.0023, its = 12
i = 135 (of 248), d = 22.0688, its = 10
i = 136 (of 248), d = 59.6584, its = 11
i = 137 (of 248), d = 52.1647, its = 11
i = 138 (of 248), d = 52.1265, its = 11
i = 139 (of 248), d = 93.0876, its = 12
i = 140 (of 248), d = 75.9048, its = 11
i = 141 (of 248), d = 75.6947, its = 12
i = 142 (of 248), d = 59.9148, its = 12
i = 143 (of 248), d = 60.7463, its = 11
i = 144 (of 248), d = 59.9666, its = 11
i = 145 (of 248), d = 36.4688, its = 11
i = 146 (of 248), d = 46.4804, its = 11
i = 147 (of 248), d = 88.2118, its = 12
i = 148 (of 248), d = 55.8488, its = 11
i = 149 (of 248), d = 75.0845, its = 12
i = 150 (of 248), d = 82.5189, its = 12
i = 151 (of 248), d = 26.2237, its = 10
i = 152 (of 248), d = 42.6733, its = 11
i = 153 (of 248), d = 66.1108, its = 11
i = 154 (of 248), d = 78.7583, its = 12
i = 155 (of 248), d = 86.4229, its = 12
i = 156 (of 248), d = 46.1663, its = 11
i = 157 (of 248), d = 80.7716, its = 12
i = 158 (of 248), d = 66.4876, its = 11
i = 159 (of 248), d = 68.1534, its = 11
i = 160 (of 248), d = 76.6152, its = 12
i = 161 (of 248), d = 30.3026, its = 10
i = 162 (of 248), d = 91.6124, its = 12
i = 163 (of 248), d = 74.8403, its = 12
i = 164 (of 248), d = 26.9695, its = 10
i = 165 (of 248), d = 56.9776, its = 11
i = 166 (of 248), d = 54.2721, its = 12
i = 167 (of 248), d = 30.0441, its = 10
i = 168 (of 248), d = 47.2179, its = 11
i = 169 (of 248), d = 71.1687, its = 11
i = 170 (of 248), d = 83.6094, its = 12
i = 171 (of 248), d = 64.7645, its = 12
i = 172 (of 248), d = 37.1517, its = 11
i = 173 (of 248), d = 70.8687, its = 11
i = 174 (of 248), d = 74.5102, its = 12
i = 175 (of 248), d = 67.0792, its = 11
i = 176 (of 248), d = 80.9677, its = 12
i = 177 (of 248), d = 59.8261, its = 11
i = 178 (of 248), d = 29.091, its = 11
i = 179 (of 248), d = 37.6996, its = 11
i = 180 (of 248), d = 84.4037, its = 12
i = 181 (of 248), d = 57.6754, its = 11
i = 182 (of 248), d = 67.4732, its = 11
i = 183 (of 248), d = 61.0358, its = 11
i = 184 (of 248), d = 26.199, its = 10
i = 185 (of 248), d = 78.8616, its = 12
i = 186 (of 248), d = 44.2469, its = 11
i = 187 (of 248), d = 37.5614, its = 11
i = 188 (of 248), d = 51.084, its = 11
i = 189 (of 248), d = 88.5725, its = 12
i = 190 (of 248), d = 24.1686, its = 10
i = 191 (of 248), d = 54.9858, its = 11
i = 192 (of 248), d = 62.7349, its = 12
i = 193 (of 248), d = 45.1594, its = 11
i = 194 (of 248), d = 26.8561, its = 11
i = 195 (of 248), d = 26.4898, its = 10
i = 196 (of 248), d = 59.4578, its = 11
i = 197 (of 248), d = 25.3252, its = 11
i = 198 (of 248), d = 61.6901, its = 11
i = 199 (of 248), d = 47.3422, its = 11
i = 200 (of 248), d = 63.1052, its = 11
i = 201 (of 248), d = 95.2579, its = 12
i = 202 (of 248), d = 40.7439, its = 11
i = 203 (of 248), d = 63.5914, its = 11
i = 204 (of 248), d = 62.8198, its = 11
i = 205 (of 248), d = 56.7053, its = 11
i = 206 (of 248), d = 72.4489, its = 12
i = 207 (of 248), d = 56.1064, its = 11
i = 208 (of 248), d = 63.9943, its = 12
i = 209 (of 248), d = 48.6337, its = 12
i = 210 (of 248), d = 71.4448, its = 12
i = 211 (of 248), d = 14.2434, its = 9
i = 212 (of 248), d = 42.9771, its = 11
i = 213 (of 248), d = 89.3082, its = 12
i = 214 (of 248), d = 90.2191, its = 12
i = 215 (of 248), d = 59.4319, its = 11
i = 216 (of 248), d = 89.596, its = 12
i = 217 (of 248), d = 67.4777, its = 11
i = 218 (of 248), d = 64.7408, its = 11
i = 219 (of 248), d = 71.7204, its = 11
i = 220 (of 248), d = 54.7208, its = 12
i = 221 (of 248), d = 71.7676, its = 12
i = 222 (of 248), d = 42.4074, its = 11
i = 223 (of 248), d = 36.0618, its = 11
i = 224 (of 248), d = 41.4985, its = 11
i = 225 (of 248), d = 77.856, its = 12
i = 226 (of 248), d = 63.8703, its = 11
i = 227 (of 248), d = 63.2242, its = 12
i = 228 (of 248), d = 95.2296, its = 12
i = 229 (of 248), d = 83.4591, its = 12
i = 230 (of 248), d = 66.9052, its = 11
i = 231 (of 248), d = 26.3586, its = 10
i = 232 (of 248), d = 27.7685, its = 10
i = 233 (of 248), d = 62.0211, its = 11
i = 234 (of 248), d = 26.2357, its = 10
i = 235 (of 248), d = 56.8073, its = 11
i = 236 (of 248), d = 37.1131, its = 10
i = 237 (of 248), d = 94.3333, its = 12
i = 238 (of 248), d = 45.7765, its = 11
i = 239 (of 248), d = 60.2689, its = 12
i = 240 (of 248), d = 52.1171, its = 11
i = 241 (of 248), d = 89.3642, its = 12
i = 242 (of 248), d = 56.695, its = 11
i = 243 (of 248), d = 63.367, its = 11
i = 244 (of 248), d = 43.193, its = 11
i = 245 (of 248), d = 83.6145, its = 12
i = 246 (of 248), d = 55.2799, its = 11
i = 247 (of 248), d = 73.9197, its = 12
i = 248 (of 248), d = 73.8857, its = 12
[1] "Fri Feb 09 16:20:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 16:20:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 16:20:03 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 16:20:04 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 16:20:21 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  12
initial  value 1856.051993 
iter  10 value 211.111990
iter  20 value 37.097916
iter  30 value 7.382431
iter  40 value 1.519306
iter  50 value 1.143806
iter  60 value 1.039634
final  value 1.039596 
converged
# weights:  12
initial  value 1500.176590 
iter  10 value 107.959737
iter  20 value 6.758269
iter  30 value 2.988026
iter  40 value 1.818138
iter  50 value 1.050418
iter  60 value 1.034972
final  value 1.034972 
converged
# weights:  12
initial  value 1091.801384 
iter  10 value 16.650665
iter  20 value 1.810600
iter  30 value 1.628732
iter  40 value 1.363116
iter  50 value 1.360541
final  value 1.360541 
converged
[Tune-y] 1: rmse.test.rmse=0.0276; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  155
initial  value 1028.904532 
iter  10 value 35.284788
iter  20 value 2.979493
iter  30 value 0.289489
iter  40 value 0.157326
iter  50 value 0.125007
iter  60 value 0.104358
iter  70 value 0.092772
iter  80 value 0.085292
iter  90 value 0.079948
iter 100 value 0.074447
final  value 0.074447 
stopped after 100 iterations
# weights:  155
initial  value 543.288209 
iter  10 value 4.627168
iter  20 value 1.032575
iter  30 value 0.218668
iter  40 value 0.137717
iter  50 value 0.101377
iter  60 value 0.084085
iter  70 value 0.071188
iter  80 value 0.061696
iter  90 value 0.057706
iter 100 value 0.054839
final  value 0.054839 
stopped after 100 iterations
# weights:  155
initial  value 4207.833102 
iter  10 value 20.145919
iter  20 value 0.829403
iter  30 value 0.380870
iter  40 value 0.221489
iter  50 value 0.130235
iter  60 value 0.106945
iter  70 value 0.093140
iter  80 value 0.083086
iter  90 value 0.076981
iter 100 value 0.070917
final  value 0.070917 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00534; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  111
initial  value 2444.273303 
iter  10 value 90.562713
iter  20 value 28.071211
iter  30 value 8.087669
iter  40 value 4.487782
iter  50 value 3.338522
iter  60 value 2.633589
iter  70 value 2.242034
iter  80 value 1.986317
iter  90 value 1.662222
iter 100 value 1.365357
final  value 1.365357 
stopped after 100 iterations
# weights:  111
initial  value 1913.718728 
iter  10 value 49.347783
iter  20 value 5.123861
iter  30 value 0.870374
iter  40 value 0.564099
iter  50 value 0.453884
iter  60 value 0.413367
iter  70 value 0.374711
iter  80 value 0.355319
iter  90 value 0.334253
iter 100 value 0.314608
final  value 0.314608 
stopped after 100 iterations
# weights:  111
initial  value 1324.710045 
iter  10 value 25.659524
iter  20 value 4.946730
iter  30 value 1.182700
iter  40 value 0.762587
iter  50 value 0.494188
iter  60 value 0.371577
iter  70 value 0.321987
iter  80 value 0.279717
iter  90 value 0.261080
iter 100 value 0.251794
final  value 0.251794 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0102; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  12
initial  value 1388.022716 
iter  10 value 210.128176
iter  20 value 209.999633
final  value 209.990646 
converged
# weights:  12
initial  value 2504.082874 
iter  10 value 11.861943
iter  20 value 4.705358
iter  30 value 1.748328
iter  40 value 0.539824
iter  50 value 0.253808
iter  60 value 0.201725
iter  70 value 0.163276
iter  80 value 0.161955
iter  90 value 0.161138
final  value 0.161116 
converged
# weights:  12
initial  value 406.895389 
iter  10 value 33.167685
iter  20 value 5.011471
iter  30 value 1.790733
iter  40 value 0.421195
iter  50 value 0.203997
iter  60 value 0.181134
iter  70 value 0.156227
iter  80 value 0.154844
iter  90 value 0.154693
iter  90 value 0.154693
iter  90 value 0.154693
final  value 0.154693 
converged
[Tune-y] 4: rmse.test.rmse=0.363; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  166
initial  value 134.797219 
iter  10 value 21.898879
iter  20 value 2.921710
iter  30 value 0.257745
iter  40 value 0.055806
iter  50 value 0.034653
iter  60 value 0.028676
iter  70 value 0.024465
iter  80 value 0.022142
iter  90 value 0.019664
iter 100 value 0.017832
final  value 0.017832 
stopped after 100 iterations
# weights:  166
initial  value 535.978929 
iter  10 value 19.953864
iter  20 value 2.456130
iter  30 value 0.518939
iter  40 value 0.078698
iter  50 value 0.041672
iter  60 value 0.028772
iter  70 value 0.023978
iter  80 value 0.021649
iter  90 value 0.018715
iter 100 value 0.017162
final  value 0.017162 
stopped after 100 iterations
# weights:  166
initial  value 10050.485140 
iter  10 value 24.626569
iter  20 value 1.706485
iter  30 value 0.186689
iter  40 value 0.058648
iter  50 value 0.041929
iter  60 value 0.035588
iter  70 value 0.031464
iter  80 value 0.027881
iter  90 value 0.026010
iter 100 value 0.023557
final  value 0.023557 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00242; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  155
initial  value 147.127461 
iter  10 value 4.135170
iter  20 value 0.997538
iter  30 value 0.583377
iter  40 value 0.365551
iter  50 value 0.316894
iter  60 value 0.294776
iter  70 value 0.279224
iter  80 value 0.263639
iter  90 value 0.251670
iter 100 value 0.244462
final  value 0.244462 
stopped after 100 iterations
# weights:  155
initial  value 1997.575845 
iter  10 value 8.924263
iter  20 value 2.305885
iter  30 value 1.051930
iter  40 value 0.545513
iter  50 value 0.364722
iter  60 value 0.331052
iter  70 value 0.309518
iter  80 value 0.292315
iter  90 value 0.280759
iter 100 value 0.273835
final  value 0.273835 
stopped after 100 iterations
# weights:  155
initial  value 775.860458 
iter  10 value 33.954207
iter  20 value 4.858029
iter  30 value 2.865859
iter  40 value 2.362514
iter  50 value 2.073174
iter  60 value 1.733048
iter  70 value 0.980986
iter  80 value 0.691953
iter  90 value 0.537783
iter 100 value 0.457124
final  value 0.457124 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.00892; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  34
initial  value 2933.954174 
iter  10 value 176.307423
iter  20 value 26.944810
iter  30 value 6.179900
iter  40 value 3.900657
iter  50 value 3.429414
iter  60 value 3.376853
iter  70 value 3.369714
iter  80 value 3.352392
iter  90 value 3.311555
iter 100 value 3.301354
final  value 3.301354 
stopped after 100 iterations
# weights:  34
initial  value 3468.618415 
iter  10 value 58.384299
iter  20 value 7.763413
iter  30 value 4.048500
iter  40 value 3.316805
iter  50 value 2.729235
iter  60 value 2.433232
iter  70 value 2.317997
iter  80 value 2.252806
iter  90 value 2.090315
iter 100 value 2.071691
final  value 2.071691 
stopped after 100 iterations
# weights:  34
initial  value 2177.150638 
iter  10 value 204.731455
iter  20 value 18.935657
iter  30 value 2.972987
iter  40 value 2.213884
iter  50 value 2.116186
iter  60 value 2.102569
iter  70 value 2.091373
iter  80 value 2.069183
iter  90 value 1.991406
iter 100 value 1.973784
final  value 1.973784 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0273; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  199
initial  value 1304.912783 
iter  10 value 6.450882
iter  20 value 1.620680
iter  30 value 0.729334
iter  40 value 0.459201
iter  50 value 0.328633
iter  60 value 0.264173
iter  70 value 0.240153
iter  80 value 0.229559
iter  90 value 0.220884
iter 100 value 0.214876
final  value 0.214876 
stopped after 100 iterations
# weights:  199
initial  value 2131.606767 
iter  10 value 35.287649
iter  20 value 9.734327
iter  30 value 4.105929
iter  40 value 2.294909
iter  50 value 1.482760
iter  60 value 1.092248
iter  70 value 0.881277
iter  80 value 0.741850
iter  90 value 0.665420
iter 100 value 0.616310
final  value 0.616310 
stopped after 100 iterations
# weights:  199
initial  value 1206.027693 
iter  10 value 9.308337
iter  20 value 1.387524
iter  30 value 0.695091
iter  40 value 0.409560
iter  50 value 0.321702
iter  60 value 0.290451
iter  70 value 0.275679
iter  80 value 0.266974
iter  90 value 0.257129
iter 100 value 0.245158
final  value 0.245158 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.008; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  155
initial  value 4994.116584 
iter  10 value 46.149327
iter  20 value 3.029992
iter  30 value 1.245865
iter  40 value 0.630622
iter  50 value 0.487273
iter  60 value 0.391961
iter  70 value 0.328192
iter  80 value 0.288187
iter  90 value 0.249250
iter 100 value 0.222638
final  value 0.222638 
stopped after 100 iterations
# weights:  155
initial  value 286.828649 
iter  10 value 7.297560
iter  20 value 1.125463
iter  30 value 0.374763
iter  40 value 0.249559
iter  50 value 0.182646
iter  60 value 0.164352
iter  70 value 0.153507
iter  80 value 0.148022
iter  90 value 0.144216
iter 100 value 0.140115
final  value 0.140115 
stopped after 100 iterations
# weights:  155
initial  value 3439.262619 
iter  10 value 32.099484
iter  20 value 7.201089
iter  30 value 1.382951
iter  40 value 0.782348
iter  50 value 0.553494
iter  60 value 0.425321
iter  70 value 0.352589
iter  80 value 0.309184
iter  90 value 0.265963
iter 100 value 0.238045
final  value 0.238045 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00595; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  12
initial  value 902.516130 
iter  10 value 42.179689
iter  20 value 4.884802
iter  30 value 1.348328
iter  40 value 0.361323
iter  50 value 0.130982
iter  60 value 0.104617
iter  70 value 0.062484
iter  80 value 0.048167
iter  90 value 0.044670
iter 100 value 0.035187
final  value 0.035187 
stopped after 100 iterations
# weights:  12
initial  value 1069.431297 
iter  10 value 23.147623
iter  20 value 3.542603
iter  30 value 1.226037
iter  40 value 0.384573
iter  50 value 0.119735
iter  60 value 0.094924
iter  70 value 0.056483
iter  80 value 0.042396
iter  90 value 0.039512
iter 100 value 0.032751
final  value 0.032751 
stopped after 100 iterations
# weights:  12
initial  value 1861.307634 
iter  10 value 202.906974
iter  20 value 172.504102
iter  30 value 41.911224
iter  40 value 6.734438
iter  50 value 1.522373
iter  60 value 0.513204
iter  70 value 0.169909
iter  80 value 0.120422
iter  90 value 0.069090
iter 100 value 0.045372
final  value 0.045372 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00886; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  89
initial  value 755.380691 
iter  10 value 27.630778
iter  20 value 3.360365
iter  30 value 0.576871
iter  40 value 0.276143
iter  50 value 0.203566
iter  60 value 0.151323
iter  70 value 0.132961
iter  80 value 0.120293
iter  90 value 0.112590
iter 100 value 0.107246
final  value 0.107246 
stopped after 100 iterations
# weights:  89
initial  value 1921.440325 
iter  10 value 34.380569
iter  20 value 7.899381
iter  30 value 0.941966
iter  40 value 0.457329
iter  50 value 0.340578
iter  60 value 0.244655
iter  70 value 0.207986
iter  80 value 0.182012
iter  90 value 0.158242
iter 100 value 0.145288
final  value 0.145288 
stopped after 100 iterations
# weights:  89
initial  value 642.549717 
iter  10 value 71.644411
iter  20 value 7.005958
iter  30 value 0.888625
iter  40 value 0.376473
iter  50 value 0.248191
iter  60 value 0.205813
iter  70 value 0.187493
iter  80 value 0.173456
iter  90 value 0.156756
iter 100 value 0.134676
final  value 0.134676 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00595; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  45
initial  value 322.985731 
iter  10 value 30.057992
iter  20 value 8.822851
iter  30 value 6.034988
iter  40 value 4.005283
iter  50 value 3.281741
iter  60 value 2.973579
iter  70 value 2.705139
iter  80 value 2.512861
iter  90 value 2.464431
iter 100 value 2.418932
final  value 2.418932 
stopped after 100 iterations
# weights:  45
initial  value 3265.121103 
iter  10 value 184.413193
iter  20 value 38.364285
iter  30 value 11.396229
iter  40 value 6.962690
iter  50 value 6.512467
iter  60 value 5.304140
iter  70 value 3.893664
iter  80 value 3.542754
iter  90 value 3.358009
iter 100 value 3.153954
final  value 3.153954 
stopped after 100 iterations
# weights:  45
initial  value 2750.784040 
iter  10 value 15.519143
iter  20 value 4.457082
iter  30 value 2.847048
iter  40 value 2.466207
iter  50 value 2.245033
iter  60 value 2.170029
iter  70 value 2.103579
iter  80 value 2.073935
iter  90 value 2.053739
iter 100 value 2.009302
final  value 2.009302 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0252; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  34
initial  value 690.830400 
iter  10 value 50.303190
iter  20 value 4.875623
iter  30 value 0.984364
iter  40 value 0.328998
iter  50 value 0.187612
iter  60 value 0.139067
iter  70 value 0.111139
iter  80 value 0.092915
iter  90 value 0.081244
iter 100 value 0.073824
final  value 0.073824 
stopped after 100 iterations
# weights:  34
initial  value 1423.499929 
iter  10 value 27.784770
iter  20 value 4.068884
iter  30 value 0.922589
iter  40 value 0.450464
iter  50 value 0.248454
iter  60 value 0.193214
iter  70 value 0.181294
iter  80 value 0.159681
iter  90 value 0.133644
iter 100 value 0.121900
final  value 0.121900 
stopped after 100 iterations
# weights:  34
initial  value 2340.811878 
iter  10 value 10.967984
iter  20 value 1.443436
iter  30 value 0.138858
iter  40 value 0.037327
iter  50 value 0.025561
iter  60 value 0.018640
iter  70 value 0.016655
iter  80 value 0.015207
iter  90 value 0.012189
iter 100 value 0.009789
final  value 0.009789 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00632; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  67
initial  value 2188.184186 
iter  10 value 23.016913
iter  20 value 2.967397
iter  30 value 0.489180
iter  40 value 0.118952
iter  50 value 0.056880
iter  60 value 0.043748
iter  70 value 0.033374
iter  80 value 0.027765
iter  90 value 0.023698
iter 100 value 0.021010
final  value 0.021010 
stopped after 100 iterations
# weights:  67
initial  value 1061.949919 
iter  10 value 20.968144
iter  20 value 2.130565
iter  30 value 0.363263
iter  40 value 0.142433
iter  50 value 0.094693
iter  60 value 0.069556
iter  70 value 0.057102
iter  80 value 0.049214
iter  90 value 0.044999
iter 100 value 0.042370
final  value 0.042370 
stopped after 100 iterations
# weights:  67
initial  value 2863.438042 
iter  10 value 30.052817
iter  20 value 3.016666
iter  30 value 0.352273
iter  40 value 0.129988
iter  50 value 0.064946
iter  60 value 0.049308
iter  70 value 0.039340
iter  80 value 0.033399
iter  90 value 0.030724
iter 100 value 0.028431
final  value 0.028431 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00416; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  67
initial  value 454.597108 
iter  10 value 25.460878
iter  20 value 1.519865
iter  30 value 0.208094
iter  40 value 0.096184
iter  50 value 0.080913
iter  60 value 0.074452
iter  70 value 0.069615
iter  80 value 0.066460
iter  90 value 0.063715
iter 100 value 0.060892
final  value 0.060892 
stopped after 100 iterations
# weights:  67
initial  value 243.382416 
iter  10 value 29.662452
iter  20 value 2.453828
iter  30 value 0.546992
iter  40 value 0.241884
iter  50 value 0.179085
iter  60 value 0.158157
iter  70 value 0.144244
iter  80 value 0.128184
iter  90 value 0.119436
iter 100 value 0.107332
final  value 0.107332 
stopped after 100 iterations
# weights:  67
initial  value 2659.449102 
iter  10 value 74.122396
iter  20 value 7.824932
iter  30 value 1.392566
iter  40 value 0.642286
iter  50 value 0.463041
iter  60 value 0.383595
iter  70 value 0.297571
iter  80 value 0.225255
iter  90 value 0.161617
iter 100 value 0.140011
final  value 0.140011 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00882; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  177
initial  value 2395.237623 
iter  10 value 134.708789
iter  20 value 92.807835
iter  30 value 77.925394
iter  40 value 61.521700
iter  50 value 47.848721
iter  60 value 41.175880
iter  70 value 39.605434
iter  80 value 39.062244
iter  90 value 38.625456
iter 100 value 38.416646
final  value 38.416646 
stopped after 100 iterations
# weights:  177
initial  value 848.533841 
iter  10 value 141.064409
iter  20 value 49.138498
iter  30 value 45.851501
iter  40 value 42.830992
iter  50 value 40.503252
iter  60 value 39.967097
iter  70 value 39.659214
iter  80 value 39.476447
iter  90 value 39.351994
iter 100 value 39.129928
final  value 39.129928 
stopped after 100 iterations
# weights:  177
initial  value 3670.664864 
iter  10 value 119.926193
iter  20 value 95.045970
iter  30 value 75.118238
iter  40 value 59.073993
iter  50 value 46.060520
iter  60 value 40.679202
iter  70 value 38.934359
iter  80 value 38.369256
iter  90 value 38.215413
iter 100 value 38.105051
final  value 38.105051 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  78
initial  value 529.799008 
iter  10 value 42.293284
iter  20 value 7.855019
iter  30 value 0.778679
iter  40 value 0.157090
iter  50 value 0.084155
iter  60 value 0.071031
iter  70 value 0.061961
iter  80 value 0.059024
iter  90 value 0.055097
iter 100 value 0.052387
final  value 0.052387 
stopped after 100 iterations
# weights:  78
initial  value 2711.861606 
iter  10 value 33.026243
iter  20 value 4.444573
iter  30 value 0.718730
iter  40 value 0.199238
iter  50 value 0.127032
iter  60 value 0.109113
iter  70 value 0.098258
iter  80 value 0.085376
iter  90 value 0.075651
iter 100 value 0.067810
final  value 0.067810 
stopped after 100 iterations
# weights:  78
initial  value 3664.200959 
iter  10 value 22.693098
iter  20 value 1.281137
iter  30 value 0.148509
iter  40 value 0.061940
iter  50 value 0.050864
iter  60 value 0.045584
iter  70 value 0.041061
iter  80 value 0.038949
iter  90 value 0.036981
iter 100 value 0.035500
final  value 0.035500 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00472; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  56
initial  value 1689.379033 
iter  10 value 13.062571
iter  20 value 1.855731
iter  30 value 1.503672
iter  40 value 1.251331
iter  50 value 1.132684
iter  60 value 1.079655
iter  70 value 1.072706
iter  80 value 1.066165
iter  90 value 1.054322
iter 100 value 1.043702
final  value 1.043702 
stopped after 100 iterations
# weights:  56
initial  value 1718.238743 
iter  10 value 11.882693
iter  20 value 3.795152
iter  30 value 2.561993
iter  40 value 1.945678
iter  50 value 1.488926
iter  60 value 1.183672
iter  70 value 1.025964
iter  80 value 0.978022
iter  90 value 0.963171
iter 100 value 0.946622
final  value 0.946622 
stopped after 100 iterations
# weights:  56
initial  value 767.901788 
iter  10 value 31.086049
iter  20 value 6.099164
iter  30 value 2.360865
iter  40 value 1.380380
iter  50 value 1.004828
iter  60 value 0.962688
iter  70 value 0.943941
iter  80 value 0.929558
iter  90 value 0.921040
iter 100 value 0.915428
final  value 0.915428 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  177
initial  value 763.108128 
iter  10 value 10.389537
iter  20 value 2.227786
iter  30 value 0.987759
iter  40 value 0.650321
iter  50 value 0.499828
iter  60 value 0.460937
iter  70 value 0.420624
iter  80 value 0.349016
iter  90 value 0.324251
iter 100 value 0.311621
final  value 0.311621 
stopped after 100 iterations
# weights:  177
initial  value 1054.449795 
iter  10 value 26.016844
iter  20 value 3.841741
iter  30 value 2.092952
iter  40 value 1.210118
iter  50 value 0.791880
iter  60 value 0.684802
iter  70 value 0.610287
iter  80 value 0.555972
iter  90 value 0.508811
iter 100 value 0.457712
final  value 0.457712 
stopped after 100 iterations
# weights:  177
initial  value 1243.177914 
iter  10 value 11.660759
iter  20 value 2.007824
iter  30 value 1.081115
iter  40 value 0.727490
iter  50 value 0.592350
iter  60 value 0.509356
iter  70 value 0.450102
iter  80 value 0.420130
iter  90 value 0.405607
iter 100 value 0.386303
final  value 0.386303 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0101; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  188
initial  value 2304.291266 
iter  10 value 16.748900
iter  20 value 1.715504
iter  30 value 0.304488
iter  40 value 0.057279
iter  50 value 0.018472
iter  60 value 0.013332
iter  70 value 0.011949
iter  80 value 0.010997
iter  90 value 0.010075
iter 100 value 0.009312
final  value 0.009312 
stopped after 100 iterations
# weights:  188
initial  value 5025.435623 
iter  10 value 15.843850
iter  20 value 0.590607
iter  30 value 0.087094
iter  40 value 0.035019
iter  50 value 0.022577
iter  60 value 0.017075
iter  70 value 0.014478
iter  80 value 0.013169
iter  90 value 0.012356
iter 100 value 0.011700
final  value 0.011700 
stopped after 100 iterations
# weights:  188
initial  value 1561.633260 
iter  10 value 7.407987
iter  20 value 1.330340
iter  30 value 0.221191
iter  40 value 0.052342
iter  50 value 0.025397
iter  60 value 0.016154
iter  70 value 0.011774
iter  80 value 0.009862
iter  90 value 0.009169
iter 100 value 0.008425
final  value 0.008425 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00146; time: 0.0 min
[Tune] Result: size=17; decay=0.000183 : rmse.test.rmse=0.00146
# weights:  188
initial  value 5807.692750 
iter  10 value 30.267506
iter  20 value 2.574438
iter  30 value 0.200177
iter  40 value 0.052535
iter  50 value 0.032794
iter  60 value 0.025950
iter  70 value 0.023572
iter  80 value 0.022002
iter  90 value 0.021006
iter 100 value 0.019807
final  value 0.019807 
stopped after 100 iterations
[1] "Fri Feb 09 16:20:40 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 608 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 367
 number of selected nodes                               : 94 
[1] "Fri Feb 09 16:20:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 16:20:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 16:21:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 16:21:12 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0332; time: 0.1 min
[Tune-x] 3: mtry=5; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0338; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0485; time: 0.0 min
[Tune-x] 5: mtry=7; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.036; time: 0.1 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0518; time: 0.0 min
[Tune-x] 8: mtry=8; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 9: mtry=7; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0453; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.033; time: 0.1 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0465; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.0328; time: 0.1 min
[Tune-x] 15: mtry=3; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0333; time: 0.1 min
[Tune-x] 16: mtry=8; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0409; time: 0.1 min
[Tune-x] 17: mtry=4; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0323; time: 0.1 min
[Tune-x] 18: mtry=3; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0353; time: 0.1 min
[Tune-x] 19: mtry=8; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0373; time: 0.1 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0357; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=3 : rmse.test.rmse=0.0323
[1] "Fri Feb 09 16:22:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 16:22:58 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.327; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.141; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.327; time: 0.0 min
[Tune] Result: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46 : rmse.test.rmse=0.118
[1] "Fri Feb 09 16:23:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 16:23:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:23:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 7 
Runtime: 1.91611 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2308 
    Y 1          1.847 
    Y 2           3.15 
    Y 3          2.033 
    Y 4          1.881 
    Y 5          1.632 
[1] "Fri Feb 09 16:23:36 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
[Tune-y] 1: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
[Tune-y] 3: rmse.test.rmse=0.192; time: 0.5 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
[Tune-y] 4: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
[Tune-y] 5: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
[Tune-y] 6: rmse.test.rmse=0.223; time: 0.2 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
[Tune-y] 8: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.262; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
[Tune-y] 10: rmse.test.rmse=0.959; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.196; time: 0.3 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
[Tune-y] 15: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.157; time: 0.8 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
[Tune-y] 18: rmse.test.rmse=0.212; time: 1.1 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
[Tune-y] 19: rmse.test.rmse=0.0739; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
[Tune-y] 20: rmse.test.rmse=0.387; time: 0.0 min
[Tune] Result: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386 : rmse.test.rmse=0.0739
[1] "Fri Feb 09 16:27:08 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 16:27:09 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.594666], n=(505,247)
**GROW** @depth 1: [4,0.550019], n=(179,66)
**GROW** @depth 1: [6,0.489538], n=(447,59)
**PRUNE** @depth 1: [6,0.492889]
**GROW** @depth 1: [1,0.316428], n=(205,299)
**GROW** @depth 1: [6,0.432497], n=(182,24)
**PRUNE** @depth 1: [6,0.432199]
**GROW** @depth 2: [3,0.516667], n=(113,181)
**GROW** @depth 3: [5,0.255448], n=(37,145)
**GROW** @depth 3: [8,0.499559], n=(50,63)
**PRUNE** @depth 3: [8,0.499559]
**GROW** @depth 3: [9,0.512821], n=(89,25)
**GROW** @depth 2: [8,0.563107], n=(124,87)
**GROW** @depth 2: [1,0.15983], n=(27,62)
**GROW** @depth 4: [6,0.155038], n=(17,70)
**GROW** @depth 3: [7,0.550962], n=(67,19)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(67,19,62,63,18,72,81,127,33,143,67)
**GROW** @depth 2: [7,0.28253], n=(53,14)
**GROW** @depth 4: [2,0.454561], n=(12,115)
**PRUNE** @depth 2: [7,0.28253]
**PRUNE** @depth 4: [9,0.512821]
**GROW** @depth 3: [4,0.27426], n=(77,68)
**GROW** @depth 4: [3,0.534375], n=(14,61)
**GROW** @depth 4: [1,0.257224], n=(28,33)
**PRUNE** @depth 4: [3,0.534375]
**GROW** @depth 4: [4,0.511053], n=(92,47)
**GROW** @depth 4: [4,0.278756], n=(46,45)
**GROW** @depth 3: [8,0.496028], n=(56,15)
**GROW** @depth 4: [8,0.403354], n=(41,15)
**GROW** @depth 3: [3,0.488542], n=(45,12)
**PRUNE** @depth 3: [3,0.488542]
**PRUNE** @depth 4: [4,0.279505]
**GROW** @depth 4: [3,0.55625], n=(31,60)
**PRUNE** @depth 3: [1,0.255001]
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(39,18,14,21,60,58,18,79,63,31,61,47,33,73,70,67)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 4: [8,0.499559]
**GROW** @depth 2: [1,0.804809], n=(43,24)
**GROW** @depth 4: [1,0.416246], n=(36,32)
**GROW** @depth 4: [1,0.224086], n=(18,40)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(39,32,20,60,19,40,18,79,35,32,32,61,43,31,71,95,20,25)
**GROW** @depth 4: [1,0.0935543], n=(26,12)
**PRUNE** @depth 4: [1,0.217418]
**GROW** @depth 4: [1,0.703172], n=(27,45)
**GROW** @depth 3: [3,0.5125], n=(12,19)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(27,12,30,21,61,59,19,78,38,35,32,56,41,13,12,35,39,98,20,26)
**PRUNE** @depth 5: [1,0.415235]
**GROW** @depth 4: [1,0.482118], n=(19,22)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(27,12,31,21,60,59,22,75,74,32,55,19,22,13,13,35,38,96,20,28)
**GROW** @depth 6: [1,0.44211], n=(48,28)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(27,14,28,21,60,60,24,45,31,76,28,55,19,22,14,12,34,38,96,20,28)
**PRUNE** @depth 3: [5,0.255448]
**GROW** @depth 4: [7,0.320251], n=(20,75)
**PRUNE** @depth 4: [7,0.320251]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(27,12,30,21,59,61,25,44,31,76,28,55,19,22,26,34,38,96,20,28)
Grow: 9.649%, Prune: 4%, Change: 67.99%, Swap: 10.03%

[1] "Fri Feb 09 16:27:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 16:27:17 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 16:27:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.4109 	 alpha= 1.8551 	 beta= 37675.81 
[1] "Fri Feb 09 16:27:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 16:27:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [4,0.0243537], n=(18,734)
**PRUNE** @depth 0: [4,0.0224803]
**GROW** @depth 0: [8,0.423654], n=(112,640)
**GROW** @depth 1: [8,0.712268], n=(341,299)
r=1000 d=[0] [0] [0]; n=(112,341,299)
**GROW** @depth 2: [6,0.232035], n=(154,187)
r=2000 d=[0] [0] [0] [0]; n=(112,154,187,299)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0]; mh=4 n=(112,152,189,299)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(112,152,189,299)
**GROW** @depth 3: [8,0.624007], n=(72,117)
r=3000 d=[0] [0] [0] [0] [0]; mh=5 n=(112,153,69,119,299)
r=4000 d=[0] [0] [0] [0] [0]; mh=5 n=(112,153,69,119,299)
r=5000 d=[0] [0] [0] [0] [0]; mh=5 n=(112,153,69,119,299)
Grow: 1.401%, Prune: 0.2899%, Change: 8.918%, Swap: 48.92%

[1] "Fri Feb 09 16:27:28 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 16:27:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 16:27:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 16:27:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 16:27:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 16:27:32 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 16:27:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 16:27:34 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:27:37 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0141; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0138; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.0213; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0124; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.00796; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.0171; time: 0.1 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0168; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0127; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.015; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0196; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.025; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0136; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.0406; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse=0.126; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0185; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0115; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.0173; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0116; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0145; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.00796
[1] "Fri Feb 09 16:28:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 16:28:02 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.061; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0116; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.0109; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0159; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0173; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0196; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0313; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0054; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0667; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0451; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0218; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.00533; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.0886; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.00533
[1] "Fri Feb 09 16:28:05 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |============================                                          |  40%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:28:14 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================                                                |  32%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:28:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:28:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |========                                                              |  12%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:28:25 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  2 2 1.935484 2 1.918082 1.932369 1.918144 1.979039 2 
  - best initial criterion value(s) :  4198.117 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -4198.1  |proj g|=       1.8835
At iterate     1  f =      -4213.8  |proj g|=        1.8338
At iterate     2  f =      -4297.8  |proj g|=        1.6525
At iterate     3  f =      -4328.1  |proj g|=         1.705
At iterate     4  f =      -4344.4  |proj g|=        1.6834
At iterate     5  f =      -4361.9  |proj g|=        1.5978
At iterate     6  f =      -4372.9  |proj g|=        1.4336
At iterate     7  f =      -4378.7  |proj g|=        1.2021
At iterate     8  f =      -4388.4  |proj g|=        1.5908
At iterate     9  f =      -4392.9  |proj g|=        1.5223
At iterate    10  f =      -4408.1  |proj g|=        1.4862
At iterate    11  f =      -4410.9  |proj g|=        1.4889
At iterate    12  f =      -4412.2  |proj g|=        1.5339
At iterate    13  f =      -4413.5  |proj g|=        1.5113
At iterate    14  f =        -4414  |proj g|=         1.448
At iterate    15  f =      -4414.3  |proj g|=         1.375
At iterate    16  f =      -4414.4  |proj g|=        1.6245
At iterate    17  f =      -4414.5  |proj g|=        1.6165
At iterate    18  f =      -4414.6  |proj g|=        1.6089
At iterate    19  f =      -4414.8  |proj g|=        1.9934
At iterate    20  f =      -4414.9  |proj g|=        1.9687
At iterate    21  f =        -4415  |proj g|=        1.4343
At iterate    22  f =      -4415.1  |proj g|=         1.557
At iterate    23  f =      -4415.2  |proj g|=        1.5016
At iterate    24  f =      -4415.3  |proj g|=        1.4607
At iterate    25  f =      -4415.4  |proj g|=       0.91515
At iterate    26  f =      -4415.4  |proj g|=        1.5524
At iterate    27  f =      -4415.5  |proj g|=        1.5545
At iterate    28  f =      -4415.5  |proj g|=        1.5582
At iterate    29  f =      -4415.6  |proj g|=        1.2139
At iterate    30  f =      -4415.6  |proj g|=       0.75576
At iterate    31  f =      -4415.6  |proj g|=        1.1697
At iterate    32  f =      -4415.6  |proj g|=        1.5027
At iterate    33  f =      -4415.7  |proj g|=        1.5013
At iterate    34  f =      -4415.7  |proj g|=        1.1452
At iterate    35  f =      -4415.8  |proj g|=        1.1353
At iterate    36  f =      -4415.8  |proj g|=         1.372
At iterate    37  f =      -4415.8  |proj g|=       0.43781
At iterate    38  f =      -4415.8  |proj g|=       0.42144
At iterate    39  f =      -4415.8  |proj g|=       0.29735
At iterate    40  f =      -4415.8  |proj g|=       0.30679
At iterate    41  f =      -4415.8  |proj g|=        0.1022
At iterate    42  f =      -4415.8  |proj g|=      0.070083
At iterate    43  f =      -4415.8  |proj g|=      0.041018
At iterate    44  f =      -4415.8  |proj g|=      0.066252

iterations 44
function evaluations 52
segments explored during Cauchy searches 52
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.066252
final function value -4415.78

F = -4415.78
final  value -4415.779341 
converged
[1] "Fri Feb 09 16:30:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 4.36909, its = 12
i = 2 (of 248), d = 3.77632, its = 12
i = 3 (of 248), d = 4.01459, its = 12
i = 4 (of 248), d = 6.23306, its = 55
i = 5 (of 248), d = 6.23306, its = 19
i = 6 (of 248), d = 4.40606, its = 11
i = 7 (of 248), d = 4.74361, its = 12
i = 8 (of 248), d = 6.23306, its = 22
i = 9 (of 248), d = 1.7516, its = 10
i = 10 (of 248), d = 6.23306, its = 25
i = 11 (of 248), d = 2.61897, its = 11
i = 12 (of 248), d = 3.40074, its = 11
i = 13 (of 248), d = 4.72292, its = 12
i = 14 (of 248), d = 6.23306, its = 61
i = 15 (of 248), d = 2.11682, its = 11
i = 16 (of 248), d = 4.32148, its = 12
i = 17 (of 248), d = 4.30486, its = 12
i = 18 (of 248), d = 3.30282, its = 12
i = 19 (of 248), d = 1.81927, its = 11
i = 20 (of 248), d = 4.32321, its = 12
i = 21 (of 248), d = 5.82329, its = 12
i = 22 (of 248), d = 2.26197, its = 11
i = 23 (of 248), d = 3.92028, its = 11
i = 24 (of 248), d = 6.23306, its = 60
i = 25 (of 248), d = 6.21751, its = 12
i = 26 (of 248), d = 6.23306, its = 25
i = 27 (of 248), d = 2.49737, its = 11
i = 28 (of 248), d = 1.85822, its = 10
i = 29 (of 248), d = 6.23306, its = 25
i = 30 (of 248), d = 2.90948, its = 12
i = 31 (of 248), d = 4.77006, its = 13
i = 32 (of 248), d = 5.60468, its = 12
i = 33 (of 248), d = 6.23306, its = 20
i = 34 (of 248), d = 1.87287, its = 11
i = 35 (of 248), d = 6.23306, its = 56
i = 36 (of 248), d = 6.23306, its = 56
i = 37 (of 248), d = 3.80412, its = 11
i = 38 (of 248), d = 5.2511, its = 12
i = 39 (of 248), d = 2.79088, its = 11
i = 40 (of 248), d = 1.64136, its = 11
i = 41 (of 248), d = 4.76615, its = 12
i = 42 (of 248), d = 1.78622, its = 11
i = 43 (of 248), d = 3.5124, its = 11
i = 44 (of 248), d = 2.77708, its = 11
i = 45 (of 248), d = 6.23306, its = 20
i = 46 (of 248), d = 2.60844, its = 11
i = 47 (of 248), d = 2.33604, its = 11
i = 48 (of 248), d = 1.70675, its = 10
i = 49 (of 248), d = 2.8399, its = 12
i = 50 (of 248), d = 1.8554, its = 10
i = 51 (of 248), d = 6.23306, its = 56
i = 52 (of 248), d = 3.4128, its = 11
i = 53 (of 248), d = 6.23306, its = 21
i = 54 (of 248), d = 2.95536, its = 11
i = 55 (of 248), d = 6.23306, its = 60
i = 56 (of 248), d = 1.56944, its = 10
i = 57 (of 248), d = 5.1752, its = 12
i = 58 (of 248), d = 5.95532, its = 11
i = 59 (of 248), d = 3.62361, its = 11
i = 60 (of 248), d = 6.23306, its = 19
i = 61 (of 248), d = 6.23306, its = 61
i = 62 (of 248), d = 6.23306, its = 24
i = 63 (of 248), d = 6.23306, its = 58
i = 64 (of 248), d = 5.96902, its = 12
i = 65 (of 248), d = 1.61582, its = 10
i = 66 (of 248), d = 6.23306, its = 62
i = 67 (of 248), d = 1.78082, its = 11
i = 68 (of 248), d = 3.4946, its = 12
i = 69 (of 248), d = 1.95983, its = 10
i = 70 (of 248), d = 1.38733, its = 11
i = 71 (of 248), d = 6.23306, its = 21
i = 72 (of 248), d = 5.70643, its = 12
i = 73 (of 248), d = 2.78805, its = 12
i = 74 (of 248), d = 6.23306, its = 25
i = 75 (of 248), d = 2.71143, its = 12
i = 76 (of 248), d = 6.23306, its = 56
i = 77 (of 248), d = 2.24605, its = 11
i = 78 (of 248), d = 4.74708, its = 12
i = 79 (of 248), d = 3.16983, its = 12
i = 80 (of 248), d = 5.372, its = 12
i = 81 (of 248), d = 2.38292, its = 11
i = 82 (of 248), d = 2.06767, its = 11
i = 83 (of 248), d = 5.43915, its = 12
i = 84 (of 248), d = 4.39528, its = 12
i = 85 (of 248), d = 4.69421, its = 12
i = 86 (of 248), d = 2.02925, its = 11
i = 87 (of 248), d = 3.4488, its = 12
i = 88 (of 248), d = 3.61737, its = 11
i = 89 (of 248), d = 4.03433, its = 12
i = 90 (of 248), d = 6.07989, its = 12
i = 91 (of 248), d = 6.23306, its = 57
i = 92 (of 248), d = 4.49338, its = 12
i = 93 (of 248), d = 4.02572, its = 12
i = 94 (of 248), d = 6.23306, its = 24
i = 95 (of 248), d = 4.47444, its = 12
i = 96 (of 248), d = 1.68968, its = 11
i = 97 (of 248), d = 3.89027, its = 12
i = 98 (of 248), d = 3.09165, its = 11
i = 99 (of 248), d = 3.07684, its = 11
i = 100 (of 248), d = 1.824, its = 10
i = 101 (of 248), d = 1.61259, its = 10
i = 102 (of 248), d = 3.33002, its = 12
i = 103 (of 248), d = 1.46057, its = 10
i = 104 (of 248), d = 2.23172, its = 12
i = 105 (of 248), d = 4.16767, its = 12
i = 106 (of 248), d = 5.99003, its = 12
i = 107 (of 248), d = 5.71781, its = 13
i = 108 (of 248), d = 3.36205, its = 11
i = 109 (of 248), d = 5.33738, its = 12
i = 110 (of 248), d = 1.71009, its = 11
i = 111 (of 248), d = 2.126, its = 11
i = 112 (of 248), d = 4.25131, its = 11
i = 113 (of 248), d = 4.99338, its = 12
i = 114 (of 248), d = 2.28034, its = 11
i = 115 (of 248), d = 1.34534, its = 10
i = 116 (of 248), d = 5.9169, its = 12
i = 117 (of 248), d = 2.23118, its = 12
i = 118 (of 248), d = 5.04489, its = 12
i = 119 (of 248), d = 6.23306, its = 57
i = 120 (of 248), d = 5.04996, its = 12
i = 121 (of 248), d = 3.89261, its = 12
i = 122 (of 248), d = 6.23306, its = 51
i = 123 (of 248), d = 5.07156, its = 12
i = 124 (of 248), d = 6.23306, its = 56
i = 125 (of 248), d = 6.23306, its = 64
i = 126 (of 248), d = 2.81263, its = 12
i = 127 (of 248), d = 1.66323, its = 11
i = 128 (of 248), d = 2.28348, its = 10
i = 129 (of 248), d = 6.23306, its = 60
i = 130 (of 248), d = 6.23306, its = 25
i = 131 (of 248), d = 6.23306, its = 58
i = 132 (of 248), d = 5.28782, its = 12
i = 133 (of 248), d = 6.23306, its = 23
i = 134 (of 248), d = 1.38449, its = 10
i = 135 (of 248), d = 2.46158, its = 11
i = 136 (of 248), d = 6.23306, its = 55
i = 137 (of 248), d = 6.23306, its = 24
i = 138 (of 248), d = 4.93607, its = 12
i = 139 (of 248), d = 5.06915, its = 13
i = 140 (of 248), d = 5.60618, its = 12
i = 141 (of 248), d = 6.23306, its = 61
i = 142 (of 248), d = 6.23306, its = 60
i = 143 (of 248), d = 6.23306, its = 19
i = 144 (of 248), d = 6.23306, its = 21
i = 145 (of 248), d = 2.30172, its = 11
i = 146 (of 248), d = 4.80837, its = 11
i = 147 (of 248), d = 3.30776, its = 11
i = 148 (of 248), d = 6.23306, its = 21
i = 149 (of 248), d = 2.97799, its = 12
i = 150 (of 248), d = 4.79319, its = 13
i = 151 (of 248), d = 2.03944, its = 11
i = 152 (of 248), d = 6.17456, its = 12
i = 153 (of 248), d = 4.48635, its = 12
i = 154 (of 248), d = 3.71518, its = 12
i = 155 (of 248), d = 4.5397, its = 11
i = 156 (of 248), d = 6.13381, its = 12
i = 157 (of 248), d = 3.80561, its = 12
i = 158 (of 248), d = 4.57996, its = 12
i = 159 (of 248), d = 1.94183, its = 10
i = 160 (of 248), d = 4.8918, its = 12
i = 161 (of 248), d = 5.17813, its = 12
i = 162 (of 248), d = 1.94285, its = 10
i = 163 (of 248), d = 6.23306, its = 58
i = 164 (of 248), d = 5.37311, its = 12
i = 165 (of 248), d = 6.0632, its = 12
i = 166 (of 248), d = 2.56996, its = 11
i = 167 (of 248), d = 4.79717, its = 12
i = 168 (of 248), d = 3.10002, its = 11
i = 169 (of 248), d = 1.74029, its = 11
i = 170 (of 248), d = 5.94577, its = 12
i = 171 (of 248), d = 3.24069, its = 11
i = 172 (of 248), d = 5.95737, its = 12
i = 173 (of 248), d = 5.35074, its = 13
i = 174 (of 248), d = 3.42683, its = 12
i = 175 (of 248), d = 6.23306, its = 54
i = 176 (of 248), d = 6.23306, its = 16
i = 177 (of 248), d = 4.67756, its = 12
i = 178 (of 248), d = 2.91925, its = 11
i = 179 (of 248), d = 2.32219, its = 11
i = 180 (of 248), d = 4.37295, its = 12
i = 181 (of 248), d = 3.87567, its = 12
i = 182 (of 248), d = 2.98813, its = 12
i = 183 (of 248), d = 6.23306, its = 60
i = 184 (of 248), d = 5.32422, its = 12
i = 185 (of 248), d = 4.11034, its = 11
i = 186 (of 248), d = 1.6333, its = 10
i = 187 (of 248), d = 6.23306, its = 58
i = 188 (of 248), d = 6.13944, its = 12
i = 189 (of 248), d = 2.38518, its = 12
i = 190 (of 248), d = 6.23306, its = 59
i = 191 (of 248), d = 1.4136, its = 10
i = 192 (of 248), d = 2.40979, its = 10
i = 193 (of 248), d = 1.67024, its = 11
i = 194 (of 248), d = 6.23306, its = 22
i = 195 (of 248), d = 1.67344, its = 10
i = 196 (of 248), d = 2.58907, its = 11
i = 197 (of 248), d = 1.70854, its = 10
i = 198 (of 248), d = 5.06599, its = 12
i = 199 (of 248), d = 6.23306, its = 20
i = 200 (of 248), d = 4.01291, its = 12
i = 201 (of 248), d = 2.08163, its = 11
i = 202 (of 248), d = 6.23306, its = 23
i = 203 (of 248), d = 3.27067, its = 13
i = 204 (of 248), d = 1.5923, its = 11
i = 205 (of 248), d = 4.82369, its = 12
i = 206 (of 248), d = 1.64166, its = 10
i = 207 (of 248), d = 5.77008, its = 12
i = 208 (of 248), d = 3.31088, its = 11
i = 209 (of 248), d = 6.23306, its = 22
i = 210 (of 248), d = 6.23306, its = 22
i = 211 (of 248), d = 1.40562, its = 11
i = 212 (of 248), d = 1.99906, its = 11
i = 213 (of 248), d = 5.04184, its = 13
i = 214 (of 248), d = 2.89558, its = 11
i = 215 (of 248), d = 5.83339, its = 12
i = 216 (of 248), d = 2.08027, its = 11
i = 217 (of 248), d = 1.84761, its = 11
i = 218 (of 248), d = 5.17007, its = 12
i = 219 (of 248), d = 1.5027, its = 10
i = 220 (of 248), d = 5.10272, its = 12
i = 221 (of 248), d = 4.1425, its = 12
i = 222 (of 248), d = 6.18787, its = 12
i = 223 (of 248), d = 5.25209, its = 12
i = 224 (of 248), d = 5.93642, its = 12
i = 225 (of 248), d = 5.16213, its = 12
i = 226 (of 248), d = 2.20438, its = 11
i = 227 (of 248), d = 6.23306, its = 21
i = 228 (of 248), d = 6.23306, its = 19
i = 229 (of 248), d = 6.23306, its = 24
i = 230 (of 248), d = 1.77674, its = 10
i = 231 (of 248), d = 4.30599, its = 12
i = 232 (of 248), d = 1.69458, its = 10
i = 233 (of 248), d = 6.23306, its = 56
i = 234 (of 248), d = 4.30177, its = 12
i = 235 (of 248), d = 1.76318, its = 11
i = 236 (of 248), d = 4.21604, its = 11
i = 237 (of 248), d = 2.00439, its = 11
i = 238 (of 248), d = 2.77677, its = 11
i = 239 (of 248), d = 3.77434, its = 11
i = 240 (of 248), d = 6.19869, its = 12
i = 241 (of 248), d = 3.27196, its = 12
i = 242 (of 248), d = 4.18221, its = 12
i = 243 (of 248), d = 4.01024, its = 12
i = 244 (of 248), d = 5.73578, its = 11
i = 245 (of 248), d = 2.34009, its = 11
i = 246 (of 248), d = 6.23306, its = 26
i = 247 (of 248), d = 2.79819, its = 11
i = 248 (of 248), d = 3.78028, its = 11
[1] "Fri Feb 09 16:31:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 16:31:51 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 16:31:52 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 16:31:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 16:31:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 16:32:02 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 502.198744 
iter  10 value 10.484568
iter  20 value 3.129288
iter  30 value 0.607632
iter  40 value 0.291838
iter  50 value 0.212741
iter  60 value 0.146513
iter  70 value 0.109479
iter  80 value 0.084011
iter  90 value 0.075256
iter 100 value 0.070513
final  value 0.070513 
stopped after 100 iterations
# weights:  210
initial  value 62.459750 
iter  10 value 0.244613
iter  20 value 0.136051
iter  30 value 0.073187
iter  40 value 0.045017
iter  50 value 0.042079
iter  60 value 0.040806
iter  70 value 0.039960
iter  80 value 0.039040
iter  90 value 0.037784
iter 100 value 0.036786
final  value 0.036786 
stopped after 100 iterations
# weights:  210
initial  value 125.675054 
iter  10 value 0.657085
iter  20 value 0.176588
iter  30 value 0.075889
iter  40 value 0.045369
iter  50 value 0.039253
iter  60 value 0.036592
iter  70 value 0.035418
iter  80 value 0.034923
iter  90 value 0.034562
iter 100 value 0.034335
final  value 0.034335 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00408; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 218.170530 
iter  10 value 0.302291
iter  20 value 0.035382
iter  30 value 0.018716
iter  40 value 0.013262
iter  50 value 0.011062
iter  60 value 0.009415
iter  70 value 0.007841
iter  80 value 0.006879
iter  90 value 0.006180
iter 100 value 0.005975
final  value 0.005975 
stopped after 100 iterations
# weights:  144
initial  value 2145.941879 
iter  10 value 1.596046
iter  20 value 0.042851
iter  30 value 0.021098
iter  40 value 0.014731
iter  50 value 0.012741
iter  60 value 0.011259
iter  70 value 0.010248
iter  80 value 0.008400
iter  90 value 0.007982
iter 100 value 0.007850
final  value 0.007850 
stopped after 100 iterations
# weights:  144
initial  value 78.165847 
iter  10 value 0.183651
iter  20 value 0.029434
iter  30 value 0.015040
iter  40 value 0.011594
iter  50 value 0.009289
iter  60 value 0.008058
iter  70 value 0.007005
iter  80 value 0.006034
iter  90 value 0.005432
iter 100 value 0.005149
final  value 0.005149 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00119; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 65.542579 
iter  10 value 0.391631
iter  20 value 0.022819
iter  30 value 0.016147
iter  40 value 0.012330
iter  50 value 0.009319
iter  60 value 0.008653
iter  70 value 0.007254
iter  80 value 0.006046
iter  90 value 0.005254
iter 100 value 0.004856
final  value 0.004856 
stopped after 100 iterations
# weights:  133
initial  value 1919.340082 
iter  10 value 6.353913
iter  20 value 1.165053
iter  30 value 0.453703
iter  40 value 0.274021
iter  50 value 0.235291
iter  60 value 0.213724
iter  70 value 0.185106
iter  80 value 0.172469
iter  90 value 0.157660
iter 100 value 0.150726
final  value 0.150726 
stopped after 100 iterations
# weights:  133
initial  value 370.931109 
iter  10 value 0.219045
iter  20 value 0.027136
iter  30 value 0.017750
iter  40 value 0.011925
iter  50 value 0.008855
iter  60 value 0.007989
iter  70 value 0.006845
iter  80 value 0.005110
iter  90 value 0.004977
iter 100 value 0.004620
final  value 0.004620 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00294; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 33.233345 
iter  10 value 0.198437
iter  20 value 0.063364
iter  30 value 0.038092
iter  40 value 0.031653
iter  50 value 0.031105
iter  60 value 0.029269
iter  70 value 0.028378
iter  80 value 0.028183
iter  90 value 0.028025
iter 100 value 0.027934
final  value 0.027934 
stopped after 100 iterations
# weights:  23
initial  value 25.876338 
iter  10 value 0.398116
iter  20 value 0.089701
iter  30 value 0.040186
iter  40 value 0.036049
iter  50 value 0.035006
iter  60 value 0.034276
iter  70 value 0.033514
iter  80 value 0.033345
iter  90 value 0.033216
iter 100 value 0.032980
final  value 0.032980 
stopped after 100 iterations
# weights:  23
initial  value 567.517163 
iter  10 value 5.453049
iter  20 value 0.587750
iter  30 value 0.130189
iter  40 value 0.047146
iter  50 value 0.040914
iter  60 value 0.035473
iter  70 value 0.032320
iter  80 value 0.031311
iter  90 value 0.031097
iter 100 value 0.030726
final  value 0.030726 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.00393; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 92.981986 
iter  10 value 1.369617
iter  20 value 0.460973
iter  30 value 0.193997
iter  40 value 0.142239
iter  50 value 0.120057
iter  60 value 0.109751
iter  70 value 0.099640
iter  80 value 0.087835
iter  90 value 0.082530
iter 100 value 0.080555
final  value 0.080555 
stopped after 100 iterations
# weights:  210
initial  value 110.091327 
iter  10 value 1.148851
iter  20 value 0.533840
iter  30 value 0.279173
iter  40 value 0.223671
iter  50 value 0.176121
iter  60 value 0.136738
iter  70 value 0.113135
iter  80 value 0.106225
iter  90 value 0.098965
iter 100 value 0.092409
final  value 0.092409 
stopped after 100 iterations
# weights:  210
initial  value 95.478421 
iter  10 value 1.435563
iter  20 value 0.643191
iter  30 value 0.316509
iter  40 value 0.177728
iter  50 value 0.123500
iter  60 value 0.110711
iter  70 value 0.101949
iter  80 value 0.097775
iter  90 value 0.095604
iter 100 value 0.094245
final  value 0.094245 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00559; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 952.643863 
iter  10 value 58.403501
iter  20 value 11.770899
iter  30 value 6.269960
iter  40 value 5.769162
iter  50 value 5.647769
iter  60 value 5.609324
iter  70 value 5.587790
iter  80 value 5.570257
iter  90 value 5.543904
iter 100 value 5.509766
final  value 5.509766 
stopped after 100 iterations
# weights:  199
initial  value 175.897010 
iter  10 value 14.676087
iter  20 value 6.114912
iter  30 value 5.615909
iter  40 value 5.571948
iter  50 value 5.556911
iter  60 value 5.550381
iter  70 value 5.541866
iter  80 value 5.532398
iter  90 value 5.523228
iter 100 value 5.520200
final  value 5.520200 
stopped after 100 iterations
# weights:  199
initial  value 85.171338 
iter  10 value 10.160552
iter  20 value 6.383196
iter  30 value 5.583030
iter  40 value 5.545525
iter  50 value 5.537074
iter  60 value 5.529280
iter  70 value 5.527544
iter  80 value 5.526199
iter  90 value 5.523546
iter 100 value 5.520598
final  value 5.520598 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0357; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 164.315892 
iter  10 value 0.114643
iter  20 value 0.045730
iter  30 value 0.026250
iter  40 value 0.019090
iter  50 value 0.014948
iter  60 value 0.012644
iter  70 value 0.011487
iter  80 value 0.010829
iter  90 value 0.010349
iter 100 value 0.010081
final  value 0.010081 
stopped after 100 iterations
# weights:  144
initial  value 36.184084 
iter  10 value 0.218417
iter  20 value 0.043101
iter  30 value 0.024355
iter  40 value 0.018670
iter  50 value 0.013849
iter  60 value 0.011646
iter  70 value 0.010492
iter  80 value 0.010006
iter  90 value 0.009710
iter 100 value 0.009496
final  value 0.009496 
stopped after 100 iterations
# weights:  144
initial  value 525.297961 
iter  10 value 0.371433
iter  20 value 0.051194
iter  30 value 0.034890
iter  40 value 0.028262
iter  50 value 0.021177
iter  60 value 0.018045
iter  70 value 0.016027
iter  80 value 0.014812
iter  90 value 0.013917
iter 100 value 0.013054
final  value 0.013054 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00175; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 802.478167 
iter  10 value 0.257156
iter  20 value 0.052954
iter  30 value 0.032365
iter  40 value 0.025889
iter  50 value 0.020654
iter  60 value 0.018786
iter  70 value 0.018158
iter  80 value 0.017601
iter  90 value 0.017169
iter 100 value 0.016878
final  value 0.016878 
stopped after 100 iterations
# weights:  89
initial  value 34.137981 
iter  10 value 0.138946
iter  20 value 0.036742
iter  30 value 0.023389
iter  40 value 0.019687
iter  50 value 0.017141
iter  60 value 0.016397
iter  70 value 0.016063
iter  80 value 0.015935
iter  90 value 0.015874
iter 100 value 0.015825
final  value 0.015825 
stopped after 100 iterations
# weights:  89
initial  value 1021.710757 
iter  10 value 0.202723
iter  20 value 0.048406
iter  30 value 0.030266
iter  40 value 0.023029
iter  50 value 0.018753
iter  60 value 0.017441
iter  70 value 0.017022
iter  80 value 0.016754
iter  90 value 0.016653
iter 100 value 0.016557
final  value 0.016557 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00272; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 215.081722 
iter  10 value 0.217280
iter  20 value 0.079364
iter  30 value 0.054035
iter  40 value 0.035926
iter  50 value 0.030074
iter  60 value 0.026527
iter  70 value 0.024562
iter  80 value 0.023056
iter  90 value 0.022011
iter 100 value 0.021222
final  value 0.021222 
stopped after 100 iterations
# weights:  210
initial  value 1351.905820 
iter  10 value 0.692761
iter  20 value 0.228664
iter  30 value 0.182207
iter  40 value 0.153681
iter  50 value 0.140089
iter  60 value 0.123940
iter  70 value 0.108736
iter  80 value 0.094425
iter  90 value 0.078212
iter 100 value 0.056831
final  value 0.056831 
stopped after 100 iterations
# weights:  210
initial  value 127.597149 
iter  10 value 0.163279
iter  20 value 0.070598
iter  30 value 0.045815
iter  40 value 0.031592
iter  50 value 0.021430
iter  60 value 0.018635
iter  70 value 0.017663
iter  80 value 0.017348
iter  90 value 0.017142
iter 100 value 0.017048
final  value 0.017048 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00325; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 852.146545 
iter  10 value 0.219604
iter  20 value 0.017408
iter  30 value 0.003762
iter  40 value 0.002067
iter  50 value 0.001623
iter  60 value 0.001155
iter  70 value 0.001042
iter  80 value 0.000957
iter  90 value 0.000894
iter 100 value 0.000819
final  value 0.000819 
stopped after 100 iterations
# weights:  56
initial  value 484.096113 
iter  10 value 0.347289
iter  20 value 0.024732
iter  30 value 0.005141
iter  40 value 0.003566
iter  50 value 0.001898
iter  60 value 0.001399
iter  70 value 0.001074
iter  80 value 0.000899
iter  90 value 0.000816
iter 100 value 0.000681
final  value 0.000681 
stopped after 100 iterations
# weights:  56
initial  value 282.021297 
iter  10 value 0.377800
iter  20 value 0.044736
iter  30 value 0.008961
iter  40 value 0.003298
iter  50 value 0.001589
iter  60 value 0.001298
iter  70 value 0.001111
iter  80 value 0.000927
iter  90 value 0.000821
iter 100 value 0.000775
final  value 0.000775 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.000775; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 1132.438702 
iter  10 value 98.142850
iter  20 value 15.846143
iter  30 value 7.696515
iter  40 value 6.149152
iter  50 value 5.664153
iter  60 value 5.459839
iter  70 value 5.290503
iter  80 value 4.966601
iter  90 value 4.663617
iter 100 value 4.567344
final  value 4.567344 
stopped after 100 iterations
# weights:  199
initial  value 105.206201 
iter  10 value 10.265930
iter  20 value 6.212725
iter  30 value 5.000531
iter  40 value 4.537103
iter  50 value 4.501294
iter  60 value 4.481239
iter  70 value 4.468040
iter  80 value 4.465817
iter  90 value 4.464003
iter 100 value 4.462346
final  value 4.462346 
stopped after 100 iterations
# weights:  199
initial  value 842.331612 
iter  10 value 17.699034
iter  20 value 5.359825
iter  30 value 4.792865
iter  40 value 4.738285
iter  50 value 4.690734
iter  60 value 4.594410
iter  70 value 4.556221
iter  80 value 4.523169
iter  90 value 4.505297
iter 100 value 4.497507
final  value 4.497507 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.0291; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 35.985949 
iter  10 value 4.995631
iter  20 value 3.914208
iter  30 value 3.882750
iter  40 value 3.880575
iter  50 value 3.876363
iter  60 value 3.870641
iter  70 value 3.863555
iter  80 value 3.846093
iter  90 value 3.840866
final  value 3.840416 
converged
# weights:  122
initial  value 245.806563 
iter  10 value 6.450202
iter  20 value 3.994150
iter  30 value 3.930347
iter  40 value 3.913611
iter  50 value 3.877265
iter  60 value 3.872825
iter  70 value 3.871751
iter  80 value 3.868396
iter  90 value 3.868110
iter 100 value 3.867895
final  value 3.867895 
stopped after 100 iterations
# weights:  122
initial  value 1019.043927 
iter  10 value 10.486386
iter  20 value 4.506149
iter  30 value 4.041154
iter  40 value 3.952217
iter  50 value 3.906040
iter  60 value 3.887309
iter  70 value 3.879307
iter  80 value 3.875697
iter  90 value 3.871184
iter 100 value 3.865062
final  value 3.865062 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0258; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 929.340109 
iter  10 value 1.074624
iter  20 value 0.363033
iter  30 value 0.245357
iter  40 value 0.220587
iter  50 value 0.196347
iter  60 value 0.184507
iter  70 value 0.179668
iter  80 value 0.177944
iter  90 value 0.174934
iter 100 value 0.170541
final  value 0.170541 
stopped after 100 iterations
# weights:  56
initial  value 242.646930 
iter  10 value 0.827858
iter  20 value 0.234390
iter  30 value 0.180101
iter  40 value 0.171993
iter  50 value 0.170090
iter  60 value 0.169055
iter  70 value 0.168853
iter  80 value 0.168745
iter  90 value 0.168687
iter 100 value 0.168654
final  value 0.168654 
stopped after 100 iterations
# weights:  56
initial  value 32.491083 
iter  10 value 0.563244
iter  20 value 0.234566
iter  30 value 0.189372
iter  40 value 0.173896
iter  50 value 0.170016
iter  60 value 0.168891
iter  70 value 0.168369
iter  80 value 0.168072
iter  90 value 0.167430
iter 100 value 0.167238
final  value 0.167238 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00661; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 37.750836 
iter  10 value 0.421787
iter  20 value 0.182615
iter  30 value 0.097857
iter  40 value 0.068098
iter  50 value 0.050663
iter  60 value 0.041564
iter  70 value 0.037917
iter  80 value 0.036317
iter  90 value 0.035298
iter 100 value 0.034775
final  value 0.034775 
stopped after 100 iterations
# weights:  144
initial  value 1186.406601 
iter  10 value 2.249356
iter  20 value 1.257849
iter  30 value 0.913666
iter  40 value 0.655098
iter  50 value 0.542143
iter  60 value 0.459487
iter  70 value 0.380033
iter  80 value 0.277556
iter  90 value 0.218403
iter 100 value 0.179037
final  value 0.179037 
stopped after 100 iterations
# weights:  144
initial  value 147.568953 
iter  10 value 0.212172
iter  20 value 0.094074
iter  30 value 0.054152
iter  40 value 0.038021
iter  50 value 0.036090
iter  60 value 0.035399
iter  70 value 0.034996
iter  80 value 0.034777
iter  90 value 0.034653
iter 100 value 0.034562
final  value 0.034562 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00497; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 310.636174 
iter  10 value 1.385417
iter  20 value 0.396269
iter  30 value 0.354388
iter  40 value 0.348899
iter  50 value 0.346189
iter  60 value 0.343573
iter  70 value 0.341573
iter  80 value 0.339893
iter  90 value 0.339321
iter 100 value 0.338794
final  value 0.338794 
stopped after 100 iterations
# weights:  122
initial  value 621.353669 
iter  10 value 2.644339
iter  20 value 1.117730
iter  30 value 0.682657
iter  40 value 0.544392
iter  50 value 0.460012
iter  60 value 0.412542
iter  70 value 0.397680
iter  80 value 0.392183
iter  90 value 0.389041
iter 100 value 0.385125
final  value 0.385125 
stopped after 100 iterations
# weights:  122
initial  value 37.327926 
iter  10 value 1.633690
iter  20 value 0.510740
iter  30 value 0.373456
iter  40 value 0.348736
iter  50 value 0.340358
iter  60 value 0.338045
iter  70 value 0.336769
iter  80 value 0.335661
iter  90 value 0.335022
iter 100 value 0.334509
final  value 0.334509 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00703; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 221.903529 
iter  10 value 0.130962
iter  20 value 0.045374
iter  30 value 0.033616
iter  40 value 0.025612
iter  50 value 0.019725
iter  60 value 0.018762
iter  70 value 0.018450
iter  80 value 0.018241
iter  90 value 0.018082
iter 100 value 0.017969
final  value 0.017969 
stopped after 100 iterations
# weights:  100
initial  value 60.957235 
iter  10 value 0.206715
iter  20 value 0.058168
iter  30 value 0.036634
iter  40 value 0.024498
iter  50 value 0.021303
iter  60 value 0.020413
iter  70 value 0.019624
iter  80 value 0.019287
iter  90 value 0.019079
iter 100 value 0.018855
final  value 0.018855 
stopped after 100 iterations
# weights:  100
initial  value 177.438391 
iter  10 value 0.223501
iter  20 value 0.050901
iter  30 value 0.032021
iter  40 value 0.023220
iter  50 value 0.021600
iter  60 value 0.020631
iter  70 value 0.019854
iter  80 value 0.019174
iter  90 value 0.018769
iter 100 value 0.018591
final  value 0.018591 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.00296; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 150.250534 
iter  10 value 4.613343
iter  20 value 4.197577
iter  30 value 4.165723
iter  40 value 4.101971
iter  50 value 4.082020
iter  60 value 4.080658
final  value 4.080502 
converged
# weights:  56
initial  value 316.023928 
iter  10 value 5.151053
iter  20 value 4.244226
iter  30 value 4.173780
iter  40 value 4.135415
iter  50 value 4.109910
iter  60 value 4.109818
iter  60 value 4.109818
iter  60 value 4.109818
final  value 4.109818 
converged
# weights:  56
initial  value 343.684687 
iter  10 value 11.672671
iter  20 value 5.864935
iter  30 value 4.798657
iter  40 value 4.448240
iter  50 value 4.241143
iter  60 value 4.096741
iter  70 value 4.095431
iter  80 value 4.094983
iter  90 value 4.094291
final  value 4.093477 
converged
[Tune-y] 17: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 346.500369 
iter  10 value 0.446703
iter  20 value 0.027198
iter  30 value 0.006643
iter  40 value 0.002607
iter  50 value 0.001661
iter  60 value 0.001439
iter  70 value 0.001244
iter  80 value 0.000958
iter  90 value 0.000907
iter 100 value 0.000818
final  value 0.000818 
stopped after 100 iterations
# weights:  155
initial  value 49.189058 
iter  10 value 0.153155
iter  20 value 0.019482
iter  30 value 0.004985
iter  40 value 0.002524
iter  50 value 0.001180
iter  60 value 0.000715
iter  70 value 0.000632
iter  80 value 0.000543
iter  90 value 0.000494
iter 100 value 0.000479
final  value 0.000479 
stopped after 100 iterations
# weights:  155
initial  value 982.438690 
iter  10 value 0.741481
iter  20 value 0.249138
iter  30 value 0.067332
iter  40 value 0.018349
iter  50 value 0.008195
iter  60 value 0.005077
iter  70 value 0.003665
iter  80 value 0.002929
iter  90 value 0.002448
iter 100 value 0.002273
final  value 0.002273 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.000851; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 135.501832 
iter  10 value 0.137050
iter  20 value 0.041371
iter  30 value 0.027364
iter  40 value 0.023237
iter  50 value 0.020824
iter  60 value 0.019318
iter  70 value 0.018420
iter  80 value 0.018132
iter  90 value 0.017810
iter 100 value 0.017686
final  value 0.017686 
stopped after 100 iterations
# weights:  45
initial  value 30.356505 
iter  10 value 0.120932
iter  20 value 0.039079
iter  30 value 0.025870
iter  40 value 0.020668
iter  50 value 0.018741
iter  60 value 0.018440
iter  70 value 0.018249
iter  80 value 0.018128
iter  90 value 0.018045
iter 100 value 0.018004
final  value 0.018004 
stopped after 100 iterations
# weights:  45
initial  value 42.971634 
iter  10 value 0.174603
iter  20 value 0.049357
iter  30 value 0.033254
iter  40 value 0.028708
iter  50 value 0.024572
iter  60 value 0.023731
iter  70 value 0.022910
iter  80 value 0.021935
iter  90 value 0.021455
iter 100 value 0.020508
final  value 0.020508 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00282; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 70.713646 
iter  10 value 0.499449
iter  20 value 0.199029
iter  30 value 0.180229
iter  40 value 0.173969
iter  50 value 0.171474
iter  60 value 0.171157
iter  70 value 0.171071
iter  80 value 0.171044
iter  90 value 0.171014
iter 100 value 0.170952
final  value 0.170952 
stopped after 100 iterations
# weights:  78
initial  value 177.617139 
iter  10 value 0.527676
iter  20 value 0.208997
iter  30 value 0.184055
iter  40 value 0.177849
iter  50 value 0.175227
iter  60 value 0.173318
iter  70 value 0.172571
iter  80 value 0.171659
iter  90 value 0.170180
iter 100 value 0.169523
final  value 0.169523 
stopped after 100 iterations
# weights:  78
initial  value 438.078926 
iter  10 value 0.540430
iter  20 value 0.205385
iter  30 value 0.183275
iter  40 value 0.175027
iter  50 value 0.172929
iter  60 value 0.171831
iter  70 value 0.171227
iter  80 value 0.170953
iter  90 value 0.170840
iter 100 value 0.170715
final  value 0.170715 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00687; time: 0.0 min
[Tune] Result: size=5; decay=1.79e-05 : rmse.test.rmse=0.000775
# weights:  56
initial  value 133.004536 
iter  10 value 0.142772
iter  20 value 0.034633
iter  30 value 0.007160
iter  40 value 0.003636
iter  50 value 0.001878
iter  60 value 0.001442
iter  70 value 0.001201
iter  80 value 0.001016
iter  90 value 0.000891
iter 100 value 0.000793
final  value 0.000793 
stopped after 100 iterations
[1] "Fri Feb 09 16:32:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 590 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 366
 number of selected nodes                               : 72 
[1] "Fri Feb 09 16:32:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 16:32:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 16:32:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 16:32:52 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0128; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0113; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0112; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0168; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0128; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0136; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0112; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0108; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0125; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0117; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0136; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0121; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0123; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0114; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0115; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0108; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.013; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0115; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0119; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0113; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0108
[1] "Fri Feb 09 16:34:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 16:34:50 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.0684; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.0684; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.0479; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.0684; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.0477; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.0415; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.056; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.0528; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.0492; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.0684; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.0466; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.0558; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.111; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.0415
[1] "Fri Feb 09 16:34:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 16:34:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:35:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 1 -----> 6 
Runtime: 0.9400542 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1         0.5571 
    Y 2           0.54 
    Y 3         0.1857 
    Y 4         0.9572 
    Y 5         0.6514 
[1] "Fri Feb 09 16:35:26 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.128; time: 0.8 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.112; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.0931; time: 0.5 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.123; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.135; time: 0.7 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.182; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.145; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.132; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse= 0.2; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.134; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.144; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.175; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.145; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.0963; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.109; time: 0.0 min
[Tune] Result: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588 : rmse.test.rmse=0.0931
[1] "Fri Feb 09 16:38:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 16:38:08 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [9,0.500986], n=(372,380)
**GROW** @depth 1: [3,0.5], n=(199,172)
**GROW** @depth 1: [9,0.677318], n=(199,182)
**GROW** @depth 2: [9,0.573767], n=(79,122)
**GROW** @depth 3: [7,0.306048], n=(63,116)
**GROW** @depth 2: [3,0.371134], n=(72,122)
**GROW** @depth 2: [8,0.487336], n=(13,162)
**GROW** @depth 3: [6,0.134417], n=(18,139)
**GROW** @depth 3: [8,0.581659], n=(30,98)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(73,125,13,18,136,81,30,98,62,116)
**GROW** @depth 4: [4,0.462826], n=(71,28)
**GROW** @depth 5: [1,0.533681], n=(12,60)
**GROW** @depth 4: [7,0.480541], n=(12,15)
**PRUNE** @depth 4: [7,0.480541]
**GROW** @depth 5: [4,0.488906], n=(96,40)
**GROW** @depth 2: [6,0.45316], n=(69,13)
**PRUNE** @depth 2: [6,0.45316]
**GROW** @depth 4: [3,0.704124], n=(29,32)
**PRUNE** @depth 3: [7,0.295268]
**GROW** @depth 5: [4,0.665239], n=(13,12)
**GROW** @depth 3: [9,0.24497], n=(31,94)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(75,31,94,15,16,96,39,79,31,13,63,13,12,57,118)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 5: [4,0.661347]
**GROW** @depth 3: [8,0.526638], n=(13,67)
**PRUNE** @depth 3: [8,0.526638]
**GROW** @depth 3: [4,0.50253], n=(59,21)
**PRUNE** @depth 5: [3,0.371134]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(83,115,17,17,94,39,62,22,29,12,62,23,58,119)
**GROW** @depth 3: [9,0.406312], n=(93,22)
**GROW** @depth 4: [4,0.488906], n=(51,67)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(82,95,22,18,16,91,39,63,21,29,13,63,25,57,51,67)
**GROW** @depth 3: [9,0.614004], n=(14,15)
**GROW** @depth 4: [9,0.0944773], n=(23,59)
**GROW** @depth 5: [1,0.364964], n=(19,20)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(26,56,98,22,18,16,65,40,20,64,22,14,15,13,64,24,57,51,67)
**GROW** @depth 6: [1,0.432534], n=(22,19)
**PRUNE** @depth 6: [1,0.432534]
**GROW** @depth 4: [9,0.755621], n=(16,35)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(26,57,101,23,15,16,63,42,20,62,22,14,14,13,64,24,58,16,35,67)
**GROW** @depth 5: [9,0.879882], n=(48,19)
**GROW** @depth 5: [9,0.166667], n=(27,30)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(26,30,28,100,23,16,15,61,44,20,62,22,14,15,13,63,24,58,16,35,48,19)
Grow: 7.887%, Prune: 1.897%, Change: 78.77%, Swap: 15.45%

[1] "Fri Feb 09 16:38:15 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 16:38:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 16:38:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.9075 	 alpha= 1.7943 	 beta= 42824.58 
[1] "Fri Feb 09 16:38:20 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 16:38:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [8,0.481223], n=(161,591)
**GROW** @depth 1: [3,0.5], n=(87,504)
r=1000 d=[0] [0] [0]; n=(161,87,504)
**GROW** @depth 1: [2,0.0438018], n=(15,146)
**GROW** @depth 2: [1,0.507404], n=(198,306)
r=2000 d=[0] [0] [0] [0] [0]; n=(15,146,87,198,306)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0] [0] [0]; mh=4 n=(15,146,87,198,306)
**GROW** @depth 3: [4,0.323083], n=(77,121)
r=2000 d=[0] [0] [0] [0] [0] [0]; mh=5 n=(15,146,87,77,121,306)
r=3000 d=[0] [0] [0] [0] [0] [0]; mh=5 n=(15,146,87,77,121,306)
r=4000 d=[0] [0] [0] [0] [0] [0]; mh=5 n=(15,146,87,77,121,306)
r=5000 d=[0] [0] [0] [0] [0] [0]; mh=5 n=(15,146,87,77,121,306)
Grow: 1.433%, Prune: 0%, Change: 8.664%, Swap: 0%

[1] "Fri Feb 09 16:38:28 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 16:38:29 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 16:38:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 16:38:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 16:38:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 16:38:33 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 16:38:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 16:38:35 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:38:37 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.0609; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.322; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0454; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0602; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.139; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.053; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.114; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0713; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.0555; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0463; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.057; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0815; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0508; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.057; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.314; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0593; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0515; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.181; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0458; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0511; time: 0.0 min
[Tune] Result: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16 : rmse.test.rmse=0.0454
[1] "Fri Feb 09 16:38:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 16:38:51 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0554; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.0374; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.082; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0243; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.116; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.07; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.0157; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0388; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.18; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0257; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.642; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0236; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.118; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.137; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0199; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.0157
[1] "Fri Feb 09 16:38:54 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |==========================================                            |  60%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:39:02 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |===========================                                           |  38%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:39:06 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:39:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:39:14 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  1.937765 1.990239 1.955645 1.925066 1.991892 1.99223 2 2 2 
  - best initial criterion value(s) :  3469.405 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3469.4  |proj g|=       1.2671
At iterate     1  f =      -3521.6  |proj g|=        1.2638
At iterate     2  f =      -3527.5  |proj g|=        1.6028
At iterate     3  f =      -3529.1  |proj g|=        1.5929
At iterate     4  f =      -3533.9  |proj g|=        1.5849
At iterate     5  f =      -3535.6  |proj g|=        1.5846
At iterate     6  f =      -3536.8  |proj g|=        1.5858
At iterate     7  f =      -3537.3  |proj g|=        1.4188
At iterate     8  f =      -3537.7  |proj g|=         1.225
At iterate     9  f =      -3537.9  |proj g|=         1.584
At iterate    10  f =      -3537.9  |proj g|=        1.5821
At iterate    11  f =        -3538  |proj g|=        1.3965
At iterate    12  f =      -3538.1  |proj g|=        1.2399
At iterate    13  f =      -3538.2  |proj g|=        1.1624
At iterate    14  f =      -3538.2  |proj g|=        1.1645
At iterate    15  f =      -3538.3  |proj g|=        1.5873
At iterate    16  f =      -3538.4  |proj g|=        1.5868
At iterate    17  f =      -3538.6  |proj g|=        1.5236
At iterate    18  f =      -3538.6  |proj g|=        1.4547
At iterate    19  f =      -3538.6  |proj g|=        1.4045
At iterate    20  f =      -3538.7  |proj g|=        0.9712
At iterate    21  f =      -3538.7  |proj g|=        0.7431
At iterate    22  f =      -3538.7  |proj g|=       0.55616
At iterate    23  f =      -3538.7  |proj g|=       0.29619
At iterate    24  f =      -3538.7  |proj g|=        0.1913
At iterate    25  f =      -3538.7  |proj g|=       0.17277
At iterate    26  f =      -3538.7  |proj g|=       0.14595
At iterate    27  f =      -3538.7  |proj g|=        0.3469
At iterate    28  f =      -3538.7  |proj g|=       0.27012
At iterate    29  f =      -3538.7  |proj g|=      0.073601
At iterate    30  f =      -3538.7  |proj g|=      0.069944
At iterate    31  f =      -3538.7  |proj g|=      0.065094
At iterate    32  f =      -3538.7  |proj g|=      0.075362
At iterate    33  f =      -3538.7  |proj g|=      0.092688
At iterate    34  f =      -3538.7  |proj g|=       0.12795
At iterate    35  f =      -3538.7  |proj g|=       0.20639
At iterate    36  f =      -3538.7  |proj g|=      0.018991
At iterate    37  f =      -3538.7  |proj g|=      0.018377

iterations 37
function evaluations 48
segments explored during Cauchy searches 45
BFGS updates skipped 0
active bounds at final generalized Cauchy point 1
norm of the final projected gradient 0.0183774
final function value -3538.67

F = -3538.67
final  value -3538.666174 
converged
[1] "Fri Feb 09 16:41:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 3.5818, its = 12
i = 2 (of 248), d = 6.23306, its = 23
i = 3 (of 248), d = 6.23306, its = 56
i = 4 (of 248), d = 6.23306, its = 57
i = 5 (of 248), d = 6.23306, its = 60
i = 6 (of 248), d = 4.57239, its = 13
i = 7 (of 248), d = 4.90429, its = 13
i = 8 (of 248), d = 3.88248, its = 11
i = 9 (of 248), d = 1.78557, its = 10
i = 10 (of 248), d = 5.97925, its = 12
i = 11 (of 248), d = 2.74108, its = 11
i = 12 (of 248), d = 2.21231, its = 11
i = 13 (of 248), d = 5.90789, its = 12
i = 14 (of 248), d = 1.53271, its = 10
i = 15 (of 248), d = 3.58982, its = 12
i = 16 (of 248), d = 2.56012, its = 11
i = 17 (of 248), d = 2.42759, its = 11
i = 18 (of 248), d = 5.97204, its = 12
i = 19 (of 248), d = 1.7046, its = 11
i = 20 (of 248), d = 4.04479, its = 11
i = 21 (of 248), d = 4.22213, its = 12
i = 22 (of 248), d = 5.69483, its = 12
i = 23 (of 248), d = 2.88378, its = 12
i = 24 (of 248), d = 4.22306, its = 12
i = 25 (of 248), d = 3.23605, its = 11
i = 26 (of 248), d = 2.15783, its = 11
i = 27 (of 248), d = 6.23306, its = 22
i = 28 (of 248), d = 6.23306, its = 24
i = 29 (of 248), d = 1.24756, its = 10
i = 30 (of 248), d = 6.23306, its = 24
i = 31 (of 248), d = 4.62411, its = 12
i = 32 (of 248), d = 6.23306, its = 56
i = 33 (of 248), d = 1.26428, its = 10
i = 34 (of 248), d = 6.23306, its = 62
i = 35 (of 248), d = 3.91376, its = 11
i = 36 (of 248), d = 2.71676, its = 11
i = 37 (of 248), d = 5.32455, its = 12
i = 38 (of 248), d = 3.04354, its = 12
i = 39 (of 248), d = 1.34857, its = 10
i = 40 (of 248), d = 4.00192, its = 12
i = 41 (of 248), d = 6.23306, its = 56
i = 42 (of 248), d = 2.47311, its = 11
i = 43 (of 248), d = 5.8181, its = 12
i = 44 (of 248), d = 4.00976, its = 12
i = 45 (of 248), d = 3.76409, its = 11
i = 46 (of 248), d = 6.23306, its = 59
i = 47 (of 248), d = 6.23306, its = 25
i = 48 (of 248), d = 4.6515, its = 12
i = 49 (of 248), d = 1.66063, its = 11
i = 50 (of 248), d = 6.23239, its = 12
i = 51 (of 248), d = 4.26747, its = 12
i = 52 (of 248), d = 5.5344, its = 12
i = 53 (of 248), d = 6.23306, its = 19
i = 54 (of 248), d = 4.77689, its = 11
i = 55 (of 248), d = 2.14049, its = 11
i = 56 (of 248), d = 6.23306, its = 20
i = 57 (of 248), d = 3.80029, its = 11
i = 58 (of 248), d = 5.58223, its = 12
i = 59 (of 248), d = 3.52142, its = 11
i = 60 (of 248), d = 6.15401, its = 12
i = 61 (of 248), d = 3.20316, its = 11
i = 62 (of 248), d = 4.64283, its = 12
i = 63 (of 248), d = 6.23306, its = 57
i = 64 (of 248), d = 2.51363, its = 11
i = 65 (of 248), d = 4.1503, its = 12
i = 66 (of 248), d = 5.28665, its = 12
i = 67 (of 248), d = 4.2458, its = 11
i = 68 (of 248), d = 3.82233, its = 12
i = 69 (of 248), d = 3.73964, its = 12
i = 70 (of 248), d = 4.82096, its = 12
i = 71 (of 248), d = 4.58059, its = 12
i = 72 (of 248), d = 2.72646, its = 11
i = 73 (of 248), d = 4.48636, its = 12
i = 74 (of 248), d = 6.18478, its = 12
i = 75 (of 248), d = 4.83622, its = 12
i = 76 (of 248), d = 5.27654, its = 11
i = 77 (of 248), d = 5.70644, its = 12
i = 78 (of 248), d = 1.84254, its = 10
i = 79 (of 248), d = 6.23306, its = 22
i = 80 (of 248), d = 3.61079, its = 12
i = 81 (of 248), d = 4.53367, its = 12
i = 82 (of 248), d = 6.23306, its = 24
i = 83 (of 248), d = 4.40741, its = 12
i = 84 (of 248), d = 2.68581, its = 11
i = 85 (of 248), d = 2.5282, its = 11
i = 86 (of 248), d = 1.31717, its = 10
i = 87 (of 248), d = 3.72739, its = 12
i = 88 (of 248), d = 4.49304, its = 11
i = 89 (of 248), d = 5.21518, its = 12
i = 90 (of 248), d = 6.01354, its = 12
i = 91 (of 248), d = 2.64511, its = 11
i = 92 (of 248), d = 4.02086, its = 11
i = 93 (of 248), d = 1.69516, its = 10
i = 94 (of 248), d = 4.63684, its = 12
i = 95 (of 248), d = 3.80624, its = 12
i = 96 (of 248), d = 4.4437, its = 12
i = 97 (of 248), d = 5.18851, its = 12
i = 98 (of 248), d = 5.00284, its = 12
i = 99 (of 248), d = 4.70611, its = 12
i = 100 (of 248), d = 3.90961, its = 12
i = 101 (of 248), d = 6.23306, its = 17
i = 102 (of 248), d = 6.23306, its = 57
i = 103 (of 248), d = 4.91039, its = 12
i = 104 (of 248), d = 4.20909, its = 12
i = 105 (of 248), d = 2.48733, its = 11
i = 106 (of 248), d = 4.21899, its = 12
i = 107 (of 248), d = 3.71242, its = 13
i = 108 (of 248), d = 4.56561, its = 12
i = 109 (of 248), d = 1.42614, its = 10
i = 110 (of 248), d = 2.20987, its = 11
i = 111 (of 248), d = 5.07087, its = 13
i = 112 (of 248), d = 1.48408, its = 10
i = 113 (of 248), d = 6.23306, its = 54
i = 114 (of 248), d = 2.07291, its = 11
i = 115 (of 248), d = 6.23306, its = 23
i = 116 (of 248), d = 2.40422, its = 11
i = 117 (of 248), d = 5.39956, its = 12
i = 118 (of 248), d = 6.23306, its = 57
i = 119 (of 248), d = 2.22019, its = 11
i = 120 (of 248), d = 1.53344, its = 11
i = 121 (of 248), d = 2.28418, its = 11
i = 122 (of 248), d = 6.23306, its = 59
i = 123 (of 248), d = 6.23306, its = 22
i = 124 (of 248), d = 4.48142, its = 12
i = 125 (of 248), d = 2.04067, its = 11
i = 126 (of 248), d = 6.23306, its = 61
i = 127 (of 248), d = 3.07082, its = 11
i = 128 (of 248), d = 5.95763, its = 12
i = 129 (of 248), d = 2.52227, its = 11
i = 130 (of 248), d = 1.64593, its = 10
i = 131 (of 248), d = 2.1501, its = 11
i = 132 (of 248), d = 3.97423, its = 11
i = 133 (of 248), d = 6.19898, its = 13
i = 134 (of 248), d = 5.0922, its = 13
i = 135 (of 248), d = 1.52691, its = 11
i = 136 (of 248), d = 6.23306, its = 59
i = 137 (of 248), d = 5.01402, its = 12
i = 138 (of 248), d = 4.73564, its = 12
i = 139 (of 248), d = 6.23306, its = 56
i = 140 (of 248), d = 6.23306, its = 20
i = 141 (of 248), d = 3.68061, its = 11
i = 142 (of 248), d = 4.88381, its = 12
i = 143 (of 248), d = 4.27768, its = 12
i = 144 (of 248), d = 4.36559, its = 12
i = 145 (of 248), d = 2.1473, its = 11
i = 146 (of 248), d = 3.12602, its = 11
i = 147 (of 248), d = 6.23306, its = 21
i = 148 (of 248), d = 3.81344, its = 10
i = 149 (of 248), d = 5.96978, its = 12
i = 150 (of 248), d = 5.81826, its = 12
i = 151 (of 248), d = 1.72725, its = 11
i = 152 (of 248), d = 3.35758, its = 11
i = 153 (of 248), d = 5.19296, its = 12
i = 154 (of 248), d = 6.23306, its = 60
i = 155 (of 248), d = 6.23306, its = 59
i = 156 (of 248), d = 4.69467, its = 11
i = 157 (of 248), d = 4.58143, its = 12
i = 158 (of 248), d = 4.72563, its = 12
i = 159 (of 248), d = 5.74622, its = 12
i = 160 (of 248), d = 5.52595, its = 12
i = 161 (of 248), d = 1.61291, its = 10
i = 162 (of 248), d = 6.23306, its = 21
i = 163 (of 248), d = 5.28068, its = 13
i = 164 (of 248), d = 2.5602, its = 11
i = 165 (of 248), d = 5.00778, its = 13
i = 166 (of 248), d = 4.83109, its = 12
i = 167 (of 248), d = 1.89075, its = 10
i = 168 (of 248), d = 1.59467, its = 10
i = 169 (of 248), d = 3.84551, its = 11
i = 170 (of 248), d = 4.59278, its = 12
i = 171 (of 248), d = 3.74635, its = 11
i = 172 (of 248), d = 4.28471, its = 12
i = 173 (of 248), d = 5.28863, its = 12
i = 174 (of 248), d = 6.23306, its = 20
i = 175 (of 248), d = 5.46529, its = 13
i = 176 (of 248), d = 6.23306, its = 61
i = 177 (of 248), d = 4.41294, its = 12
i = 178 (of 248), d = 1.95476, its = 11
i = 179 (of 248), d = 2.39783, its = 11
i = 180 (of 248), d = 6.23306, its = 52
i = 181 (of 248), d = 6.23306, its = 23
i = 182 (of 248), d = 3.98778, its = 12
i = 183 (of 248), d = 3.71166, its = 12
i = 184 (of 248), d = 2.52082, its = 11
i = 185 (of 248), d = 6.23306, its = 56
i = 186 (of 248), d = 2.82575, its = 11
i = 187 (of 248), d = 2.33854, its = 11
i = 188 (of 248), d = 3.93764, its = 12
i = 189 (of 248), d = 5.63596, its = 13
i = 190 (of 248), d = 1.39664, its = 10
i = 191 (of 248), d = 3.0914, its = 11
i = 192 (of 248), d = 6.20074, its = 12
i = 193 (of 248), d = 2.28234, its = 11
i = 194 (of 248), d = 3.35581, its = 11
i = 195 (of 248), d = 1.56721, its = 10
i = 196 (of 248), d = 5.41997, its = 12
i = 197 (of 248), d = 1.82322, its = 11
i = 198 (of 248), d = 3.78554, its = 12
i = 199 (of 248), d = 3.71101, its = 12
i = 200 (of 248), d = 5.54124, its = 12
i = 201 (of 248), d = 6.23306, its = 26
i = 202 (of 248), d = 2.59152, its = 11
i = 203 (of 248), d = 5.90379, its = 12
i = 204 (of 248), d = 6.01982, its = 12
i = 205 (of 248), d = 3.18147, its = 12
i = 206 (of 248), d = 6.05716, its = 12
i = 207 (of 248), d = 3.89829, its = 12
i = 208 (of 248), d = 5.0117, its = 13
i = 209 (of 248), d = 3.24466, its = 11
i = 210 (of 248), d = 5.28647, its = 12
i = 211 (of 248), d = 1.59624, its = 11
i = 212 (of 248), d = 4.1706, its = 12
i = 213 (of 248), d = 6.23306, its = 20
i = 214 (of 248), d = 6.23306, its = 58
i = 215 (of 248), d = 4.34689, its = 13
i = 216 (of 248), d = 6.23306, its = 18
i = 217 (of 248), d = 5.16168, its = 12
i = 218 (of 248), d = 5.25945, its = 13
i = 219 (of 248), d = 6.23306, its = 21
i = 220 (of 248), d = 2.94302, its = 11
i = 221 (of 248), d = 5.17959, its = 12
i = 222 (of 248), d = 3.88766, its = 12
i = 223 (of 248), d = 3.44521, its = 11
i = 224 (of 248), d = 2.61027, its = 11
i = 225 (of 248), d = 4.8545, its = 12
i = 226 (of 248), d = 4.85518, its = 11
i = 227 (of 248), d = 6.23306, its = 60
i = 228 (of 248), d = 6.0809, its = 12
i = 229 (of 248), d = 5.53644, its = 12
i = 230 (of 248), d = 4.5144, its = 12
i = 231 (of 248), d = 1.38575, its = 10
i = 232 (of 248), d = 1.54547, its = 10
i = 233 (of 248), d = 6.23306, its = 19
i = 234 (of 248), d = 1.61222, its = 11
i = 235 (of 248), d = 5.95545, its = 11
i = 236 (of 248), d = 3.20428, its = 12
i = 237 (of 248), d = 5.46913, its = 13
i = 238 (of 248), d = 3.85615, its = 11
i = 239 (of 248), d = 6.09636, its = 12
i = 240 (of 248), d = 2.80896, its = 11
i = 241 (of 248), d = 5.38171, its = 12
i = 242 (of 248), d = 6.23306, its = 15
i = 243 (of 248), d = 4.19291, its = 12
i = 244 (of 248), d = 3.64095, its = 11
i = 245 (of 248), d = 6.23306, its = 21
i = 246 (of 248), d = 4.69579, its = 11
i = 247 (of 248), d = 6.23306, its = 63
i = 248 (of 248), d = 3.46954, its = 12
[1] "Fri Feb 09 16:42:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 16:42:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 16:42:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 16:42:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 16:42:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 16:42:43 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=1; decay=0.0282
# weights:  12
initial  value 1907.379895 
iter  10 value 20.801743
iter  20 value 3.525647
iter  30 value 1.375781
iter  40 value 1.115872
iter  50 value 1.115721
final  value 1.115721 
converged
# weights:  12
initial  value 1333.194474 
iter  10 value 201.523094
iter  20 value 9.096772
iter  30 value 3.066962
iter  40 value 1.365995
iter  50 value 1.142388
iter  60 value 1.142082
final  value 1.142082 
converged
# weights:  12
initial  value 1204.813258 
iter  10 value 205.249505
iter  20 value 204.873664
iter  30 value 193.617368
iter  40 value 33.852504
iter  50 value 2.207681
iter  60 value 1.524352
iter  70 value 1.474969
iter  80 value 1.474502
iter  80 value 1.474502
iter  80 value 1.474502
final  value 1.474502 
converged
[Tune-y] 1: rmse.test.rmse=0.0282; time: 0.0 min
[Tune-x] 2: size=14; decay=0.00201
# weights:  155
initial  value 390.657686 
iter  10 value 8.210887
iter  20 value 0.310307
iter  30 value 0.165911
iter  40 value 0.133434
iter  50 value 0.098710
iter  60 value 0.091182
iter  70 value 0.082576
iter  80 value 0.079120
iter  90 value 0.074219
iter 100 value 0.071544
final  value 0.071544 
stopped after 100 iterations
# weights:  155
initial  value 662.491347 
iter  10 value 15.018316
iter  20 value 0.951258
iter  30 value 0.405100
iter  40 value 0.330770
iter  50 value 0.251102
iter  60 value 0.191670
iter  70 value 0.159701
iter  80 value 0.130892
iter  90 value 0.116501
iter 100 value 0.105083
final  value 0.105083 
stopped after 100 iterations
# weights:  155
initial  value 4401.070267 
iter  10 value 5.225215
iter  20 value 0.371833
iter  30 value 0.245425
iter  40 value 0.211991
iter  50 value 0.188798
iter  60 value 0.177642
iter  70 value 0.169843
iter  80 value 0.160701
iter  90 value 0.148571
iter 100 value 0.139382
final  value 0.139382 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.0049; time: 0.0 min
[Tune-x] 3: size=10; decay=0.0146
# weights:  111
initial  value 2349.676483 
iter  10 value 1.664067
iter  20 value 0.588190
iter  30 value 0.417507
iter  40 value 0.393192
iter  50 value 0.384752
iter  60 value 0.379484
iter  70 value 0.375918
iter  80 value 0.372597
iter  90 value 0.370956
iter 100 value 0.370284
final  value 0.370284 
stopped after 100 iterations
# weights:  111
initial  value 2438.385806 
iter  10 value 2.492078
iter  20 value 0.700425
iter  30 value 0.470049
iter  40 value 0.429370
iter  50 value 0.409361
iter  60 value 0.400006
iter  70 value 0.393650
iter  80 value 0.390643
iter  90 value 0.388690
iter 100 value 0.385884
final  value 0.385884 
stopped after 100 iterations
# weights:  111
initial  value 1448.722615 
iter  10 value 5.801902
iter  20 value 0.988868
iter  30 value 0.620117
iter  40 value 0.500366
iter  50 value 0.445151
iter  60 value 0.417051
iter  70 value 0.397959
iter  80 value 0.386907
iter  90 value 0.383351
iter 100 value 0.379369
final  value 0.379369 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.0129; time: 0.0 min
[Tune-x] 4: size=1; decay=0.00151
# weights:  12
initial  value 1184.655374 
iter  10 value 210.183060
iter  20 value 210.066864
final  value 210.054020 
converged
# weights:  12
initial  value 2123.632413 
iter  10 value 52.153735
iter  20 value 6.175389
iter  30 value 1.439030
iter  40 value 0.324631
iter  50 value 0.173399
iter  60 value 0.158618
iter  70 value 0.150875
iter  80 value 0.150569
iter  90 value 0.150432
iter 100 value 0.150349
final  value 0.150349 
stopped after 100 iterations
# weights:  12
initial  value 440.316943 
iter  10 value 203.088918
iter  20 value 166.903465
iter  30 value 43.814044
iter  40 value 15.738727
iter  50 value 1.949600
iter  60 value 0.453318
iter  70 value 0.310544
iter  80 value 0.166013
iter  90 value 0.146149
iter 100 value 0.142577
final  value 0.142577 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.363; time: 0.0 min
[Tune-x] 5: size=15; decay=0.000446
# weights:  166
initial  value 352.695913 
iter  10 value 4.751422
iter  20 value 0.386380
iter  30 value 0.086346
iter  40 value 0.062307
iter  50 value 0.052756
iter  60 value 0.049161
iter  70 value 0.044872
iter  80 value 0.042284
iter  90 value 0.040342
iter 100 value 0.038141
final  value 0.038141 
stopped after 100 iterations
# weights:  166
initial  value 778.119565 
iter  10 value 1.743521
iter  20 value 0.155931
iter  30 value 0.067406
iter  40 value 0.050772
iter  50 value 0.046431
iter  60 value 0.043040
iter  70 value 0.040283
iter  80 value 0.036576
iter  90 value 0.034117
iter 100 value 0.032654
final  value 0.032654 
stopped after 100 iterations
# weights:  166
initial  value 7824.547243 
iter  10 value 72.052274
iter  20 value 16.708647
iter  30 value 2.041451
iter  40 value 0.574699
iter  50 value 0.476593
iter  60 value 0.406719
iter  70 value 0.350253
iter  80 value 0.286508
iter  90 value 0.242803
iter 100 value 0.194212
final  value 0.194212 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00531; time: 0.0 min
[Tune-x] 6: size=14; decay=0.0189
# weights:  155
initial  value 185.989549 
iter  10 value 8.564830
iter  20 value 1.792009
iter  30 value 1.241965
iter  40 value 1.008045
iter  50 value 0.824289
iter  60 value 0.688414
iter  70 value 0.591476
iter  80 value 0.530389
iter  90 value 0.489647
iter 100 value 0.468128
final  value 0.468128 
stopped after 100 iterations
# weights:  155
initial  value 2196.385361 
iter  10 value 4.511376
iter  20 value 1.044155
iter  30 value 0.621469
iter  40 value 0.559248
iter  50 value 0.532705
iter  60 value 0.517848
iter  70 value 0.500442
iter  80 value 0.482314
iter  90 value 0.470689
iter 100 value 0.457046
final  value 0.457046 
stopped after 100 iterations
# weights:  155
initial  value 836.122817 
iter  10 value 2.100514
iter  20 value 0.861172
iter  30 value 0.564078
iter  40 value 0.520748
iter  50 value 0.493015
iter  60 value 0.477683
iter  70 value 0.461385
iter  80 value 0.450679
iter  90 value 0.444570
iter 100 value 0.441018
final  value 0.441018 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0138; time: 0.0 min
[Tune-x] 7: size=3; decay=0.118
# weights:  34
initial  value 3411.313991 
iter  10 value 46.072959
iter  20 value 6.338804
iter  30 value 3.522396
iter  40 value 2.859878
iter  50 value 2.747386
iter  60 value 2.736955
iter  70 value 2.721952
iter  80 value 2.705328
iter  90 value 2.662366
iter 100 value 2.659964
final  value 2.659964 
stopped after 100 iterations
# weights:  34
initial  value 2747.338985 
iter  10 value 87.556978
iter  20 value 9.914569
iter  30 value 4.294899
iter  40 value 3.049294
iter  50 value 2.827840
iter  60 value 2.779663
iter  70 value 2.765442
iter  80 value 2.724350
iter  90 value 2.682065
iter 100 value 2.674548
final  value 2.674548 
stopped after 100 iterations
# weights:  34
initial  value 2344.413533 
iter  10 value 34.286577
iter  20 value 4.587215
iter  30 value 2.896889
iter  40 value 2.684339
iter  50 value 2.676661
iter  60 value 2.675397
iter  70 value 2.667848
iter  80 value 2.661283
iter  90 value 2.635377
iter 100 value 2.632479
final  value 2.632479 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.0275; time: 0.0 min
[Tune-x] 8: size=18; decay=0.0178
# weights:  199
initial  value 1479.436280 
iter  10 value 3.116036
iter  20 value 0.992670
iter  30 value 0.720780
iter  40 value 0.628686
iter  50 value 0.577643
iter  60 value 0.511886
iter  70 value 0.474275
iter  80 value 0.450097
iter  90 value 0.435208
iter 100 value 0.423406
final  value 0.423406 
stopped after 100 iterations
# weights:  199
initial  value 869.742843 
iter  10 value 6.830229
iter  20 value 1.104669
iter  30 value 0.619704
iter  40 value 0.501519
iter  50 value 0.451103
iter  60 value 0.429948
iter  70 value 0.422815
iter  80 value 0.418932
iter  90 value 0.416925
iter 100 value 0.414852
final  value 0.414852 
stopped after 100 iterations
# weights:  199
initial  value 1843.270505 
iter  10 value 46.078185
iter  20 value 10.396943
iter  30 value 3.918107
iter  40 value 1.839482
iter  50 value 1.221932
iter  60 value 0.958467
iter  70 value 0.791012
iter  80 value 0.702403
iter  90 value 0.674780
iter 100 value 0.655898
final  value 0.655898 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.0136; time: 0.0 min
[Tune-x] 9: size=14; decay=0.00728
# weights:  155
initial  value 4198.179724 
iter  10 value 30.040643
iter  20 value 1.986923
iter  30 value 1.420401
iter  40 value 1.076141
iter  50 value 0.873974
iter  60 value 0.772405
iter  70 value 0.659073
iter  80 value 0.593137
iter  90 value 0.549007
iter 100 value 0.518312
final  value 0.518312 
stopped after 100 iterations
# weights:  155
initial  value 323.701959 
iter  10 value 1.225142
iter  20 value 0.389707
iter  30 value 0.293274
iter  40 value 0.244389
iter  50 value 0.227784
iter  60 value 0.219963
iter  70 value 0.215829
iter  80 value 0.213018
iter  90 value 0.208581
iter 100 value 0.203607
final  value 0.203607 
stopped after 100 iterations
# weights:  155
initial  value 2914.498925 
iter  10 value 60.967417
iter  20 value 7.081142
iter  30 value 2.243606
iter  40 value 0.793000
iter  50 value 0.642640
iter  60 value 0.541532
iter  70 value 0.500672
iter  80 value 0.450151
iter  90 value 0.404781
iter 100 value 0.361595
final  value 0.361595 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.0088; time: 0.0 min
[Tune-x] 10: size=1; decay=2.84e-05
# weights:  12
initial  value 1121.684972 
iter  10 value 47.837904
iter  20 value 5.173817
iter  30 value 1.169099
iter  40 value 0.265879
iter  50 value 0.075435
iter  60 value 0.061319
iter  70 value 0.032775
iter  80 value 0.023280
iter  90 value 0.020492
iter 100 value 0.016584
final  value 0.016584 
stopped after 100 iterations
# weights:  12
initial  value 1004.268080 
iter  10 value 202.933850
iter  20 value 174.036945
iter  30 value 34.319254
iter  40 value 2.339666
iter  50 value 0.519739
iter  60 value 0.157738
iter  70 value 0.058884
iter  80 value 0.046975
iter  90 value 0.026642
iter 100 value 0.021508
final  value 0.021508 
stopped after 100 iterations
# weights:  12
initial  value 1659.852418 
iter  10 value 28.863571
iter  20 value 5.312924
iter  30 value 1.155096
iter  40 value 0.337324
iter  50 value 0.124222
iter  60 value 0.066412
iter  70 value 0.043657
iter  80 value 0.026974
iter  90 value 0.024781
iter 100 value 0.020213
final  value 0.020213 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00605; time: 0.0 min
[Tune-x] 11: size=8; decay=0.00379
# weights:  89
initial  value 822.809527 
iter  10 value 6.556102
iter  20 value 0.666950
iter  30 value 0.266799
iter  40 value 0.203209
iter  50 value 0.179510
iter  60 value 0.165199
iter  70 value 0.158246
iter  80 value 0.151096
iter  90 value 0.145401
iter 100 value 0.141822
final  value 0.141822 
stopped after 100 iterations
# weights:  89
initial  value 1902.502942 
iter  10 value 11.210671
iter  20 value 0.928334
iter  30 value 0.404951
iter  40 value 0.295706
iter  50 value 0.221746
iter  60 value 0.186210
iter  70 value 0.166725
iter  80 value 0.153321
iter  90 value 0.146879
iter 100 value 0.142645
final  value 0.142645 
stopped after 100 iterations
# weights:  89
initial  value 870.021276 
iter  10 value 2.605117
iter  20 value 0.318535
iter  30 value 0.192058
iter  40 value 0.167497
iter  50 value 0.152525
iter  60 value 0.145944
iter  70 value 0.139517
iter  80 value 0.133161
iter  90 value 0.128255
iter 100 value 0.126113
final  value 0.126113 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.00652; time: 0.0 min
[Tune-x] 12: size=4; decay=0.149
# weights:  45
initial  value 364.108131 
iter  10 value 21.655320
iter  20 value 8.024382
iter  30 value 4.673455
iter  40 value 3.598923
iter  50 value 3.253344
iter  60 value 3.105796
iter  70 value 2.966728
iter  80 value 2.950310
iter  90 value 2.940250
iter 100 value 2.917781
final  value 2.917781 
stopped after 100 iterations
# weights:  45
initial  value 2425.568160 
iter  10 value 44.317141
iter  20 value 5.025021
iter  30 value 3.109632
iter  40 value 2.978145
iter  50 value 2.967540
iter  60 value 2.964351
iter  70 value 2.960355
iter  80 value 2.959314
iter  90 value 2.959175
iter 100 value 2.957643
final  value 2.957643 
stopped after 100 iterations
# weights:  45
initial  value 2207.160028 
iter  10 value 10.714401
iter  20 value 4.751295
iter  30 value 3.464261
iter  40 value 3.246050
iter  50 value 3.192360
iter  60 value 3.127957
iter  70 value 3.051775
iter  80 value 2.977588
iter  90 value 2.905411
iter 100 value 2.863387
final  value 2.863387 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0257; time: 0.0 min
[Tune-x] 13: size=3; decay=7.16e-05
# weights:  34
initial  value 723.881779 
iter  10 value 23.436059
iter  20 value 3.530779
iter  30 value 0.514572
iter  40 value 0.082528
iter  50 value 0.031040
iter  60 value 0.022698
iter  70 value 0.020501
iter  80 value 0.018839
iter  90 value 0.016706
iter 100 value 0.016041
final  value 0.016041 
stopped after 100 iterations
# weights:  34
initial  value 1514.343642 
iter  10 value 12.564952
iter  20 value 1.257366
iter  30 value 0.274519
iter  40 value 0.141857
iter  50 value 0.089021
iter  60 value 0.050654
iter  70 value 0.043571
iter  80 value 0.041349
iter  90 value 0.036709
iter 100 value 0.031895
final  value 0.031895 
stopped after 100 iterations
# weights:  34
initial  value 2064.217911 
iter  10 value 4.859427
iter  20 value 0.220473
iter  30 value 0.087481
iter  40 value 0.050529
iter  50 value 0.038566
iter  60 value 0.026667
iter  70 value 0.021158
iter  80 value 0.019516
iter  90 value 0.017078
iter 100 value 0.014359
final  value 0.014359 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.00338; time: 0.0 min
[Tune-x] 14: size=6; decay=0.000467
# weights:  67
initial  value 2074.681191 
iter  10 value 2.566796
iter  20 value 0.373811
iter  30 value 0.104560
iter  40 value 0.062232
iter  50 value 0.046115
iter  60 value 0.039100
iter  70 value 0.034116
iter  80 value 0.031553
iter  90 value 0.030282
iter 100 value 0.029224
final  value 0.029224 
stopped after 100 iterations
# weights:  67
initial  value 880.497921 
iter  10 value 4.138866
iter  20 value 0.292380
iter  30 value 0.094366
iter  40 value 0.073444
iter  50 value 0.061226
iter  60 value 0.054446
iter  70 value 0.050139
iter  80 value 0.046960
iter  90 value 0.043787
iter 100 value 0.039580
final  value 0.039580 
stopped after 100 iterations
# weights:  67
initial  value 2576.556084 
iter  10 value 1.913053
iter  20 value 0.269758
iter  30 value 0.048996
iter  40 value 0.033984
iter  50 value 0.031007
iter  60 value 0.028525
iter  70 value 0.027481
iter  80 value 0.025667
iter  90 value 0.024992
iter 100 value 0.024471
final  value 0.024471 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00323; time: 0.0 min
[Tune-x] 15: size=6; decay=0.000951
# weights:  67
initial  value 453.556311 
iter  10 value 33.789929
iter  20 value 0.877098
iter  30 value 0.224768
iter  40 value 0.146176
iter  50 value 0.112614
iter  60 value 0.089887
iter  70 value 0.079006
iter  80 value 0.074011
iter  90 value 0.070718
iter 100 value 0.068860
final  value 0.068860 
stopped after 100 iterations
# weights:  67
initial  value 319.845896 
iter  10 value 4.890284
iter  20 value 0.266338
iter  30 value 0.170722
iter  40 value 0.147335
iter  50 value 0.134571
iter  60 value 0.123885
iter  70 value 0.114427
iter  80 value 0.103263
iter  90 value 0.097670
iter 100 value 0.091238
final  value 0.091238 
stopped after 100 iterations
# weights:  67
initial  value 2833.727770 
iter  10 value 8.512367
iter  20 value 1.336250
iter  30 value 0.169277
iter  40 value 0.077346
iter  50 value 0.064139
iter  60 value 0.056615
iter  70 value 0.052995
iter  80 value 0.051952
iter  90 value 0.050138
iter 100 value 0.049381
final  value 0.049381 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.00462; time: 0.0 min
[Tune-x] 16: size=16; decay=8.97
# weights:  177
initial  value 3391.618814 
iter  10 value 424.921183
iter  20 value 218.350228
iter  30 value 128.361350
iter  40 value 115.104982
iter  50 value 111.600107
iter  60 value 107.505927
iter  70 value 102.308644
iter  80 value 101.166019
iter  90 value 99.780744
iter 100 value 98.555813
final  value 98.555813 
stopped after 100 iterations
# weights:  177
initial  value 1503.325425 
iter  10 value 303.398408
iter  20 value 136.999428
iter  30 value 122.604755
iter  40 value 113.193269
iter  50 value 108.239420
iter  60 value 106.263334
iter  70 value 103.466405
iter  80 value 101.380326
iter  90 value 99.685021
iter 100 value 98.935476
final  value 98.935476 
stopped after 100 iterations
# weights:  177
initial  value 2105.973489 
iter  10 value 228.734297
iter  20 value 115.047703
iter  30 value 101.860602
iter  40 value 100.909542
iter  50 value 100.538918
iter  60 value 100.054931
iter  70 value 99.513882
iter  80 value 99.113440
iter  90 value 98.606037
iter 100 value 98.380911
final  value 98.380911 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.211; time: 0.0 min
[Tune-x] 17: size=7; decay=0.000548
# weights:  78
initial  value 447.750717 
iter  10 value 0.592869
iter  20 value 0.094191
iter  30 value 0.048042
iter  40 value 0.037347
iter  50 value 0.032480
iter  60 value 0.030542
iter  70 value 0.029078
iter  80 value 0.027589
iter  90 value 0.025530
iter 100 value 0.024702
final  value 0.024702 
stopped after 100 iterations
# weights:  78
initial  value 2018.161033 
iter  10 value 5.680239
iter  20 value 0.234728
iter  30 value 0.107294
iter  40 value 0.077975
iter  50 value 0.064643
iter  60 value 0.055522
iter  70 value 0.048156
iter  80 value 0.043407
iter  90 value 0.039700
iter 100 value 0.037842
final  value 0.037842 
stopped after 100 iterations
# weights:  78
initial  value 3937.214336 
iter  10 value 4.216382
iter  20 value 0.212135
iter  30 value 0.084007
iter  40 value 0.046414
iter  50 value 0.040525
iter  60 value 0.034909
iter  70 value 0.030406
iter  80 value 0.029219
iter  90 value 0.028133
iter 100 value 0.027223
final  value 0.027223 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.00284; time: 0.0 min
[Tune-x] 18: size=5; decay=0.0554
# weights:  56
initial  value 1366.034723 
iter  10 value 94.085207
iter  20 value 17.586619
iter  30 value 8.487771
iter  40 value 3.485330
iter  50 value 1.856411
iter  60 value 1.505417
iter  70 value 1.443860
iter  80 value 1.385332
iter  90 value 1.313812
iter 100 value 1.296727
final  value 1.296727 
stopped after 100 iterations
# weights:  56
initial  value 1608.735499 
iter  10 value 126.181760
iter  20 value 5.830742
iter  30 value 2.163189
iter  40 value 1.628826
iter  50 value 1.523267
iter  60 value 1.453851
iter  70 value 1.403194
iter  80 value 1.383260
iter  90 value 1.371781
iter 100 value 1.359284
final  value 1.359284 
stopped after 100 iterations
# weights:  56
initial  value 689.482368 
iter  10 value 17.050312
iter  20 value 3.315037
iter  30 value 2.284965
iter  40 value 1.802987
iter  50 value 1.547271
iter  60 value 1.404347
iter  70 value 1.350654
iter  80 value 1.329460
iter  90 value 1.310620
iter 100 value 1.290845
final  value 1.290845 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.021; time: 0.0 min
[Tune-x] 19: size=16; decay=0.0217
# weights:  177
initial  value 889.306132 
iter  10 value 8.650300
iter  20 value 3.266166
iter  30 value 2.225732
iter  40 value 1.463957
iter  50 value 0.923480
iter  60 value 0.745975
iter  70 value 0.670030
iter  80 value 0.634198
iter  90 value 0.619334
iter 100 value 0.606084
final  value 0.606084 
stopped after 100 iterations
# weights:  177
initial  value 615.964767 
iter  10 value 30.996818
iter  20 value 3.008940
iter  30 value 1.559863
iter  40 value 1.162906
iter  50 value 0.917960
iter  60 value 0.761549
iter  70 value 0.666010
iter  80 value 0.591028
iter  90 value 0.552990
iter 100 value 0.541690
final  value 0.541690 
stopped after 100 iterations
# weights:  177
initial  value 1460.273814 
iter  10 value 5.810830
iter  20 value 3.072023
iter  30 value 2.225615
iter  40 value 1.510899
iter  50 value 1.154088
iter  60 value 0.942626
iter  70 value 0.796525
iter  80 value 0.696910
iter  90 value 0.632090
iter 100 value 0.600992
final  value 0.600992 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.0145; time: 0.0 min
[Tune-x] 20: size=17; decay=0.000183
# weights:  188
initial  value 2555.112593 
iter  10 value 3.450055
iter  20 value 0.200647
iter  30 value 0.049751
iter  40 value 0.029088
iter  50 value 0.022613
iter  60 value 0.019490
iter  70 value 0.017862
iter  80 value 0.016210
iter  90 value 0.015076
iter 100 value 0.014272
final  value 0.014272 
stopped after 100 iterations
# weights:  188
initial  value 4091.289640 
iter  10 value 44.673161
iter  20 value 3.372154
iter  30 value 0.974617
iter  40 value 0.506885
iter  50 value 0.413654
iter  60 value 0.398157
iter  70 value 0.386250
iter  80 value 0.375414
iter  90 value 0.355095
iter 100 value 0.338416
final  value 0.338416 
stopped after 100 iterations
# weights:  188
initial  value 1742.845898 
iter  10 value 10.950606
iter  20 value 0.267919
iter  30 value 0.044767
iter  40 value 0.026055
iter  50 value 0.018908
iter  60 value 0.016829
iter  70 value 0.015387
iter  80 value 0.014165
iter  90 value 0.013553
iter 100 value 0.012454
final  value 0.012454 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.00299; time: 0.0 min
[Tune] Result: size=7; decay=0.000548 : rmse.test.rmse=0.00284
# weights:  78
initial  value 2216.805722 
iter  10 value 3.229820
iter  20 value 0.322569
iter  30 value 0.074106
iter  40 value 0.049735
iter  50 value 0.044152
iter  60 value 0.039843
iter  70 value 0.037706
iter  80 value 0.034545
iter  90 value 0.033394
iter 100 value 0.032415
final  value 0.032415 
stopped after 100 iterations
[1] "Fri Feb 09 16:42:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 619 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 363
 number of selected nodes                               : 86 
[1] "Fri Feb 09 16:43:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 16:43:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 16:43:25 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 16:43:29 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=1; min.node.size=6
[Tune-y] 1: rmse.test.rmse=0.0512; time: 0.0 min
[Tune-x] 2: mtry=6; min.node.size=4
[Tune-y] 2: rmse.test.rmse=0.0332; time: 0.1 min
[Tune-x] 3: mtry=5; min.node.size=6
[Tune-y] 3: rmse.test.rmse=0.0338; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=4
[Tune-y] 4: rmse.test.rmse=0.0485; time: 0.0 min
[Tune-x] 5: mtry=7; min.node.size=3
[Tune-y] 5: rmse.test.rmse=0.0342; time: 0.1 min
[Tune-x] 6: mtry=7; min.node.size=6
[Tune-y] 6: rmse.test.rmse=0.036; time: 0.1 min
[Tune-x] 7: mtry=1; min.node.size=7
[Tune-y] 7: rmse.test.rmse=0.0518; time: 0.0 min
[Tune-x] 8: mtry=8; min.node.size=6
[Tune-y] 8: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 9: mtry=7; min.node.size=5
[Tune-y] 9: rmse.test.rmse=0.035; time: 0.1 min
[Tune-x] 10: mtry=1; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0452; time: 0.0 min
[Tune-x] 11: mtry=4; min.node.size=5
[Tune-y] 11: rmse.test.rmse=0.033; time: 0.1 min
[Tune-x] 12: mtry=2; min.node.size=7
[Tune-y] 12: rmse.test.rmse=0.0382; time: 0.0 min
[Tune-x] 13: mtry=1; min.node.size=2
[Tune-y] 13: rmse.test.rmse=0.0465; time: 0.0 min
[Tune-x] 14: mtry=3; min.node.size=3
[Tune-y] 14: rmse.test.rmse=0.0328; time: 0.1 min
[Tune-x] 15: mtry=3; min.node.size=4
[Tune-y] 15: rmse.test.rmse=0.0333; time: 0.1 min
[Tune-x] 16: mtry=8; min.node.size=10
[Tune-y] 16: rmse.test.rmse=0.0408; time: 0.1 min
[Tune-x] 17: mtry=4; min.node.size=3
[Tune-y] 17: rmse.test.rmse=0.0324; time: 0.1 min
[Tune-x] 18: mtry=3; min.node.size=7
[Tune-y] 18: rmse.test.rmse=0.0353; time: 0.1 min
[Tune-x] 19: mtry=8; min.node.size=6
[Tune-y] 19: rmse.test.rmse=0.0372; time: 0.1 min
[Tune-x] 20: mtry=8; min.node.size=3
[Tune-y] 20: rmse.test.rmse=0.0357; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=3 : rmse.test.rmse=0.0324
[1] "Fri Feb 09 16:45:11 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 16:45:14 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.000993; maxdepth=19; minbucket=35; minsplit=22
[Tune-y] 1: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 2: cp=0.0283; maxdepth=17; minbucket=5; minsplit=21
[Tune-y] 2: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 3: cp=0.15; maxdepth=10; minbucket=36; minsplit=30
[Tune-y] 3: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 4: cp=0.00204; maxdepth=22; minbucket=44; minsplit=29
[Tune-y] 4: rmse.test.rmse=0.159; time: 0.0 min
[Tune-x] 5: cp=0.114; maxdepth=16; minbucket=5; minsplit=8
[Tune-y] 5: rmse.test.rmse=0.327; time: 0.0 min
[Tune-x] 6: cp=0.0111; maxdepth=15; minbucket=13; minsplit=36
[Tune-y] 6: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 7: cp=0.00205; maxdepth=6; minbucket=18; minsplit=17
[Tune-y] 7: rmse.test.rmse=0.125; time: 0.0 min
[Tune-x] 8: cp=0.00688; maxdepth=12; minbucket=40; minsplit=50
[Tune-y] 8: rmse.test.rmse=0.157; time: 0.0 min
[Tune-x] 9: cp=0.0101; maxdepth=11; minbucket=15; minsplit=33
[Tune-y] 9: rmse.test.rmse=0.153; time: 0.0 min
[Tune-x] 10: cp=0.232; maxdepth=18; minbucket=43; minsplit=14
[Tune-y] 10: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 11: cp=0.00603; maxdepth=15; minbucket=24; minsplit=41
[Tune-y] 11: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 12: cp=0.00224; maxdepth=11; minbucket=30; minsplit=49
[Tune-y] 12: rmse.test.rmse=0.142; time: 0.0 min
[Tune-x] 13: cp=0.0203; maxdepth=21; minbucket=12; minsplit=40
[Tune-y] 13: rmse.test.rmse=0.197; time: 0.0 min
[Tune-x] 14: cp=0.29; maxdepth=16; minbucket=42; minsplit=42
[Tune-y] 14: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 15: cp=0.0461; maxdepth=23; minbucket=5; minsplit=7
[Tune-y] 15: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 16: cp=0.162; maxdepth=27; minbucket=47; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 17: cp=0.314; maxdepth=4; minbucket=30; minsplit=17
[Tune-y] 17: rmse.test.rmse=0.374; time: 0.0 min
[Tune-x] 18: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46
[Tune-y] 18: rmse.test.rmse=0.119; time: 0.0 min
[Tune-x] 19: cp=0.0242; maxdepth=20; minbucket=36; minsplit=20
[Tune-y] 19: rmse.test.rmse=0.222; time: 0.0 min
[Tune-x] 20: cp=0.123; maxdepth=21; minbucket=44; minsplit=17
[Tune-y] 20: rmse.test.rmse=0.327; time: 0.0 min
[Tune] Result: cp=0.00156; maxdepth=8; minbucket=14; minsplit=46 : rmse.test.rmse=0.119
[1] "Fri Feb 09 16:45:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 16:45:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:45:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.261129 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2308 
    Y 1          1.855 
    Y 2          3.131 
    Y 3          2.043 
    Y 4           1.88 
    Y 5          1.641 
[1] "Fri Feb 09 16:45:50 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=10; max_depth=6; eta=0.393; gamma=3.84; colsample_bytree=0.494; min_child_weight=10.5; subsample=0.259
[Tune-y] 1: rmse.test.rmse=0.239; time: 0.0 min
[Tune-x] 2: nrounds=88; max_depth=8; eta=0.166; gamma=6.83; colsample_bytree=0.518; min_child_weight=2.13; subsample=0.759
[Tune-y] 2: rmse.test.rmse=0.199; time: 0.0 min
[Tune-x] 3: nrounds=1.77e+03; max_depth=6; eta=0.412; gamma=4.77; colsample_bytree=0.304; min_child_weight=1.51; subsample=0.513
[Tune-y] 3: rmse.test.rmse=0.192; time: 0.5 min
[Tune-x] 4: nrounds=131; max_depth=2; eta=0.418; gamma=1.07; colsample_bytree=0.357; min_child_weight=5.7; subsample=0.459
[Tune-y] 4: rmse.test.rmse=0.117; time: 0.0 min
[Tune-x] 5: nrounds=54; max_depth=4; eta=0.468; gamma=9.92; colsample_bytree=0.435; min_child_weight=5.8; subsample=0.418
[Tune-y] 5: rmse.test.rmse=0.254; time: 0.0 min
[Tune-x] 6: nrounds=420; max_depth=8; eta=0.334; gamma=8.42; colsample_bytree=0.384; min_child_weight=5.25; subsample=0.582
[Tune-y] 6: rmse.test.rmse=0.223; time: 0.2 min
[Tune-x] 7: nrounds=133; max_depth=8; eta=0.0728; gamma=2.98; colsample_bytree=0.525; min_child_weight=19.4; subsample=0.579
[Tune-y] 7: rmse.test.rmse=0.151; time: 0.0 min
[Tune-x] 8: nrounds=583; max_depth=2; eta=0.461; gamma=8.21; colsample_bytree=0.5; min_child_weight=16.3; subsample=0.863
[Tune-y] 8: rmse.test.rmse=0.188; time: 0.1 min
[Tune-x] 9: nrounds=280; max_depth=8; eta=0.00596; gamma=0.619; colsample_bytree=0.595; min_child_weight=17.5; subsample=0.939
[Tune-y] 9: rmse.test.rmse=0.262; time: 0.1 min
[Tune-x] 10: nrounds=11; max_depth=9; eta=0.0251; gamma=5.51; colsample_bytree=0.411; min_child_weight=1.35; subsample=0.391
[Tune-y] 10: rmse.test.rmse=0.959; time: 0.0 min
[Tune-x] 11: nrounds=36; max_depth=9; eta=0.278; gamma=6.14; colsample_bytree=0.572; min_child_weight=6.95; subsample=0.773
[Tune-y] 11: rmse.test.rmse=0.188; time: 0.0 min
[Tune-x] 12: nrounds=508; max_depth=9; eta=0.16; gamma=3.71; colsample_bytree=0.597; min_child_weight=16.1; subsample=0.786
[Tune-y] 12: rmse.test.rmse=0.153; time: 0.2 min
[Tune-x] 13: nrounds=767; max_depth=7; eta=0.25; gamma=7.33; colsample_bytree=0.633; min_child_weight=14.6; subsample=0.843
[Tune-y] 13: rmse.test.rmse=0.196; time: 0.3 min
[Tune-x] 14: nrounds=175; max_depth=6; eta=0.0983; gamma=9.05; colsample_bytree=0.55; min_child_weight=19.9; subsample=0.513
[Tune-y] 14: rmse.test.rmse=0.247; time: 0.0 min
[Tune-x] 15: nrounds=189; max_depth=3; eta=0.42; gamma=4.21; colsample_bytree=0.309; min_child_weight=18.5; subsample=0.399
[Tune-y] 15: rmse.test.rmse=0.19; time: 0.0 min
[Tune-x] 16: nrounds=172; max_depth=2; eta=0.412; gamma=9.59; colsample_bytree=0.598; min_child_weight=13.6; subsample=0.58
[Tune-y] 16: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 17: nrounds=1.79e+03; max_depth=9; eta=0.0552; gamma=3.22; colsample_bytree=0.631; min_child_weight=3.64; subsample=0.526
[Tune-y] 17: rmse.test.rmse=0.158; time: 0.8 min
[Tune-x] 18: nrounds=3.93e+03; max_depth=6; eta=0.284; gamma=9.03; colsample_bytree=0.361; min_child_weight=3.27; subsample=0.666
[Tune-y] 18: rmse.test.rmse=0.212; time: 1.1 min
[Tune-x] 19: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386
[Tune-y] 19: rmse.test.rmse=0.0763; time: 0.0 min
[Tune-x] 20: nrounds=32; max_depth=8; eta=0.0486; gamma=7.04; colsample_bytree=0.369; min_child_weight=13.6; subsample=0.295
[Tune-y] 20: rmse.test.rmse=0.387; time: 0.0 min
[Tune] Result: nrounds=292; max_depth=2; eta=0.595; gamma=0.0358; colsample_bytree=0.465; min_child_weight=12.2; subsample=0.386 : rmse.test.rmse=0.0763
[1] "Fri Feb 09 16:49:22 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 16:49:23 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [1,0.594666], n=(505,247)
**GROW** @depth 1: [4,0.550019], n=(179,66)
**GROW** @depth 1: [6,0.489538], n=(447,59)
**PRUNE** @depth 1: [6,0.492889]
**GROW** @depth 1: [1,0.316428], n=(205,299)
**GROW** @depth 1: [6,0.432497], n=(182,24)
**PRUNE** @depth 1: [6,0.432199]
**GROW** @depth 2: [3,0.4875], n=(79,215)
**GROW** @depth 3: [5,0.255448], n=(37,145)
**GROW** @depth 2: [4,0.380667], n=(179,28)
**GROW** @depth 3: [2,0.498822], n=(133,82)
**GROW** @depth 4: [3,0.554167], n=(19,126)
**GROW** @depth 2: [5,0.222214], n=(132,47)
**GROW** @depth 3: [6,0.294512], n=(68,16)
**PRUNE** @depth 3: [6,0.29682]
**GROW** @depth 4: [2,0.397173], n=(50,81)
**GROW** @depth 2: [1,0.79309], n=(38,28)
**GROW** @depth 3: [6,0.154442], n=(15,69)
**GROW** @depth 4: [2,0.262538], n=(31,16)
**GROW** @depth 5: [7,0.469042], n=(61,20)
**PRUNE** @depth 5: [7,0.465994]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(131,32,14,29,17,68,49,82,84,40,16,142,20,28)
**GROW** @depth 4: [1,0.678723], n=(54,88)
**GROW** @depth 4: [2,0.227701], n=(100,29)
**GROW** @depth 5: [1,0.149525], n=(52,48)
**GROW** @depth 5: [3,0.779167], n=(67,20)
**GROW** @depth 4: [9,0.507495], n=(56,16)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(51,49,22,30,24,27,18,57,17,44,83,86,39,15,53,62,29,20,26)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 4: [2,0.260855]
**GROW** @depth 5: [3,0.779167], n=(38,13)
**GROW** @depth 5: [7,0.626405], n=(61,22)
**PRUNE** @depth 5: [7,0.626405]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(52,17,55,51,27,19,61,19,40,83,85,40,14,39,13,63,30,20,24)
**GROW** @depth 2: [1,0.252778], n=(12,16)
**GROW** @depth 3: [3,0.504167], n=(41,12)
**PRUNE** @depth 4: [3,0.504167]
**GROW** @depth 4: [8,0.54722], n=(18,36)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(53,17,55,19,35,12,16,17,59,19,39,80,86,41,13,42,13,63,28,20,25)
**GROW** @depth 4: [3,0.684375], n=(53,34)
**GROW** @depth 5: [2,0.443958], n=(31,49)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(52,17,56,19,35,12,16,16,60,19,39,31,49,54,35,41,12,41,13,62,25,20,28)
**PRUNE** @depth 3: [8,0.546337]
**GROW** @depth 7: [3,0.682292], n=(34,29)
**GROW** @depth 7: [3,0.658333], n=(22,20)
**GROW** @depth 5: [5,0.0761606], n=(37,15)
**GROW** @depth 4: [8,0.496028], n=(30,26)
**PRUNE** @depth 7: [3,0.779167]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(36,16,17,30,25,54,12,16,19,58,19,37,34,49,53,34,41,13,21,33,31,31,28,20,25)
**GROW** @depth 3: [3,0.5], n=(43,12)
**PRUNE** @depth 3: [3,0.501042]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=9 n=(37,16,16,29,25,58,12,18,17,57,20,36,33,47,53,35,40,14,20,33,32,31,28,20,25)
Grow: 9.855%, Prune: 2.95%, Change: 78.33%, Swap: 18.83%

[1] "Fri Feb 09 16:49:31 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 16:49:32 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 16:49:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 19.4109 	 alpha= 1.8551 	 beta= 37675.81 
[1] "Fri Feb 09 16:49:35 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 16:49:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [4,0.0243537], n=(18,734)
**PRUNE** @depth 0: [4,0.0224803]
**GROW** @depth 0: [8,0.423654], n=(112,640)
**GROW** @depth 1: [3,0.529167], n=(211,429)
r=1000 d=[0] [0] [0]; n=(112,212,428)
r=2000 d=[0] [0] [0]; n=(112,212,428)

Sampling @ nn=0 pred locs:
r=1000 d=[0] [0] [0]; mh=3 n=(112,212,428)
**GROW** @depth 2: [9,0.45858], n=(87,341)
r=2000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
r=3000 d=[0] [0] [0] [0]; mh=4 n=(112,212,87,341)
**GROW** @depth 2: [4,0.341701], n=(192,20)
r=4000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
r=5000 d=[0] [0] [0] [0] [0]; mh=4 n=(112,192,20,87,341)
Grow: 1.42%, Prune: 0.277%, Change: 17.16%, Swap: 0%

[1] "Fri Feb 09 16:49:42 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 16:49:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 16:49:44 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 16:49:45 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 16:49:46 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 16:49:47 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 16:49:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 16:49:49 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:49:52 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=792; interaction.depth=5; shrinkage=0.363; n.minobsinnode=10
[Tune-y] 1: rmse.test.rmse=0.0435; time: 0.0 min
[Tune-x] 2: n.trees=141; interaction.depth=3; shrinkage=0.0423; n.minobsinnode=13
[Tune-y] 2: rmse.test.rmse=0.0439; time: 0.0 min
[Tune-x] 3: n.trees=723; interaction.depth=6; shrinkage=0.539; n.minobsinnode=23
[Tune-y] 3: rmse.test.rmse=0.0645; time: 0.0 min
[Tune-x] 4: n.trees=181; interaction.depth=4; shrinkage=0.235; n.minobsinnode=12
[Tune-y] 4: rmse.test.rmse=0.0391; time: 0.0 min
[Tune-x] 5: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5
[Tune-y] 5: rmse.test.rmse=0.0251; time: 0.0 min
[Tune-x] 6: n.trees=608; interaction.depth=9; shrinkage=0.33; n.minobsinnode=22
[Tune-y] 6: rmse.test.rmse=0.055; time: 0.1 min
[Tune-x] 7: n.trees=26; interaction.depth=6; shrinkage=0.382; n.minobsinnode=14
[Tune-y] 7: rmse.test.rmse=0.0535; time: 0.0 min
[Tune-x] 8: n.trees=103; interaction.depth=7; shrinkage=0.253; n.minobsinnode=13
[Tune-y] 8: rmse.test.rmse=0.0417; time: 0.0 min
[Tune-x] 9: n.trees=25; interaction.depth=9; shrinkage=0.409; n.minobsinnode=5
[Tune-y] 9: rmse.test.rmse=0.046; time: 0.0 min
[Tune-x] 10: n.trees=23; interaction.depth=4; shrinkage=0.194; n.minobsinnode=17
[Tune-y] 10: rmse.test.rmse=0.0621; time: 0.0 min
[Tune-x] 11: n.trees=54; interaction.depth=2; shrinkage=0.512; n.minobsinnode=24
[Tune-y] 11: rmse.test.rmse=0.0777; time: 0.0 min
[Tune-x] 12: n.trees=970; interaction.depth=4; shrinkage=0.386; n.minobsinnode=7
[Tune-y] 12: rmse.test.rmse=0.0418; time: 0.0 min
[Tune-x] 13: n.trees=24; interaction.depth=1; shrinkage=0.593; n.minobsinnode=22
[Tune-y] 13: rmse.test.rmse=0.129; time: 0.0 min
[Tune-x] 14: n.trees=475; interaction.depth=6; shrinkage=0.00124; n.minobsinnode=13
[Tune-y] 14: rmse.test.rmse=0.373; time: 0.0 min
[Tune-x] 15: n.trees=57; interaction.depth=6; shrinkage=0.0089; n.minobsinnode=11
[Tune-y] 15: rmse.test.rmse= 0.4; time: 0.0 min
[Tune-x] 16: n.trees=96; interaction.depth=8; shrinkage=0.565; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0544; time: 0.0 min
[Tune-x] 17: n.trees=44; interaction.depth=10; shrinkage=0.249; n.minobsinnode=7
[Tune-y] 17: rmse.test.rmse=0.0368; time: 0.0 min
[Tune-x] 18: n.trees=64; interaction.depth=2; shrinkage=0.427; n.minobsinnode=5
[Tune-y] 18: rmse.test.rmse=0.056; time: 0.0 min
[Tune-x] 19: n.trees=491; interaction.depth=6; shrinkage=0.301; n.minobsinnode=7
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.0 min
[Tune-x] 20: n.trees=239; interaction.depth=8; shrinkage=0.382; n.minobsinnode=11
[Tune-y] 20: rmse.test.rmse=0.0466; time: 0.0 min
[Tune] Result: n.trees=660; interaction.depth=4; shrinkage=0.126; n.minobsinnode=5 : rmse.test.rmse=0.0251
[1] "Fri Feb 09 16:50:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 16:50:16 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.949; lambda=0.0584
[Tune-y] 1: rmse.test.rmse=0.0678; time: 0.0 min
[Tune-x] 2: alpha=0.604; lambda=0.00999
[Tune-y] 2: rmse.test.rmse=0.0226; time: 0.0 min
[Tune-x] 3: alpha=0.575; lambda=0.0092
[Tune-y] 3: rmse.test.rmse=0.022; time: 0.0 min
[Tune-x] 4: alpha=0.069; lambda=0.0374
[Tune-y] 4: rmse.test.rmse=0.0256; time: 0.0 min
[Tune-x] 5: alpha=0.93; lambda=0.116
[Tune-y] 5: rmse.test.rmse=0.122; time: 0.0 min
[Tune-x] 6: alpha=0.898; lambda=2.36
[Tune-y] 6: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 7: alpha=0.628; lambda=0.0178
[Tune-y] 7: rmse.test.rmse=0.0278; time: 0.0 min
[Tune-x] 8: alpha=0.39; lambda=0.0288
[Tune-y] 8: rmse.test.rmse=0.0315; time: 0.0 min
[Tune-x] 9: alpha=0.91; lambda=0.0298
[Tune-y] 9: rmse.test.rmse=0.0428; time: 0.0 min
[Tune-x] 10: alpha=0.209; lambda=0.00143
[Tune-y] 10: rmse.test.rmse=0.0149; time: 0.0 min
[Tune-x] 11: alpha=0.892; lambda=2.02
[Tune-y] 11: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 12: alpha=0.549; lambda=1.8
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: alpha=0.204; lambda=0.184
[Tune-y] 13: rmse.test.rmse=0.0844; time: 0.0 min
[Tune-x] 14: alpha=0.636; lambda=0.0565
[Tune-y] 14: rmse.test.rmse=0.0555; time: 0.0 min
[Tune-x] 15: alpha=0.506; lambda=0.332
[Tune-y] 15: rmse.test.rmse=0.215; time: 0.0 min
[Tune-x] 16: alpha=0.42; lambda=0.0316
[Tune-y] 16: rmse.test.rmse=0.0334; time: 0.0 min
[Tune-x] 17: alpha=0.202; lambda=1.8
[Tune-y] 17: rmse.test.rmse=0.483; time: 0.0 min
[Tune-x] 18: alpha=0.681; lambda=0.00106
[Tune-y] 18: rmse.test.rmse=0.0143; time: 0.0 min
[Tune-x] 19: alpha=0.179; lambda=0.029
[Tune-y] 19: rmse.test.rmse=0.0272; time: 0.0 min
[Tune-x] 20: alpha=0.322; lambda=0.197
[Tune-y] 20: rmse.test.rmse=0.106; time: 0.0 min
[Tune] Result: alpha=0.681; lambda=0.00106 : rmse.test.rmse=0.0143
[1] "Fri Feb 09 16:50:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |=================================================                     |  70%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:50:28 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==================                                                    |  26%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:50:33 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:50:36 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 16:50:40 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  9.898 11.884 1.92 5.338 27.442 26.858 10.498 2.266 10.14 
  - best initial criterion value(s) :  3328.615 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -3328.6  |proj g|=       7.8814
At iterate     1  f =      -3340.1  |proj g|=        6.7544
At iterate     2  f =      -3398.2  |proj g|=        7.8537
At iterate     3  f =      -3435.6  |proj g|=        4.7662
At iterate     4  f =      -3460.8  |proj g|=        4.4083
At iterate     5  f =      -3467.3  |proj g|=        4.1755
At iterate     6  f =      -3470.3  |proj g|=        4.1391
At iterate     7  f =      -3481.4  |proj g|=        4.2088
At iterate     8  f =      -3486.9  |proj g|=        4.2328
At iterate     9  f =      -3488.9  |proj g|=        4.1909
At iterate    10  f =      -3494.8  |proj g|=        4.1931
At iterate    11  f =      -3496.4  |proj g|=        4.1637
At iterate    12  f =      -3498.1  |proj g|=        4.0611
At iterate    13  f =      -3500.2  |proj g|=        3.9477
At iterate    14  f =      -3504.3  |proj g|=        2.1749
At iterate    15  f =      -3508.6  |proj g|=        2.8156
At iterate    16  f =      -3509.3  |proj g|=        3.1922
At iterate    17  f =      -3509.4  |proj g|=        3.1865
At iterate    18  f =      -3509.9  |proj g|=        4.3269
At iterate    19  f =        -3510  |proj g|=        4.0227
At iterate    20  f =      -3510.2  |proj g|=        2.6501
At iterate    21  f =      -3511.7  |proj g|=        2.3562
At iterate    22  f =      -3512.6  |proj g|=        3.2352
At iterate    23  f =      -3513.1  |proj g|=        3.0041
At iterate    24  f =      -3514.1  |proj g|=        3.0241
At iterate    25  f =      -3521.7  |proj g|=        7.0185
At iterate    26  f =      -3525.8  |proj g|=        3.9145
At iterate    27  f =      -3527.5  |proj g|=        3.4709
At iterate    28  f =      -3528.2  |proj g|=        3.4542
At iterate    29  f =        -3529  |proj g|=        3.4147
At iterate    30  f =      -3529.2  |proj g|=        3.4077
At iterate    31  f =      -3531.5  |proj g|=        3.2684
At iterate    32  f =      -3538.3  |proj g|=         7.811
At iterate    33  f =      -3541.1  |proj g|=        2.8562
At iterate    34  f =      -3541.4  |proj g|=         3.201
At iterate    35  f =      -3541.5  |proj g|=        3.5902
At iterate    36  f =      -3541.7  |proj g|=        3.5338
At iterate    37  f =      -3542.2  |proj g|=        2.9428
At iterate    38  f =      -3542.6  |proj g|=        2.1265
At iterate    39  f =      -3542.7  |proj g|=        1.5167
At iterate    40  f =      -3542.9  |proj g|=        1.2604
At iterate    41  f =      -3542.9  |proj g|=        1.1795
At iterate    42  f =      -3542.9  |proj g|=        1.4852
At iterate    43  f =        -3543  |proj g|=        1.4883
At iterate    44  f =        -3543  |proj g|=        1.4913
At iterate    45  f =      -3543.1  |proj g|=        1.4924
At iterate    46  f =      -3543.3  |proj g|=        2.2509
At iterate    47  f =      -3543.5  |proj g|=        2.4122
At iterate    48  f =      -3543.6  |proj g|=        2.4062
At iterate    49  f =      -3543.8  |proj g|=        2.2959
At iterate    50  f =        -3544  |proj g|=        2.7622
At iterate    51  f =      -3544.2  |proj g|=        1.5037
At iterate    52  f =      -3544.2  |proj g|=        1.5299
At iterate    53  f =      -3544.3  |proj g|=        1.5253
At iterate    54  f =      -3544.3  |proj g|=        1.5126
At iterate    55  f =      -3544.3  |proj g|=       0.80832
At iterate    56  f =      -3544.3  |proj g|=        1.0986
At iterate    57  f =      -3544.4  |proj g|=       0.28595
At iterate    58  f =      -3544.4  |proj g|=       0.67442
At iterate    59  f =      -3544.4  |proj g|=       0.78913
At iterate    60  f =      -3544.4  |proj g|=        1.3157
At iterate    61  f =      -3544.4  |proj g|=       0.77041
At iterate    62  f =      -3544.4  |proj g|=         1.488
At iterate    63  f =      -3544.5  |proj g|=        1.4839
At iterate    64  f =      -3544.5  |proj g|=       0.92549
At iterate    65  f =      -3544.5  |proj g|=        1.3703
At iterate    66  f =      -3544.5  |proj g|=       0.78486
At iterate    67  f =      -3544.5  |proj g|=       0.45217
At iterate    68  f =      -3544.5  |proj g|=       0.42495
At iterate    69  f =      -3544.6  |proj g|=       0.33318
At iterate    70  f =      -3544.6  |proj g|=        1.4839
At iterate    71  f =      -3544.6  |proj g|=        1.1859
At iterate    72  f =      -3544.6  |proj g|=       0.55789
At iterate    73  f =      -3544.6  |proj g|=        0.6882
At iterate    74  f =      -3544.6  |proj g|=       0.69684
At iterate    75  f =      -3544.6  |proj g|=       0.56766
At iterate    76  f =      -3544.6  |proj g|=         1.482
At iterate    77  f =      -3544.7  |proj g|=        1.1465
At iterate    78  f =      -3544.7  |proj g|=       0.44315
At iterate    79  f =      -3544.7  |proj g|=       0.58944
At iterate    80  f =      -3544.7  |proj g|=       0.23528
At iterate    81  f =      -3544.7  |proj g|=       0.23733
At iterate    82  f =      -3544.7  |proj g|=       0.59329
At iterate    83  f =      -3544.7  |proj g|=       0.81767
At iterate    84  f =      -3544.7  |proj g|=       0.85629
At iterate    85  f =      -3544.7  |proj g|=       0.93306
At iterate    86  f =      -3544.8  |proj g|=       0.63527
At iterate    87  f =      -3544.8  |proj g|=       0.70299
At iterate    88  f =      -3544.8  |proj g|=        1.4776
At iterate    89  f =      -3544.8  |proj g|=       0.75748
At iterate    90  f =      -3544.8  |proj g|=        0.6528
At iterate    91  f =      -3544.8  |proj g|=       0.55715
At iterate    92  f =      -3544.8  |proj g|=       0.31567
At iterate    93  f =      -3544.8  |proj g|=       0.19976
At iterate    94  f =      -3544.8  |proj g|=        1.4375
At iterate    95  f =      -3544.8  |proj g|=        1.0246
At iterate    96  f =      -3544.8  |proj g|=       0.48397
At iterate    97  f =      -3544.8  |proj g|=       0.52294
At iterate    98  f =      -3544.8  |proj g|=       0.52864
At iterate    99  f =      -3544.9  |proj g|=       0.44264
At iterate   100  f =      -3544.9  |proj g|=        1.4798
At iterate   101  f =      -3544.9  |proj g|=        1.4476
final  value -3544.880687 
stopped after 101 iterations
[1] "Fri Feb 09 16:55:10 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 81.6381, its = 11
i = 2 (of 248), d = 50.4835, its = 11
i = 3 (of 248), d = 53.5942, its = 11
i = 4 (of 248), d = 122.859, its = 12
i = 5 (of 248), d = 85.1172, its = 12
i = 6 (of 248), d = 40.8853, its = 11
i = 7 (of 248), d = 88.4914, its = 11
i = 8 (of 248), d = 100.562, its = 11
i = 9 (of 248), d = 28.7216, its = 9
i = 10 (of 248), d = 107.053, its = 11
i = 11 (of 248), d = 37.9501, its = 10
i = 12 (of 248), d = 42.2289, its = 10
i = 13 (of 248), d = 63.3826, its = 11
i = 14 (of 248), d = 211.263, its = 12
i = 15 (of 248), d = 49.6758, its = 10
i = 16 (of 248), d = 60.4139, its = 11
i = 17 (of 248), d = 43.0261, its = 10
i = 18 (of 248), d = 76.0365, its = 11
i = 19 (of 248), d = 20.3274, its = 9
i = 20 (of 248), d = 67.6309, its = 11
i = 21 (of 248), d = 95.7672, its = 11
i = 22 (of 248), d = 29.5729, its = 10
i = 23 (of 248), d = 74.2623, its = 12
i = 24 (of 248), d = 153.261, its = 12
i = 25 (of 248), d = 107.241, its = 12
i = 26 (of 248), d = 126.005, its = 11
i = 27 (of 248), d = 61.6181, its = 10
i = 28 (of 248), d = 31.829, its = 10
i = 29 (of 248), d = 153.888, its = 13
i = 30 (of 248), d = 78.6486, its = 11
i = 31 (of 248), d = 84.493, its = 11
i = 32 (of 248), d = 70.9572, its = 11
i = 33 (of 248), d = 124.829, its = 12
i = 34 (of 248), d = 27.2667, its = 10
i = 35 (of 248), d = 126.943, its = 12
i = 36 (of 248), d = 123.006, its = 11
i = 37 (of 248), d = 86.5268, its = 11
i = 38 (of 248), d = 93.8513, its = 11
i = 39 (of 248), d = 34.6424, its = 10
i = 40 (of 248), d = 32.4118, its = 9
i = 41 (of 248), d = 59.4162, its = 11
i = 42 (of 248), d = 22.2584, its = 9
i = 43 (of 248), d = 57.314, its = 10
i = 44 (of 248), d = 44.9622, its = 10
i = 45 (of 248), d = 89.8757, its = 11
i = 46 (of 248), d = 49.8736, its = 10
i = 47 (of 248), d = 38.2174, its = 10
i = 48 (of 248), d = 19.8057, its = 9
i = 49 (of 248), d = 49.6311, its = 10
i = 50 (of 248), d = 33.8076, its = 10
i = 51 (of 248), d = 156.372, its = 12
i = 52 (of 248), d = 57.5233, its = 11
i = 53 (of 248), d = 121.299, its = 11
i = 54 (of 248), d = 46.6802, its = 10
i = 55 (of 248), d = 120.731, its = 12
i = 56 (of 248), d = 21.4573, its = 9
i = 57 (of 248), d = 137.217, its = 13
i = 58 (of 248), d = 90.9739, its = 12
i = 59 (of 248), d = 68.8682, its = 11
i = 60 (of 248), d = 109.753, its = 12
i = 61 (of 248), d = 143.46, its = 13
i = 62 (of 248), d = 94.4793, its = 11
i = 63 (of 248), d = 113.162, its = 11
i = 64 (of 248), d = 83.9198, its = 11
i = 65 (of 248), d = 26.2431, its = 9
i = 66 (of 248), d = 125.068, its = 11
i = 67 (of 248), d = 19.5886, its = 9
i = 68 (of 248), d = 68.4551, its = 11
i = 69 (of 248), d = 32.363, its = 9
i = 70 (of 248), d = 17.375, its = 9
i = 71 (of 248), d = 156.267, its = 13
i = 72 (of 248), d = 96.4112, its = 11
i = 73 (of 248), d = 41.77, its = 10
i = 74 (of 248), d = 85.3034, its = 11
i = 75 (of 248), d = 42.2964, its = 10
i = 76 (of 248), d = 115.922, its = 12
i = 77 (of 248), d = 45.0567, its = 10
i = 78 (of 248), d = 74.683, its = 11
i = 79 (of 248), d = 69.6093, its = 11
i = 80 (of 248), d = 83.3347, its = 11
i = 81 (of 248), d = 35.6791, its = 10
i = 82 (of 248), d = 32.7981, its = 10
i = 83 (of 248), d = 73.8158, its = 11
i = 84 (of 248), d = 96.1281, its = 11
i = 85 (of 248), d = 71.8901, its = 11
i = 86 (of 248), d = 28.1677, its = 9
i = 87 (of 248), d = 55.9643, its = 11
i = 88 (of 248), d = 48.9644, its = 11
i = 89 (of 248), d = 53.4303, its = 10
i = 90 (of 248), d = 92.1895, its = 11
i = 91 (of 248), d = 114.369, its = 12
i = 92 (of 248), d = 54.2139, its = 11
i = 93 (of 248), d = 50.4993, its = 10
i = 94 (of 248), d = 108.794, its = 11
i = 95 (of 248), d = 74.9373, its = 11
i = 96 (of 248), d = 17.3664, its = 9
i = 97 (of 248), d = 53.5812, its = 10
i = 98 (of 248), d = 34.5303, its = 10
i = 99 (of 248), d = 41.3775, its = 11
i = 100 (of 248), d = 25.2923, its = 9
i = 101 (of 248), d = 21.1352, its = 9
i = 102 (of 248), d = 45.4291, its = 10
i = 103 (of 248), d = 19.1583, its = 9
i = 104 (of 248), d = 43.337, its = 11
i = 105 (of 248), d = 79.7952, its = 11
i = 106 (of 248), d = 75.669, its = 11
i = 107 (of 248), d = 105.899, its = 11
i = 108 (of 248), d = 40.2151, its = 10
i = 109 (of 248), d = 124.589, its = 12
i = 110 (of 248), d = 26.967, its = 9
i = 111 (of 248), d = 24.8384, its = 9
i = 112 (of 248), d = 144.982, its = 12
i = 113 (of 248), d = 64.592, its = 11
i = 114 (of 248), d = 34.2722, its = 10
i = 115 (of 248), d = 20.8262, its = 10
i = 116 (of 248), d = 95.4175, its = 12
i = 117 (of 248), d = 42.7777, its = 10
i = 118 (of 248), d = 67.4093, its = 10
i = 119 (of 248), d = 160.209, its = 13
i = 120 (of 248), d = 70.5525, its = 11
i = 121 (of 248), d = 46.5869, its = 11
i = 122 (of 248), d = 130.863, its = 11
i = 123 (of 248), d = 67.4338, its = 11
i = 124 (of 248), d = 89.1165, its = 11
i = 125 (of 248), d = 158.681, its = 13
i = 126 (of 248), d = 54.6375, its = 11
i = 127 (of 248), d = 39.9938, its = 10
i = 128 (of 248), d = 25.499, its = 10
i = 129 (of 248), d = 84.9714, its = 11
i = 130 (of 248), d = 181.942, its = 12
i = 131 (of 248), d = 103.082, its = 11
i = 132 (of 248), d = 69.6513, its = 11
i = 133 (of 248), d = 118.543, its = 11
i = 134 (of 248), d = 23.4147, its = 9
i = 135 (of 248), d = 51.1434, its = 10
i = 136 (of 248), d = 119.697, its = 11
i = 137 (of 248), d = 107.513, its = 11
i = 138 (of 248), d = 71.958, its = 11
i = 139 (of 248), d = 70.734, its = 11
i = 140 (of 248), d = 118.219, its = 11
i = 141 (of 248), d = 109.166, its = 12
i = 142 (of 248), d = 131.509, its = 11
i = 143 (of 248), d = 152.703, its = 13
i = 144 (of 248), d = 111.201, its = 11
i = 145 (of 248), d = 35.9442, its = 10
i = 146 (of 248), d = 73.7117, its = 11
i = 147 (of 248), d = 67.7912, its = 11
i = 148 (of 248), d = 144.357, its = 12
i = 149 (of 248), d = 67.5248, its = 11
i = 150 (of 248), d = 119.162, its = 13
i = 151 (of 248), d = 31.7838, its = 9
i = 152 (of 248), d = 119.782, its = 11
i = 153 (of 248), d = 100.789, its = 11
i = 154 (of 248), d = 53.3399, its = 11
i = 155 (of 248), d = 92.1972, its = 12
i = 156 (of 248), d = 96.9205, its = 11
i = 157 (of 248), d = 62.7226, its = 11
i = 158 (of 248), d = 96.3754, its = 11
i = 159 (of 248), d = 17.0282, its = 9
i = 160 (of 248), d = 50.6264, its = 10
i = 161 (of 248), d = 111.173, its = 12
i = 162 (of 248), d = 40.6053, its = 10
i = 163 (of 248), d = 137.811, its = 12
i = 164 (of 248), d = 95.1407, its = 12
i = 165 (of 248), d = 64.3601, its = 11
i = 166 (of 248), d = 39.709, its = 10
i = 167 (of 248), d = 68.5079, its = 11
i = 168 (of 248), d = 41.163, its = 10
i = 169 (of 248), d = 28.7441, its = 10
i = 170 (of 248), d = 137.169, its = 12
i = 171 (of 248), d = 65.1753, its = 11
i = 172 (of 248), d = 110.921, its = 13
i = 173 (of 248), d = 247.727, its = 15
i = 174 (of 248), d = 44.7524, its = 10
i = 175 (of 248), d = 141.238, its = 12
i = 176 (of 248), d = 141.581, its = 12
i = 177 (of 248), d = 58.5192, its = 11
i = 178 (of 248), d = 32.7251, its = 9
i = 179 (of 248), d = 45.1026, its = 11
i = 180 (of 248), d = 88.0771, its = 11
i = 181 (of 248), d = 70.4445, its = 11
i = 182 (of 248), d = 41.9674, its = 10
i = 183 (of 248), d = 167.955, its = 13
i = 184 (of 248), d = 94.3134, its = 12
i = 185 (of 248), d = 62.4957, its = 11
i = 186 (of 248), d = 36.2491, its = 10
i = 187 (of 248), d = 122.821, its = 12
i = 188 (of 248), d = 92.4164, its = 12
i = 189 (of 248), d = 49.2627, its = 11
i = 190 (of 248), d = 130.051, its = 12
i = 191 (of 248), d = 22.8844, its = 9
i = 192 (of 248), d = 35.5348, its = 10
i = 193 (of 248), d = 36.829, its = 10
i = 194 (of 248), d = 92.8097, its = 12
i = 195 (of 248), d = 21.9829, its = 9
i = 196 (of 248), d = 37.3593, its = 11
i = 197 (of 248), d = 32.4234, its = 10
i = 198 (of 248), d = 97.5475, its = 11
i = 199 (of 248), d = 63.2311, its = 11
i = 200 (of 248), d = 65.9531, its = 11
i = 201 (of 248), d = 21.3636, its = 9
i = 202 (of 248), d = 99.1268, its = 12
i = 203 (of 248), d = 63.3029, its = 10
i = 204 (of 248), d = 25.7021, its = 10
i = 205 (of 248), d = 84.1015, its = 11
i = 206 (of 248), d = 25.0211, its = 9
i = 207 (of 248), d = 111.012, its = 12
i = 208 (of 248), d = 41.877, its = 10
i = 209 (of 248), d = 136.782, its = 12
i = 210 (of 248), d = 111.164, its = 12
i = 211 (of 248), d = 23.004, its = 9
i = 212 (of 248), d = 44.3854, its = 10
i = 213 (of 248), d = 106.141, its = 11
i = 214 (of 248), d = 45.9094, its = 10
i = 215 (of 248), d = 61.8428, its = 11
i = 216 (of 248), d = 35.7859, its = 10
i = 217 (of 248), d = 28.4618, its = 9
i = 218 (of 248), d = 79.8788, its = 12
i = 219 (of 248), d = 16.6131, its = 8
i = 220 (of 248), d = 69.3035, its = 11
i = 221 (of 248), d = 55.3285, its = 10
i = 222 (of 248), d = 118.965, its = 11
i = 223 (of 248), d = 102.234, its = 11
i = 224 (of 248), d = 81.7287, its = 11
i = 225 (of 248), d = 94.0626, its = 11
i = 226 (of 248), d = 41.4353, its = 10
i = 227 (of 248), d = 138.562, its = 11
i = 228 (of 248), d = 103.526, its = 12
i = 229 (of 248), d = 127.687, its = 11
i = 230 (of 248), d = 18.2559, its = 9
i = 231 (of 248), d = 87.0673, its = 11
i = 232 (of 248), d = 29.3141, its = 10
i = 233 (of 248), d = 128.234, its = 11
i = 234 (of 248), d = 70.6027, its = 11
i = 235 (of 248), d = 19.0193, its = 9
i = 236 (of 248), d = 59.1825, its = 11
i = 237 (of 248), d = 36.401, its = 10
i = 238 (of 248), d = 56.2294, its = 11
i = 239 (of 248), d = 50.2586, its = 11
i = 240 (of 248), d = 92.1659, its = 11
i = 241 (of 248), d = 40.8583, its = 10
i = 242 (of 248), d = 68.093, its = 11
i = 243 (of 248), d = 69.1133, its = 11
i = 244 (of 248), d = 77.5975, its = 11
i = 245 (of 248), d = 38.4981, its = 10
i = 246 (of 248), d = 188.831, its = 12
i = 247 (of 248), d = 57.716, its = 11
i = 248 (of 248), d = 84.6372, its = 11
[1] "Fri Feb 09 16:56:13 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L1SVR no default is available.
[1] "Fri Feb 09 16:56:16 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.LiblineaRL2L2SVR no default is available.
[1] "Fri Feb 09 16:56:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.lm no default is available.
[1] "Fri Feb 09 16:56:18 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mars no default is available.
[1] "Fri Feb 09 16:56:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.mob no default is available.
[1] "Fri Feb 09 16:56:27 2018"
[Tune] Started tuning learner regr.nnet for parameter set:
         Type len   Def  Constr Req Tunable Trafo
size  integer   -     3 1 to 20   -    TRUE     -
decay numeric   - 1e-05 -5 to 1   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: size=19; decay=0.0053
# weights:  210
initial  value 2543.484541 
iter  10 value 12.067619
iter  20 value 1.449677
iter  30 value 0.458507
iter  40 value 0.258877
iter  50 value 0.189027
iter  60 value 0.140282
iter  70 value 0.118401
iter  80 value 0.109604
iter  90 value 0.105634
iter 100 value 0.102051
final  value 0.102051 
stopped after 100 iterations
# weights:  210
initial  value 813.324229 
iter  10 value 6.713533
iter  20 value 1.393011
iter  30 value 0.530417
iter  40 value 0.295328
iter  50 value 0.209775
iter  60 value 0.157097
iter  70 value 0.126951
iter  80 value 0.118305
iter  90 value 0.112587
iter 100 value 0.109138
final  value 0.109138 
stopped after 100 iterations
# weights:  210
initial  value 553.331027 
iter  10 value 13.211015
iter  20 value 1.195954
iter  30 value 0.438350
iter  40 value 0.309172
iter  50 value 0.242162
iter  60 value 0.195939
iter  70 value 0.176528
iter  80 value 0.166150
iter  90 value 0.156925
iter 100 value 0.149163
final  value 0.149163 
stopped after 100 iterations
[Tune-y] 1: rmse.test.rmse=0.00463; time: 0.0 min
[Tune-x] 2: size=13; decay=0.000354
# weights:  144
initial  value 308.452632 
iter  10 value 9.296977
iter  20 value 1.673860
iter  30 value 0.222488
iter  40 value 0.060094
iter  50 value 0.029802
iter  60 value 0.022999
iter  70 value 0.018192
iter  80 value 0.016663
iter  90 value 0.015708
iter 100 value 0.014424
final  value 0.014424 
stopped after 100 iterations
# weights:  144
initial  value 6910.333001 
iter  10 value 36.852088
iter  20 value 5.655185
iter  30 value 0.480044
iter  40 value 0.139952
iter  50 value 0.072200
iter  60 value 0.045339
iter  70 value 0.036111
iter  80 value 0.028733
iter  90 value 0.025161
iter 100 value 0.021826
final  value 0.021826 
stopped after 100 iterations
# weights:  144
initial  value 1747.404566 
iter  10 value 81.933792
iter  20 value 20.771334
iter  30 value 3.260150
iter  40 value 0.572280
iter  50 value 0.264238
iter  60 value 0.175164
iter  70 value 0.139502
iter  80 value 0.120616
iter  90 value 0.102525
iter 100 value 0.089978
final  value 0.089978 
stopped after 100 iterations
[Tune-y] 2: rmse.test.rmse=0.00497; time: 0.0 min
[Tune-x] 3: size=12; decay=0.000312
# weights:  133
initial  value 640.123230 
iter  10 value 20.793671
iter  20 value 3.277362
iter  30 value 0.635231
iter  40 value 0.122760
iter  50 value 0.043086
iter  60 value 0.029573
iter  70 value 0.022930
iter  80 value 0.020675
iter  90 value 0.017852
iter 100 value 0.016722
final  value 0.016722 
stopped after 100 iterations
# weights:  133
initial  value 6722.932607 
iter  10 value 46.862906
iter  20 value 7.280622
iter  30 value 0.444243
iter  40 value 0.157630
iter  50 value 0.054426
iter  60 value 0.035143
iter  70 value 0.025247
iter  80 value 0.021686
iter  90 value 0.018049
iter 100 value 0.016688
final  value 0.016688 
stopped after 100 iterations
# weights:  133
initial  value 1886.239490 
iter  10 value 63.443629
iter  20 value 28.006690
iter  30 value 3.689073
iter  40 value 0.792477
iter  50 value 0.302340
iter  60 value 0.141450
iter  70 value 0.073584
iter  80 value 0.054391
iter  90 value 0.046808
iter 100 value 0.040990
final  value 0.040990 
stopped after 100 iterations
[Tune-y] 3: rmse.test.rmse=0.00309; time: 0.0 min
[Tune-x] 4: size=2; decay=0.00268
# weights:  23
initial  value 598.899214 
iter  10 value 25.194309
iter  20 value 1.730931
iter  30 value 0.413256
iter  40 value 0.201173
iter  50 value 0.165744
iter  60 value 0.142449
iter  70 value 0.109599
iter  80 value 0.088646
iter  90 value 0.080979
iter 100 value 0.080279
final  value 0.080279 
stopped after 100 iterations
# weights:  23
initial  value 830.918577 
iter  10 value 137.258589
iter  20 value 45.256428
iter  30 value 24.199743
iter  40 value 6.394196
iter  50 value 2.856434
iter  60 value 0.972731
iter  70 value 0.383890
iter  80 value 0.257693
iter  90 value 0.225113
iter 100 value 0.218960
final  value 0.218960 
stopped after 100 iterations
# weights:  23
initial  value 1865.663082 
iter  10 value 57.332816
iter  20 value 3.960219
iter  30 value 0.700403
iter  40 value 0.399667
iter  50 value 0.286195
iter  60 value 0.220176
iter  70 value 0.175747
iter  80 value 0.152174
iter  90 value 0.139471
iter 100 value 0.136224
final  value 0.136224 
stopped after 100 iterations
[Tune-y] 4: rmse.test.rmse=0.00751; time: 0.0 min
[Tune-x] 5: size=19; decay=0.0153
# weights:  210
initial  value 293.403010 
iter  10 value 23.081515
iter  20 value 3.382767
iter  30 value 1.025342
iter  40 value 0.602894
iter  50 value 0.426807
iter  60 value 0.361710
iter  70 value 0.327529
iter  80 value 0.298512
iter  90 value 0.272365
iter 100 value 0.257786
final  value 0.257786 
stopped after 100 iterations
# weights:  210
initial  value 2086.177955 
iter  10 value 16.645414
iter  20 value 4.658501
iter  30 value 1.487783
iter  40 value 0.764106
iter  50 value 0.496057
iter  60 value 0.370216
iter  70 value 0.333305
iter  80 value 0.308826
iter  90 value 0.292443
iter 100 value 0.279662
final  value 0.279662 
stopped after 100 iterations
# weights:  210
initial  value 672.883463 
iter  10 value 11.027668
iter  20 value 2.648454
iter  30 value 0.845734
iter  40 value 0.543418
iter  50 value 0.371458
iter  60 value 0.325069
iter  70 value 0.302650
iter  80 value 0.280175
iter  90 value 0.262494
iter 100 value 0.252190
final  value 0.252190 
stopped after 100 iterations
[Tune-y] 5: rmse.test.rmse=0.00686; time: 0.0 min
[Tune-x] 6: size=18; decay=1.53
# weights:  199
initial  value 5233.867158 
iter  10 value 51.945830
iter  20 value 18.306633
iter  30 value 13.758725
iter  40 value 12.160336
iter  50 value 10.815236
iter  60 value 9.841766
iter  70 value 9.462043
iter  80 value 9.142112
iter  90 value 8.975765
iter 100 value 8.937460
final  value 8.937460 
stopped after 100 iterations
# weights:  199
initial  value 818.326742 
iter  10 value 59.354831
iter  20 value 23.911641
iter  30 value 17.925420
iter  40 value 14.644672
iter  50 value 11.787656
iter  60 value 10.470600
iter  70 value 9.922330
iter  80 value 9.672140
iter  90 value 9.510570
iter 100 value 9.370019
final  value 9.370019 
stopped after 100 iterations
# weights:  199
initial  value 2108.146636 
iter  10 value 206.948506
iter  20 value 155.752447
iter  30 value 134.470903
iter  40 value 100.619564
iter  50 value 54.288637
iter  60 value 27.354347
iter  70 value 20.780023
iter  80 value 14.455907
iter  90 value 12.432157
iter 100 value 11.036605
final  value 11.036605 
stopped after 100 iterations
[Tune-y] 6: rmse.test.rmse=0.0663; time: 0.0 min
[Tune-x] 7: size=13; decay=0.000855
# weights:  144
initial  value 1872.350783 
iter  10 value 20.589154
iter  20 value 1.352590
iter  30 value 0.471146
iter  40 value 0.355103
iter  50 value 0.317765
iter  60 value 0.300779
iter  70 value 0.287336
iter  80 value 0.274283
iter  90 value 0.264631
iter 100 value 0.244282
final  value 0.244282 
stopped after 100 iterations
# weights:  144
initial  value 1483.905007 
iter  10 value 55.705682
iter  20 value 13.913587
iter  30 value 2.425611
iter  40 value 0.783706
iter  50 value 0.407474
iter  60 value 0.329849
iter  70 value 0.272490
iter  80 value 0.237060
iter  90 value 0.205862
iter 100 value 0.173622
final  value 0.173622 
stopped after 100 iterations
# weights:  144
initial  value 2142.134178 
iter  10 value 21.271809
iter  20 value 3.420112
iter  30 value 0.633158
iter  40 value 0.135450
iter  50 value 0.069217
iter  60 value 0.055148
iter  70 value 0.046325
iter  80 value 0.040256
iter  90 value 0.036037
iter 100 value 0.033314
final  value 0.033314 
stopped after 100 iterations
[Tune-y] 7: rmse.test.rmse=0.00734; time: 0.0 min
[Tune-x] 8: size=8; decay=0.00179
# weights:  89
initial  value 2998.010327 
iter  10 value 41.102744
iter  20 value 14.372036
iter  30 value 2.811739
iter  40 value 0.581095
iter  50 value 0.320959
iter  60 value 0.188371
iter  70 value 0.148823
iter  80 value 0.125617
iter  90 value 0.101837
iter 100 value 0.093969
final  value 0.093969 
stopped after 100 iterations
# weights:  89
initial  value 1397.102741 
iter  10 value 66.357744
iter  20 value 9.551179
iter  30 value 1.510497
iter  40 value 0.355115
iter  50 value 0.187488
iter  60 value 0.122849
iter  70 value 0.085334
iter  80 value 0.070674
iter  90 value 0.065933
iter 100 value 0.061935
final  value 0.061935 
stopped after 100 iterations
# weights:  89
initial  value 3045.598422 
iter  10 value 18.282584
iter  20 value 3.611159
iter  30 value 0.628419
iter  40 value 0.171803
iter  50 value 0.109853
iter  60 value 0.088855
iter  70 value 0.068648
iter  80 value 0.060009
iter  90 value 0.053994
iter 100 value 0.049491
final  value 0.049491 
stopped after 100 iterations
[Tune-y] 8: rmse.test.rmse=0.00387; time: 0.0 min
[Tune-x] 9: size=19; decay=0.00189
# weights:  210
initial  value 2443.200252 
iter  10 value 56.720549
iter  20 value 4.522461
iter  30 value 0.873640
iter  40 value 0.547720
iter  50 value 0.440033
iter  60 value 0.373587
iter  70 value 0.301960
iter  80 value 0.244984
iter  90 value 0.218735
iter 100 value 0.185924
final  value 0.185924 
stopped after 100 iterations
# weights:  210
initial  value 3118.848635 
iter  10 value 25.551786
iter  20 value 2.784568
iter  30 value 0.519225
iter  40 value 0.188712
iter  50 value 0.140307
iter  60 value 0.108623
iter  70 value 0.086591
iter  80 value 0.072857
iter  90 value 0.068458
iter 100 value 0.063999
final  value 0.063999 
stopped after 100 iterations
# weights:  210
initial  value 310.687596 
iter  10 value 14.670294
iter  20 value 2.939559
iter  30 value 0.423727
iter  40 value 0.160553
iter  50 value 0.101801
iter  60 value 0.082294
iter  70 value 0.064415
iter  80 value 0.057339
iter  90 value 0.051564
iter 100 value 0.047403
final  value 0.047403 
stopped after 100 iterations
[Tune-y] 9: rmse.test.rmse=0.00466; time: 0.0 min
[Tune-x] 10: size=5; decay=1.79e-05
# weights:  56
initial  value 3137.631093 
iter  10 value 29.361503
iter  20 value 4.223758
iter  30 value 0.366907
iter  40 value 0.076931
iter  50 value 0.053183
iter  60 value 0.037220
iter  70 value 0.028120
iter  80 value 0.019975
iter  90 value 0.014731
iter 100 value 0.012252
final  value 0.012252 
stopped after 100 iterations
# weights:  56
initial  value 2397.342722 
iter  10 value 61.333094
iter  20 value 9.919273
iter  30 value 2.056787
iter  40 value 0.572479
iter  50 value 0.183901
iter  60 value 0.082299
iter  70 value 0.047447
iter  80 value 0.028811
iter  90 value 0.022789
iter 100 value 0.021172
final  value 0.021172 
stopped after 100 iterations
# weights:  56
initial  value 2188.483715 
iter  10 value 30.048571
iter  20 value 2.726216
iter  30 value 0.583973
iter  40 value 0.168426
iter  50 value 0.063196
iter  60 value 0.041094
iter  70 value 0.026028
iter  80 value 0.017760
iter  90 value 0.013409
iter 100 value 0.008569
final  value 0.008569 
stopped after 100 iterations
[Tune-y] 10: rmse.test.rmse=0.00491; time: 0.0 min
[Tune-x] 11: size=18; decay=1.21
# weights:  199
initial  value 2856.050265 
iter  10 value 114.444159
iter  20 value 55.787513
iter  30 value 33.464858
iter  40 value 24.665426
iter  50 value 21.826794
iter  60 value 18.989135
iter  70 value 14.747343
iter  80 value 11.972069
iter  90 value 9.505293
iter 100 value 8.869169
final  value 8.869169 
stopped after 100 iterations
# weights:  199
initial  value 1774.976045 
iter  10 value 54.725727
iter  20 value 23.656840
iter  30 value 12.687424
iter  40 value 10.293064
iter  50 value 8.750562
iter  60 value 8.287927
iter  70 value 7.998771
iter  80 value 7.839674
iter  90 value 7.721450
iter 100 value 7.627767
final  value 7.627767 
stopped after 100 iterations
# weights:  199
initial  value 280.522469 
iter  10 value 42.994491
iter  20 value 16.708404
iter  30 value 12.206404
iter  40 value 10.210419
iter  50 value 9.233679
iter  60 value 8.538285
iter  70 value 8.122363
iter  80 value 7.730598
iter  90 value 7.572702
iter 100 value 7.499406
final  value 7.499406 
stopped after 100 iterations
[Tune-y] 11: rmse.test.rmse=0.062; time: 0.0 min
[Tune-x] 12: size=11; decay=1.02
# weights:  122
initial  value 637.027601 
iter  10 value 46.170123
iter  20 value 21.330540
iter  30 value 13.177239
iter  40 value 11.403893
iter  50 value 10.436990
iter  60 value 9.918260
iter  70 value 9.545151
iter  80 value 9.215486
iter  90 value 8.808993
iter 100 value 8.620048
final  value 8.620048 
stopped after 100 iterations
# weights:  122
initial  value 1343.075721 
iter  10 value 41.023207
iter  20 value 20.670698
iter  30 value 15.723159
iter  40 value 12.839412
iter  50 value 11.764107
iter  60 value 10.851768
iter  70 value 9.996229
iter  80 value 9.195876
iter  90 value 8.531951
iter 100 value 8.209945
final  value 8.209945 
stopped after 100 iterations
# weights:  122
initial  value 5329.328466 
iter  10 value 291.574058
iter  20 value 70.919911
iter  30 value 26.964556
iter  40 value 19.502556
iter  50 value 16.475684
iter  60 value 14.961146
iter  70 value 13.569156
iter  80 value 12.285466
iter  90 value 11.125163
iter 100 value 10.656649
final  value 10.656649 
stopped after 100 iterations
[Tune-y] 12: rmse.test.rmse=0.0643; time: 0.0 min
[Tune-x] 13: size=5; decay=0.0307
# weights:  56
initial  value 3149.302140 
iter  10 value 34.628464
iter  20 value 10.068225
iter  30 value 2.821098
iter  40 value 2.036383
iter  50 value 1.615466
iter  60 value 1.359294
iter  70 value 1.049959
iter  80 value 0.844009
iter  90 value 0.741874
iter 100 value 0.683613
final  value 0.683613 
stopped after 100 iterations
# weights:  56
initial  value 601.709563 
iter  10 value 95.397082
iter  20 value 18.439859
iter  30 value 12.265603
iter  40 value 8.824025
iter  50 value 4.800147
iter  60 value 2.950865
iter  70 value 1.637809
iter  80 value 1.094973
iter  90 value 0.869752
iter 100 value 0.709190
final  value 0.709190 
stopped after 100 iterations
# weights:  56
initial  value 1241.892560 
iter  10 value 58.315950
iter  20 value 19.483580
iter  30 value 6.031265
iter  40 value 3.374827
iter  50 value 2.333127
iter  60 value 1.831011
iter  70 value 1.358034
iter  80 value 1.031012
iter  90 value 0.918938
iter 100 value 0.876674
final  value 0.876674 
stopped after 100 iterations
[Tune-y] 13: rmse.test.rmse=0.0124; time: 0.0 min
[Tune-x] 14: size=13; decay=0.00503
# weights:  144
initial  value 369.979848 
iter  10 value 10.451916
iter  20 value 2.223334
iter  30 value 0.662553
iter  40 value 0.371023
iter  50 value 0.273608
iter  60 value 0.201645
iter  70 value 0.163375
iter  80 value 0.144422
iter  90 value 0.133567
iter 100 value 0.128020
final  value 0.128020 
stopped after 100 iterations
# weights:  144
initial  value 1350.820036 
iter  10 value 27.560982
iter  20 value 2.727714
iter  30 value 0.647322
iter  40 value 0.451568
iter  50 value 0.354496
iter  60 value 0.315140
iter  70 value 0.279887
iter  80 value 0.255064
iter  90 value 0.240329
iter 100 value 0.225107
final  value 0.225107 
stopped after 100 iterations
# weights:  144
initial  value 1807.350673 
iter  10 value 41.844261
iter  20 value 10.726076
iter  30 value 1.491593
iter  40 value 0.503768
iter  50 value 0.284786
iter  60 value 0.219878
iter  70 value 0.169013
iter  80 value 0.135866
iter  90 value 0.121085
iter 100 value 0.112582
final  value 0.112582 
stopped after 100 iterations
[Tune-y] 14: rmse.test.rmse=0.00556; time: 0.0 min
[Tune-x] 15: size=11; decay=0.076
# weights:  122
initial  value 1878.480939 
iter  10 value 79.959950
iter  20 value 31.803180
iter  30 value 9.645332
iter  40 value 3.418266
iter  50 value 1.992778
iter  60 value 1.585467
iter  70 value 1.366546
iter  80 value 1.179967
iter  90 value 1.083423
iter 100 value 1.027514
final  value 1.027514 
stopped after 100 iterations
# weights:  122
initial  value 1616.809533 
iter  10 value 27.065885
iter  20 value 5.080728
iter  30 value 2.588934
iter  40 value 2.064002
iter  50 value 1.829511
iter  60 value 1.671918
iter  70 value 1.441309
iter  80 value 1.292627
iter  90 value 1.186532
iter 100 value 1.132477
final  value 1.132477 
stopped after 100 iterations
# weights:  122
initial  value 188.029690 
iter  10 value 12.824519
iter  20 value 3.187587
iter  30 value 1.725936
iter  40 value 1.349429
iter  50 value 1.165037
iter  60 value 1.078348
iter  70 value 1.039255
iter  80 value 1.018260
iter  90 value 1.005968
iter 100 value 0.995528
final  value 0.995528 
stopped after 100 iterations
[Tune-y] 15: rmse.test.rmse=0.015; time: 0.0 min
[Tune-x] 16: size=9; decay=0.00207
# weights:  100
initial  value 2232.231629 
iter  10 value 65.416617
iter  20 value 17.479683
iter  30 value 3.181380
iter  40 value 0.672082
iter  50 value 0.450487
iter  60 value 0.326297
iter  70 value 0.262990
iter  80 value 0.225356
iter  90 value 0.212973
iter 100 value 0.201596
final  value 0.201596 
stopped after 100 iterations
# weights:  100
initial  value 1639.148156 
iter  10 value 17.790156
iter  20 value 5.636315
iter  30 value 0.843515
iter  40 value 0.158557
iter  50 value 0.106178
iter  60 value 0.084400
iter  70 value 0.072228
iter  80 value 0.063063
iter  90 value 0.059522
iter 100 value 0.057951
final  value 0.057951 
stopped after 100 iterations
# weights:  100
initial  value 269.569061 
iter  10 value 12.739542
iter  20 value 1.138631
iter  30 value 0.166514
iter  40 value 0.088689
iter  50 value 0.068531
iter  60 value 0.060090
iter  70 value 0.054517
iter  80 value 0.050716
iter  90 value 0.049306
iter 100 value 0.047647
final  value 0.047647 
stopped after 100 iterations
[Tune-y] 16: rmse.test.rmse=0.0043; time: 0.0 min
[Tune-x] 17: size=5; decay=1.02
# weights:  56
initial  value 1380.409632 
iter  10 value 72.739213
iter  20 value 25.693224
iter  30 value 18.624003
iter  40 value 15.484713
iter  50 value 13.483779
iter  60 value 12.441081
iter  70 value 11.894107
iter  80 value 11.505200
iter  90 value 11.229781
iter 100 value 11.057624
final  value 11.057624 
stopped after 100 iterations
# weights:  56
initial  value 1587.024979 
iter  10 value 72.153790
iter  20 value 40.939361
iter  30 value 17.921107
iter  40 value 14.002948
iter  50 value 13.225989
iter  60 value 12.983609
iter  70 value 12.909839
iter  80 value 12.882775
iter  90 value 12.869110
iter 100 value 12.866588
final  value 12.866588 
stopped after 100 iterations
# weights:  56
initial  value 2044.760813 
iter  10 value 83.701804
iter  20 value 22.375939
iter  30 value 15.830842
iter  40 value 13.455801
iter  50 value 12.159397
iter  60 value 11.642866
iter  70 value 11.412208
iter  80 value 11.160191
iter  90 value 10.919193
iter 100 value 10.824881
final  value 10.824881 
stopped after 100 iterations
[Tune-y] 17: rmse.test.rmse=0.0666; time: 0.0 min
[Tune-x] 18: size=14; decay=1.13e-05
# weights:  155
initial  value 248.681698 
iter  10 value 22.435792
iter  20 value 5.905874
iter  30 value 0.254590
iter  40 value 0.045682
iter  50 value 0.014007
iter  60 value 0.006663
iter  70 value 0.003859
iter  80 value 0.002894
iter  90 value 0.002152
iter 100 value 0.001739
final  value 0.001739 
stopped after 100 iterations
# weights:  155
initial  value 990.257493 
iter  10 value 15.415218
iter  20 value 1.501850
iter  30 value 0.246928
iter  40 value 0.041726
iter  50 value 0.010799
iter  60 value 0.004120
iter  70 value 0.001859
iter  80 value 0.001543
iter  90 value 0.001348
iter 100 value 0.001205
final  value 0.001205 
stopped after 100 iterations
# weights:  155
initial  value 4475.394005 
iter  10 value 36.762674
iter  20 value 3.167285
iter  30 value 0.688563
iter  40 value 0.232469
iter  50 value 0.082358
iter  60 value 0.039666
iter  70 value 0.015391
iter  80 value 0.007772
iter  90 value 0.005253
iter 100 value 0.004145
final  value 0.004145 
stopped after 100 iterations
[Tune-y] 18: rmse.test.rmse=0.00231; time: 0.0 min
[Tune-x] 19: size=4; decay=0.00181
# weights:  45
initial  value 1240.023062 
iter  10 value 76.248610
iter  20 value 19.927452
iter  30 value 2.612708
iter  40 value 0.811507
iter  50 value 0.442594
iter  60 value 0.254143
iter  70 value 0.142040
iter  80 value 0.087912
iter  90 value 0.073837
iter 100 value 0.070460
final  value 0.070460 
stopped after 100 iterations
# weights:  45
initial  value 1085.842345 
iter  10 value 113.024968
iter  20 value 9.554805
iter  30 value 1.036887
iter  40 value 0.620567
iter  50 value 0.568695
iter  60 value 0.495058
iter  70 value 0.452938
iter  80 value 0.423418
iter  90 value 0.401759
iter 100 value 0.393114
final  value 0.393114 
stopped after 100 iterations
# weights:  45
initial  value 895.075212 
iter  10 value 34.799518
iter  20 value 12.596052
iter  30 value 1.586428
iter  40 value 0.339488
iter  50 value 0.163879
iter  60 value 0.118840
iter  70 value 0.108335
iter  80 value 0.101610
iter  90 value 0.097646
iter 100 value 0.094721
final  value 0.094721 
stopped after 100 iterations
[Tune-y] 19: rmse.test.rmse=0.00789; time: 0.0 min
[Tune-x] 20: size=7; decay=0.0341
# weights:  78
initial  value 1229.781946 
iter  10 value 28.833807
iter  20 value 3.504737
iter  30 value 1.458393
iter  40 value 1.179932
iter  50 value 1.050020
iter  60 value 0.993691
iter  70 value 0.944567
iter  80 value 0.854683
iter  90 value 0.789433
iter 100 value 0.695964
final  value 0.695964 
stopped after 100 iterations
# weights:  78
initial  value 149.767690 
iter  10 value 30.118197
iter  20 value 6.171456
iter  30 value 1.673832
iter  40 value 0.935809
iter  50 value 0.785754
iter  60 value 0.715077
iter  70 value 0.646547
iter  80 value 0.607520
iter  90 value 0.585360
iter 100 value 0.568652
final  value 0.568652 
stopped after 100 iterations
# weights:  78
initial  value 3381.543146 
iter  10 value 55.109505
iter  20 value 19.192290
iter  30 value 4.443704
iter  40 value 2.116215
iter  50 value 1.375393
iter  60 value 0.972903
iter  70 value 0.851891
iter  80 value 0.743101
iter  90 value 0.657276
iter 100 value 0.618499
final  value 0.618499 
stopped after 100 iterations
[Tune-y] 20: rmse.test.rmse=0.0128; time: 0.0 min
[Tune] Result: size=14; decay=1.13e-05 : rmse.test.rmse=0.00231
# weights:  155
initial  value 5541.523714 
iter  10 value 30.923686
iter  20 value 2.873398
iter  30 value 0.297169
iter  40 value 0.076277
iter  50 value 0.023881
iter  60 value 0.010634
iter  70 value 0.005223
iter  80 value 0.003559
iter  90 value 0.002281
iter 100 value 0.001841
final  value 0.001841 
stopped after 100 iterations
[1] "Fri Feb 09 16:56:50 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.nodeHarvest no default is available.

 ... generating 1000 nodes ...
 total number of nodes in initial set                   : 1081
 total number of nodes after removal of identical nodes : 550 
 ... computing node means ... 
 ... computing node weights ...
 dimension of null space of I                           : 342
 number of selected nodes                               : 79 
[1] "Fri Feb 09 16:57:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.pcr no default is available.
[1] "Fri Feb 09 16:57:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.plsr no default is available.
In addition: Warning messages:
1: package '!penalized' is not available (for R version 3.4.3) 
2: package '!penalized' is not available (for R version 3.4.3) 
3: package '!penalized' is not available (for R version 3.4.3) 
[1] "Fri Feb 09 16:57:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.randomForestSRC no default is available.
[1] "Fri Feb 09 16:57:20 2018"
[Tune] Started tuning learner regr.ranger for parameter set:
                 Type len Def  Constr Req Tunable Trafo
mtry          integer   -   3  1 to 9   -    TRUE     -
min.node.size integer   -   5 1 to 10   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: mtry=9; min.node.size=5
[Tune-y] 1: rmse.test.rmse=0.0407; time: 0.1 min
[Tune-x] 2: mtry=6; min.node.size=3
[Tune-y] 2: rmse.test.rmse=0.0357; time: 0.1 min
[Tune-x] 3: mtry=6; min.node.size=3
[Tune-y] 3: rmse.test.rmse=0.0354; time: 0.1 min
[Tune-x] 4: mtry=1; min.node.size=5
[Tune-y] 4: rmse.test.rmse=0.0534; time: 0.0 min
[Tune-x] 5: mtry=9; min.node.size=6
[Tune-y] 5: rmse.test.rmse=0.0405; time: 0.1 min
[Tune-x] 6: mtry=9; min.node.size=9
[Tune-y] 6: rmse.test.rmse=0.0432; time: 0.1 min
[Tune-x] 7: mtry=6; min.node.size=4
[Tune-y] 7: rmse.test.rmse=0.0356; time: 0.1 min
[Tune-x] 8: mtry=4; min.node.size=4
[Tune-y] 8: rmse.test.rmse=0.0345; time: 0.1 min
[Tune-x] 9: mtry=9; min.node.size=4
[Tune-y] 9: rmse.test.rmse=0.0397; time: 0.1 min
[Tune-x] 10: mtry=2; min.node.size=1
[Tune-y] 10: rmse.test.rmse=0.0371; time: 0.1 min
[Tune-x] 11: mtry=9; min.node.size=9
[Tune-y] 11: rmse.test.rmse=0.0433; time: 0.1 min
[Tune-x] 12: mtry=5; min.node.size=9
[Tune-y] 12: rmse.test.rmse=0.0384; time: 0.1 min
[Tune-x] 13: mtry=2; min.node.size=6
[Tune-y] 13: rmse.test.rmse=0.0389; time: 0.0 min
[Tune-x] 14: mtry=6; min.node.size=5
[Tune-y] 14: rmse.test.rmse=0.0362; time: 0.1 min
[Tune-x] 15: mtry=5; min.node.size=7
[Tune-y] 15: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 16: mtry=4; min.node.size=4
[Tune-y] 16: rmse.test.rmse=0.0343; time: 0.1 min
[Tune-x] 17: mtry=2; min.node.size=9
[Tune-y] 17: rmse.test.rmse=0.0412; time: 0.0 min
[Tune-x] 18: mtry=7; min.node.size=1
[Tune-y] 18: rmse.test.rmse=0.0365; time: 0.1 min
[Tune-x] 19: mtry=2; min.node.size=4
[Tune-y] 19: rmse.test.rmse=0.0378; time: 0.1 min
[Tune-x] 20: mtry=3; min.node.size=6
[Tune-y] 20: rmse.test.rmse=0.0359; time: 0.1 min
[Tune] Result: mtry=4; min.node.size=4 : rmse.test.rmse=0.0343
[1] "Fri Feb 09 16:59:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rknn no default is available.
[1] "Fri Feb 09 16:59:20 2018"
[Tune] Started tuning learner regr.rpart for parameter set:
             Type len   Def   Constr Req Tunable Trafo
cp        numeric   - -6.64 -10 to 0   -    TRUE     Y
maxdepth  integer   -    30  3 to 30   -    TRUE     -
minbucket integer   -     7  5 to 50   -    TRUE     -
minsplit  integer   -    20  5 to 50   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: cp=0.703; maxdepth=15; minbucket=32; minsplit=16
[Tune-y] 1: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 2: cp=0.0527; maxdepth=9; minbucket=8; minsplit=23
[Tune-y] 2: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 3: cp=0.614; maxdepth=17; minbucket=46; minsplit=44
[Tune-y] 3: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 4: cp=0.0761; maxdepth=12; minbucket=22; minsplit=22
[Tune-y] 4: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 5: cp=0.535; maxdepth=13; minbucket=14; minsplit=6
[Tune-y] 5: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 6: cp=0.473; maxdepth=26; minbucket=30; minsplit=43
[Tune-y] 6: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 7: cp=0.00402; maxdepth=19; minbucket=34; minsplit=25
[Tune-y] 7: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 8: cp=0.0325; maxdepth=21; minbucket=24; minsplit=22
[Tune-y] 8: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 9: cp=0.00396; maxdepth=26; minbucket=36; minsplit=5
[Tune-y] 9: rmse.test.rmse=0.152; time: 0.0 min
[Tune-x] 10: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32
[Tune-y] 10: rmse.test.rmse=0.132; time: 0.0 min
[Tune-x] 11: cp=0.0123; maxdepth=8; minbucket=44; minsplit=46
[Tune-y] 11: rmse.test.rmse=0.178; time: 0.0 min
[Tune-x] 12: cp=0.955; maxdepth=13; minbucket=34; minsplit=10
[Tune-y] 12: rmse.test.rmse=0.637; time: 0.0 min
[Tune-x] 13: cp=0.00371; maxdepth=3; minbucket=50; minsplit=43
[Tune-y] 13: rmse.test.rmse=0.168; time: 0.0 min
[Tune-x] 14: cp=0.326; maxdepth=18; minbucket=5; minsplit=22
[Tune-y] 14: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 15: cp=0.0133; maxdepth=18; minbucket=5; minsplit=18
[Tune-y] 15: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 16: cp=0.0295; maxdepth=24; minbucket=48; minsplit=6
[Tune-y] 16: rmse.test.rmse=0.217; time: 0.0 min
[Tune-x] 17: cp=0.00899; maxdepth=30; minbucket=24; minsplit=10
[Tune-y] 17: rmse.test.rmse=0.148; time: 0.0 min
[Tune-x] 18: cp=0.0161; maxdepth=6; minbucket=37; minsplit=6
[Tune-y] 18: rmse.test.rmse=0.177; time: 0.0 min
[Tune-x] 19: cp=0.343; maxdepth=19; minbucket=28; minsplit=10
[Tune-y] 19: rmse.test.rmse=0.368; time: 0.0 min
[Tune-x] 20: cp=0.116; maxdepth=22; minbucket=34; minsplit=19
[Tune-y] 20: rmse.test.rmse=0.354; time: 0.0 min
[Tune] Result: cp=0.00337; maxdepth=13; minbucket=19; minsplit=32 : rmse.test.rmse=0.132
[1] "Fri Feb 09 16:59:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rsm no default is available.
[1] "Fri Feb 09 16:59:23 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.rvm no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 16:59:53 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.slim no default is available.
Sparse Linear Regression with L1 Regularization.
Square root Lasso with screening.

slim options summary: 
5 lambdas used:
[1] 0.9850 0.4770 0.2310 0.1120 0.0541
Method = lq 
q = 2 loss, SQRT Lasso
Degree of freedom: 0 -----> 6 
Runtime: 2.245128 secs 

 Values of predicted responses: 
   index             3 
   lambda       0.2307 
    Y 1          1.857 
    Y 2          1.808 
    Y 3         0.6813 
    Y 4          3.133 
    Y 5          2.161 
[1] "Fri Feb 09 16:59:57 2018"
[Tune] Started tuning learner regr.xgboost for parameter set:
                    Type len Def       Constr Req Tunable Trafo
nrounds          numeric   -   0    0 to 8.64   -    TRUE     Y
max_depth        integer   -   6      1 to 10   -    TRUE     -
eta              numeric   - 0.3 0.001 to 0.6   -    TRUE     -
gamma            numeric   -   0      0 to 10   -    TRUE     -
colsample_bytree numeric   - 0.5   0.3 to 0.7   -    TRUE     -
min_child_weight numeric   -   1      0 to 20   -    TRUE     -
subsample        numeric   -   1    0.25 to 1   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: nrounds=2.95e+03; max_depth=5; eta=0.363; gamma=2.58; colsample_bytree=0.53; min_child_weight=4.98; subsample=0.302
[Tune-y] 1: rmse.test.rmse=0.154; time: 0.7 min
[Tune-x] 2: nrounds=113; max_depth=10; eta=0.319; gamma=8.98; colsample_bytree=0.646; min_child_weight=12.6; subsample=0.491
[Tune-y] 2: rmse.test.rmse=0.234; time: 0.0 min
[Tune-x] 3: nrounds=104; max_depth=4; eta=0.546; gamma=3.79; colsample_bytree=0.384; min_child_weight=0.84; subsample=0.919
[Tune-y] 3: rmse.test.rmse=0.156; time: 0.0 min
[Tune-x] 4: nrounds=1.6e+03; max_depth=6; eta=0.501; gamma=2.04; colsample_bytree=0.533; min_child_weight=12.7; subsample=0.588
[Tune-y] 4: rmse.test.rmse=0.148; time: 0.5 min
[Tune-x] 5: nrounds=207; max_depth=7; eta=0.253; gamma=3.86; colsample_bytree=0.381; min_child_weight=16.7; subsample=0.761
[Tune-y] 5: rmse.test.rmse=0.16; time: 0.1 min
[Tune-x] 6: nrounds=11; max_depth=2; eta=0.226; gamma=3.22; colsample_bytree=0.536; min_child_weight=7.31; subsample=0.397
[Tune-y] 6: rmse.test.rmse=0.212; time: 0.0 min
[Tune-x] 7: nrounds=1.65e+03; max_depth=10; eta=0.596; gamma=3.89; colsample_bytree=0.557; min_child_weight=2.32; subsample=0.394
[Tune-y] 7: rmse.test.rmse=0.171; time: 0.9 min
[Tune-x] 8: nrounds=12; max_depth=10; eta=0.508; gamma=8.38; colsample_bytree=0.526; min_child_weight=0.00786; subsample=0.54
[Tune-y] 8: rmse.test.rmse=0.23; time: 0.0 min
[Tune-x] 9: nrounds=95; max_depth=6; eta=0.0089; gamma=2.96; colsample_bytree=0.497; min_child_weight=15; subsample=0.956
[Tune-y] 9: rmse.test.rmse=0.559; time: 0.0 min
[Tune-x] 10: nrounds=12; max_depth=4; eta=0.588; gamma=4.14; colsample_bytree=0.352; min_child_weight=8.09; subsample=0.344
[Tune-y] 10: rmse.test.rmse=0.233; time: 0.0 min
[Tune-x] 11: nrounds=708; max_depth=1; eta=0.508; gamma=5.84; colsample_bytree=0.501; min_child_weight=2.42; subsample=0.767
[Tune-y] 11: rmse.test.rmse=0.198; time: 0.1 min
[Tune-x] 12: nrounds=687; max_depth=7; eta=0.183; gamma=5.27; colsample_bytree=0.334; min_child_weight=14.8; subsample=0.982
[Tune-y] 12: rmse.test.rmse=0.173; time: 0.2 min
[Tune-x] 13: nrounds=90; max_depth=9; eta=0.565; gamma=6.89; colsample_bytree=0.313; min_child_weight=3.19; subsample=0.414
[Tune-y] 13: rmse.test.rmse=0.268; time: 0.0 min
[Tune-x] 14: nrounds=47; max_depth=6; eta=0.301; gamma=7.94; colsample_bytree=0.585; min_child_weight=2.33; subsample=0.323
[Tune-y] 14: rmse.test.rmse=0.276; time: 0.0 min
[Tune-x] 15: nrounds=85; max_depth=3; eta=0.497; gamma=4.9; colsample_bytree=0.441; min_child_weight=8.08; subsample=0.632
[Tune-y] 15: rmse.test.rmse=0.187; time: 0.0 min
[Tune-x] 16: nrounds=27; max_depth=3; eta=0.477; gamma=7.9; colsample_bytree=0.662; min_child_weight=10.1; subsample=0.835
[Tune-y] 16: rmse.test.rmse=0.192; time: 0.0 min
[Tune-x] 17: nrounds=15; max_depth=1; eta=0.201; gamma=5.82; colsample_bytree=0.695; min_child_weight=7.05; subsample=0.405
[Tune-y] 17: rmse.test.rmse=0.242; time: 0.0 min
[Tune-x] 18: nrounds=22; max_depth=8; eta=0.0934; gamma=6.12; colsample_bytree=0.52; min_child_weight=12.2; subsample=0.99
[Tune-y] 18: rmse.test.rmse=0.226; time: 0.0 min
[Tune-x] 19: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683
[Tune-y] 19: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 20: nrounds=26; max_depth=4; eta=0.294; gamma=3.02; colsample_bytree=0.33; min_child_weight=18.9; subsample=0.861
[Tune-y] 20: rmse.test.rmse=0.155; time: 0.0 min
[Tune] Result: nrounds=36; max_depth=7; eta=0.538; gamma=2.16; colsample_bytree=0.411; min_child_weight=14.9; subsample=0.683 : rmse.test.rmse=0.143
[1] "Fri Feb 09 17:02:32 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.xyf no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.xyf: Error in !toroidal : invalid argument type

[1] "Fri Feb 09 17:02:35 2018"
Warning in preProcess.default(df.toprocess[, trans.y:length(df.toprocess[1,  :
  No variation for for: V11, V12
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.bartMachine please install the following packages: bartMachine
Error in getDefaultParConfig(learner) : 
  For the learner regr.bcart no default is available.

burn in:
**GROW** @depth 0: [7,0.527027], n=(402,350)
**GROW** @depth 1: [8,0.608108], n=(87,263)
**GROW** @depth 1: [3,0.502004], n=(337,115)
**GROW** @depth 2: [1,0.324473], n=(29,235)
**GROW** @depth 2: [7,0.449449], n=(85,31)
**PRUNE** @depth 2: [7,0.449449]
**GROW** @depth 2: [4,0.498495], n=(295,38)
**GROW** @depth 3: [2,0.5], n=(14,218)
**GROW** @depth 2: [7,0.491992], n=(99,22)
**GROW** @depth 4: [1,0.67001], n=(86,132)
**GROW** @depth 3: [6,0.411323], n=(60,38)
**GROW** @depth 4: [3,0.651303], n=(36,24)
**GROW** @depth 3: [5,0.491483], n=(261,34)
**PRUNE** @depth 4: [3,0.651303]
**GROW** @depth 5: [2,0.656313], n=(51,36)
**GROW** @depth 4: [8,0.336837], n=(243,18)
**PRUNE** @depth 4: [8,0.336837]
**GROW** @depth 4: [2,0.5], n=(216,44)
**GROW** @depth 4: [3,0.256012], n=(12,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(215,21,12,28,60,60,38,20,35,31,15,52,35,130)
**GROW** @depth 6: [7,0.753253], n=(27,25)
**PRUNE** @depth 4: [5,0.480962]
**GROW** @depth 4: [9,0.423924], n=(27,33)
**PRUNE** @depth 4: [4,0.490471]
**PRUNE** @depth 6: [7,0.754755]
**GROW** @depth 6: [3,0.509519], n=(12,40)
**GROW** @depth 5: [1,0.702106], n=(12,117)
**GROW** @depth 4: [2,0.350701], n=(156,14)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(156,14,106,59,27,34,38,20,34,30,15,13,39,37,14,116)

Sampling @ nn=0 pred locs:
**GROW** @depth 5: [3,0.104709], n=(71,85)
**GROW** @depth 5: [1,0.957874], n=(93,22)
**GROW** @depth 4: [2,0.746493], n=(44,13)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(71,90,67,44,44,13,30,36,41,23,27,35,14,13,40,35,15,92,22)
**GROW** @depth 5: [5,0.39479], n=(23,21)
**PRUNE** @depth 6: [1,0.71013]
**PRUNE** @depth 6: [3,0.504509]
**GROW** @depth 4: [9,0.156156], n=(19,47)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(71,90,19,46,47,23,21,13,30,36,41,24,26,16,36,50,35,106,22)
**GROW** @depth 5: [6,0.187375], n=(16,14)
**PRUNE** @depth 5: [6,0.187375]
**GROW** @depth 5: [2,0.490982], n=(14,21)
**GROW** @depth 6: [6,0.837675], n=(16,19)
**GROW** @depth 6: [3,0.685371], n=(21,85)
**GROW** @depth 6: [3,0.725451], n=(29,19)
**GROW** @depth 6: [3,0.287575], n=(31,14)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(58,31,14,75,50,46,23,21,13,29,14,20,43,24,26,17,37,29,19,16,20,21,84,22)
**GROW** @depth 7: [9,0.0560561], n=(32,26)
**GROW** @depth 4: [2,0.760521], n=(12,25)
**GROW** @depth 4: [3,0.749499], n=(32,12)
**PRUNE** @depth 4: [3,0.276553]
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(32,24,33,14,124,47,22,22,13,30,14,19,32,12,23,25,18,13,26,29,19,16,18,21,84,22)
**GROW** @depth 5: [1,0.124373], n=(12,111)
**PRUNE** @depth 4: [3,0.753507]
**GROW** @depth 4: [3,0.700401], n=(23,20)
**PRUNE** @depth 4: [3,0.700401]
**GROW** @depth 6: [3,0.242485], n=(59,51)
**GROW** @depth 5: [2,0.397796], n=(24,24)
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=8 n=(33,24,32,17,12,61,46,24,24,21,23,13,31,14,21,42,22,26,18,13,26,29,19,16,19,20,84,22)
Grow: 10.03%, Prune: 3.571%, Change: 76.42%, Swap: 18.36%

[1] "Fri Feb 09 17:02:43 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bdk no default is available.
Warning in train(allmodel, regr.task) :
  Could not train learner regr.bdk: Error : 'bdk' is not an exported object from 'namespace:kohonen'

[1] "Fri Feb 09 17:02:44 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.blackboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.blm no default is available.

burn in:
r=1000 d=[0]; n=752

Sampling @ nn=0 pred locs:
r=1000 d=[0]; mh=1 n=752
r=2000 d=[0]; mh=1 n=752
r=3000 d=[0]; mh=1 n=752

[1] "Fri Feb 09 17:02:47 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.brnn no default is available.
Number of parameters (weights and biases) to estimate: 22 
Nguyen-Widrow method
Scaling factor= 0.7006455 
gamma= 20.2677 	 alpha= 1.3584 	 beta= 2159.192 
[1] "Fri Feb 09 17:02:48 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.bst no default is available.
[1] "Fri Feb 09 17:02:49 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.btlm no default is available.

burn in:
**GROW** @depth 0: [4,0.448345], n=(341,411)
**GROW** @depth 1: [7,0.499499], n=(81,329)
**GROW** @depth 2: [3,0.815631], n=(224,105)
**PRUNE** @depth 2: [7,0.499499]
**GROW** @depth 2: [5,0.346693], n=(46,229)
**GROW** @depth 2: [6,0.506012], n=(21,114)
**GROW** @depth 3: [9,0.507508], n=(109,121)
**GROW** @depth 1: [2,0.486974], n=(229,112)
**GROW** @depth 4: [8,0.626126], n=(14,106)
**GROW** @depth 5: [2,0.748497], n=(69,28)
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(230,142,17,16,68,28,116,22,113)
**GROW** @depth 2: [4,0.219659], n=(131,99)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; n=(129,101,141,20,21,65,28,119,18,110)

Sampling @ nn=0 pred locs:
**PRUNE** @depth 5: [8,0.624124]
r=1000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(127,103,141,21,20,127,85,18,110)
**GROW** @depth 3: [1,0.404213], n=(121,19)
r=2000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(127,103,121,20,20,22,131,80,18,110)
r=3000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(125,105,121,20,21,20,136,75,19,110)
r=4000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=6 n=(122,108,121,19,20,21,135,78,18,110)
**GROW** @depth 5: [5,0.759519], n=(88,45)
**PRUNE** @depth 5: [5,0.757515]
r=5000 d=[0] [0] [0] [0] [0] [0] [0] [0] [0] [0]; mh=7 n=(126,104,121,19,20,19,135,76,20,112)
Grow: 3.39%, Prune: 0.8721%, Change: 77.45%, Swap: 16.38%

[1] "Fri Feb 09 17:02:56 2018"
Loading required package: crs
Error: package or namespace load failed for 'crs' in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]):
 there is no package called 'MatrixModels'
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.crs please install the following packages: crs
Error in getDefaultParConfig(learner) : 
  For the learner regr.ctree no default is available.
[1] "Fri Feb 09 17:02:57 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cubist no default is available.
[1] "Fri Feb 09 17:02:58 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.cvglmnet no default is available.
[1] "Fri Feb 09 17:02:59 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.earth no default is available.
[1] "Fri Feb 09 17:03:00 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.elmNN no default is available.
[1] "Fri Feb 09 17:03:01 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.evtree please install the following packages: evtree
Error in getDefaultParConfig(learner) : 
  For the learner regr.featureless no default is available.
[1] "Fri Feb 09 17:03:01 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.fnn no default is available.
[1] "Fri Feb 09 17:03:02 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.gamboost please install the following packages: mboost
Error in getDefaultParConfig(learner) : 
  For the learner regr.gausspr no default is available.
Using automatic sigma estimation (sigest) for RBF or laplace kernel 
[1] "Fri Feb 09 17:03:05 2018"
[Tune] Started tuning learner regr.gbm for parameter set:
                     Type len   Def       Constr Req Tunable Trafo
n.trees           numeric   -  5.64    0 to 6.64   -    TRUE     Y
interaction.depth integer   -     1      1 to 10   -    TRUE     -
shrinkage         numeric   - 0.001 0.001 to 0.6   -    TRUE     -
n.minobsinnode    integer   -    10      5 to 25   -    TRUE     -
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: n.trees=10; interaction.depth=6; shrinkage=0.393; n.minobsinnode=13
[Tune-y] 1: rmse.test.rmse=0.023; time: 0.0 min
[Tune-x] 2: n.trees=94; interaction.depth=6; shrinkage=0.00808; n.minobsinnode=12
[Tune-y] 2: rmse.test.rmse=0.143; time: 0.0 min
[Tune-x] 3: n.trees=283; interaction.depth=3; shrinkage=0.41; n.minobsinnode=16
[Tune-y] 3: rmse.test.rmse=0.0172; time: 0.0 min
[Tune-x] 4: n.trees=16; interaction.depth=7; shrinkage=0.518; n.minobsinnode=16
[Tune-y] 4: rmse.test.rmse=0.0244; time: 0.0 min
[Tune-x] 5: n.trees=236; interaction.depth=5; shrinkage=0.00765; n.minobsinnode=6
[Tune-y] 5: rmse.test.rmse=0.0597; time: 0.0 min
[Tune-x] 6: n.trees=50; interaction.depth=5; shrinkage=0.107; n.minobsinnode=19
[Tune-y] 6: rmse.test.rmse=0.0169; time: 0.0 min
[Tune-x] 7: n.trees=16; interaction.depth=2; shrinkage=0.172; n.minobsinnode=10
[Tune-y] 7: rmse.test.rmse=0.0432; time: 0.0 min
[Tune-x] 8: n.trees=37; interaction.depth=4; shrinkage=0.468; n.minobsinnode=25
[Tune-y] 8: rmse.test.rmse=0.0242; time: 0.0 min
[Tune-x] 9: n.trees=47; interaction.depth=3; shrinkage=0.135; n.minobsinnode=18
[Tune-y] 9: rmse.test.rmse=0.019; time: 0.0 min
[Tune-x] 10: n.trees=378; interaction.depth=6; shrinkage=0.505; n.minobsinnode=9
[Tune-y] 10: rmse.test.rmse=0.0189; time: 0.0 min
[Tune-x] 11: n.trees=34; interaction.depth=5; shrinkage=0.26; n.minobsinnode=21
[Tune-y] 11: rmse.test.rmse=0.0175; time: 0.0 min
[Tune-x] 12: n.trees=17; interaction.depth=3; shrinkage=0.337; n.minobsinnode=25
[Tune-y] 12: rmse.test.rmse=0.0277; time: 0.0 min
[Tune-x] 13: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21
[Tune-y] 13: rmse.test.rmse=0.0142; time: 0.0 min
[Tune-x] 14: n.trees=439; interaction.depth=5; shrinkage=0.489; n.minobsinnode=22
[Tune-y] 14: rmse.test.rmse=0.0209; time: 0.0 min
[Tune-x] 15: n.trees=129; interaction.depth=8; shrinkage=0.00596; n.minobsinnode=6
[Tune-y] 15: rmse.test.rmse=0.14; time: 0.0 min
[Tune-x] 16: n.trees=299; interaction.depth=9; shrinkage=0.551; n.minobsinnode=5
[Tune-y] 16: rmse.test.rmse=0.0251; time: 0.0 min
[Tune-x] 17: n.trees=463; interaction.depth=1; shrinkage=0.331; n.minobsinnode=10
[Tune-y] 17: rmse.test.rmse=0.0204; time: 0.0 min
[Tune-x] 18: n.trees=14; interaction.depth=2; shrinkage=0.128; n.minobsinnode=23
[Tune-y] 18: rmse.test.rmse=0.0718; time: 0.0 min
[Tune-x] 19: n.trees=84; interaction.depth=7; shrinkage=0.408; n.minobsinnode=12
[Tune-y] 19: rmse.test.rmse=0.0189; time: 0.0 min
[Tune-x] 20: n.trees=249; interaction.depth=7; shrinkage=0.513; n.minobsinnode=10
[Tune-y] 20: rmse.test.rmse=0.0198; time: 0.0 min
[Tune] Result: n.trees=75; interaction.depth=7; shrinkage=0.0941; n.minobsinnode=21 : rmse.test.rmse=0.0142
[1] "Fri Feb 09 17:03:17 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.glm no default is available.
[1] "Fri Feb 09 17:03:18 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.glmboost please install the following packages: mboost
[Tune] Started tuning learner regr.glmnet for parameter set:
          Type len Def   Constr Req Tunable Trafo
alpha  numeric   -   1   0 to 1   -    TRUE     -
lambda numeric   -   0 -10 to 3   -    TRUE     Y
With control class: TuneControlRandom
Imputation value: Inf
[Tune-x] 1: alpha=0.00236; lambda=0.174
[Tune-y] 1: rmse.test.rmse=0.0482; time: 0.0 min
[Tune-x] 2: alpha=0.654; lambda=0.0311
[Tune-y] 2: rmse.test.rmse=0.031; time: 0.0 min
[Tune-x] 3: alpha=0.486; lambda=0.113
[Tune-y] 3: rmse.test.rmse=0.0739; time: 0.0 min
[Tune-x] 4: alpha=0.0118; lambda=0.0258
[Tune-y] 4: rmse.test.rmse=0.0212; time: 0.0 min
[Tune-x] 5: alpha=0.726; lambda=0.0116
[Tune-y] 5: rmse.test.rmse=0.0194; time: 0.0 min
[Tune-x] 6: alpha=0.683; lambda=0.134
[Tune-y] 6: rmse.test.rmse=0.107; time: 0.0 min
[Tune-x] 7: alpha=0.106; lambda=0.443
[Tune-y] 7: rmse.test.rmse=0.112; time: 0.0 min
[Tune-x] 8: alpha=0.864; lambda=0.129
[Tune-y] 8: rmse.test.rmse=0.123; time: 0.0 min
[Tune-x] 9: alpha=0.686; lambda=0.0719
[Tune-y] 9: rmse.test.rmse=0.0613; time: 0.0 min
[Tune-x] 10: alpha=0.0111; lambda=0.00193
[Tune-y] 10: rmse.test.rmse=0.0158; time: 0.0 min
[Tune-x] 11: alpha=0.351; lambda=0.0469
[Tune-y] 11: rmse.test.rmse=0.0331; time: 0.0 min
[Tune-x] 12: alpha=0.176; lambda=0.514
[Tune-y] 12: rmse.test.rmse=0.15; time: 0.0 min
[Tune-x] 13: alpha=0.107; lambda=0.00353
[Tune-y] 13: rmse.test.rmse=0.0165; time: 0.0 min
[Tune-x] 14: alpha=0.285; lambda=0.012
[Tune-y] 14: rmse.test.rmse=0.0197; time: 0.0 min
[Tune-x] 15: alpha=0.282; lambda=0.0191
[Tune-y] 15: rmse.test.rmse=0.0219; time: 0.0 min
[Tune-x] 16: alpha=0.779; lambda=7.45
[Tune-y] 16: rmse.test.rmse=0.289; time: 0.0 min
[Tune-x] 17: alpha=0.337; lambda=0.0133
[Tune-y] 17: rmse.test.rmse=0.0201; time: 0.0 min
[Tune-x] 18: alpha=0.224; lambda=0.27
[Tune-y] 18: rmse.test.rmse=0.102; time: 0.0 min
[Tune-x] 19: alpha=0.789; lambda=0.146
[Tune-y] 19: rmse.test.rmse=0.13; time: 0.0 min
[Tune-x] 20: alpha=0.842; lambda=0.00651
[Tune-y] 20: rmse.test.rmse=0.0168; time: 0.0 min
[Tune] Result: alpha=0.0111; lambda=0.00193 : rmse.test.rmse=0.0158
[1] "Fri Feb 09 17:03:21 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.deeplearning no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |=======                                                               |  10%  |                                                                              |============================                                          |  40%  |                                                                              |========================================================              |  80%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 17:03:30 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.gbm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |====================                                                  |  28%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 17:03:34 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.glm no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 17:03:37 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.h2o.randomForest no default is available.
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |==========                                                            |  14%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
  |                                                                              |                                                                      |   0%  |                                                                              |======================================================================| 100%
[1] "Fri Feb 09 17:03:42 2018"
Error in requirePackages(package, why = stri_paste("learner", id, sep = " "),  : 
  For learner regr.IBk please install the following packages: RWeka
Error in getDefaultParConfig(learner) : 
  For the learner regr.km no default is available.
In addition: Warning message:
package '!kknn' is not available (for R version 3.4.3) 

optimisation start
------------------
* estimation method   : MLE 
* optimisation method : BFGS 
* analytical gradient : used
* trend model : ~1
* covariance model : 
  - type :  matern5_2 
  - nugget : NO
  - parameters lower bounds :  1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 1e-10 
  - parameters upper bounds :  1.995996 1.997998 1.997998 1.995996 1.997998 1.997998 2 2 2 
  - best initial criterion value(s) :  2405.886 

N = 9, M = 5 machine precision = 2.22045e-16
At X0, 0 variables are exactly at the bounds
At iterate     0  f=      -2405.9  |proj g|=       1.6446
At iterate     1  f =      -2411.1  |proj g|=        1.6137
At iterate     2  f =      -2449.6  |proj g|=        1.4637
At iterate     3  f =      -2456.6  |proj g|=        1.7748
At iterate     4  f =      -2463.1  |proj g|=        1.7758
At iterate     5  f =      -2468.2  |proj g|=        1.7622
At iterate     6  f =      -2471.2  |proj g|=        1.7551
At iterate     7  f =      -2475.8  |proj g|=        1.7652
At iterate     8  f =      -2477.2  |proj g|=        1.7518
At iterate     9  f =      -2477.6  |proj g|=        1.7493
At iterate    10  f =      -2478.5  |proj g|=        1.7567
At iterate    11  f =      -2479.3  |proj g|=        1.7492
At iterate    12  f =      -2480.4  |proj g|=        1.7454
At iterate    13  f =      -2481.5  |proj g|=        1.7597
At iterate    14  f =      -2481.8  |proj g|=        1.7565
At iterate    15  f =      -2481.8  |proj g|=        1.7548
At iterate    16  f =      -2481.9  |proj g|=        1.7433
At iterate    17  f =      -2482.1  |proj g|=        1.7446
At iterate    18  f =      -2482.2  |proj g|=        1.2865
At iterate    19  f =      -2482.4  |proj g|=        1.7414
At iterate    20  f =      -2482.4  |proj g|=        1.7399
At iterate    21  f =      -2482.5  |proj g|=        1.7537
At iterate    22  f =      -2482.5  |proj g|=        1.7533
At iterate    23  f =      -2482.6  |proj g|=        1.5306
At iterate    24  f =      -2482.6  |proj g|=        1.5289
At iterate    25  f =      -2482.7  |proj g|=        1.7395
At iterate    26  f =      -2482.7  |proj g|=        1.7398
At iterate    27  f =      -2482.7  |proj g|=        1.3555
At iterate    28  f =      -2482.7  |proj g|=        1.4723
At iterate    29  f =      -2482.7  |proj g|=        1.6332
At iterate    30  f =      -2482.7  |proj g|=        1.7521
At iterate    31  f =      -2482.7  |proj g|=        1.6243
At iterate    32  f =      -2482.7  |proj g|=        1.0347
At iterate    33  f =      -2482.7  |proj g|=       0.97199
At iterate    34  f =      -2482.7  |proj g|=       0.82779
At iterate    35  f =      -2482.8  |proj g|=        1.7417
At iterate    36  f =      -2482.8  |proj g|=       0.97447
At iterate    37  f =      -2482.8  |proj g|=         1.752
At iterate    38  f =      -2482.8  |proj g|=       0.60905
At iterate    39  f =      -2482.8  |proj g|=        1.4301
At iterate    40  f =      -2482.8  |proj g|=       0.53273
At iterate    41  f =      -2482.8  |proj g|=       0.49426
At iterate    42  f =      -2482.8  |proj g|=       0.42392
At iterate    43  f =      -2482.8  |proj g|=       0.41084
At iterate    44  f =      -2482.8  |proj g|=       0.76076
At iterate    45  f =      -2482.8  |proj g|=       0.24818
At iterate    46  f =      -2482.8  |proj g|=       0.36953
At iterate    47  f =      -2482.8  |proj g|=       0.39105
At iterate    48  f =      -2482.8  |proj g|=       0.22698
At iterate    49  f =      -2482.8  |proj g|=       0.11636
At iterate    50  f =      -2482.8  |proj g|=       0.18771
At iterate    51  f =      -2482.8  |proj g|=       0.19192
At iterate    52  f =      -2482.8  |proj g|=        0.3843
At iterate    53  f =      -2482.8  |proj g|=       0.13395
At iterate    54  f =      -2482.8  |proj g|=      0.094236
At iterate    55  f =      -2482.8  |proj g|=       0.18763

iterations 55
function evaluations 64
segments explored during Cauchy searches 63
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 0.187633
final function value -2482.79

F = -2482.79
final  value -2482.790521 
converged
[1] "Fri Feb 09 17:06:19 2018"
Error in getDefaultParConfig(learner) : 
  For the learner regr.laGP no default is available.
i = 1 (of 248), d = 5.40504, its = 11
i = 2 (of 248), d = 4.86546, its = 12
i = 3 (of 248), d = 4.01059, its = 11
i = 4 (of 248), d = 4.19697, its = 11
i = 5 (of 248), d = 6.41088, its = 11
i = 6 (of 248), d = 4.75039, its = 11
i = 7 (of 248), d = 8.3371, its = 21
i = 8 (of 248), d = 4.7212, its = 12
i = 9 (of 248), d = 1.38338, its = 9
i = 10 (of 248), d = 8.3371, its = 26
i = 11 (of 248), d = 7.42431, its = 12
i = 12 (of 248), d = 3.19553, its = 11
i = 13 (of 248), d = 6.02626, its = 12
i = 14 (of 248), d = 0.832478, its = 10
i = 15 (of 248), d = 6.67722, its = 11
i = 16 (of 248), d = 6.52512, its = 12
i = 17 (of 248), d = 4.66319, its = 11
i = 18 (of 248), d = 6.60458, its = 12
i = 19 (of 248), d = 2.00481, its = 10
i = 20 (of 248), d = 2.82979, its = 11
i = 21 (of 248), d = 5.12788, its = 11
i = 22 (of 248), d = 6.27654, its = 12
i = 23 (of 248), d = 4.29538, its = 12
i = 24 (of 248), d = 5.50836, its = 11
i = 25 (of 248), d = 3.31834, its = 11
i = 26 (of 248), d = 1.46258, its = 9
i = 27 (of 248), d = 5.71515, its = 12
i = 28 (of 248), d = 5.90874, its = 11
i = 29 (of 248), d = 0.79724, its = 8
i = 30 (of 248), d = 8.07921, its = 12
i = 31 (of 248), d = 6.33182, its = 12
i = 32 (of 248), d = 8.3371, its = 56
i = 33 (of 248), d = 1.50923, its = 8
i = 34 (of 248), d = 7.64473, its = 11
i = 35 (of 248), d = 3.89655, its = 12
i = 36 (of 248), d = 8.3371, its = 24
i = 37 (of 248), d = 8.3371, its = 21
i = 38 (of 248), d = 3.63449, its = 11
i = 39 (of 248), d = 1.40099, its = 8
i = 40 (of 248), d = 4.21827, its = 11
i = 41 (of 248), d = 4.64565, its = 12
i = 42 (of 248), d = 5.60211, its = 11
i = 43 (of 248), d = 5.70782, its = 12
i = 44 (of 248), d = 4.45505, its = 12
i = 45 (of 248), d = 5.81069, its = 12
i = 46 (of 248), d = 6.74914, its = 11
i = 47 (of 248), d = 4.15217, its = 12
i = 48 (of 248), d = 5.37328, its = 11
i = 49 (of 248), d = 2.02606, its = 9
i = 50 (of 248), d = 8.3371, its = 23
i = 51 (of 248), d = 5.74874, its = 11
i = 52 (of 248), d = 7.29256, its = 12
i = 53 (of 248), d = 6.8576, its = 11
i = 54 (of 248), d = 4.33177, its = 11
i = 55 (of 248), d = 6.68086, its = 11
i = 56 (of 248), d = 7.84461, its = 12
i = 57 (of 248), d = 8.3371, its = 22
i = 58 (of 248), d = 7.17224, its = 12
i = 59 (of 248), d = 5.81767, its = 11
i = 60 (of 248), d = 4.28804, its = 11
i = 61 (of 248), d = 6.31193, its = 11
i = 62 (of 248), d = 6.45623, its = 12
i = 63 (of 248), d = 5.33446, its = 11
i = 64 (of 248), d = 2.71059, its = 10
i = 65 (of 248), d = 4.19517, its = 11
i = 66 (of 248), d = 4.56009, its = 12
i = 67 (of 248), d = 7.3077, its = 11
i = 68 (of 248), d = 7.05751, its = 11
i = 69 (of 248), d = 8.3371, its = 20
i = 70 (of 248), d = 4.13615, its = 10
i = 71 (of 248), d = 6.27906, its = 11
i = 72 (of 248), d = 4.75118, its = 12
i = 73 (of 248), d = 6.83247, its = 12
i = 74 (of 248), d = 7.531, its = 11
i = 75 (of 248), d = 6.57587, its = 11
i = 76 (of 248), d = 8.3371, its = 17
i = 77 (of 248), d = 7.132, its = 12
i = 78 (of 248), d = 3.74221, its = 31
i = 79 (of 248), d = 6.00942, its = 12
i = 80 (of 248), d = 6.91093, its = 11
i = 81 (of 248), d = 4.91317, its = 11
i = 82 (of 248), d = 6.78103, its = 11
i = 83 (of 248), d = 8.3371, its = 28
i = 84 (of 248), d = 6.44608, its = 12
i = 85 (of 248), d = 4.86002, its = 11
i = 86 (of 248), d = 1.84824, its = 9
i = 87 (of 248), d = 2.4898, its = 10
i = 88 (of 248), d = 8.3371, its = 57
i = 89 (of 248), d = 3.87089, its = 10
i = 90 (of 248), d = 4.12584, its = 11
i = 91 (of 248), d = 7.91492, its = 12
i = 92 (of 248), d = 3.78669, its = 10
i = 93 (of 248), d = 2.77997, its = 10
i = 94 (of 248), d = 2.10578, its = 10
i = 95 (of 248), d = 6.58654, its = 11
i = 96 (of 248), d = 3.61381, its = 11
i = 97 (of 248), d = 4.86621, its = 12
i = 98 (of 248), d = 6.24425, its = 11
i = 99 (of 248), d = 8.3371, its = 47
i = 100 (of 248), d = 5.31153, its = 11
i = 101 (of 248), d = 7.49759, its = 11
i = 102 (of 248), d = 6.39517, its = 11
i = 103 (of 248), d = 7.45012, its = 12
i = 104 (of 248), d = 7.92957, its = 11
i = 105 (of 248), d = 1.77527, its = 10
i = 106 (of 248), d = 4.89607, its = 11
i = 107 (of 248), d = 8.3371, its = 22
i = 108 (of 248), d = 4.4913, its = 11
i = 109 (of 248), d = 2.46428, its = 9
i = 110 (of 248), d = 3.10623, its = 10
i = 111 (of 248), d = 3.66962, its = 10
i = 112 (of 248), d = 1.68741, its = 8
i = 113 (of 248), d = 7.79107, its = 12
i = 114 (of 248), d = 3.35197, its = 10
i = 115 (of 248), d = 6.35416, its = 11
i = 116 (of 248), d = 2.40008, its = 9
i = 117 (of 248), d = 3.39965, its = 11
i = 118 (of 248), d = 6.89563, its = 11
i = 119 (of 248), d = 4.23188, its = 11
i = 120 (of 248), d = 2.95207, its = 9
i = 121 (of 248), d = 6.37248, its = 11
i = 122 (of 248), d = 8.1134, its = 12
i = 123 (of 248), d = 4.51312, its = 12
i = 124 (of 248), d = 5.81442, its = 11
i = 125 (of 248), d = 3.19356, its = 10
i = 126 (of 248), d = 7.21348, its = 12
i = 127 (of 248), d = 3.65966, its = 10
i = 128 (of 248), d = 6.84336, its = 12
i = 129 (of 248), d = 4.1494, its = 11
i = 130 (of 248), d = 2.5378, its = 10
i = 131 (of 248), d = 2.48877, its = 10
i = 132 (of 248), d = 5.70845, its = 11
i = 133 (of 248), d = 8.01862, its = 12
i = 134 (of 248), d = 8.3371, its = 60
i = 135 (of 248), d = 1.35058, its = 8
i = 136 (of 248), d = 8.19381, its = 11
i = 137 (of 248), d = 6.9433, its = 11
i = 138 (of 248), d = 5.0515, its = 11
i = 139 (of 248), d = 8.04126, its = 12
i = 140 (of 248), d = 7.39938, its = 12
i = 141 (of 248), d = 5.41512, its = 11
i = 142 (of 248), d = 8.3371, its = 59
i = 143 (of 248), d = 6.43314, its = 11
i = 144 (of 248), d = 4.48979, its = 11
i = 145 (of 248), d = 6.15798, its = 12
i = 146 (of 248), d = 7.05618, its = 11
i = 147 (of 248), d = 2.48555, its = 10
i = 148 (of 248), d = 7.28556, its = 12
i = 149 (of 248), d = 5.75875, its = 11
i = 150 (of 248), d = 6.23492, its = 12
i = 151 (of 248), d = 3.94035, its = 11
i = 152 (of 248), d = 5.8037, its = 11
i = 153 (of 248), d = 7.95632, its = 11
i = 154 (of 248), d = 7.33849, its = 12
i = 155 (of 248), d = 6.86123, its = 11
i = 156 (of 248), d = 4.22365, its = 11
i = 157 (of 248), d = 6.86942, its = 11
i = 158 (of 248), d = 5.93872, its = 11

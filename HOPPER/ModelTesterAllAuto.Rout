
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> options(repos=structure(c(CRAN="https://rweb.crmda.ku.edu/cran/")))
> ## capture messages and errors to a file.https://rweb.crmda.ku.edu/cran/
> #zz <- file("all.Rout", open="wt")https://cran.cnr.berkeley.edu
> #sink(zz, type="message") edit for rebaseless
> #chek for R package updates
> #try(log("a")) ## test --no-edit
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> memory.limit()
[1] 3562
> task.subject<-"14th20hp3cv"
> pc.mlr<-c("ACE")#"ALTA","HOPPER"
> which.computer<-Sys.info()[['nodename']]
> out.file<-paste("out",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,".csv",sep="")
> importance.file<-paste("importance",task.subject,which.computer,.Platform$OS.type,.Platform$r_arch,sep="")
> 
> if(exists("base.folder")){setwd(base.folder)}
> base.folder<-getwd()
> cpout.folder<-paste(base.folder,"/",which.computer,sep = "")
> setwd(cpout.folder)
> 
> if(length(which(list.files() == out.file))<1) write.table( "0.01,0.01,100,100,100,Wed Aug 02 16:37:25 2017,dummy,8,1,basic latent features,ignore,none,asis,1.12784979099243,random,333,53,adaptive_cv,16,5,2,2,19,0.0107744822639878,FALSE,,,,,,,,,," ,file =,out.file,  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,".csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,".csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> if(length(which(list.files() == paste(importance.file,"mlr.csv",sep="")))<1) write.table( ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," ,file = paste(importance.file,"mlr.csv",sep=""),  quote = F, sep = ",", row.names = F,col.names = F)
> 
> cv.iters=3
> tuneLength=20
> tuneLength2=8
> normings=c("YeoJohnson","ICA", "centernscale","expoTrans","range01","asis","quantile")#,"centernscale"
> 
> gensTTesto<-c(56,53,4,12,13,14,15,20,45,54,55, 44,3,1,52)#,  51,c(4)#c(1:40)#c(5,10,11,13,14,15,16,17,18,19,20,21,24,28,38,39,40)
> gensTTest<-vector()
> write.table( t(gensTTesto),file = "initial tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
> try({
+   gensTTest<-t(read.csv("tasks to test.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   gensTTest<-as.vector(gensTTest)
+ })
> if(!exists("gensTTest")) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> gensTTesto<-c(gensTTesto[length(gensTTesto):1])
> if(length(gensTTest)<1) gensTTest<-c(gensTTesto)#reversion[length(reversion):1]
> 
> 
> ########packages install check######
> 
> #list.of.packages <- c("caret","caretEnsemble","mlr","MLmetrics","tgp")
> #list.of.packages <- c("gower","dimRed","DEoptimR","caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> #new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> #if(length(new.packages)) install.packages(new.packages, dep = TRUE)
> 
> 
> #install.packages("mlr", dependencies = c("Depends", "Suggests"))
> #install.packages("caret", dependencies = c("Depends", "Suggests"))
> #install.packages("caret",repos = "http://cran.r-project.org",dependencies = c("Depends", "Imports", "Suggests"))
> #install.packages("SuperLearner", dependencies = c("Depends", "Suggests"))
> #install.packages("rattle", dependencies = c("Depends", "Suggests"))
> 
> # Load libraries
> #library(mlbench)
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> #library(caretEnsemble)
> library(MLmetrics)

Attaching package: 'MLmetrics'

The following objects are masked from 'package:caret':

    MAE, RMSE

The following object is masked from 'package:base':

    Recall

> 
> ########error no repeat#########
> 
> 
> try({
+   before.last.alg<-as.matrix(read.csv("beforelast algorithm.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.alg<-as.matrix(read.csv("last algorithm tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   #write.table(paste(date(), last.alg,.Platform$OS.type,.Platform$r_arch,which.computer,sep=" "),file = "algos after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.alg==before.last.alg){print("algorithm may be broken")}
+   write.table(last.alg,file = "beforelast algorithm.csv",  quote = F, row.names = F,col.names = F)
+ })
> try({
+   before.last.tsk<-as.matrix(read.csv("beforelast task.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   last.tsk<-as.matrix(read.csv("last task tried.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   write.table(paste(date(),last.alg, last.tsk,cv.iters,tuneLength,.Platform$OS.type,.Platform$r_arch,which.computer,sep=","),file = "test after which reset.csv",  quote = F, row.names = F,col.names = F,append = T)
+   if(last.tsk==before.last.tsk){print("task may be broken")}
+   write.table(last.tsk,file = "beforelast task.csv",  quote = F, row.names = F,col.names = F)
+ })
> bad.models=c("spaccceeee")
> previous.fails<-(read.csv("test after which reset.csv", sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
> previous.fails<-previous.fails[previous.fails[,8]==which.computer,]
> lgf<-length(previous.fails[,2])
> for(lt in 2:lgf)  {
+   if(previous.fails[lt,2]==previous.fails[lt-1,2])  {
+     bad.models=union(bad.models,c(paste(previous.fails[lt,2])))  }}
> 
> #######not to redo a test function#####
> check.redundant<-function(df=df.previous.calcs,norming="asis",trans.y=1,withextra="missing",missingdata="leaveempty",datasource="mean" ,column.to.predict=200,allmodel="ctree")
+ {
+   for(intern in 1:length(df[,1])){
+     if((any(df[intern,] == norming, na.rm=T))&&
+        (any(df[intern,] == withextra, na.rm=T))&&
+        (any(df[intern,] == missingdata, na.rm=T))&&
+        (any(df[intern,] == datasource, na.rm=T))&&
+        (any(df[intern,] == column.to.predict, na.rm=T))&&
+        (any(df[intern,] == allmodel, na.rm=T))&&
+        (  (df[intern,9] == trans.y)))
+     {return(TRUE)}
+   }
+   return(FALSE)
+ }
> #####caret init#####
> best.ranged <- c("avNNet", "nnet", "pcaNNet", "glm.nb")
> best.asis <- c("svmLinear3", "relaxo", "superpc", "xgbTree")
> best.cns <- c("gam", "bam", "svmLinear2", "msaenet", "BstLm", "gbm")
> 
> cv6hp5 <- c( "BstLm", "qrnn")#earth
> cv3hp32 <- c("Rborist", "pcaNNet", "SBC")
> cv7x5hp32 <- c("gbm", "krlsPoly", "kknn", "xgbLinear","RRF", "cubist", "rlm" )
> cv6hp5.avoid <- c("pcaNNet")
> cv3hp32.avoid <- c("glm.nb", "gamboost", "ctree2","glmboost", "leapSeq","ctree","svmLinear2")
> cv7x5hp32.avoid <- c("SBC","bagearthgcv","gcvearth","lmStepAIC","glmStepAIC","bridge","lm","glm","bayesglm","blassoAveraged","treebag","rpart1SE")
> 
> allmodels <- c("avNNet", "bagEarth", "bagEarthGCV",
+                "bayesglm", "bdk", "blackboost", "Boruta", "brnn", "BstLm" ,
+                "bstTree", "cforest", "ctree", "ctree2", "cubist", "DENFIS",
+                "dnn", "earth", "elm", "enet",   "evtree",
+                "extraTrees",  "gamLoess",  "gaussprLinear", "gaussprPoly", "gaussprRadial",
+                "gcvEarth","glm", "glmboost",  "icr", "kernelpls",
+                "kknn", "knn",  "krlsRadial", "lars" , "lasso",
+                "leapBackward", "leapForward", "leapSeq", "lm", "M5", "M5Rules",
+                "mlpWeightDecay", "neuralnet" , "partDSA",
+                "pcaNNet", "pcr", "penalized", "pls", "plsRglm", "ppr",
+                "qrf" , "ranger",  "rf")
> allmodels <- c("rlm", "rpart", "rpart2",
+                "RRF", "RRFglobal",  "simpls",
+                "svmLinear", "svmPoly", "svmRadial", "svmRadialCost",
+                "widekernelpls",  "xgbLinear",
+                "xgbTree")
> allmodels <- c("avNNet","BstLm","bstTree","cforest","ctree","ctree2",
+                "cubist","earth","enet","evtree","glmboost",
+                "icr","kernelpls","kknn","lasso","pcaNNet",
+                "pcr","pls","qrf","ranger","rf")
> 
> allmodels <- c("kknn", "cubist", "avNNet", "xgbLinear", "RRF", "pcaNNet","earth","nnet","gbm","enet","lasso","BstLm",
+                "foba", "leapBackward", "gcvEarth", "SBC","glm.nb","gamboost","ctree2","relaxo",
+                "bartMachine","extraTrees","bam","gam","randomGLM")
> #allmodels <- c("bam")
> #allmodels <- c("rf")"rqlasso",, "xyf" "rvmPoly", "rvmRadial",    "spls", "superpc" ,   "treebag",  "svmLinear2",  "SBC",
> #allmodels <- c("bartMachine", "xgbLinear", "pcaNNet","svmLinear","glmnet","cforest","cubist","rf","ranger")"glmnet",
> #wow rfRules is really slow "rfRules","WM", takes 50min
> # brak everythig "rbfDDA","ridge","rqnc",
> # use "rf" to test all
> library(caret)
> allmodels <- unique(modelLookup()[modelLookup()$forReg,c(1)])
> #allmodels <-c("avNNet", "nnet", "pcaNNet",  "glm.nb", "gam" ,
> #              "bam","msaenet", "svmLinear2","svmLinear3",
> #              "relaxo",  "superpc", "xgbTree", "BstLm")
> #allmodels<- c("svmLinear","svmPoly","svmRadial")
> #library(doParallel); cl <- makeCluster(detectCores()); registerDoParallel(cl)
> #allmodels<-c("bartMachine","extraTrees")#,"randomGLM"
> 
> 
> adaptControl <- trainControl(method = "adaptive_cv",
+                              number = 7, repeats = 5,
+                              adaptive = list(min = 4, alpha = 0.05,
+                                              method = "gls", complete = FALSE),
+                              search = "random")
> adaptControl <-trainControl(method = "cv", number = cv.iters,  search = "random")
> simpleControl <- trainControl(method = "cv",
+                               number = cv.iters,
+                               search = "random")
> 
> 
> #########MLR init######
> #R.utils::gcDLLs()
> #list.of.packages <- c("ParamHelpers","devtools","mlrMBO","RJSONIO","plot3D","plotly")
> #install.packages("mlrMBO", dependencies = c("Depends", "Suggests"))
> list.of.packages <- c("caretEnsemble","logicFS"," RWeka","ordinalNet","xgboost","mlr","caret","MLmetrics","bartMachine","spikeslab","party","rqPen","monomvn","foba","logicFS","rPython","qrnn","randomGLM","msaenet","Rborist","relaxo","ordinalNet","rrf","frbs","extraTrees","ipred","elasticnet","bst","brnn","Boruta","arm","elmNN","evtree","extraTrees","deepnet","kknn","KRLS","RSNNS","partDSA","plsRglm","quantregForest","ranger","inTrees")
> new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
> if(length(new.packages)) install.packages(new.packages, dep = TRUE)
Warning message:
packages 'logicFS', ' RWeka', 'rPython', 'rrf' are not available (for R version 3.4.3) 
> 
> #devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
> #devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
> 
> tuneLengthMLR<-tuneLength
> mlr.iters<-cv.iters
> #######data read process start#####
> seed.var =222+round(runif(1,min=0,max=100))
> column.to.predict=1
> print(date());
[1] "Mon Mar 12 14:12:21 2018"
> 
> setwd(base.folder)
> if(!exists("gen.count")){gen.count=56}
> gens.names<-as.matrix(read.table("gens names.csv", sep = ",",header = FALSE,row.names=1,fill=TRUE, quote="",dec="."))
> count.toy.data.passed<-1
> for(gend.data in gensTTest){
+   count.toy.data.passed<-count.toy.data.passed+1
+   setwd(base.folder)
+   data.source<-as.matrix(read.csv(paste("Generats/",gens.names[gend.data],".csv", sep = ""), sep = ",",fill=TRUE, header = FALSE,quote="",dec="."))
+   datasource<-gens.names[gend.data,1]
+   setwd(cpout.folder)
+   missingdatas=c("ignore")
+   for(missingdata in missingdatas){
+     withextras=c("none")
+     for(withextra in withextras){
+       ################data wrestling###############
+ 
+       dependant.selection=complete.cases(data.source[,column.to.predict])
+       df.previous.calcs=as.data.frame(read.csv(file=out.file, header = FALSE, sep = ",", quote = "",
+                                                dec = ".", fill = TRUE, comment.char = ""))
+       unimportant.computations<-vector(mode = "logical",length=length(df.previous.calcs[,1])  )
+       for(intern in 1:length(df.previous.calcs[,1])){
+         if((any(df.previous.calcs[intern,] == withextra, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == missingdata, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == datasource, na.rm=T))&&
+            (any(df.previous.calcs[intern,] == column.to.predict, na.rm=T)))
+         {unimportant.computations[intern]<-T}}
+ 
+       df.previous.calcs<-df.previous.calcs[unimportant.computations,]
+ 
+       #data.source=data.frame( data.source[,column.to.predict],data.source[,1:2], data.source[,4:(column.to.predict-1)], data.source[,(column.to.predict+1):length( data.source[1,])])
+ 
+         for(norming in normings) {
+         for(trans.y in 1:2) {
+           df.toprocess=data.source
+           y.untransformed<-df.toprocess[,1]
+ 
+           if(norming=="centernscale"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("center", "scale"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="range01"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="expoTrans"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("expoTrans"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+           if(norming=="YeoJohnson"){
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("YeoJohnson"))#"center", "scale",
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           if((norming=="asis")&&(trans.y==2)){next}
+ 
+ 
+           ################preprocess###########
+           df.toprocess=data.frame(df.toprocess[dependant.selection,])
+           y.untransformed=y.untransformed[dependant.selection]
+           if(norming=="quantile"){
+             for(Clol in trans.y:length(data.source[1,])){
+               df.toprocess[,Clol]<- (rank(df.toprocess[,Clol],na.last = "keep",ties.method = "average")-1) }
+             preProcValues= preProcess(df.toprocess[,trans.y:length(df.toprocess[1,])],method = c("range"))
+             df.toprocess[,trans.y:length(df.toprocess[1,])]<- predict(preProcValues, df.toprocess[,trans.y:length(df.toprocess[1,])])}
+ 
+           loess.model<-loess(y.untransformed~ df.toprocess[,1],span = 0.21, degree = 1)
+ 
+           #df.toprocess = data.frame(df.toprocess,)
+           nzv <- nearZeroVar(df.toprocess[,])#, saveMetrics= TRUE
+           #nzv[nzv$nzv,][1:10,]
+           if(length(nzv)>1){
+             df.toprocess = (df.toprocess[, -nzv])}
+           df.toprocess = signif(df.toprocess,digits = 3)
+ 
+           seed.var =222+round(runif(1,min=0,max=100))
+           set.seed(seed.var)
+           inTrain <- createDataPartition(y = df.toprocess[,1],
+                                          p = .75,
+                                          list = FALSE)
+           training <- df.toprocess[ inTrain,]
+           testing  <- df.toprocess[-inTrain,]
+           write.table(df.toprocess,file = "sanity check 1.csv",  quote = F, row.names = F,col.names = F)
+ 
+           ###########for all models#################
+           setwd(base.folder)
+           if(max(which.computer==pc.mlr)>0)
+             source("MLR part.R")
+           else
+             source("Caret part.R")
+ 
+          setwd(cpout.folder)
+           if(norming == normings[length(normings)]){
+             if(count.toy.data.passed>length(gensTTest)){gensTTest<-c(gensTTesto)}
+             write.table( t(gensTTest[count.toy.data.passed:length(gensTTest)]),file = "tasks to test.csv",  quote = F, sep = ",", row.names = F,col.names = F)
+ 
+             }
+ 
+         }
+       }
+     }
+   }
+ 
+ }
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:41:16 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "glmnet_h2o"              
Start:  AIC=232.91
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V11   1   191.38 231.01
- V10   1   191.47 231.03
- V8    1   192.56 231.31
- V7    1   193.01 231.43
- V4    1   193.02 231.43
- V9    1   196.26 232.26
<none>      191.01 232.91
- V6    1   334.22 258.88
- V2    1   364.83 263.26
- V5    1   372.47 264.30
- V3    1   556.89 284.41

Step:  AIC=231.01
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance    AIC
- V10   1   191.98 229.16
- V8    1   193.66 229.60
- V4    1   193.67 229.60
- V7    1   194.26 229.75
- V9    1   196.26 230.26
<none>      191.38 231.01
- V6    1   342.02 258.04
- V2    1   367.52 261.63
- V5    1   386.15 264.10
- V3    1   561.61 282.83

Step:  AIC=229.16
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Deviance    AIC
- V4    1   193.82 227.64
- V8    1   193.99 227.68
- V7    1   194.89 227.91
- V9    1   197.40 228.56
<none>      191.98 229.16
- V6    1   342.62 256.12
- V2    1   370.47 260.03
- V5    1   394.49 263.17
- V3    1   562.30 280.89

Step:  AIC=227.64
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9

       Df Deviance    AIC
- V8    1   195.12 225.97
- V7    1   196.94 226.44
- V9    1   199.12 226.99
<none>      193.82 227.64
- V6    1   344.00 254.32
- V2    1   370.91 258.09
- V5    1   399.38 261.79
- V3    1   571.76 279.73

Step:  AIC=225.97
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9

       Df Deviance    AIC
- V7    1   197.84 224.66
- V9    1   200.26 225.27
<none>      195.12 225.97
- V6    1   349.33 253.09
- V2    1   388.82 258.45
- V5    1   403.06 260.25
- V3    1   571.86 277.74

Step:  AIC=224.66
.outcome ~ V2 + V3 + V5 + V6 + V9

       Df Deviance    AIC
- V9    1   204.29 224.27
<none>      197.84 224.66
- V6    1   351.97 251.47
- V2    1   389.92 256.59
- V5    1   404.99 258.49
- V3    1   573.33 275.87

Step:  AIC=224.27
.outcome ~ V2 + V3 + V5 + V6

       Df Deviance    AIC
<none>      204.29 224.27
- V6    1   369.07 251.84
- V5    1   405.26 256.52
- V2    1   413.88 257.57
- V3    1   574.02 273.93
Start:  AIC=250.43
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V4    1   271.39 248.47
- V8    1   271.53 248.50
- V10   1   275.90 249.29
- V9    1   277.53 249.59
- V11   1   280.88 250.19
<none>      271.15 250.43
- V7    1   290.43 251.86
- V6    1   349.97 261.19
- V2    1   501.26 279.15
- V5    1   511.38 280.15
- V3    1   599.36 288.09

Step:  AIC=248.47
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V8    1   271.75 246.54
- V10   1   275.99 247.31
- V9    1   277.70 247.62
- V11   1   280.89 248.19
<none>      271.39 248.47
- V7    1   292.16 250.16
- V6    1   350.47 259.26
- V2    1   501.42 277.17
- V5    1   524.59 279.42
- V3    1   602.72 286.37

Step:  AIC=246.54
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V10 + V11

       Df Deviance    AIC
- V10   1   276.15 245.34
- V9    1   277.81 245.64
- V11   1   281.13 246.23
<none>      271.75 246.54
- V7    1   292.58 248.23
- V6    1   350.62 257.28
- V2    1   510.32 276.04
- V5    1   525.10 277.47
- V3    1   602.91 284.38

Step:  AIC=245.34
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V11

       Df Deviance    AIC
- V9    1   283.72 244.69
- V11   1   286.63 245.20
<none>      276.15 245.34
- V7    1   294.80 246.61
- V6    1   354.76 255.87
- V2    1   510.51 274.06
- V5    1   539.50 276.82
- V3    1   606.80 282.70

Step:  AIC=244.69
.outcome ~ V2 + V3 + V5 + V6 + V7 + V11

       Df Deviance    AIC
<none>      283.72 244.69
- V11   1   295.63 244.75
- V7    1   305.09 246.32
- V6    1   383.07 257.70
- V2    1   530.38 273.97
- V5    1   550.20 275.81
- V3    1   615.41 281.41
Start:  AIC=250.52
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V10   1   237.70 248.60
- V9    1   237.76 248.61
- V4    1   238.33 248.74
- V11   1   238.51 248.77
- V7    1   240.57 249.22
- V8    1   245.46 250.27
<none>      237.33 250.52
- V2    1   347.98 268.42
- V6    1   374.85 272.28
- V5    1   434.50 279.96
- V3    1   536.92 290.97

Step:  AIC=248.6
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V11

       Df Deviance    AIC
- V9    1   238.09 246.68
- V4    1   238.85 246.85
- V11   1   238.92 246.86
- V7    1   241.18 247.35
- V8    1   246.32 248.45
<none>      237.70 248.60
- V2    1   349.13 266.59
- V6    1   377.02 270.58
- V5    1   448.94 279.66
- V3    1   543.82 289.63

Step:  AIC=246.68
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
- V4    1   239.49 244.99
- V11   1   239.52 245.00
- V7    1   241.53 245.43
- V8    1   246.51 246.49
<none>      238.09 246.68
- V2    1   350.40 264.78
- V6    1   377.26 268.62
- V5    1   453.43 278.18
- V3    1   558.27 289.00

Step:  AIC=244.99
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V11

       Df Deviance    AIC
- V11   1   240.56 243.22
- V7    1   243.36 243.82
- V8    1   247.01 244.60
<none>      239.49 244.99
- V2    1   359.02 264.04
- V6    1   397.53 269.34
- V5    1   467.18 277.73
- V3    1   558.69 287.04

Step:  AIC=243.22
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8

       Df Deviance    AIC
- V7    1   246.24 242.43
- V8    1   247.75 242.75
<none>      240.56 243.22
- V6    1   398.11 267.41
- V2    1   399.38 267.58
- V5    1   492.91 278.52
- V3    1   560.76 285.23

Step:  AIC=242.43
.outcome ~ V2 + V3 + V5 + V6 + V8

       Df Deviance    AIC
- V8    1   254.13 242.07
<none>      246.24 242.43
- V2    1   400.03 265.67
- V6    1   401.18 265.81
- V5    1   493.90 276.63
- V3    1   561.44 283.29

Step:  AIC=242.07
.outcome ~ V2 + V3 + V5 + V6

       Df Deviance    AIC
<none>      254.13 242.07
- V6    1   406.47 264.50
- V2    1   433.55 267.85
- V5    1   502.28 275.50
- V3    1   565.09 281.63
Start:  AIC=358.93
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V4    1   366.03 357.15
- V11   1   366.60 357.27
- V10   1   367.11 357.37
- V8    1   369.61 357.89
- V9    1   370.49 358.07
<none>      364.99 358.93
- V7    1   377.84 359.56
- V6    1   564.02 390.01
- V2    1   653.30 401.18
- V5    1   690.08 405.34
- V3    1   875.21 423.40

Step:  AIC=357.15
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Deviance    AIC
- V11   1   367.59 355.47
- V10   1   367.86 355.53
- V8    1   370.16 356.00
- V9    1   371.53 356.28
<none>      366.03 357.15
- V7    1   379.84 357.96
- V6    1   568.58 388.62
- V2    1   654.38 399.30
- V5    1   705.19 404.99
- V3    1   879.53 421.78

Step:  AIC=355.47
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Deviance    AIC
- V10   1   369.40 353.85
- V8    1   371.35 354.25
- V9    1   373.57 354.70
<none>      367.59 355.47
- V7    1   380.19 356.03
- V6    1   570.47 386.87
- V2    1   655.53 397.44
- V5    1   717.66 404.32
- V3    1   879.71 419.79

Step:  AIC=353.85
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9

       Df Deviance    AIC
- V8    1   373.04 352.59
- V9    1   376.00 353.19
<none>      369.40 353.85
- V7    1   382.04 354.40
- V6    1   570.49 384.88
- V2    1   656.35 395.53
- V5    1   738.44 404.49
- V3    1   881.18 417.92

Step:  AIC=352.59
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9

       Df Deviance    AIC
- V9    1   378.80 351.75
<none>      373.04 352.59
- V7    1   385.50 353.09
- V6    1   575.30 383.51
- V2    1   687.71 397.08
- V5    1   743.13 402.97
- V3    1   881.46 415.94

Step:  AIC=351.75
.outcome ~ V2 + V3 + V5 + V6 + V7

       Df Deviance    AIC
<none>      378.80 351.75
- V7    1   393.02 352.56
- V6    1   595.87 384.18
- V2    1   699.17 396.33
- V5    1   743.17 400.97
- V3    1   890.55 414.72
Generalized Linear Model with Stepwise Feature Selection 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE     Rsquared   MAE     
  2.41942  0.7699172  1.957556

[1] "Mon Mar 12 15:41:31 2018"
Independent Component Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  n.comp  RMSE      Rsquared   MAE       Selected
  2       4.456505  0.2322641  3.651291          
  3       4.420417  0.2579628  3.628015          
  4       3.967199  0.3988114  3.120795          
  5       3.571830  0.5093497  2.807852          
  6       3.181960  0.6364943  2.610964          
  7       3.157757  0.6452545  2.566282          
  8       3.172414  0.6279956  2.561744          
  9       2.840059  0.6964576  2.292754  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was n.comp = 9.
[1] "Mon Mar 12 15:41:48 2018"
Partial Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE      Rsquared   MAE       Selected
  2      2.554676  0.7457723  2.072901          
  3      2.483519  0.7575385  2.027530          
  4      2.455220  0.7624006  2.005845          
  5      2.442977  0.7641861  1.988915          
  6      2.438827  0.7648748  1.987122          
  7      2.438280  0.7649700  1.987337          
  8      2.437872  0.7650387  1.987021  *       
  9      2.437903  0.7650324  1.987045          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 15:42:02 2018"
Error in MSEP(object) : could not find function "MSEP"
k-Nearest Neighbors 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  kmax  distance   kernel        RMSE      Rsquared   MAE       Selected
   3    2.6158190  epanechnikov  4.250916  0.3172816  3.452742          
   4    0.7598070  biweight      3.925993  0.4104821  3.164293          
   5    0.2854615  triangular    3.688103  0.4671441  2.909052          
   6    1.3559582  inv           3.640879  0.5106641  3.062637          
   7    1.6500853  rectangular   3.756677  0.4690491  3.110855          
   9    1.9718086  rectangular   3.682535  0.4838671  3.106667          
   9    2.8497932  triangular    4.087472  0.3504588  3.393203          
  11    1.4028958  cos           3.599227  0.5467730  2.945758          
  12    0.8039695  cos           3.578520  0.5608088  2.870075          
  14    2.0422501  cos           3.881909  0.4539041  3.224694          
  14    2.9120675  biweight      3.922752  0.4016199  3.206967          
  16    0.1645523  rectangular   3.733531  0.5042289  3.022780          
  16    0.5812913  gaussian      3.525830  0.5651767  2.865057  *       
  17    0.2259030  triangular    3.603290  0.5441641  2.895478          
  17    2.6778800  epanechnikov  4.037743  0.3718524  3.353114          
  19    1.7773804  triweight     3.657349  0.5122124  2.978327          
  20    0.4741031  biweight      3.563016  0.5619533  2.859060          
  21    1.9943767  inv           3.668641  0.4923443  3.098451          
  22    0.2752325  biweight      3.589647  0.5461517  2.876653          
  22    1.8695967  epanechnikov  3.714567  0.4977327  3.100154          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were kmax = 16, distance = 0.5812913
 and kernel = gaussian.
[1] "Mon Mar 12 15:42:18 2018"
k-Nearest Neighbors 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  k   RMSE      Rsquared   MAE       Selected
   3  3.932203  0.4044385  3.242390          
   4  3.959913  0.3920680  3.309754          
   5  3.743237  0.4635656  3.088391          
   6  3.737538  0.4727076  3.052742  *       
   7  3.759677  0.4734975  3.062651          
   9  3.890834  0.4506810  3.201490          
  11  3.909534  0.4649514  3.196631          
  12  3.882273  0.4917649  3.188373          
  14  3.858057  0.5437840  3.142044          
  16  3.964667  0.5090268  3.199045          
  17  3.964904  0.5172756  3.183121          
  19  3.982858  0.5694897  3.220849          
  20  3.998372  0.5584767  3.223934          
  21  3.994251  0.5982069  3.259593          
  22  3.997006  0.6313248  3.264393          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was k = 6.
[1] "Mon Mar 12 15:42:33 2018"
Polynomial Kernel Regularized Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        degree  RMSE          Rsquared    MAE           Selected
  3.398331e-05  3           4.976930  0.47090045      4.175146          
  4.908678e-05  1       17628.021249  0.01794696  14734.716499          
  6.869763e-05  1       12595.722192  0.01794767  10528.301095          
  1.388171e-04  2           4.927235  0.35373024      4.109910          
  1.700434e-04  2           4.927235  0.35373787      4.109911          
  4.205922e-04  2           4.927232  0.35379908      4.109914          
  5.295414e-04  3           4.976930  0.47090065      4.175146          
  1.226861e-03  2           4.927225  0.35399598      4.109924          
  1.706586e-03  1         506.738287  0.01800695    423.300765          
  4.649139e-03  3           4.976930  0.47090228      4.175146          
  6.163082e-03  3           4.976930  0.47090287      4.175146          
  1.139410e-02  1          75.774664  0.01836058     63.248963          
  1.227144e-02  1          70.357601  0.01839286     58.735101          
  1.847174e-02  1          46.777128  0.01862216     39.079424          
  2.146598e-02  3           4.976930  0.47090892      4.175147          
  6.302247e-02  2           4.926697  0.36879064      4.110654          
  8.405619e-02  1          11.121233  0.02117520      9.339051          
  1.012562e-01  2           4.926418  0.37764440      4.111082          
  1.791958e-01  2           4.925945  0.39496424      4.111903  *       
  2.364973e-01  1           5.919770  0.02800012      4.694320          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.1791958 and degree = 2.
[1] "Mon Mar 12 15:42:48 2018"

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.428425284  0.587458597 -0.109540350  0.507284968  0.298848145  0.047005370 
          V8           V9          V10          V11 
-0.051867694 -0.001933391  0.035863662 -0.006832478 

 Quartiles of Marginal Effects:
 
            V2        V3          V4        V5         V6          V7
25% 0.05257546 0.1744481 -0.49810808 0.1530875 0.02000654 -0.19669821
50% 0.38445949 0.4478487 -0.05383207 0.4628338 0.23257298  0.05246703
75% 0.64247370 1.0929079  0.19284954 0.8167438 0.57335441  0.20346313
            V8            V9          V10         V11
25% -0.3016736 -0.3328087429 -0.198872105 -0.32178451
50% -0.0851319 -0.0007548379 -0.002367641  0.08311887
75%  0.2242459  0.2704254463  0.230690928  0.41501162

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.06830808  2.72167442 -0.41039273  2.35088849  1.41882171 -0.13318005 
         V8          V9         V10         V11 
-0.16456620  0.37234232 -0.03158926 -0.10317549 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7          V8
25% 1.796915 2.295506 -0.85419520 2.151266 1.121060 -0.30134980 -0.31990765
50% 2.115159 2.830313 -0.34291551 2.347432 1.412464 -0.10910717 -0.18405545
75% 2.480726 3.109084  0.04302964 2.580569 1.667241  0.07341625  0.02201675
           V9         V10         V11
25% 0.1545022 -0.32071003 -0.33990865
50% 0.4083322 -0.07856207 -0.05853456
75% 0.7545977  0.17832906  0.14123152

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.16700631  3.14902731 -0.89846432  3.12709022  1.23272028  0.09751390 
         V8          V9         V10         V11 
-0.24465433  0.42973774  0.05551218 -0.09977270 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7         V8
25% 1.343042 1.983280 -2.1658872 2.199595 0.5931052 -0.64954815 -0.8293641
50% 2.089152 3.243906 -0.9694358 3.171119 1.1811493  0.04130733 -0.2123691
75% 3.133986 4.329201  0.4013264 3.974023 1.9251353  0.70980174  0.3551966
            V9         V10         V11
25% -0.1261989 -0.56958888 -0.44056744
50%  0.3962955  0.06755269 -0.06281051
75%  1.1521609  0.71709877  0.35851349

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.74972949  0.91742328  0.10536114  0.84911067  0.61944791  0.08258999 
         V8          V9         V10         V11 
-0.17062608  0.09567563  0.06286065 -0.20838629 

 Quartiles of Marginal Effects:
 
           V2        V3        V4        V5        V6         V7         V8
25% 0.7483808 0.9165443 0.1048105 0.8483371 0.6186858 0.08223115 -0.1714454
50% 0.7501310 0.9176669 0.1056549 0.8495261 0.6194719 0.08276337 -0.1707637
75% 0.7509254 0.9188751 0.1061209 0.8508978 0.6205340 0.08326953 -0.1698753
            V9        V10        V11
25% 0.09511626 0.06228157 -0.2092090
50% 0.09581074 0.06257878 -0.2084651
75% 0.09640949 0.06334734 -0.2074416

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.267391012  0.374973290 -0.080088083  0.327076175  0.201717083  0.039250727 
          V8           V9          V10          V11 
-0.039490497  0.005134237  0.022448200 -0.004265060 

 Quartiles of Marginal Effects:
 
               V2         V3          V4         V5          V6          V7
25% -0.0006426263 0.09097293 -0.34052469 0.05921873 0.003880157 -0.14109862
50%  0.1396884423 0.25451585 -0.04173592 0.27222471 0.137504911  0.01998601
75%  0.4538302708 0.71587560  0.13608302 0.54821230 0.379402241  0.14705653
             V8          V9          V10         V11
25% -0.21100121 -0.23271763 -0.161599061 -0.24960618
50% -0.05736201 -0.01021974 -0.003842309  0.05165847
75%  0.17611108  0.21777698  0.156138737  0.27634755

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.235028159  3.137439787 -0.688055821  2.930661418  1.521275872 -0.002496246 
          V8           V9          V10          V11 
-0.224138776  0.507626896 -0.059350361 -0.081837307 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6          V7          V8
25% 1.836364 2.230117 -1.7038856 2.393467 1.188355 -0.42283793 -0.57894619
50% 2.214516 3.350944 -0.8106580 2.997186 1.445669 -0.06497751 -0.12135468
75% 2.733908 3.932685  0.3495842 3.472969 1.972636  0.36817914  0.04468287
             V9         V10         V11
25% 0.005862659 -0.56047896 -0.39583758
50% 0.423558514 -0.05094079 -0.06943478
75% 1.100865625  0.38653717  0.22537016

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.031208985  0.044187247 -0.012331213  0.035340693  0.021565355  0.007032835 
          V8           V9          V10          V11 
-0.003983341  0.007591466 -0.002401941  0.000817717 

 Quartiles of Marginal Effects:
 
               V2           V3           V4           V5           V6
25% -0.0204022190 -0.002572603 -0.030509573 -0.003610856 -0.002417686
50% -0.0006110996  0.007415315 -0.001328406  0.010307432  0.006007442
75%  0.0470639728  0.060309323  0.019442667  0.060897660  0.055255177
               V7           V8            V9           V10          V11
25% -0.0196999181 -0.027414380 -0.0325967627 -0.0300559778 -0.030895813
50% -0.0008297726 -0.002651041  0.0002912791 -0.0008599872  0.002639395
75%  0.0111733540  0.015751788  0.0252065361  0.0191025671  0.028584539

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.924592578  2.542424855 -0.405568035  2.200396408  1.230428825 -0.134632373 
          V8           V9          V10          V11 
-0.121900263  0.287631694  0.007387752 -0.083004683 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7          V8
25% 1.535169 1.980055 -0.83470087 1.913543 0.875908 -0.37153686 -0.33673081
50% 1.968696 2.631967 -0.30692045 2.252806 1.195181 -0.09629449 -0.16679321
75% 2.393805 3.026200  0.03261264 2.494673 1.559589  0.12136853  0.09441291
            V9         V10         V11
25% 0.04303804 -0.37889105 -0.34192511
50% 0.31629369 -0.03377124 -0.03874541
75% 0.73201825  0.29419492  0.25593988

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.845141984  2.328361270 -0.073215705  1.884728262  1.442200902 -0.066007142 
          V8           V9          V10          V11 
-0.191905816  0.317772468  0.006841753 -0.223921075 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7         V8
25% 1.841292 2.324140 -0.07794711 1.881808 1.438009 -0.06814508 -0.1954551
50% 1.847094 2.329097 -0.07237186 1.885097 1.442120 -0.06542452 -0.1913006
75% 1.851632 2.333884 -0.06891544 1.891095 1.448185 -0.06303307 -0.1891170
           V9         V10        V11
25% 0.3153226 0.003585738 -0.2273465
50% 0.3193262 0.006894369 -0.2236947
75% 0.3225063 0.009542475 -0.2196236

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.31449977  1.64026365  0.06011990  1.40630009  1.04492834  0.04522633 
         V8          V9         V10         V11 
-0.19724183  0.21178650  0.06503118 -0.26272072 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7         V8
25% 1.313516 1.639657 0.05954903 1.405704 1.044296 0.04489863 -0.1977550
50% 1.314865 1.640500 0.06022729 1.406528 1.044906 0.04530458 -0.1973222
75% 1.315435 1.641171 0.06073822 1.407472 1.045737 0.04577992 -0.1967714
           V9        V10        V11
25% 0.2113912 0.06465525 -0.2633477
50% 0.2119073 0.06489114 -0.2627422
75% 0.2123807 0.06534063 -0.2621967

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 2.2554059  2.8936396 -0.3067431  2.3258490  1.7393753 -0.1804979 -0.2211990 
        V9        V10        V11 
 0.3988288 -0.1060104 -0.1091151 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7         V8
25% 2.162072 2.788150 -0.4553465 2.257132 1.670354 -0.2238139 -0.2979899
50% 2.266943 2.916407 -0.2884370 2.316892 1.745515 -0.1908946 -0.2116862
75% 2.359659 2.972370 -0.1649744 2.402701 1.792290 -0.1447001 -0.1548073
           V9         V10         V11
25% 0.3193344 -0.17087674 -0.18194323
50% 0.3834223 -0.11945196 -0.10463290
75% 0.4829661 -0.03824025 -0.01262784

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.22299051  2.84721421 -0.28817542  2.29112179  1.70986670 -0.17206363 
         V8          V9         V10         V11 
-0.21475487  0.39221812 -0.09374049 -0.11951314 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7         V8
25% 2.121954 2.747396 -0.4309093 2.231765 1.637570 -0.2139218 -0.2859932
50% 2.232105 2.878412 -0.2659327 2.287755 1.715860 -0.1768911 -0.2083000
75% 2.324425 2.931567 -0.1597901 2.377940 1.769198 -0.1366103 -0.1531899
           V9         V10         V11
25% 0.3153952 -0.15558053 -0.19608512
50% 0.3780493 -0.10785485 -0.10690777
75% 0.4859476 -0.03096368 -0.02764269

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 2.194610970  3.114141724 -0.736897632  2.946963322  1.394993209  0.008485636 
          V8           V9          V10          V11 
-0.214787731  0.478876265 -0.003271705 -0.084018133 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6          V7         V8
25% 1.625344 2.128826 -1.8435444 2.351635 0.913734 -0.46379143 -0.5577746
50% 2.211995 3.301702 -0.8529066 2.913957 1.335535  0.01584679 -0.1525317
75% 2.847037 3.998711  0.3364080 3.590948 1.931362  0.42833427  0.1260763
              V9        V10           V11
25% -0.003061458 -0.5678167 -0.4720458335
50%  0.436869942  0.0248965 -0.0006309215
75%  1.138611293  0.4931084  0.2956860410

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0588547958  0.0845352477 -0.0235123957  0.0721126354  0.0453525488 
           V7            V8            V9           V10           V11 
 0.0123683815 -0.0086832111  0.0097308872 -0.0002194434  0.0006461687 

 Quartiles of Marginal Effects:
 
              V2          V3           V4           V5           V6
25% -0.031714867 -0.00428621 -0.063820654 -0.009495936 -0.007161003
50%  0.003986613  0.02124869 -0.003621977  0.032153082  0.019398843
75%  0.095432566  0.13902036  0.034720362  0.131800931  0.104817847
              V7          V8            V9          V10         V11
25% -0.039884343 -0.05483957 -7.665729e-02 -0.055429644 -0.06339373
50% -0.007454417 -0.01290856 -2.408027e-05 -0.001109521  0.00350903
75%  0.037090264  0.03251808  5.687954e-02  0.040758130  0.07523378

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.20319543  0.24097103  0.05041567  0.24499463  0.17735443  0.03886363 
         V8          V9         V10         V11 
-0.06689109  0.01579443  0.02124157 -0.07293601 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5        V6         V7          V8
25% 0.2029804 0.2408413 0.05032640 0.2448730 0.1772549 0.03880243 -0.06703631
50% 0.2032607 0.2410133 0.05047274 0.2450488 0.1773500 0.03890075 -0.06691470
75% 0.2033803 0.2411793 0.05054234 0.2452605 0.1775073 0.03895840 -0.06678582
            V9        V10         V11
25% 0.01568332 0.02114557 -0.07308127
50% 0.01580029 0.02121539 -0.07296202
75% 0.01588476 0.02132471 -0.07279581

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.05304036  2.80756079 -0.65014068  2.51318059  1.08409128 -0.05333824 
         V8          V9         V10         V11 
-0.14494292  0.27407664  0.05760020 -0.05407708 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7         V8
25% 1.493426 1.891707 -1.5170675 1.994938 0.5709462 -0.48235171 -0.6153104
50% 2.035032 2.804093 -0.4926731 2.455416 1.0614392  0.02239993 -0.2037940
75% 2.844615 3.760436  0.2379270 3.040998 1.5893726  0.41978055  0.3830548
            V9         V10         V11
25% -0.2214836 -0.53230791 -0.45856357
50%  0.4442187 -0.02844009 -0.02471793
75%  0.8321992  0.55621483  0.39973644

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.15309585  1.43226144  0.08282135  1.25294946  0.92485804  0.06404508 
         V8          V9         V10         V11 
-0.19527336  0.17710880  0.06995234 -0.25616297 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6         V7         V8
25% 1.152078 1.431641 0.08225852 1.252333 0.9242234 0.06374848 -0.1958070
50% 1.153459 1.432456 0.08296856 1.253195 0.9248307 0.06414786 -0.1953901
75% 1.154032 1.433293 0.08342368 1.254122 0.9256995 0.06460305 -0.1947334
           V9        V10        V11
25% 0.1766735 0.06957599 -0.2567946
50% 0.1771856 0.06975386 -0.2561902
75% 0.1776949 0.07029665 -0.2556218

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.902115113  2.451949400 -0.286437793  2.081670162  1.325360275 -0.121759069 
          V8           V9          V10          V11 
-0.146564543  0.309686130 -0.006768741 -0.131486472 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7          V8
25% 1.673587 2.160167 -0.58999916 1.937535 1.105696 -0.25736417 -0.30356538
50% 1.967325 2.519312 -0.19825068 2.090685 1.316953 -0.10571119 -0.16024674
75% 2.240396 2.784494  0.01566969 2.285577 1.577665  0.04652501 -0.02418981
           V9         V10         V11
25% 0.1548143 -0.20325356 -0.32394234
50% 0.3320446 -0.01091133 -0.11998691
75% 0.6074583  0.19699903  0.08208135

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.99026741  2.68694081 -0.58063435  2.36613347  1.06725683 -0.08015648 
         V8          V9         V10         V11 
-0.12084529  0.23757730  0.05134183 -0.04464622 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7         V8
25% 1.490111 1.849732 -1.3745003 1.883120 0.5578936 -0.41323727 -0.5743729
50% 1.912744 2.590119 -0.4427241 2.304622 0.9942139 -0.07723976 -0.1921114
75% 2.752088 3.625885  0.2221853 2.900517 1.6125271  0.32764729  0.3930027
            V9         V10           V11
25% -0.2137353 -0.50771303 -0.4079429932
50%  0.3491730 -0.01368182  0.0006101309
75%  0.8160486  0.60083108  0.4294585579

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.15122295  3.08168740 -0.83445334  2.94818022  1.15957183  0.05036293 
         V8          V9         V10         V11 
-0.21854246  0.39492510  0.07163302 -0.09125425 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7         V8
25% 1.438597 1.864751 -1.9743266 2.208101 0.5676599 -0.48307131 -0.7941132
50% 2.102877 3.175072 -0.8150319 3.071110 1.0716683  0.04651155 -0.1869284
75% 3.089619 4.260800  0.2944965 3.771004 1.7795833  0.63530958  0.3991268
            V9         V10         V11
25% -0.1246579 -0.59273315 -0.49497924
50%  0.5322986  0.05651809 -0.01064141
75%  1.0598162  0.67541856  0.36387583

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.383505003  0.565757754 -0.040822373  0.380244850  0.224470606 -0.004478161 
          V8           V9          V10          V11 
-0.057918896  0.089328640  0.003458460  0.019375454 

 Quartiles of Marginal Effects:
 
            V2        V3          V4        V5          V6          V7
25% 0.06656633 0.2008260 -0.29094316 0.1282291 -0.01390515 -0.20874387
50% 0.26186655 0.5016123 -0.02496109 0.3653333  0.13312392  0.02785746
75% 0.64932585 0.8492464  0.17631969 0.7614122  0.50176631  0.19682224
             V8          V9         V10         V11
25% -0.34039372 -0.18277343 -0.21143410 -0.19396729
50%  0.02103443  0.02679296 -0.04497617 -0.08145695
75%  0.19830359  0.32087331  0.22635326  0.22563527

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.85195303  2.65871687 -0.17928471  2.13951245  1.22084584 -0.43428287 
         V8          V9         V10         V11 
-0.11525425  0.31100610 -0.08832339  0.32102310 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6         V7          V8
25% 1.663989 2.166122 -0.7431843 1.926614 0.9149985 -0.7172322 -0.33435822
50% 1.823927 2.630624 -0.1117546 2.169764 1.1098954 -0.5314359 -0.06808276
75% 2.172158 3.103325  0.3466580 2.463234 1.5620890 -0.1871937  0.12821799
             V9        V10        V11
25% -0.07446479 -0.3384538 0.05631716
50%  0.38578294 -0.1373938 0.39376405
75%  0.76429522  0.1271716 0.63811376

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 2.2892778  3.3723446 -0.5350995  2.3372952  1.5483553 -0.3136281 -0.4298553 
        V9        V10        V11 
-0.1276859 -0.1259891  0.6415110 

 Quartiles of Marginal Effects:
 
          V2       V3        V4       V5        V6         V7          V8
25% 1.787017 2.270090 -2.344715 1.562662 0.6798691 -0.9665232 -0.97745038
50% 2.375790 3.369550 -0.485545 2.421458 1.4877348 -0.3191748 -0.40393058
75% 2.922554 4.626527  1.131676 3.311217 2.3176224  0.2759046  0.09477846
            V9        V10       V11
25% -1.2115513 -0.6891209 0.2292634
50% -0.1529895 -0.0972370 0.7400618
75%  0.9235866  0.5360485 1.1497618

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.66089879  0.86415380  0.10024187  0.86470969  0.43060084  0.05344649 
         V8          V9         V10         V11 
-0.04018061  0.26003035 -0.12916811 -0.04996124 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5        V6         V7          V8
25% 0.6602313 0.8631831 0.09974467 0.8634700 0.4296977 0.05255510 -0.04098315
50% 0.6609603 0.8643382 0.10005424 0.8652707 0.4308078 0.05357500 -0.04011455
75% 0.6620241 0.8656298 0.10081209 0.8661106 0.4318166 0.05407618 -0.03938763
           V9        V10         V11
25% 0.2594874 -0.1299324 -0.05094471
50% 0.2601992 -0.1292948 -0.05007774
75% 0.2606847 -0.1286598 -0.04919741

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.239351027  0.357801186 -0.020611626  0.231636756  0.139762382  0.001246971 
          V8           V9          V10          V11 
-0.040941472  0.058119467  0.006313188  0.022045968 

 Quartiles of Marginal Effects:
 
            V2         V3           V4         V5          V6          V7
25% 0.01439566 0.09173265 -0.188397155 0.04520687 -0.04151508 -0.14068793
50% 0.14260053 0.29459983 -0.003071684 0.22693046  0.05699783  0.01383368
75% 0.40948507 0.56459214  0.138271147 0.45840848  0.33294968  0.14452796
              V8          V9         V10          V11
25% -0.232738821 -0.10832100 -0.16190602 -0.147052884
50%  0.001779848  0.01300947 -0.01442232 -0.002643706
75%  0.160600967  0.22422029  0.14080151  0.158860824

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 2.1618193  3.0919643 -0.2454083  2.3015688  1.4219617 -0.4456132 -0.2099296 
        V9        V10        V11 
 0.1698344 -0.1581830  0.4765123 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7         V8
25% 1.763773 2.157056 -1.2948424 1.749591 0.8152832 -0.94225479 -0.5170374
50% 2.097653 3.182723 -0.3143590 2.318214 1.3851867 -0.43987464 -0.1111569
75% 2.627418 3.847877  0.9507923 2.943483 2.0376070 -0.03635748  0.1115799
            V9        V10       V11
25% -0.4273779 -0.5348346 0.1338649
50%  0.1653793 -0.2612599 0.5736889
75%  0.8941933  0.3070321 0.8242534

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.025289172  0.042907784  0.002926458  0.016143148  0.015938337 -0.006520447 
          V8           V9          V10          V11 
-0.006192660  0.004898301  0.007711204  0.011816475 

 Quartiles of Marginal Effects:
 
              V2           V3            V4           V5            V6
25% -0.007340407 -0.002643745 -0.0187924893 -0.006875528 -0.0085374206
50%  0.005624057  0.011970083  0.0003151714  0.007037250  0.0002628694
75%  0.033748072  0.068446415  0.0077029098  0.030165325  0.0282811052
              V7            V8           V9          V10          V11
25% -0.008307317 -0.0178100494 -0.014396848 -0.023001266 -0.007453260
50%  0.001372787  0.0002459403  0.002713152 -0.001443991  0.002892218
75%  0.012693368  0.0172783452  0.021987982  0.015701419  0.019421869

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 1.6958161  2.5256678 -0.2134762  1.9705179  1.1462130 -0.3911103 -0.1215889 
        V9        V10        V11 
 0.2489876 -0.0315300  0.2696245 

 Quartiles of Marginal Effects:
 
          V2       V3           V4       V5        V6           V7          V8
25% 1.388926 2.000296 -0.855656881 1.619716 0.7703044 -0.740481167 -0.40968689
50% 1.678285 2.387372 -0.001233344 2.027551 1.0535671 -0.478964289 -0.02909819
75% 2.086578 3.059334  0.382978402 2.448438 1.6629478 -0.004790732  0.16363994
            V9        V10         V11
25% -0.2641961 -0.3438520 -0.05302585
50%  0.3504966 -0.1342099  0.27543265
75%  0.8203707  0.2561186  0.59907111

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.685673542  2.233980042  0.009410985  2.062325311  1.088705638 -0.341007495 
          V8           V9          V10          V11 
-0.088306377  0.414725280 -0.261213198  0.239933653 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7          V8
25% 1.680562 2.227328 0.003912982 2.057364 1.082799 -0.3458350 -0.09189474
50% 1.686066 2.233558 0.009743161 2.062938 1.088234 -0.3422262 -0.08702222
75% 1.691169 2.241105 0.013587447 2.069631 1.096500 -0.3375480 -0.08481798
           V9        V10       V11
25% 0.4082748 -0.2657250 0.2347792
50% 0.4155900 -0.2616277 0.2405750
75% 0.4223184 -0.2560336 0.2459536

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.17527071  1.55318117  0.08236971  1.48021679  0.76144589 -0.07743958 
         V8          V9         V10         V11 
-0.06850523  0.38244656 -0.19925875  0.04206919 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7          V8
25% 1.174491 1.552375 0.08184114 1.479300 0.7605354 -0.07803965 -0.06902882
50% 1.175292 1.552989 0.08221116 1.480591 0.7615332 -0.07745635 -0.06848613
75% 1.176219 1.554250 0.08289436 1.481190 0.7624269 -0.07707482 -0.06791176
           V9        V10        V11
25% 0.3819665 -0.1997761 0.04142034
50% 0.3825358 -0.1993635 0.04226023
75% 0.3830920 -0.1987026 0.04260227

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.09983365  2.79888192 -0.10211422  2.47438212  1.34777143 -0.58559903 
         V8          V9         V10         V11 
-0.09812335  0.39014264 -0.26694603  0.44953862 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7          V8
25% 2.013289 2.692023 -0.26950742 2.412180 1.238354 -0.6757459 -0.18319073
50% 2.112291 2.790273 -0.10512558 2.491175 1.342410 -0.5862072 -0.09202895
75% 2.187820 2.885647  0.05242448 2.559475 1.446488 -0.5247654 -0.01697072
           V9        V10       V11
25% 0.2427006 -0.3504134 0.3676246
50% 0.4045246 -0.2767655 0.4610350
75% 0.5430793 -0.1847330 0.5386455

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.06212713  2.75170648 -0.09571921  2.43623350  1.32466314 -0.56428808 
         V8          V9         V10         V11 
-0.09763630  0.39204461 -0.26119935  0.42863081 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7          V8
25% 1.980200 2.652847 -0.25337045 2.385309 1.224407 -0.6554374 -0.18103241
50% 2.071897 2.738048 -0.09482913 2.448741 1.318227 -0.5607796 -0.08752126
75% 2.152804 2.838593  0.04737539 2.507832 1.424977 -0.5036765 -0.02279555
           V9        V10       V11
25% 0.2502647 -0.3359456 0.3462632
50% 0.4035906 -0.2657656 0.4402552
75% 0.5428446 -0.1827237 0.5185106

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.14747301  3.15750858 -0.33664257  2.29858501  1.45807969 -0.42268794 
         V8          V9         V10         V11 
-0.24568594  0.06783903 -0.12510393  0.51079181 

 Quartiles of Marginal Effects:
 
          V2       V3        V4       V5       V6          V7          V8
25% 1.706835 2.125591 -1.552313 1.674681 0.741651 -0.97145867 -0.59230846
50% 2.128732 3.172632 -0.366101 2.382349 1.418128 -0.41821577 -0.16821458
75% 2.700638 4.039829  1.033619 3.026012 2.169783  0.01810041  0.08469741
            V9        V10       V11
25% -0.5940168 -0.5246243 0.1376590
50%  0.2695876 -0.1520923 0.6061179
75%  0.8726509  0.2806135 0.8913988

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0506582798  0.0812542965  0.0005602859  0.0394055423  0.0314917110 
           V7            V8            V9           V10           V11 
-0.0055582136 -0.0111475662  0.0110943561  0.0098879166  0.0177460952 

 Quartiles of Marginal Effects:
 
             V2            V3            V4           V5           V6
25% -0.01227815 -0.0001088572 -0.0471446319 -0.004454678 -0.026519788
50%  0.01685759  0.0305899004 -0.0002252882  0.021850946  0.001621557
75%  0.07231183  0.1333876068  0.0215639964  0.077145532  0.078230843
              V7           V8           V9          V10          V11
25% -0.024671751 -0.043262740 -0.032420518 -0.040519853 -0.022986566
50%  0.006625951 -0.004588679  0.004202389 -0.005155672  0.009087655
75%  0.028061548  0.040983961  0.044295956  0.037306516  0.043254506

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.17765927  0.22860034  0.04373706  0.24328952  0.11633589  0.03996512 
         V8          V9         V10         V11 
-0.01078864  0.07895672 -0.04144151 -0.03208390 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5        V6         V7          V8
25% 0.1775465 0.2284469 0.04368148 0.2431089 0.1161901 0.03983505 -0.01090226
50% 0.1776837 0.2286354 0.04373477 0.2433525 0.1163605 0.03998079 -0.01076723
75% 0.1778198 0.2288008 0.04384636 0.2435050 0.1164990 0.04009386 -0.01066072
            V9         V10         V11
25% 0.07886601 -0.04154259 -0.03222482
50% 0.07898147 -0.04145170 -0.03210679
75% 0.07905065 -0.04135911 -0.03196610

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.90839018  3.02294095 -0.51241097  2.22226032  1.41685929 -0.37572259 
         V8          V9         V10         V11 
-0.21031245  0.05899966  0.01670357  0.48275354 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7          V8
25% 1.573302 2.184869 -1.7723268 1.870278 0.5987881 -0.90741890 -0.61702286
50% 1.971318 2.941521 -0.3434886 2.335385 1.4110467 -0.46618465 -0.05334245
75% 2.498006 3.567427  0.7147246 2.795258 2.1223036  0.07397967  0.20896179
            V9         V10        V11
25% -0.6308255 -0.54794116 0.07399297
50%  0.2392209 -0.03264683 0.52535016
75%  0.8142094  0.33615579 0.89517405

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.025910260  1.352744627  0.094270949  1.305159489  0.665679295 -0.024695626 
          V8           V9          V10          V11 
-0.060903205  0.355519080 -0.180209017  0.003766791 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6          V7          V8
25% 1.025157 1.351887 0.09384642 1.304262 0.6648987 -0.02534885 -0.06147432
50% 1.025935 1.352547 0.09404537 1.305564 0.6658030 -0.02464839 -0.06085930
75% 1.026828 1.353851 0.09475761 1.306175 0.6666205 -0.02432552 -0.06029981
           V9        V10         V11
25% 0.3550826 -0.1807530 0.003071755
50% 0.3556607 -0.1803259 0.003882827
75% 0.3561656 -0.1797032 0.004299129

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.67538083  2.36510431 -0.12495617  1.96741057  1.09399244 -0.37022691 
         V8          V9         V10         V11 
-0.09878951  0.33612562 -0.09763248  0.23375117 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5        V6         V7          V8
25% 1.492757 2.009400 -0.53176904 1.766428 0.8257888 -0.6366502 -0.28790919
50% 1.660142 2.280868 -0.07369013 1.985323 1.0161542 -0.4095447 -0.05766505
75% 1.965411 2.717305  0.22298497 2.288194 1.4648028 -0.1591764  0.07992704
              V9        V10          V11
25% -0.005861183 -0.3077622 -0.004208347
50%  0.404166461 -0.1410899  0.277691289
75%  0.748255559  0.1125313  0.484327140

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.81339753  2.86644566 -0.44769493  2.11514692  1.33294158 -0.37565838 
         V8          V9         V10         V11 
-0.18270184  0.09929522  0.02255979  0.41301009 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6         V7          V8
25% 1.381893 2.115106 -1.6592020 1.736873 0.6615001 -0.9117695 -0.58175019
50% 1.864696 2.782007 -0.2313175 2.189017 1.2409703 -0.5067086 -0.08982936
75% 2.317178 3.441649  0.6120672 2.755896 1.9633893  0.1067185  0.29191445
            V9         V10          V11
25% -0.5272349 -0.58309952 0.0007366184
50%  0.2667837 -0.04153309 0.4531518909
75%  0.8588137  0.29498583 0.7963058578

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.16823859  3.33340365 -0.59585366  2.37182601  1.55417144 -0.34243982 
         V8          V9         V10         V11 
-0.34497090 -0.07303518 -0.05621463  0.63151489 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5        V6         V7         V8
25% 1.737559 2.389247 -2.3648749 1.764161 0.6634876 -1.0267133 -0.7776644
50% 2.329239 3.207058 -0.6305355 2.417285 1.5317666 -0.3479551 -0.2655457
75% 2.721283 4.306713  0.9340982 3.115064 2.2542473  0.2160806  0.1930767
            V9         V10       V11
25% -0.9378037 -0.59663299 0.1298829
50%  0.1648475 -0.06326202 0.7623661
75%  0.8116077  0.53074788 1.0785931

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.317569674  0.495897950 -0.052626227  0.388428163  0.389657439  0.021473243 
          V8           V9          V10          V11 
-0.154231059 -0.007849529  0.008286719 -0.056192294 

 Quartiles of Marginal Effects:
 
             V2         V3          V4           V5         V6          V7
25% -0.04270849 0.08676718 -0.23530431 0.0009861016 0.04625313 -0.13894353
50%  0.15673634 0.33025767 -0.03994713 0.2336999294 0.24671324 -0.01557242
75%  0.58412750 0.98537516  0.22835742 0.7732702603 0.62071982  0.21093370
             V8          V9         V10         V11
25% -0.47964348 -0.27977707 -0.20291479 -0.24028180
50% -0.06611229 -0.02865437 -0.02387789 -0.01209659
75%  0.20127858  0.20613785  0.24352025  0.25527677

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 1.6669665  2.5249894 -0.1098814  1.9856463  1.6301465 -0.2331350 -0.4855904 
        V9        V10        V11 
 0.1331487  0.1071573 -0.1208134 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7         V8
25% 1.349810 2.046740 -0.77623429 1.721121 1.294837 -0.44153953 -0.7514026
50% 1.659704 2.608307 -0.05624085 1.921808 1.665864 -0.26307662 -0.5600848
75% 1.899969 2.866568  0.39387962 2.291909 2.000433 -0.02187705 -0.2478583
             V9         V10         V11
25% -0.15677342 -0.08341765 -0.44031244
50%  0.08182396  0.12832551 -0.03458141
75%  0.46799262  0.29973018  0.19774477

 Average Marginal Effects:
 
        V2         V3         V4         V5         V6         V7         V8 
 2.1913658  2.7633894  0.1043662  2.0896746  1.9133188 -0.1430083 -0.4918517 
        V9        V10        V11 
 0.2143924  0.2339374  0.2233583 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6          V7          V8
25% 1.883679 2.193970 -1.0216150 1.354608 1.118366 -0.60297747 -1.07518163
50% 2.097077 2.652878  0.2236338 1.949862 2.018306 -0.09227173 -0.58664638
75% 2.457385 3.403260  0.9445453 2.754719 2.698626  0.37870946 -0.04516589
            V9        V10        V11
25% -0.3065236 -0.3080158 -0.4201394
50%  0.2303464  0.1984954  0.0929984
75%  0.6131624  0.7919262  0.7918935

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.607419456  0.971035461  0.001006758  0.783600733  0.642200035  0.072051203 
          V8           V9          V10          V11 
-0.162273931 -0.023901467 -0.089879332 -0.292628799 

 Quartiles of Marginal Effects:
 
           V2        V3           V4        V5        V6         V7         V8
25% 0.6063085 0.9693614 0.0001234959 0.7822092 0.6412724 0.07122406 -0.1632359
50% 0.6077113 0.9711714 0.0008590384 0.7835962 0.6425068 0.07185062 -0.1625428
75% 0.6085083 0.9727525 0.0017899671 0.7850516 0.6433727 0.07247805 -0.1612437
             V9         V10        V11
25% -0.02469225 -0.09067599 -0.2935499
50% -0.02377103 -0.09015388 -0.2927384
75% -0.02306252 -0.08886224 -0.2919012

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.18959458  0.32214525 -0.03150460  0.25311833  0.24738283  0.02672858 
         V8          V9         V10         V11 
-0.10271345 -0.01256484  0.00806770 -0.03098214 

 Quartiles of Marginal Effects:
 
             V2         V3          V4          V5          V6            V7
25% -0.07340366 0.04473827 -0.14954234 -0.03698767 -0.01551333 -0.0994658761
50%  0.08523700 0.16946464 -0.01004869  0.13163993  0.13149372 -0.0002539802
75%  0.35451536 0.64212402  0.16285292  0.48934155  0.39835815  0.1525675775
             V8          V9         V10         V11
25% -0.31591064 -0.20088472 -0.14252652 -0.17370088
50% -0.04444502 -0.03674256 -0.02424063  0.02980082
75%  0.14123548  0.14086732  0.16025832  0.21638470

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.98343217  2.80528281  0.02339985  2.20629032  1.85779914 -0.16328126 
         V8          V9         V10         V11 
-0.55673066  0.19869470  0.23604919  0.05536485 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7         V8
25% 1.574297 2.417814 -0.90173295 1.829431 1.332142 -0.51000383 -0.8624494
50% 2.028841 2.810158  0.01941911 2.125779 1.844086 -0.09539636 -0.6100452
75% 2.224214 3.311960  0.71138537 2.709001 2.426151  0.14847203 -0.2423696
            V9        V10         V11
25% -0.1835967 -0.1586591 -0.44160691
50%  0.1243974  0.2760682  0.06835532
75%  0.5710305  0.5791675  0.52767824

 Average Marginal Effects:
 
           V2            V3            V4            V5            V6 
 0.0169553120  0.0362783742  0.0006321560  0.0222281144  0.0224365534 
           V7            V8            V9           V10           V11 
-0.0004037281 -0.0108583677 -0.0070401990  0.0052714400  0.0027670472 

 Quartiles of Marginal Effects:
 
              V2           V3           V4           V5           V6
25% -0.012255097 -0.002773209 -0.018654949 -0.006997251 -0.002355279
50%  0.002585981  0.003489372  0.001467102  0.005668282  0.004423714
75%  0.034938637  0.055233325  0.015181817  0.036866366  0.035291239
              V7            V8           V9           V10          V11
25% -0.010949999 -0.0281595436 -0.035148103 -0.0199031057 -0.019272180
50%  0.002322789 -0.0009252889 -0.001934681  0.0001538581  0.007567733
75%  0.019005139  0.0130869338  0.021409723  0.0155113593  0.026667456

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.56988039  2.29153893 -0.11186608  1.76486151  1.51994675 -0.23943454 
         V8          V9         V10         V11 
-0.46126107  0.12432557  0.10720826 -0.09855621 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7         V8
25% 1.180870 1.787255 -0.82881169 1.373662 1.090783 -0.47227466 -0.8525616
50% 1.565990 2.392414 -0.04967405 1.698622 1.595685 -0.29506534 -0.5748411
75% 1.935328 2.691209  0.46366566 2.039940 1.914266 -0.05047043 -0.1588776
             V9         V10         V11
25% -0.21706031 -0.21503082 -0.42037518
50%  0.08270753  0.09583695 -0.01882215
75%  0.47514623  0.42725686  0.26667984

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.43471797  2.30337288 -0.11656508  1.92272286  1.50535738 -0.08625426 
         V8          V9         V10         V11 
-0.33690598  0.03979287 -0.12115110 -0.33711649 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6          V7         V8
25% 1.429161 2.294892 -0.1262456 1.918218 1.498625 -0.09093520 -0.3414903
50% 1.434827 2.304613 -0.1167190 1.922879 1.506670 -0.08643549 -0.3381607
75% 1.440633 2.310306 -0.1084839 1.928205 1.511598 -0.08214515 -0.3333431
            V9        V10        V11
25% 0.03316334 -0.1272007 -0.3426384
50% 0.03764031 -0.1207290 -0.3368859
75% 0.04677017 -0.1160378 -0.3313527

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 1.029180321  1.675434842 -0.048421925  1.362470163  1.110420977  0.037118042 
          V8           V9          V10          V11 
-0.260264646 -0.006055109 -0.125248802 -0.382038421 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7         V8
25% 1.028232 1.674344 -0.04931569 1.361501 1.109488 0.03646955 -0.2608695
50% 1.029335 1.675376 -0.04856620 1.362481 1.110558 0.03708143 -0.2604994
75% 1.029958 1.676655 -0.04767326 1.363409 1.111210 0.03746248 -0.2595919
              V9        V10        V11
25% -0.006571012 -0.1259502 -0.3825633
50% -0.006014833 -0.1253728 -0.3821349
75% -0.005168533 -0.1245700 -0.3813794

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.79328362  2.79924514 -0.16328214  2.36287748  1.76147340 -0.24953876 
         V8          V9         V10         V11 
-0.43866665  0.12220801 -0.01477562 -0.16037689 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7         V8
25% 1.689082 2.684075 -0.40688129 2.263119 1.670576 -0.3255861 -0.5250643
50% 1.781518 2.814098 -0.15525217 2.363149 1.769115 -0.2408462 -0.4306121
75% 1.877915 2.891846 -0.03027053 2.448470 1.849130 -0.1629998 -0.3653400
             V9         V10         V11
25% -0.02821885 -0.09070101 -0.26431898
50%  0.10081066 -0.01242036 -0.12590492
75%  0.25983367  0.07597207 -0.05887233

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.76065019  2.75853588 -0.16067448  2.32145815  1.74100647 -0.23844098 
         V8          V9         V10         V11 
-0.43220640  0.11546701 -0.02078847 -0.17788758 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6         V7         V8
25% 1.671826 2.640212 -0.39233388 2.227837 1.642610 -0.3144404 -0.5170211
50% 1.753723 2.782646 -0.15127913 2.320621 1.746262 -0.2248789 -0.4306842
75% 1.846781 2.856778 -0.02981315 2.412853 1.834210 -0.1609759 -0.3611861
             V9         V10         V11
25% -0.02911226 -0.09523919 -0.27653508
50%  0.09219829 -0.01634172 -0.15234437
75%  0.23976331  0.06382021 -0.07343014

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.01585707  2.74777028  0.04331733  2.12304025  1.84348404 -0.16809700 
         V8          V9         V10         V11 
-0.53867175  0.19973370  0.23327295  0.10653778 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7         V8
25% 1.634445 2.296994 -0.9947500 1.597381 1.271097 -0.5193258 -0.9340599
50% 2.004292 2.756256  0.1005101 1.937140 1.901315 -0.1913667 -0.6085695
75% 2.250116 3.295891  0.8362529 2.725994 2.500157  0.1900189 -0.0970110
            V9        V10         V11
25% -0.2214716 -0.3204695 -0.45078945
50%  0.1909314  0.2845117  0.09815961
75%  0.5901703  0.6327610  0.58455419

 Average Marginal Effects:
 
          V2           V3           V4           V5           V6           V7 
 0.035184773  0.071512590 -0.003062332  0.051202684  0.048667543  0.005149669 
          V8           V9          V10          V11 
-0.022088417 -0.010511986  0.006657502  0.002095737 

 Quartiles of Marginal Effects:
 
             V2           V3           V4          V5           V6           V7
25% -0.03117588 -0.004009751 -0.042846360 -0.01806848 -0.006481319 -0.021321400
50%  0.01243237  0.014004806  0.009914837  0.01410380  0.009546223  0.006113765
75%  0.08030082  0.124918231  0.037116260  0.08589700  0.074470572  0.044032193
              V8           V9           V10         V11
25% -0.066958666 -0.074100961 -0.0374630945 -0.04238434
50% -0.004124991 -0.000377811  0.0001890914  0.01747106
75%  0.028223962  0.046789362  0.0320507565  0.04807174

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.17228653  0.26484418  0.01029897  0.21574660  0.17201512  0.03106664 
         V8          V9         V10         V11 
-0.04828272 -0.01291563 -0.02866047 -0.09765248 

 Quartiles of Marginal Effects:
 
           V2        V3         V4        V5        V6         V7          V8
25% 0.1721071 0.2645908 0.01018450 0.2155618 0.1718834 0.03096166 -0.04841722
50% 0.1723156 0.2648612 0.01029396 0.2157476 0.1720512 0.03103036 -0.04831987
75% 0.1724444 0.2650785 0.01037225 0.2159693 0.1721755 0.03112345 -0.04813375
             V9         V10         V11
25% -0.01303635 -0.02879117 -0.09782806
50% -0.01291290 -0.02870069 -0.09767696
75% -0.01279560 -0.02853985 -0.09754227

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.92543729  2.40578844 -0.07570161  1.76872223  1.65965489 -0.19395521 
         V8          V9         V10         V11 
-0.46286103  0.13722172  0.17512663  0.13211228 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5        V6         V7          V8
25% 1.468007 1.981338 -1.09732945 1.251940 0.9310531 -0.6438683 -1.08155044
50% 1.862492 2.401798  0.01627026 1.699503 1.8725569 -0.2878388 -0.50908553
75% 2.342459 2.942221  0.81515018 2.319673 2.3820083  0.2751270  0.01228982
            V9        V10        V11
25% -0.3756869 -0.4086332 -0.4769520
50%  0.1558118  0.2273190  0.1559128
75%  0.6819218  0.6121799  0.5989326

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 0.90974644  1.47757818 -0.03120948  1.19657294  0.98028166  0.05603279 
         V8          V9         V10         V11 
-0.23409257 -0.01399896 -0.11854328 -0.36917631 

 Quartiles of Marginal Effects:
 
           V2       V3          V4       V5        V6         V7         V8
25% 0.9088188 1.476367 -0.03195997 1.195549 0.9794453 0.05544325 -0.2347390
50% 0.9099426 1.477611 -0.03137149 1.196546 0.9804158 0.05594937 -0.2343328
75% 0.9104931 1.478856 -0.03050410 1.197544 0.9810904 0.05642335 -0.2333703
             V9        V10        V11
25% -0.01461240 -0.1192596 -0.3697556
50% -0.01398379 -0.1186549 -0.3693257
75% -0.01316397 -0.1178179 -0.3685668

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.49115703  2.31658847 -0.12313332  1.83055075  1.50861817 -0.20210579 
         V8          V9         V10         V11 
-0.43076948  0.09950104  0.04809849 -0.20823800 

 Quartiles of Marginal Effects:
 
          V2       V3          V4       V5       V6          V7         V8
25% 1.254327 1.915031 -0.60789450 1.543417 1.223927 -0.37919008 -0.6657935
50% 1.495225 2.407043 -0.01916461 1.797166 1.542694 -0.23837699 -0.5178971
75% 1.726570 2.607460  0.20273275 2.035745 1.780493 -0.05387449 -0.2393501
             V9         V10         V11
25% -0.13471541 -0.13948984 -0.43705280
50%  0.04842915  0.07051339 -0.16689299
75%  0.37823151  0.26848983  0.07260085

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.80855839  2.30032447 -0.10184141  1.69921610  1.58832781 -0.21361394 
         V8          V9         V10         V11 
-0.45634749  0.13035279  0.15657725  0.08892496 

 Quartiles of Marginal Effects:
 
          V2       V3            V4       V5        V6         V7           V8
25% 1.377988 1.853246 -1.1591643113 1.237901 0.9464508 -0.6102726 -1.041739408
50% 1.758335 2.413152  0.0006330883 1.575369 1.7862336 -0.3077564 -0.499883526
75% 2.238331 2.715470  0.7762896621 2.249439 2.2638994  0.1840891 -0.009188809
            V9        V10        V11
25% -0.3193894 -0.3913864 -0.3715619
50%  0.1556630  0.1824340  0.1714331
75%  0.6759261  0.6750415  0.5498540

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 2.15455606  2.67309781  0.04370447  1.97843344  1.84792776 -0.14469847 
         V8          V9         V10         V11 
-0.48185274  0.17825027  0.21003288  0.20672433 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7          V8
25% 1.812529 2.034708 -1.2085346 1.286791 1.026463 -0.5753973 -1.01696505
50% 2.100023 2.583438  0.1978152 1.858751 2.108709 -0.1518792 -0.51851990
75% 2.574486 3.342099  1.0087801 2.656905 2.666267  0.3343114 -0.01427425
            V9        V10        V11
25% -0.3998262 -0.2478697 -0.5565525
50%  0.2013314  0.2574641  0.1802334
75%  0.6344487  0.7506850  0.7689000

 Average Marginal Effects:
 
         V2          V3          V4          V5          V6          V7 
 1.97027789  2.80395914 -0.27125731  2.20025632  1.48659197 -0.32959979 
         V8          V9         V10         V11 
-0.31959221  0.23850880  0.03580386  0.12573140 

 Quartiles of Marginal Effects:
 
          V2       V3         V4       V5       V6         V7          V8
25% 1.695561 2.307813 -1.0693189 1.952731 1.179796 -0.5601271 -0.60608934
50% 1.992296 2.804682 -0.1936514 2.269718 1.506743 -0.3302491 -0.34787233
75% 2.288563 3.333210  0.3904606 2.577050 1.869199 -0.1421319 -0.02619807
             V9         V10        V11
25% -0.05694347 -0.26012117 -0.1112349
50%  0.29023270  0.01261489  0.1915637
75%  0.59633417  0.34553399  0.3883058
Radial Basis Function Kernel Regularized Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        sigma        RMSE      Rsquared   MAE       Selected
  3.398331e-05     3.252680  4.282474  0.4550149  3.508017          
  4.908678e-05   970.340184  2.333681  0.7943392  1.860642          
  6.869763e-05  4162.791485  2.354944  0.7837961  1.882850          
  1.388171e-04   155.616545  2.495025  0.7753726  1.897380          
  1.700434e-04    63.079210  2.438920  0.7823321  1.876566          
  4.205922e-04    23.492104  2.448505  0.7800237  1.915440          
  5.295414e-04     1.585900  4.893214  0.2456523  4.085365          
  1.226861e-03   134.732467  2.344149  0.7925868  1.846553          
  1.706586e-03   847.306707  2.357431  0.7832920  1.879174          
  4.649139e-03    18.923428  2.490729  0.7742559  1.945309          
  6.163082e-03     1.309916  4.942683  0.1891312  4.136881          
  1.139410e-02  6033.883856  3.004467  0.7524249  2.409848          
  1.227144e-02  1678.590965  2.554475  0.7618017  1.958433          
  1.847174e-02  4997.997663  3.189387  0.7492391  2.583196          
  2.146598e-02     2.688397  4.523124  0.4031955  3.725630          
  6.302247e-02    42.673624  2.325433  0.7970148  1.814422  *       
  8.405619e-02  2332.719750  3.730206  0.7394496  3.084269          
  1.012562e-01    21.919521  2.444835  0.7836748  1.884270          
  1.791958e-01    32.151738  2.431892  0.7888785  1.858294          
  2.364973e-01  4295.595018  4.613716  0.7205631  3.857241          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.06302247 and sigma
 = 42.67362.
[1] "Mon Mar 12 15:43:03 2018"
Least Angle Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  fraction   RMSE      Rsquared   MAE       Selected
  0.1062531  4.474355  0.4668666  3.754426          
  0.1381929  4.335014  0.5344411  3.640164          
  0.1673884  4.211066  0.5921821  3.532891          
  0.2284886  3.959377  0.6627683  3.308392          
  0.2461119  3.888940  0.6750944  3.243638          
  0.3247722  3.577101  0.7179272  2.953740          
  0.3447800  3.495909  0.7290139  2.880661          
  0.4177590  3.212676  0.7565032  2.621026          
  0.4464257  3.107775  0.7634581  2.524868          
  0.5334745  2.819743  0.7767482  2.247010          
  0.5579596  2.753822  0.7787883  2.176983          
  0.6113360  2.624083  0.7819971  2.038005          
  0.6177791  2.609817  0.7822904  2.022066          
  0.6533015  2.537186  0.7835930  1.936386          
  0.6663501  2.515610  0.7836394  1.914377          
  0.7598991  2.439736  0.7779685  1.896627          
  0.7849139  2.436305  0.7754554  1.908931          
  0.8010843  2.436547  0.7737151  1.921305          
  0.8506656  2.434737  0.7704434  1.953657          
  0.8747652  2.434006  0.7692318  1.962285  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.8747652.
[1] "Mon Mar 12 15:43:17 2018"
Least Angle Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  step  RMSE      Rsquared   MAE       Selected
  2     4.849112  0.2243442  4.055054          
  3     4.469735  0.4525865  3.757937          
  4     4.165801  0.5327032  3.460556          
  5     2.610721  0.7757162  2.030217          
  6     2.471437  0.7823540  1.926037          
  7     2.447505  0.7786782  1.920268          
  8     2.434913  0.7787144  1.920193  *       
  9     2.441677  0.7662001  1.973877          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was step = 8.
[1] "Mon Mar 12 15:43:31 2018"
The lasso 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  fraction   RMSE      Rsquared   MAE       Selected
  0.1062531  4.474355  0.4668666  3.754426          
  0.1381929  4.335014  0.5344411  3.640164          
  0.1673884  4.211066  0.5921821  3.532891          
  0.2284886  3.959377  0.6627683  3.308392          
  0.2461119  3.888940  0.6750944  3.243638          
  0.3247722  3.577101  0.7179272  2.953740          
  0.3447800  3.495909  0.7290139  2.880661          
  0.4177590  3.212676  0.7565032  2.621026          
  0.4464257  3.107775  0.7634581  2.524868          
  0.5334745  2.819743  0.7767482  2.247010          
  0.5579596  2.753822  0.7787883  2.176983          
  0.6113360  2.624083  0.7819971  2.038005          
  0.6177791  2.609817  0.7822904  2.022066          
  0.6533015  2.537186  0.7835930  1.936386          
  0.6663501  2.515610  0.7836394  1.914377          
  0.7598991  2.439736  0.7779685  1.896627          
  0.7849139  2.436305  0.7754554  1.908931          
  0.8010843  2.436547  0.7737151  1.921305          
  0.8506656  2.434737  0.7704434  1.953657          
  0.8747652  2.434006  0.7692318  1.962285  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was fraction = 0.8747652.
[1] "Mon Mar 12 15:43:46 2018"
Linear Regression with Backwards Selection 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nvmax  RMSE      Rsquared   MAE       Selected
  2      3.512991  0.5042993  2.827049          
  3      2.882954  0.6821255  2.309500          
  4      2.328541  0.7873506  1.854937  *       
  5      2.381618  0.7764826  1.974352          
  6      2.389443  0.7734789  1.929682          
  7      2.426427  0.7680502  1.966955          
  8      2.457804  0.7614341  1.987610          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 4.
[1] "Mon Mar 12 15:44:00 2018"
Linear Regression with Forward Selection 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nvmax  RMSE      Rsquared   MAE       Selected
  2      3.512991  0.5042993  2.827049          
  3      2.882954  0.6821255  2.309500          
  4      2.328541  0.7873506  1.854937  *       
  5      2.381618  0.7764826  1.974352          
  6      2.389443  0.7734789  1.929682          
  7      2.426427  0.7680502  1.966955          
  8      2.457804  0.7614341  1.987610          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nvmax = 4.
[1] "Mon Mar 12 15:44:13 2018"
Linear Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE      Rsquared   MAE     
  2.437903  0.7650326  1.987049

Tuning parameter 'intercept' was held constant at a value of TRUE
[1] "Mon Mar 12 15:44:28 2018"
Start:  AIC=89.02
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS     AIC
- V11   1      0.37 191.38  87.112
- V10   1      0.46 191.47  87.135
- V8    1      1.55 192.56  87.420
- V7    1      2.00 193.01  87.537
- V4    1      2.01 193.02  87.539
- V9    1      5.24 196.26  88.370
<none>              191.01  89.016
- V6    1    143.21 334.22 114.989
- V2    1    173.82 364.83 119.371
- V5    1    181.46 372.47 120.407
- V3    1    365.88 556.89 140.517

Step:  AIC=87.11
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS     AIC
- V10   1      0.60 191.98  85.268
- V8    1      2.28 193.66  85.704
- V4    1      2.29 193.67  85.706
- V7    1      2.88 194.26  85.860
- V9    1      4.88 196.26  86.370
<none>              191.38  87.112
- V6    1    150.64 342.02 114.143
- V2    1    176.14 367.52 117.737
- V5    1    194.77 386.15 120.210
- V3    1    370.23 561.61 138.939

Step:  AIC=85.27
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq    RSS     AIC
- V4    1      1.84 193.82  83.745
- V8    1      2.02 193.99  83.790
- V7    1      2.91 194.89  84.021
- V9    1      5.43 197.40  84.661
<none>              191.98  85.268
- V6    1    150.65 342.62 112.230
- V2    1    178.50 370.47 116.138
- V5    1    202.51 394.49 119.278
- V3    1    370.33 562.30 137.001

Step:  AIC=83.75
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq    RSS     AIC
- V8    1      1.30 195.12  82.079
- V7    1      3.12 196.94  82.544
- V9    1      5.30 199.12  83.093
<none>              193.82  83.745
- V6    1    150.18 344.00 110.431
- V2    1    177.09 370.91 114.197
- V5    1    205.56 399.38 117.895
- V3    1    377.94 571.76 135.835

Step:  AIC=82.08
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9

       Df Sum of Sq    RSS     AIC
- V7    1      2.72 197.84  80.771
- V9    1      5.14 200.26  81.380
<none>              195.12  82.079
- V6    1    154.21 349.33 109.200
- V2    1    193.71 388.82 114.555
- V5    1    207.94 403.06 116.353
- V3    1    376.74 571.86 133.844

Step:  AIC=80.77
.outcome ~ V2 + V3 + V5 + V6 + V9

       Df Sum of Sq    RSS     AIC
- V9    1      6.45 204.29  80.376
<none>              197.84  80.771
- V6    1    154.13 351.97 107.576
- V2    1    192.08 389.92 112.696
- V5    1    207.16 404.99 114.592
- V3    1    375.49 573.33 131.972

Step:  AIC=80.38
.outcome ~ V2 + V3 + V5 + V6

       Df Sum of Sq    RSS     AIC
<none>              204.29  80.376
- V6    1    164.78 369.07 107.948
- V5    1    200.97 405.26 112.626
- V2    1    209.59 413.88 113.678
- V3    1    369.73 574.02 130.032
Start:  AIC=106.53
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS    AIC
- V4    1      0.23 271.39 104.58
- V8    1      0.38 271.53 104.60
- V10   1      4.75 275.90 105.40
- V9    1      6.38 277.53 105.70
- V11   1      9.72 280.88 106.29
<none>              271.15 106.53
- V7    1     19.28 290.43 107.97
- V6    1     78.82 349.97 117.29
- V2    1    230.10 501.26 135.25
- V5    1    240.23 511.38 136.25
- V3    1    328.20 599.36 144.19

Step:  AIC=104.58
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS    AIC
- V8    1      0.37 271.75 102.64
- V10   1      4.60 275.99 103.42
- V9    1      6.32 277.70 103.73
- V11   1      9.51 280.89 104.30
<none>              271.39 104.58
- V7    1     20.77 292.16 106.26
- V6    1     79.08 350.47 115.36
- V2    1    230.04 501.42 133.27
- V5    1    253.21 524.59 135.53
- V3    1    331.33 602.72 142.47

Step:  AIC=102.64
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V10 + V11

       Df Sum of Sq    RSS    AIC
- V10   1      4.39 276.15 101.44
- V9    1      6.06 277.81 101.75
- V11   1      9.38 281.13 102.34
<none>              271.75 102.64
- V7    1     20.83 292.58 104.34
- V6    1     78.87 350.62 113.38
- V2    1    238.56 510.32 132.15
- V5    1    253.34 525.10 133.58
- V3    1    331.15 602.91 140.49

Step:  AIC=101.45
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9 + V11

       Df Sum of Sq    RSS    AIC
- V9    1      7.57 283.72 100.80
- V11   1     10.49 286.63 101.31
<none>              276.15 101.44
- V7    1     18.65 294.80 102.71
- V6    1     78.62 354.76 111.97
- V2    1    234.36 510.51 130.17
- V5    1    263.35 539.50 132.93
- V3    1    330.65 606.80 138.81

Step:  AIC=100.8
.outcome ~ V2 + V3 + V5 + V6 + V7 + V11

       Df Sum of Sq    RSS    AIC
<none>              283.72 100.80
- V11   1     11.92 295.63 100.86
- V7    1     21.38 305.09 102.43
- V6    1     99.35 383.07 113.81
- V2    1    246.67 530.38 130.08
- V5    1    266.49 550.20 131.91
- V3    1    331.69 615.41 137.51
Start:  AIC=100.95
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS     AIC
- V10   1     0.365 237.70  99.027
- V9    1     0.432 237.76  99.042
- V4    1     0.997 238.33  99.165
- V11   1     1.178 238.51  99.204
- V7    1     3.242 240.57  99.652
- V8    1     8.128 245.46 100.698
<none>              237.33 100.947
- V2    1   110.646 347.98 118.846
- V6    1   137.521 374.85 122.715
- V5    1   197.172 434.50 130.394
- V3    1   299.590 536.92 141.400

Step:  AIC=99.03
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V11

       Df Sum of Sq    RSS     AIC
- V9    1     0.390 238.09  97.112
- V4    1     1.150 238.85  97.278
- V11   1     1.223 238.92  97.294
- V7    1     3.482 241.18  97.783
- V8    1     8.621 246.32  98.879
<none>              237.70  99.027
- V2    1   111.437 349.13 117.019
- V6    1   139.321 377.02 121.014
- V5    1   211.241 448.94 130.093
- V3    1   306.127 543.82 140.064

Step:  AIC=97.11
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V11

       Df Sum of Sq    RSS     AIC
- V4    1      1.41 239.49  95.418
- V11   1      1.44 239.52  95.425
- V7    1      3.44 241.53  95.858
- V8    1      8.42 246.51  96.920
<none>              238.09  97.112
- V2    1    112.32 350.40 115.207
- V6    1    139.18 377.26 119.048
- V5    1    215.34 453.43 128.611
- V3    1    320.19 558.27 139.427

Step:  AIC=95.42
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V11

       Df Sum of Sq    RSS     AIC
- V11   1      1.07 240.56  93.650
- V7    1      3.87 243.36  94.251
- V8    1      7.52 247.01  95.027
<none>              239.49  95.418
- V2    1    119.52 359.02 114.470
- V6    1    158.04 397.53 119.769
- V5    1    227.69 467.18 128.165
- V3    1    319.20 558.69 137.467

Step:  AIC=93.65
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8

       Df Sum of Sq    RSS     AIC
- V7    1      5.68 246.24  92.864
- V8    1      7.19 247.75  93.182
<none>              240.56  93.650
- V6    1    157.55 398.11 117.845
- V2    1    158.82 399.38 118.011
- V5    1    252.35 492.91 128.952
- V3    1    320.20 560.76 135.658

Step:  AIC=92.86
.outcome ~ V2 + V3 + V5 + V6 + V8

       Df Sum of Sq    RSS     AIC
- V8    1     7.884 254.13  92.503
<none>              246.24  92.864
- V2    1   153.788 400.03 116.095
- V6    1   154.941 401.18 116.245
- V5    1   247.656 493.90 127.056
- V3    1   315.194 561.44 133.721

Step:  AIC=92.5
.outcome ~ V2 + V3 + V5 + V6

       Df Sum of Sq    RSS     AIC
<none>              254.13  92.503
- V6    1    152.34 406.47 114.925
- V2    1    179.43 433.55 118.280
- V5    1    248.15 502.28 125.931
- V3    1    310.96 565.09 132.058
Start:  AIC=141.26
.outcome ~ V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS    AIC
- V4    1      1.03 366.03 139.47
- V11   1      1.61 366.60 139.59
- V10   1      2.12 367.11 139.69
- V8    1      4.62 369.61 140.21
- V9    1      5.49 370.49 140.39
<none>              364.99 141.25
- V7    1     12.85 377.84 141.88
- V6    1    199.03 564.02 172.33
- V2    1    288.30 653.30 183.50
- V5    1    325.09 690.08 187.66
- V3    1    510.22 875.21 205.72

Step:  AIC=139.47
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10 + V11

       Df Sum of Sq    RSS    AIC
- V11   1      1.56 367.59 137.79
- V10   1      1.83 367.86 137.85
- V8    1      4.13 370.16 138.32
- V9    1      5.50 371.53 138.60
<none>              366.03 139.47
- V7    1     13.81 379.84 140.28
- V6    1    202.55 568.58 170.94
- V2    1    288.35 654.38 181.62
- V5    1    339.16 705.19 187.31
- V3    1    513.50 879.53 204.10

Step:  AIC=137.79
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9 + V10

       Df Sum of Sq    RSS    AIC
- V10   1      1.81 369.40 136.17
- V8    1      3.76 371.35 136.57
- V9    1      5.99 373.57 137.02
<none>              367.59 137.79
- V7    1     12.60 380.19 138.35
- V6    1    202.89 570.47 169.20
- V2    1    287.94 655.53 179.76
- V5    1    350.07 717.66 186.64
- V3    1    512.13 879.71 202.11

Step:  AIC=136.17
.outcome ~ V2 + V3 + V5 + V6 + V7 + V8 + V9

       Df Sum of Sq    RSS    AIC
- V8    1      3.64 373.04 134.91
- V9    1      6.61 376.00 135.51
<none>              369.40 136.17
- V7    1     12.64 382.04 136.72
- V6    1    201.10 570.49 167.20
- V2    1    286.95 656.35 177.85
- V5    1    369.04 738.44 186.81
- V3    1    511.78 881.18 200.24

Step:  AIC=134.91
.outcome ~ V2 + V3 + V5 + V6 + V7 + V9

       Df Sum of Sq    RSS    AIC
- V9    1      5.76 378.80 134.08
<none>              373.04 134.91
- V7    1     12.46 385.50 135.41
- V6    1    202.26 575.30 165.84
- V2    1    314.68 687.71 179.40
- V5    1    370.10 743.13 185.29
- V3    1    508.43 881.46 198.26

Step:  AIC=134.08
.outcome ~ V2 + V3 + V5 + V6 + V7

       Df Sum of Sq    RSS    AIC
<none>              378.80 134.08
- V7    1     14.22 393.02 134.88
- V6    1    217.07 595.87 166.51
- V2    1    320.37 699.17 178.66
- V5    1    364.37 743.17 183.29
- V3    1    511.76 890.55 197.04
Linear Regression with Stepwise Selection 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE     Rsquared   MAE     
  2.41942  0.7699172  1.957556

[1] "Mon Mar 12 15:44:42 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:45:17 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "logreg"                  
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:45:31 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "M5"                      
Error : package RWeka is required
Error : package RWeka is required
Error : package RWeka is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:45:45 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "M5Rules"                 
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:46:00 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDecay"           
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
Error : Could not start a sequential model. `tensorflow` might not be installed. See `?install_tensorflow`.
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:46:14 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "mlpKerasDropout"         
Multi-Step Adaptive MCP-Net 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  alphas     nsteps  scale      RMSE      Rsquared   MAE       Selected
  0.1456278   9      1.5289136  2.426255  0.7690482  1.988533          
  0.1743736   4      1.8282158  2.449625  0.7642361  2.004594          
  0.2006495   2      0.9967927  2.444890  0.7640653  1.979414          
  0.2556397   6      3.1845993  2.438136  0.7648853  1.984233          
  0.2715008   6      0.2946906  2.438095  0.7648975  1.984455          
  0.3422950   7      0.3474640  2.439175  0.7647438  1.985884          
  0.3603020  10      1.1830360  2.422717  0.7683418  1.985295          
  0.4259831   6      2.7611275  2.437924  0.7649680  1.985750          
  0.4517831   4      3.0401748  2.437913  0.7649751  1.985879          
  0.5301271   8      2.9723633  2.437892  0.7649922  1.986195          
  0.5521636  10      1.6584653  2.443469  0.7658306  2.026892          
  0.6002024   2      0.3900431  2.454550  0.7616710  1.982888          
  0.6060012   3      3.8186972  2.441315  0.7655437  1.987517          
  0.6379714   2      0.8594890  2.421695  0.7676335  1.973775  *       
  0.6497151  10      1.6272731  2.441438  0.7660562  2.022345          
  0.7339092   7      2.5108996  2.459908  0.7619028  1.988970          
  0.7564225   3      1.7085173  2.449132  0.7649210  2.036237          
  0.7709759   7      3.1745240  2.455853  0.7624081  1.988311          
  0.8155990   7      1.5821910  2.444428  0.7655009  2.026107          
  0.8372887   2      2.0561542  2.461183  0.7633345  2.025773          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alphas = 0.6379714, nsteps = 2
 and scale = 0.859489.
[1] "Mon Mar 12 15:46:36 2018"
# weights:  37
initial  value 11030.824801 
iter  10 value 10374.458051
iter  20 value 10373.698546
iter  20 value 10373.698458
iter  20 value 10373.698414
final  value 10373.698414 
converged
# weights:  193
initial  value 11128.617244 
iter  10 value 10348.199025
iter  20 value 10344.984895
final  value 10344.984424 
converged
# weights:  61
initial  value 11133.520862 
iter  10 value 10347.271210
iter  20 value 10344.882113
iter  30 value 10344.578544
iter  40 value 10344.537775
final  value 10344.536148 
converged
# weights:  193
initial  value 11123.215658 
iter  10 value 10346.798436
iter  20 value 10344.349662
final  value 10344.325651 
converged
# weights:  169
initial  value 11497.613456 
iter  10 value 10366.244826
iter  20 value 10364.017979
final  value 10364.000237 
converged
# weights:  37
initial  value 11223.112508 
iter  10 value 10347.105332
iter  20 value 10344.359566
iter  20 value 10344.359470
iter  20 value 10344.359391
final  value 10344.359391 
converged
# weights:  145
initial  value 11652.796981 
iter  10 value 10391.404020
iter  20 value 10390.916318
iter  20 value 10390.916218
iter  20 value 10390.916135
final  value 10390.916135 
converged
# weights:  205
initial  value 11161.628571 
iter  10 value 10348.569257
final  value 10345.794028 
converged
# weights:  157
initial  value 11381.580345 
iter  10 value 10346.177534
iter  20 value 10344.342503
final  value 10344.329481 
converged
# weights:  157
initial  value 10767.605827 
iter  10 value 10344.623349
iter  20 value 10344.324585
final  value 10344.322312 
converged
# weights:  49
initial  value 11253.785359 
iter  10 value 10344.672753
iter  20 value 10344.326428
final  value 10344.326120 
converged
# weights:  109
initial  value 11223.854805 
iter  10 value 10351.591393
iter  20 value 10344.404921
final  value 10344.346818 
converged
# weights:  109
initial  value 10738.566893 
iter  10 value 10345.542878
iter  20 value 10344.642407
iter  30 value 10344.542958
final  value 10344.524612 
converged
# weights:  85
initial  value 10708.071489 
iter  10 value 10394.346988
iter  20 value 10392.557771
final  value 10392.557133 
converged
# weights:  217
initial  value 10990.797521 
iter  10 value 10345.322088
iter  20 value 10344.332641
final  value 10344.322718 
converged
# weights:  85
initial  value 10807.685960 
iter  10 value 10346.971651
iter  20 value 10346.402901
final  value 10346.402749 
converged
# weights:  169
initial  value 11265.479332 
iter  10 value 10344.815137
iter  20 value 10344.326796
final  value 10344.322902 
converged
# weights:  217
initial  value 11046.201477 
iter  10 value 10348.676467
iter  20 value 10345.207321
final  value 10345.206857 
converged
# weights:  133
initial  value 11079.394448 
iter  10 value 10348.018169
final  value 10346.540143 
converged
# weights:  61
initial  value 11167.339679 
iter  10 value 10355.649365
iter  20 value 10345.424590
iter  30 value 10345.015122
final  value 10345.007498 
converged
# weights:  37
initial  value 11398.686598 
iter  10 value 10615.192440
iter  20 value 10613.754925
final  value 10613.754276 
converged
# weights:  193
initial  value 11625.967091 
iter  10 value 10586.820506
iter  20 value 10585.001309
final  value 10584.999713 
converged
# weights:  61
initial  value 11665.158119 
iter  10 value 10590.485817
iter  20 value 10584.620240
iter  30 value 10584.554690
final  value 10584.550920 
converged
# weights:  193
initial  value 10768.652036 
iter  10 value 10584.833278
iter  20 value 10584.341832
final  value 10584.340120 
converged
# weights:  169
initial  value 11102.995975 
iter  10 value 10605.107065
iter  20 value 10604.054609
final  value 10604.054109 
converged
# weights:  37
initial  value 11396.700876 
iter  10 value 10587.284993
iter  20 value 10584.375984
iter  30 value 10584.364733
final  value 10584.361481 
converged
# weights:  145
initial  value 11832.170759 
iter  10 value 10632.182141
iter  20 value 10631.060653
final  value 10631.059700 
converged
# weights:  205
initial  value 11127.709965 
iter  10 value 10586.263211
final  value 10585.809965 
converged
# weights:  157
initial  value 11482.869323 
iter  10 value 10587.156032
iter  20 value 10584.368612
final  value 10584.344218 
converged
# weights:  157
initial  value 11049.798188 
iter  10 value 10584.762677
iter  20 value 10584.341018
final  value 10584.337288 
converged
# weights:  49
initial  value 10979.014408 
iter  10 value 10584.597735
iter  20 value 10584.340445
final  value 10584.340251 
converged
# weights:  109
initial  value 11290.235242 
iter  10 value 10591.907653
iter  20 value 10584.423394
final  value 10584.361735 
converged
# weights:  109
initial  value 10899.223594 
iter  10 value 10587.900894
iter  20 value 10584.731082
iter  30 value 10584.554189
final  value 10584.539843 
converged
# weights:  85
initial  value 11489.117479 
iter  10 value 10635.285548
iter  20 value 10632.679316
final  value 10632.677446 
converged
# weights:  217
initial  value 10905.834319 
iter  10 value 10584.751751
iter  20 value 10584.340892
final  value 10584.337652 
converged
# weights:  85
initial  value 11060.692422 
iter  10 value 10587.162701
iter  20 value 10586.418503
iter  20 value 10586.418449
iter  20 value 10586.418433
final  value 10586.418433 
converged
# weights:  169
initial  value 10831.378404 
iter  10 value 10584.559037
iter  20 value 10584.338670
final  value 10584.337610 
converged
# weights:  217
initial  value 10873.661253 
iter  10 value 10585.774911
final  value 10585.222339 
converged
# weights:  133
initial  value 11276.028999 
iter  10 value 10587.152042
final  value 10586.556115 
converged
# weights:  61
initial  value 11247.111525 
iter  10 value 10587.116409
iter  20 value 10585.082818
iter  30 value 10585.022895
final  value 10585.022578 
converged
# weights:  37
initial  value 11809.189258 
iter  10 value 10811.367108
final  value 10809.850980 
converged
# weights:  193
initial  value 11739.397074 
iter  10 value 10780.929089
iter  20 value 10780.889385
final  value 10780.889162 
converged
# weights:  61
initial  value 11613.488990 
iter  10 value 10783.457053
iter  20 value 10780.548136
iter  30 value 10780.457201
iter  40 value 10780.439615
iter  40 value 10780.439550
iter  40 value 10780.439515
final  value 10780.439515 
converged
# weights:  193
initial  value 11536.946941 
iter  10 value 10782.479030
iter  20 value 10780.248813
final  value 10780.227343 
converged
# weights:  169
initial  value 11242.962404 
iter  10 value 10801.707680
iter  20 value 10800.082889
final  value 10800.082088 
converged
# weights:  37
initial  value 11506.568330 
iter  10 value 10783.442329
iter  20 value 10780.263040
final  value 10780.252518 
converged
# weights:  145
initial  value 11940.204722 
iter  10 value 10829.417744
iter  20 value 10827.357158
final  value 10827.351982 
converged
# weights:  205
initial  value 11030.555993 
iter  10 value 10782.038930
final  value 10781.703479 
converged
# weights:  157
initial  value 11535.488446 
iter  10 value 10784.156097
iter  20 value 10780.268148
final  value 10780.230812 
converged
# weights:  157
initial  value 11861.942656 
iter  10 value 10780.526745
iter  20 value 10780.226304
final  value 10780.224102 
converged
# weights:  49
initial  value 11080.875377 
iter  10 value 10780.408138
final  value 10780.226574 
converged
# weights:  109
initial  value 11599.943076 
iter  10 value 10787.904361
iter  20 value 10780.311362
final  value 10780.250506 
converged
# weights:  109
initial  value 11476.928931 
iter  10 value 10781.726949
iter  20 value 10780.491754
iter  30 value 10780.429983
final  value 10780.427045 
converged
# weights:  85
initial  value 11468.828049 
iter  10 value 10832.304596
iter  20 value 10828.961771
final  value 10828.961090 
converged
# weights:  217
initial  value 11323.911757 
iter  10 value 10781.205572
iter  20 value 10780.234131
final  value 10780.224389 
converged
# weights:  85
initial  value 11558.451528 
iter  10 value 10783.538311
iter  20 value 10782.314882
final  value 10782.314481 
converged
# weights:  169
initial  value 11440.951090 
iter  10 value 10780.878191
iter  20 value 10780.230356
final  value 10780.224326 
converged
# weights:  217
initial  value 11668.480358 
iter  10 value 10785.496512
iter  20 value 10781.113140
final  value 10781.112864 
converged
# weights:  133
initial  value 11452.487016 
iter  10 value 10787.614596
iter  20 value 10782.453605
final  value 10782.453141 
converged
# weights:  61
initial  value 11667.927141 
iter  10 value 10782.449593
iter  20 value 10780.934380
iter  30 value 10780.912139
iter  30 value 10780.912072
iter  30 value 10780.912038
final  value 10780.912038 
converged
# weights:  157
initial  value 17591.758639 
iter  10 value 15854.899889
iter  20 value 15854.445302
final  value 15854.441464 
converged
Neural Network 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared   MAE       Selected
   3    3.308369e-04  14.44513  0.3658387  13.55904          
   3    1.704661e+00  14.45053  0.5166400  13.56505          
   4    3.723257e-05  14.44512  0.4255518  13.55904          
   5    5.151294e-03  14.44514  0.4889018  13.55906          
   5    1.996046e-02  14.44520  0.4867903  13.55913          
   7    8.782479e-02  14.44539  0.5041045  13.55934          
   7    5.007101e+00  14.45508  0.5334252  13.56998          
   9    4.054516e-04  14.44512  0.2444262  13.55904          
   9    6.394279e-03  14.44514  0.4633553  13.55906          
  11    1.214787e-01  14.44542  0.5173651  13.55937          
  12    6.670140e+00  14.45481  0.5352453  13.56959          
  13    2.133559e-05  14.44512        NaN  13.55904  *       
  13    1.454061e-04  14.44512  0.1511318  13.55904          
  14    2.830127e-05  14.44512  0.1133616  13.55904          
  14    2.268611e+00  14.44865  0.5339888  13.56291          
  16    8.875772e-05  14.44512  0.2954037  13.55904          
  16    3.587242e-02  14.44520  0.5244930  13.55913          
  17    9.744362e-02  14.44531  0.5246576  13.55925          
  18    3.551935e-05  14.44512        NaN  13.55904          
  18    5.485211e-02  14.44523  0.5214407  13.55916          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 13 and decay = 2.133559e-05.
[1] "Mon Mar 12 15:47:25 2018"
Non-Negative Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE      Rsquared   MAE     
  15.33606  0.6487955  15.00063

[1] "Mon Mar 12 15:47:39 2018"
Non-Informative Model 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE      Rsquared  MAE     
  4.980649  NaN       4.177808

[1] "Mon Mar 12 15:47:52 2018"
Parallel Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE       Selected
  2     3.475397  0.6762591  2.880247          
  3     3.302768  0.6816629  2.736588          
  4     3.204379  0.6828377  2.641509          
  5     3.241539  0.6485353  2.674743          
  6     3.162274  0.6659957  2.596060  *       
  7     3.165231  0.6572920  2.606094          
  8     3.195616  0.6395338  2.626913          
  9     3.193932  0.6352388  2.631317          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 6.
[1] "Mon Mar 12 15:48:11 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: Warning messages:
1: package 'mxnet' is not available (for R version 3.4.3) 
2: package 'mxnet' is not available (for R version 3.4.3) 
3: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
5: executing %dopar% sequentially: no parallel backend registered 
# weights:  37
initial  value 11021.027744 
iter  10 value 10375.952912
iter  20 value 10373.719192
final  value 10373.718817 
converged
# weights:  193
initial  value 11123.466787 
iter  10 value 10345.020900
iter  20 value 10344.985258
final  value 10344.984952 
converged
# weights:  61
initial  value 11137.111421 
iter  10 value 10347.256141
iter  20 value 10344.716997
iter  30 value 10344.560221
final  value 10344.537035 
converged
# weights:  193
initial  value 11124.759864 
iter  10 value 10346.854259
iter  20 value 10344.350305
final  value 10344.325594 
converged
# weights:  169
initial  value 11481.820522 
iter  10 value 10366.353011
iter  20 value 10364.009342
final  value 10364.006828 
converged
# weights:  37
initial  value 11220.310304 
iter  10 value 10347.086945
iter  20 value 10344.359301
iter  20 value 10344.359232
iter  20 value 10344.359150
final  value 10344.359150 
converged
# weights:  145
initial  value 11658.269609 
iter  10 value 10391.492658
final  value 10390.928419 
converged
# weights:  205
initial  value 11157.084514 
iter  10 value 10349.021731
final  value 10345.794836 
converged
# weights:  157
initial  value 11348.289596 
iter  10 value 10346.382969
iter  20 value 10344.344872
final  value 10344.329476 
converged
# weights:  157
initial  value 10744.296189 
iter  10 value 10344.607645
iter  20 value 10344.324404
final  value 10344.322330 
converged
# weights:  49
initial  value 11260.323696 
iter  10 value 10344.668721
iter  20 value 10344.326564
final  value 10344.326192 
converged
# weights:  109
initial  value 11218.401850 
iter  10 value 10351.421558
iter  20 value 10344.402963
final  value 10344.346885 
converged
# weights:  109
initial  value 10726.753951 
iter  10 value 10345.269018
iter  20 value 10344.646246
iter  30 value 10344.530036
final  value 10344.524699 
converged
# weights:  85
initial  value 10704.065055 
iter  10 value 10396.301246
iter  20 value 10392.583124
final  value 10392.577796 
converged
# weights:  217
initial  value 10990.683143 
iter  10 value 10345.340179
iter  20 value 10344.332849
final  value 10344.322638 
converged
# weights:  85
initial  value 10794.043893 
iter  10 value 10346.746196
final  value 10346.404560 
converged
# weights:  169
initial  value 11249.219573 
iter  10 value 10344.833435
iter  20 value 10344.327007
final  value 10344.322884 
converged
# weights:  217
initial  value 11043.506414 
iter  10 value 10348.783033
iter  20 value 10345.207490
iter  20 value 10345.207406
iter  20 value 10345.207380
final  value 10345.207380 
converged
# weights:  133
initial  value 11058.961376 
iter  10 value 10346.758234
final  value 10346.541702 
converged
# weights:  61
initial  value 11168.083727 
iter  10 value 10356.038466
iter  20 value 10345.497415
iter  30 value 10345.020915
iter  40 value 10345.008042
iter  40 value 10345.007993
iter  40 value 10345.007979
final  value 10345.007979 
converged
# weights:  37
initial  value 11408.981242 
iter  10 value 10615.120262
iter  20 value 10613.863616
final  value 10613.862744 
converged
# weights:  193
initial  value 11617.892859 
iter  10 value 10585.372034
iter  20 value 10585.003769
final  value 10585.001970 
converged
# weights:  61
initial  value 11664.435694 
iter  10 value 10589.634675
iter  20 value 10584.604351
iter  30 value 10584.555798
final  value 10584.551597 
converged
# weights:  193
initial  value 10765.474643 
iter  10 value 10584.831226
iter  20 value 10584.341808
final  value 10584.340081 
converged
# weights:  169
initial  value 11090.819820 
iter  10 value 10608.057549
iter  20 value 10604.106724
final  value 10604.104105 
converged
# weights:  37
initial  value 11393.710240 
iter  10 value 10587.476505
iter  20 value 10584.375245
final  value 10584.362614 
converged
# weights:  145
initial  value 11839.165091 
iter  10 value 10631.715319
iter  20 value 10631.164438
final  value 10631.163517 
converged
# weights:  205
initial  value 11101.784473 
iter  10 value 10588.105769
final  value 10585.814518 
converged
# weights:  157
initial  value 11473.816201 
iter  10 value 10587.241059
iter  20 value 10584.369592
final  value 10584.344143 
converged
# weights:  157
initial  value 11088.586809 
iter  10 value 10584.799066
iter  20 value 10584.341438
final  value 10584.337288 
converged
# weights:  49
initial  value 10984.369592 
iter  10 value 10584.600867
iter  20 value 10584.340507
final  value 10584.340328 
converged
# weights:  109
initial  value 11286.209390 
iter  10 value 10591.553572
iter  20 value 10584.419312
final  value 10584.361700 
converged
# weights:  109
initial  value 10883.303826 
iter  10 value 10585.166386
iter  20 value 10584.623897
iter  30 value 10584.544441
final  value 10584.540432 
converged
# weights:  85
initial  value 11480.246883 
iter  10 value 10635.384243
iter  20 value 10632.829039
final  value 10632.825985 
converged
# weights:  217
initial  value 10875.561620 
iter  10 value 10584.734420
iter  20 value 10584.340692
final  value 10584.337659 
converged
# weights:  85
initial  value 11053.409977 
iter  10 value 10586.661599
final  value 10586.426267 
converged
# weights:  169
initial  value 10824.794104 
iter  10 value 10584.551810
iter  20 value 10584.338587
final  value 10584.337558 
converged
# weights:  217
initial  value 10896.960287 
iter  10 value 10586.268522
final  value 10585.225113 
converged
# weights:  133
initial  value 11261.217271 
iter  10 value 10587.073487
final  value 10586.564020 
converged
# weights:  61
initial  value 11283.686971 
iter  10 value 10595.754264
iter  20 value 10585.288806
iter  30 value 10585.027419
final  value 10585.024986 
converged
# weights:  37
initial  value 11815.100286 
iter  10 value 10811.524973
iter  20 value 10810.025076
final  value 10810.024372 
converged
# weights:  193
initial  value 11756.363592 
iter  10 value 10788.755528
iter  20 value 10781.159225
iter  30 value 10780.893163
final  value 10780.892627 
converged
# weights:  61
initial  value 11620.981385 
iter  10 value 10783.436382
iter  20 value 10780.547040
iter  30 value 10780.454671
iter  40 value 10780.441036
final  value 10780.439635 
converged
# weights:  193
initial  value 11598.708458 
iter  10 value 10782.322897
iter  20 value 10780.247012
final  value 10780.227303 
converged
# weights:  169
initial  value 11257.477277 
iter  10 value 10801.687789
iter  20 value 10800.167524
final  value 10800.165942 
converged
# weights:  37
initial  value 11486.723012 
iter  10 value 10783.288778
iter  20 value 10780.261369
final  value 10780.260478 
converged
# weights:  145
initial  value 11924.012901 
iter  10 value 10830.796717
iter  20 value 10827.529703
iter  20 value 10827.529621
final  value 10827.529454 
converged
# weights:  205
initial  value 11020.682621 
iter  10 value 10782.441836
final  value 10781.710903 
converged
# weights:  157
initial  value 11572.880446 
iter  10 value 10783.916447
iter  20 value 10780.265385
final  value 10780.230822 
converged
# weights:  157
initial  value 11872.141646 
iter  10 value 10780.518632
iter  20 value 10780.226211
final  value 10780.224067 
converged
# weights:  49
initial  value 11081.432631 
iter  10 value 10780.407875
iter  20 value 10780.226667
iter  20 value 10780.226610
iter  20 value 10780.226610
final  value 10780.226610 
converged
# weights:  109
initial  value 11541.230338 
iter  10 value 10787.711262
iter  20 value 10780.309136
final  value 10780.249766 
converged
# weights:  109
initial  value 11468.484570 
iter  10 value 10781.159202
iter  20 value 10780.490783
iter  30 value 10780.434352
iter  40 value 10780.427971
final  value 10780.427777 
converged
# weights:  85
initial  value 11477.299991 
iter  10 value 10831.328713
iter  20 value 10829.210208
final  value 10829.209546 
converged
# weights:  217
initial  value 11348.017404 
iter  10 value 10781.161955
iter  20 value 10780.233628
final  value 10780.224457 
converged
# weights:  85
initial  value 11534.533779 
iter  10 value 10782.983142
final  value 10782.326905 
converged
# weights:  169
initial  value 11475.869355 
iter  10 value 10780.866718
iter  20 value 10780.230224
final  value 10780.224299 
converged
# weights:  217
initial  value 11642.830724 
iter  10 value 10785.608134
iter  20 value 10781.117867
final  value 10781.117310 
converged
# weights:  133
initial  value 11472.702974 
iter  10 value 10787.450966
iter  20 value 10782.466425
final  value 10782.465834 
converged
# weights:  61
initial  value 11674.478315 
iter  10 value 10782.655610
iter  20 value 10780.949357
iter  30 value 10780.915790
iter  30 value 10780.915728
iter  30 value 10780.915706
final  value 10780.915706 
converged
# weights:  157
initial  value 17629.321140 
iter  10 value 15854.847185
iter  20 value 15854.444695
final  value 15854.441772 
converged
Neural Networks with Feature Extraction 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  size  decay         RMSE      Rsquared    MAE       Selected
   3    3.308369e-04  14.44513  0.49097559  13.55904          
   3    1.704661e+00  14.45026  0.73991503  13.56475          
   4    3.723257e-05  14.44512  0.31857907  13.55904          
   5    5.151294e-03  14.44514  0.70765026  13.55906          
   5    1.996046e-02  14.44519  0.73758355  13.55912          
   7    8.782479e-02  14.44537  0.73978174  13.55932          
   7    5.007101e+00  14.45475  0.73418356  13.56961          
   9    4.054516e-04  14.44512  0.38355730  13.55904          
   9    6.394279e-03  14.44514  0.72094473  13.55906          
  11    1.214787e-01  14.44540  0.74046654  13.55935          
  12    6.670140e+00  14.45456  0.72662323  13.56931          
  13    2.133559e-05  14.44512  0.08795725  13.55904  *       
  13    1.454061e-04  14.44512  0.16264093  13.55904          
  14    2.830127e-05  14.44512  0.33818675  13.55904          
  14    2.268611e+00  14.44853  0.73035558  13.56277          
  16    8.875772e-05  14.44512  0.17698043  13.55904          
  16    3.587242e-02  14.44519  0.73185128  13.55912          
  17    9.744362e-02  14.44530  0.73646729  13.55924          
  18    3.551935e-05  14.44512  0.25629878  13.55904          
  18    5.485211e-02  14.44522  0.74006460  13.55915          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 13 and decay = 2.133559e-05.
[1] "Mon Mar 12 15:48:26 2018"
Principal Component Analysis 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE      Rsquared   MAE       Selected
  1      4.536507  0.1958683  3.746913          
  2      4.520166  0.2041050  3.743488          
  3      4.457691  0.2416024  3.708226          
  4      4.242153  0.3295731  3.470471          
  5      3.958678  0.4184467  3.342496          
  6      3.347149  0.5943017  2.803610          
  7      3.399777  0.5729133  2.782677          
  8      3.224850  0.6049637  2.611621  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 15:48:40 2018"
Loading required package: plsRglm
____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.118492023693398) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0903972115833312) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0316068716812879) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.178525331569836) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0506537991575897) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.194137832149863) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.132958446955308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0387527559418231) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0109701526816934) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0190307671204209) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0535979677923024) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0935263869352639) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.189986211294308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0183488334994763) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.131453903950751) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0150601975619793) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.124639777326956) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.136150009883568) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.110005687689409) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.118492023693398) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0903972115833312) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0316068716812879) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.178525331569836) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.194137832149863) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.132958446955308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0387527559418231) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0109701526816934) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0535979677923024) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0935263869352639) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.189986211294308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0183488334994763) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.131453903950751) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.0150601975619793) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.124639777326956) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.136150009883568) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.110005687689409) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.118492023693398) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0903972115833312) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0316068716812879) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.178525331569836) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0506537991575897) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.194137832149863) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.132958446955308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0387527559418231) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0109701526816934) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0190307671204209) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0535979677923024) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0935263869352639) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.189986211294308) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0183488334994763) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.131453903950751) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0150601975619793) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.124639777326956) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.136150009883568) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
____Component____ 2 ____
No more significant predictors (<0.110005687689409) found
Warning only 2 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

____************************************************____

Family: gaussian 
Link function: identity 

____Component____ 1 ____
No more significant predictors (<0.0935263869352639) found
Warning only 1 components were thus extracted
____Predicting X without NA neither in X nor in Y____
****________________________________________________****

Partial Least Squares Generalized Linear Models  

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nt  alpha.pvals.expli  RMSE      Rsquared   MAE       Selected
  2   0.01903077         2.475971  0.7611774  2.039956          
  2   0.05065380         2.388106  0.7815779  1.920405          
  2   0.17438793         2.525342  0.7509608  2.020415          
  3   0.09039721         2.358238  0.7821738  1.861826          
  3   0.11000569         2.469100  0.7609241  1.985831          
  4   0.13145390         2.469100  0.7609241  1.985831          
  4   0.18998621         2.513601  0.7533238  2.024308          
  5   0.05359797         2.388106  0.7815779  1.920405          
  5   0.09352639         2.358238  0.7821738  1.861826  *       
  6   0.13615001         2.469100  0.7609241  1.985831          
  6   0.19413783         2.513601  0.7533238  2.024308          
  7   0.01097015         2.475971  0.7611774  2.039956          
  7   0.01506020         2.475971  0.7611774  2.039956          
  7   0.03875276         2.475971  0.7611774  2.039956          
  7   0.17852533         2.525342  0.7509608  2.020415          
  8   0.03160687         2.475971  0.7611774  2.039956          
  8   0.11849202         2.469100  0.7609241  1.985831          
  9   0.01834883         2.475971  0.7611774  2.039956          
  9   0.12463978         2.469100  0.7609241  1.985831          
  9   0.13295845         2.469100  0.7609241  1.985831          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nt = 5 and alpha.pvals.expli
 = 0.09352639.
[1] "Mon Mar 12 15:49:29 2018"
Projection Pursuit Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  nterms  RMSE      Rsquared   MAE       Selected
   1      2.659781  0.7323793  2.154546  *       
   2      2.955679  0.6662400  2.268920          
   3      3.236344  0.6096291  2.491368          
   4      3.296426  0.6200772  2.528342          
   5      3.099284  0.6660551  2.384605          
   6      3.130592  0.6579369  2.397792          
   7      3.069295  0.6667172  2.385753          
   8      3.167884  0.6598670  2.466503          
   9      3.127058  0.6661086  2.423006          
  10      3.109138  0.6650993  2.400094          
  11      3.114837  0.6647817  2.409722          
  12      3.114501  0.6648644  2.407548          
  13      3.117390  0.6647382  2.409934          
  14      3.117030  0.6649397  2.410165          
  15      3.117761  0.6648392  2.408458          
  16      3.115479  0.6653772  2.406242          
  17      3.116197  0.6652712  2.406138          
  18      3.116487  0.6651186  2.407084          
  19      3.116537  0.6651569  2.406835          
  20      3.116661  0.6651385  2.407028          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was nterms = 1.
[1] "Mon Mar 12 15:49:44 2018"
Quantile Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE       Selected
  2     7.034757  0.5225552  6.170780          
  3     6.542746  0.5724874  5.735295          
  4     6.394924  0.5762808  5.610373          
  5     6.308552  0.5325216  5.502840          
  6     6.197096  0.5660617  5.399715          
  7     6.010913  0.5780190  5.226158          
  8     6.128495  0.5244094  5.292796          
  9     5.899248  0.5600185  5.073003  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 9.
[1] "Mon Mar 12 15:50:22 2018"
Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  min.node.size  mtry  splitrule   RMSE      Rsquared   MAE       Selected
   3              3    extratrees  3.504302  0.7518259  2.872832          
   3              9    extratrees  3.181867  0.7261932  2.624071          
   4              1    variance    3.794383  0.6261573  3.172954          
   5              5    maxstat     3.281979  0.6583474  2.732420          
   5              6    variance    3.174326  0.6560439  2.602901          
   7              7    variance    3.172240  0.6554857  2.611008  *       
   7             10    variance    3.245579  0.6135194  2.662286          
   9              3    maxstat     3.485982  0.6652398  2.884867          
   9              5    maxstat     3.309652  0.6468281  2.745143          
  11              7    maxstat     3.246598  0.6369086  2.694266          
  12             10    extratrees  3.263454  0.7490381  2.723865          
  13              1    variance    3.937869  0.6482346  3.285283          
  13              2    maxstat     3.706469  0.6822791  3.086146          
  14              1    variance    3.947936  0.6328286  3.312815          
  14              9    extratrees  3.383380  0.7332425  2.810919          
  16              2    extratrees  4.054309  0.7548419  3.368938          
  16              6    extratrees  3.564362  0.7464502  2.950866          
  17              7    maxstat     3.295233  0.6338464  2.739337          
  18              1    extratrees  4.383216  0.7338771  3.653139          
  18              7    extratrees  3.547008  0.7517454  2.972884          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, splitrule = variance
 and min.node.size = 7.
[1] "Mon Mar 12 15:50:42 2018"
Error in code$varImp(object$finalModel, ...) : 
  No importance values available
In addition: Warning messages:
1: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
2: package 'gpls' is not available (for R version 3.4.3) 
3: package 'rPython' is not available (for R version 3.4.3) 
Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  predFixed  minNode  RMSE      Rsquared   MAE       Selected
  2           2       3.471820  0.6467149  2.881828          
  2           6       3.531591  0.6503337  2.947636          
  2          18       3.861499  0.6415039  3.236237          
  3          10       3.432680  0.6572167  2.849115          
  3          12       3.420575  0.6918893  2.856487          
  4          14       3.460901  0.6430394  2.885383          
  4          19       3.616689  0.6482019  3.065809          
  5           6       3.194383  0.6730602  2.646340          
  5          10       3.271870  0.6679366  2.713094          
  6          14       3.354570  0.6366727  2.783144          
  6          20       3.571077  0.6250293  2.992732          
  7           2       3.085472  0.6722833  2.516512  *       
  7           4       3.133161  0.6631020  2.572920          
  7          18       3.457922  0.6244877  2.888481          
  8           4       3.179771  0.6390315  2.599581          
  8          12       3.286135  0.6285420  2.714782          
  9           2       3.159240  0.6413686  2.562315          
  9          13       3.334791  0.6099706  2.765839          
  9          14       3.336874  0.6108151  2.762809          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were predFixed = 7 and minNode = 2.
[1] "Mon Mar 12 15:53:08 2018"
Loading required package: lars
Loaded lars 1.2

Relaxed Lasso 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  phi         lambda     RMSE      Rsquared   MAE       Selected
  0.05485076   71.12814  14.92805  0.5647029  14.49737          
  0.07530099   80.57873  14.93737  0.4098410  14.20525          
  0.09174417  155.64591  15.46891  0.4681819  14.48329          
  0.09515384   19.00595  14.55076  0.7573226  14.09584          
  0.15803436  119.16167  15.55630  0.3118225  14.64205          
  0.19376378   72.50363  14.87110  0.5451987  14.42521          
  0.25326900   17.42598  14.55228  0.7544073  14.17458          
  0.26798984   43.56488  14.56117  0.7014527  14.12376          
  0.45198606   22.79146  14.53553  0.7693187  14.24365  *       
  0.46763193   40.00616  14.56411  0.7075108  14.19873          
  0.55002844   24.01732  14.54569  0.7719362  14.28426          
  0.59246012  110.62201  15.11941  0.3278489  14.45131          
  0.62319889  144.88526  15.28825  0.3299875  14.55003          
  0.65726952   30.34434  14.57277  0.7690884  14.34204          
  0.66479223  125.02967  15.32638  0.3700440  14.56467          
  0.68075005   56.43124  14.66685  0.7087184  14.34817          
  0.87193966   15.84754  14.62489  0.7769670  14.41904          
  0.89262666   83.76577  15.18797  0.4544718  14.57682          
  0.94993106   32.20389  14.66923  0.7655667  14.46821          
  0.97068916   60.69189  14.69437  0.7225155  14.39861          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 22.79146 and phi = 0.4519861.
[1] "Mon Mar 12 15:53:24 2018"
Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE       Selected
  2     3.457606  0.6638352  2.884281          
  3     3.278285  0.6846012  2.699301          
  4     3.217789  0.6725196  2.664509          
  5     3.228903  0.6522431  2.656087          
  6     3.176040  0.6611830  2.619425          
  7     3.168628  0.6471112  2.602408  *       
  8     3.184951  0.6449205  2.624890          
  9     3.178658  0.6440850  2.595821          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 7.
[1] "Mon Mar 12 15:53:42 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Ridge Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        RMSE      Rsquared   MAE       Selected
  4.340291e-05  2.437914  0.7650317  1.987061  *       
  6.747726e-05  2.437921  0.7650312  1.987068          
  1.010020e-04  2.437929  0.7650305  1.987078          
  2.349262e-04  2.437964  0.7650278  1.987116          
  2.996896e-04  2.437981  0.7650264  1.987135          
  8.884509e-04  2.438136  0.7650141  1.987305          
  1.171332e-03  2.438210  0.7650082  1.987386          
  3.210364e-03  2.438745  0.7649655  1.987968          
  4.770390e-03  2.439155  0.7649327  1.988408          
  1.587988e-02  2.442076  0.7646956  1.991439          
  2.227191e-02  2.443757  0.7645569  1.993102          
  4.656033e-02  2.450129  0.7640194  1.998936          
  5.089497e-02  2.451260  0.7639224  1.999904          
  8.313985e-02  2.459582  0.7631965  2.006503          
  9.956366e-02  2.463745  0.7628271  2.009506          
  3.625722e-01  2.522134  0.7574439  2.039246          
  5.122520e-01  2.549445  0.7548641  2.050888          
  6.404805e-01  2.570327  0.7528777  2.060110          
  1.270555e+00  2.650230  0.7452371  2.089159          
  1.772521e+00  2.696260  0.7408613  2.103010          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 4.340291e-05.
[1] "Mon Mar 12 15:53:57 2018"
Robust Linear Model 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  intercept  psi           RMSE       Rsquared   MAE        Selected
  FALSE      psi.huber     16.829245  0.3892195  15.660222          
  FALSE      psi.hampel    16.829245  0.3892195  15.660222          
  FALSE      psi.bisquare  17.130069  0.3664816  15.782290          
   TRUE      psi.huber      2.459593  0.7602623   1.967332  *       
   TRUE      psi.hampel     2.464175  0.7602798   1.977237          
   TRUE      psi.bisquare   2.463133  0.7594117   1.951267          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were intercept = TRUE and psi = psi.huber.
[1] "Mon Mar 12 15:54:11 2018"
CART 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cp          RMSE      Rsquared   MAE       Selected
  0.00000000  4.284580  0.3366933  3.490659          
  0.04197456  4.210228  0.3537563  3.394322          
  0.10611804  4.145633  0.3664757  3.332748  *       
  0.12581168  4.298321  0.3162973  3.483746          
  0.35827386  4.353422  0.3404407  3.573257          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.106118.
[1] "Mon Mar 12 15:54:26 2018"
CART 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE     Rsquared   MAE     
  4.28458  0.3366933  3.490659

[1] "Mon Mar 12 15:54:40 2018"
CART 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  maxdepth  RMSE      Rsquared   MAE       Selected
  1         4.016283  0.3524193  3.312087  *       
  2         4.256215  0.3104329  3.480641          
  3         4.145633  0.3664757  3.332748          
  4         4.284580  0.3366933  3.490659          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was maxdepth = 1.
[1] "Mon Mar 12 15:54:54 2018"
Quantile Regression with LASSO penalty 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        RMSE      Rsquared   MAE       Selected
  4.340291e-05  2.638554  0.7260527  1.995518          
  6.747726e-05  2.638554  0.7260527  1.995518          
  1.010020e-04  2.638554  0.7260527  1.995518          
  2.349262e-04  2.638554  0.7260527  1.995518          
  2.996896e-04  2.638554  0.7260527  1.995518          
  8.884509e-04  2.638554  0.7260527  1.995518          
  1.171332e-03  2.613805  0.7299324  2.017903          
  3.210364e-03  2.568728  0.7366760  2.006168          
  4.770390e-03  2.557746  0.7388311  1.997066          
  1.587988e-02  2.496051  0.7501332  1.950599          
  2.227191e-02  2.477245  0.7536179  1.942849          
  4.656033e-02  2.444589  0.7739049  1.914655          
  5.089497e-02  2.436349  0.7748990  1.924384          
  8.313985e-02  2.426252  0.7896599  1.919928  *       
  9.956366e-02  2.508644  0.7871848  2.007194          
  3.625722e-01  5.009840        NaN  4.195449          
  5.122520e-01  5.009840        NaN  4.195449          
  6.404805e-01  5.009840        NaN  4.195449          
  1.270555e+00  5.009840        NaN  4.195449          
  1.772521e+00  5.009840        NaN  4.195449          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was lambda = 0.08313985.
[1] "Mon Mar 12 15:55:09 2018"
Non-Convex Penalized Quantile Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        penalty  RMSE      Rsquared   MAE       Selected
  4.340291e-05  SCAD     2.638554  0.7260527  1.995518          
  6.747726e-05  MCP      2.638554  0.7260527  1.995518          
  1.010020e-04  MCP      2.638554  0.7260527  1.995518          
  2.349262e-04  MCP      2.638554  0.7260527  1.995518          
  2.996896e-04  SCAD     2.638554  0.7260527  1.995518          
  8.884509e-04  SCAD     2.638554  0.7260527  1.995518          
  1.171332e-03  SCAD     2.638554  0.7260527  1.995518          
  3.210364e-03  MCP      2.638554  0.7260527  1.995518          
  4.770390e-03  MCP      2.638554  0.7260527  1.995518          
  1.587988e-02  SCAD     2.616519  0.7296093  2.023213          
  2.227191e-02  SCAD     2.630491  0.7272368  2.031758          
  4.656033e-02  MCP      2.558220  0.7440962  2.037542          
  5.089497e-02  MCP      2.558220  0.7440962  2.037542          
  8.313985e-02  MCP      2.313150  0.7869865  1.912943  *       
  9.956366e-02  SCAD     2.326301  0.7842200  1.919494          
  3.625722e-01  SCAD     5.009840        NaN  4.195449          
  5.122520e-01  MCP      5.009840        NaN  4.195449          
  6.404805e-01  SCAD     5.009840        NaN  4.195449          
  1.270555e+00  SCAD     5.009840        NaN  4.195449          
  1.772521e+00  MCP      5.009840        NaN  4.195449          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were lambda = 0.08313985 and penalty = MCP.
[1] "Mon Mar 12 15:55:24 2018"
Regularized Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  coefReg     coefImp     RMSE      Rsquared   MAE       Selected
  2     0.09515384  0.19914471  3.455494  0.6658266  2.858141          
  2     0.25326900  0.42085754  3.472684  0.6530233  2.894966          
  2     0.87193966  0.34104363  3.510241  0.6618971  2.916655          
  3     0.45198606  0.78255982  3.223609  0.7007270  2.656088          
  3     0.55002844  0.01191750  3.309212  0.6723969  2.746339          
  4     0.65726952  0.02599040  3.251943  0.6608670  2.680578          
  4     0.94993106  0.24880959  3.257248  0.6694215  2.698690          
  5     0.26798984  0.74404662  2.993560  0.7021494  2.433256          
  5     0.46763193  0.66963399  3.035039  0.6950206  2.518064          
  6     0.68075005  0.72596356  2.972742  0.7000770  2.431521          
  6     0.97068916  0.37559074  3.189486  0.6503748  2.624221          
  7     0.05485076  0.03734481  3.075685  0.6407325  2.585606          
  7     0.07530099  0.16253041  2.928601  0.6869425  2.382223  *       
  7     0.19376378  0.95165259  2.989057  0.6756544  2.441096          
  7     0.89262666  0.36727283  3.196012  0.6510287  2.634549          
  8     0.15803436  0.38893795  2.995597  0.6669416  2.448226          
  8     0.59246012  0.60290656  3.079606  0.6488018  2.512898          
  9     0.09174417  0.48164113  3.120443  0.6380560  2.544630          
  9     0.62319889  0.35525093  3.148388  0.6360243  2.567835          
  9     0.66479223  0.77987306  3.012706  0.6722372  2.462729          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7, coefReg = 0.07530099
 and coefImp = 0.1625304.
[1] "Mon Mar 12 15:55:55 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
In addition: There were 48 warnings (use warnings() to see them)
Regularized Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  mtry  coefReg     RMSE      Rsquared   MAE       Selected
  2     0.09515384  3.464817  0.6810492  2.886481          
  2     0.25326900  3.490620  0.6720129  2.910029          
  2     0.87193966  3.511469  0.6661673  2.924834          
  3     0.45198606  3.284294  0.6794236  2.723032          
  3     0.55002844  3.331009  0.6759247  2.777015          
  4     0.65726952  3.261720  0.6592971  2.706403          
  4     0.94993106  3.263692  0.6633583  2.709005          
  5     0.26798984  3.196936  0.6504247  2.628142          
  5     0.46763193  3.197291  0.6706545  2.652346          
  6     0.68075005  3.211266  0.6421600  2.623650          
  6     0.97068916  3.193183  0.6506982  2.623961          
  7     0.05485076  3.300835  0.5639199  2.688023          
  7     0.07530099  3.516206  0.5040387  2.939037          
  7     0.19376378  3.196332  0.6332776  2.652381          
  7     0.89262666  3.177504  0.6468468  2.619915  *       
  8     0.15803436  3.343932  0.5700667  2.660843          
  8     0.59246012  3.178040  0.6485272  2.606282          
  9     0.09174417  4.171799  0.3012611  3.401393          
  9     0.62319889  3.207133  0.6324495  2.641524          
  9     0.66479223  3.208434  0.6293986  2.637188          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 7 and coefReg = 0.8926267.
[1] "Mon Mar 12 15:56:16 2018"
Error in varImp[, "%IncMSE"] : subscript out of bounds
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: model fit failed for Resample07: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
4: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
5: model fit failed for Resample10: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
6: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
7: model fit failed for Resample12: parameter=none Error in crossprod(Kr)/var + diag(1/thetatmp) : non-conformable arrays
 
8: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
9: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:56:36 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "rvmLinear"               
Error in .local(object, ...) : test vector does not match model !
In addition: There were 25 warnings (use warnings() to see them)
Error in .local(object, ...) : test vector does not match model !
In addition: Warning messages:
1: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
2: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
3: In max(abs(logtheta[thetavec[which(nzindex)] != 0] + log(thetavec[thetavec !=  :
  no non-missing arguments to max; returning -Inf
Error in .local(object, ...) : test vector does not match model !
In addition: There were 39 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:57:15 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "rvmPoly"                 
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:57:38 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "rvmRadial"               
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |=========================================                             |  58%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |====================                                                  |  28%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  32%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |==================================                                    |  48%  |                                                                              |===================================                                   |  50%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   8%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |===========                                                           |  15%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  19%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  23%  |                                                                              |==================                                                    |  25%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  29%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  35%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |============================                                          |  40%  |                                                                              |==============================                                        |  42%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%
  |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   5%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  18%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  22%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%
Subtractive Clustering and Fuzzy c-Means Rules 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  r.a  eps.high   eps.low       RMSE      Rsquared   MAE       Selected
   2   0.4901618  0.3653186886  3.725188  0.5335362  3.028024  *       
   3   0.7744290  0.2739380442  4.778722  0.3987302  3.956646          
   4   0.6531605  0.4581178804  4.778722  0.3987219  3.956646          
   4   0.9697338  0.0797232995  4.778722  0.3987219  3.956646          
   5   0.8618884  0.3841470121       NaN        NaN       NaN          
   6   0.8159539  0.5843512511       NaN        NaN       NaN          
   6   0.9184213  0.7899983230       NaN        NaN       NaN          
   7   0.8547220  0.4210153152       NaN        NaN       NaN          
   8   0.7265472  0.2603582565       NaN        NaN       NaN          
   8   0.9351366  0.6780259046       NaN        NaN       NaN          
   9   0.9589623  0.7043351901       NaN        NaN       NaN          
  10   0.6124102  0.5344782129       NaN        NaN       NaN          
  13   0.9663533  0.2551320577       NaN        NaN       NaN          
  13   0.9884499  0.7252866123       NaN        NaN       NaN          
  16   0.4367257  0.3712804178       NaN        NaN       NaN          
  16   0.5517438  0.0100329830       NaN        NaN       NaN          
  16   0.8352474  0.6042729241       NaN        NaN       NaN          
  17   0.4286347  0.0008654157       NaN        NaN       NaN          
  18   0.6554791  0.0566454902       NaN        NaN       NaN          
  18   0.7005309  0.2671539867       NaN        NaN       NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were r.a = 2, eps.high = 0.4901618
 and eps.low = 0.3653187.
[1] "Mon Mar 12 15:58:11 2018"
Partial Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE      Rsquared   MAE       Selected
  1      2.873145  0.7062988  2.182776          
  2      2.554676  0.7457723  2.072901          
  3      2.483519  0.7575385  2.027530          
  4      2.455220  0.7624006  2.005845          
  5      2.442977  0.7641861  1.988915          
  6      2.438827  0.7648748  1.987122          
  7      2.438280  0.7649700  1.987337          
  8      2.437872  0.7650387  1.987021  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 15:58:25 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Loading required package: spikeslab
Loading required package: randomForest
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.

Attaching package: 'randomForest'

The following object is masked from 'package:ggplot2':

    margin

Loading required package: parallel

 spikeslab 1.1.5 
 
 Type spikeslab.news() to see new features, changes, and bug fixes. 
 

Spike and Slab Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  vars  RMSE      Rsquared   MAE       Selected
  2     4.479395  0.4535120  3.765208          
  3     4.178670  0.5333136  3.471984          
  4     2.576204  0.7783487  1.998932          
  5     2.462622  0.7832156  1.913686          
  6     2.435912  0.7813264  1.895977          
  7     2.424549  0.7807913  1.895587          
  8     2.412661  0.7734773  1.929545          
  9     2.411183  0.7716863  1.944348  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was vars = 9.
[1] "Mon Mar 12 15:58:51 2018"
Sparse Partial Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  kappa       eta         K   RMSE      Rsquared   MAE       Selected
  0.05312657  0.87193966   4  2.356720  0.7795473  1.914491          
  0.06909645  0.25326900   5  2.441398  0.7644874  1.989214          
  0.08369418  0.09515384   2  2.561589  0.7461479  2.094643          
  0.11424429  0.45198606   8  2.437895  0.7650335  1.987038          
  0.12305597  0.55002844   1  2.621074  0.7292023  2.071287          
  0.16238612  0.65726952   1  2.621074  0.7292023  2.071287          
  0.17238999  0.94993106   3  2.882954  0.6821255  2.309500          
  0.20887952  0.46763193   7  2.438067  0.7650035  1.987163          
  0.22321283  0.26798984   8  2.437895  0.7650335  1.987038          
  0.26673726  0.68075005   8  2.437895  0.7650335  1.987038          
  0.27897979  0.97068916   4  2.328541  0.7873506  1.854937  *       
  0.30566800  0.05485076   1  2.838586  0.7110594  2.151565          
  0.30888957  0.19376378  10  2.437903  0.7650326  1.987049          
  0.32665077  0.07530099   2  2.561589  0.7461479  2.094643          
  0.33317507  0.89262666   4  2.354966  0.7804410  1.934027          
  0.37994954  0.59246012   7  2.438067  0.7650035  1.987163          
  0.39245697  0.15803436   4  2.455715  0.7620966  1.998907          
  0.40054216  0.66479223   8  2.437895  0.7650335  1.987038          
  0.42533278  0.62319889   4  2.468067  0.7601629  2.006084          
  0.43738262  0.09174417   5  2.441398  0.7644874  1.989214          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were K = 4, eta = 0.9706892 and kappa
 = 0.2789798.
[1] "Mon Mar 12 15:59:08 2018"
Supervised Principal Component Analysis 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  threshold  n.components  RMSE      Rsquared   MAE       Selected
  0.1062531  3             15.28180  0.2919945  14.67676          
  0.1381929  1             15.23328  0.1968765  14.54337          
  0.1673884  1             15.23328  0.1968765  14.54337          
  0.2284886  2             15.29691  0.2480243  14.65958          
  0.2461119  2             15.30362  0.2701761  14.68665          
  0.3247722  2             15.31195  0.2617553  14.68040          
  0.3447800  3             15.09175  0.3421261  14.53249          
  0.4177590  2             15.25244  0.2963532  14.64550          
  0.4464257  1             15.33331  0.1953804  14.64493          
  0.5334745  3             15.08470  0.5093326  14.65883          
  0.5579596  3             15.08470  0.5093326  14.65883          
  0.6113360  1             15.34971  0.2271034  14.68842          
  0.6177791  1             15.45396  0.2024592  14.77845          
  0.6533015  1             15.45396  0.2024592  14.77845          
  0.6663501  3             14.91865  0.6180926  14.57858  *       
  0.7598991  2             15.07096  0.5016577  14.64605          
  0.7849139  1             15.32204  0.2284056  14.65810          
  0.8010843  2             15.07096  0.5016577  14.64605          
  0.8506656  2             15.07096  0.5016577  14.64605          
  0.8747652  1             15.32204  0.2284056  14.65810          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were threshold = 0.6663501 and
 n.components = 3.
[1] "Mon Mar 12 15:59:24 2018"
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:59:39 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "svmBoundrangeString"     
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 15:59:53 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "svmExpoString"           
Support Vector Machines with Linear Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  C             RMSE      Rsquared   MAE       Selected
    0.09432586  2.635205  0.7331807  2.045618          
    0.13147802  2.627480  0.7339903  2.073307          
    0.17810813  2.613720  0.7347084  2.061361          
    0.33618531  2.570859  0.7413276  2.026764          
    0.40379059  2.565657  0.7413648  2.025703  *       
    0.91483506  2.579736  0.7390700  2.018292          
    1.12638478  2.588074  0.7382371  2.027489          
    2.40558150  2.606701  0.7358211  2.043116          
    3.24087819  2.606979  0.7358002  2.042737          
    8.01175170  2.606405  0.7358867  2.042770          
   10.33447960  2.606672  0.7358492  2.042434          
   18.00143638  2.606797  0.7358051  2.043010          
   19.24867267  2.606887  0.7358388  2.042692          
   27.84841151  2.608271  0.7355826  2.044561          
   31.89486254  2.605952  0.7359722  2.042426          
   84.35994357  2.606493  0.7358568  2.042740          
  109.41821848  2.608488  0.7355604  2.043980          
  129.45122160  2.608378  0.7355604  2.044447          
  216.76432101  2.606704  0.7358286  2.042943          
  278.48937563  2.608403  0.7355759  2.044529          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 0.4037906.
[1] "Mon Mar 12 16:00:23 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Linear Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cost          RMSE      Rsquared   MAE       Selected
    0.09432586  2.635205  0.7331807  2.045618          
    0.13147802  2.627480  0.7339903  2.073307          
    0.17810813  2.613720  0.7347084  2.061361          
    0.33618531  2.570859  0.7413276  2.026764          
    0.40379059  2.565657  0.7413648  2.025703  *       
    0.91483506  2.579736  0.7390700  2.018292          
    1.12638478  2.588074  0.7382371  2.027489          
    2.40558150  2.606701  0.7358211  2.043116          
    3.24087819  2.606979  0.7358002  2.042737          
    8.01175170  2.606405  0.7358867  2.042770          
   10.33447960  2.606672  0.7358492  2.042434          
   18.00143638  2.606797  0.7358051  2.043010          
   19.24867267  2.606887  0.7358388  2.042692          
   27.84841151  2.608271  0.7355826  2.044561          
   31.89486254  2.605952  0.7359722  2.042426          
   84.35994357  2.606493  0.7358568  2.042740          
  109.41821848  2.608488  0.7355604  2.043980          
  129.45122160  2.608378  0.7355604  2.044447          
  216.76432101  2.606704  0.7358286  2.042943          
  278.48937563  2.608403  0.7355759  2.044529          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cost = 0.4037906.
[1] "Mon Mar 12 16:00:50 2018"
L2 Regularized Support Vector Machine (dual) with Linear Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  cost          Loss  RMSE       Rsquared   MAE        Selected
  4.259982e-03  L2    15.200018  0.1510264  14.362838          
  6.632912e-03  L1     9.800955  0.6117392   9.090151          
  9.942105e-03  L1     8.405626  0.6380063   7.679451          
  2.319201e-02  L1     5.619094  0.6906064   4.862555          
  2.961022e-02  L2    14.092823  0.1510264  13.195288          
  8.810971e-02  L2    11.501235  0.1805292  10.484587          
  1.162739e-01  L2    10.309047  0.2533772   9.274613          
  3.197865e-01  L1     2.522812  0.7584673   1.979280          
  4.758283e-01  L1     2.500107  0.7578869   1.974402          
  1.590511e+00  L2     2.610578  0.7398592   1.998787          
  2.233323e+00  L2     2.617333  0.7329720   2.019914          
  4.680687e+00  L1     2.452451  0.7625447   1.979080          
  5.118009e+00  L1     2.452279  0.7625699   1.979414  *       
  8.374661e+00  L1     2.457718  0.7621075   1.986038          
  1.003524e+01  L2     2.540385  0.7416176   1.978844          
  3.670696e+01  L2     2.630762  0.7308008   2.016990          
  5.192217e+01  L1     2.456871  0.7622615   1.987794          
  6.496930e+01  L2     2.670549  0.7255353   2.055976          
  1.291865e+02  L2     2.694626  0.7262807   2.090358          
  1.804312e+02  L1     2.456765  0.7622825   1.988035          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were cost = 5.118009 and Loss = L1.
[1] "Mon Mar 12 16:01:06 2018"
Support Vector Machines with Polynomial Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  degree  scale         C             RMSE      Rsquared   MAE       Selected
  1       3.194577e-05    0.24778671  4.990368  0.6744899  4.183465          
  1       2.200830e-04    2.48434089  4.890510  0.6744899  4.095512          
  1       2.488809e-03  106.77257440  2.576511  0.7413984  2.026551          
  1       8.236017e-03    0.03537227  4.937406  0.6744899  4.136689          
  1       3.049337e-02    0.04094571  4.774114  0.6846375  3.996651          
  1       4.189655e-01    1.08346647  2.565544  0.7414011  2.025644          
  2       1.953285e-05    0.04607641  4.991510  0.6744889  4.184467          
  2       2.507107e-05    0.16933548  4.990261  0.6744886  4.183371          
  2       1.064505e-04  619.42631922  2.626366  0.7343702  2.073402          
  2       2.634039e-04   71.54104347  2.633333  0.7526787  2.024786          
  2       3.012527e-03   33.00264776  2.568016  0.7489360  2.006610          
  2       4.061406e-02   59.27917160  2.480938  0.7745950  1.913462          
  2       5.393128e-01    1.42315466  2.900519  0.6819000  2.175041          
  2       1.085458e+00    0.41527643  3.449934  0.5546839  2.709394          
  2       1.398467e+00    1.55171234  3.723889  0.4889384  2.971067          
  3       3.064352e-05    4.67386774  4.911666  0.6744867  4.113779          
  3       6.882479e-05    1.78270660  4.923070  0.6744828  4.123916          
  3       1.382448e-02   16.49090237  2.247404  0.8066275  1.778455  *       
  3       2.011852e-02    1.25593690  2.458791  0.7868349  1.915678          
  3       3.342594e-02  103.83117368  2.346984  0.7928091  1.817707          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were degree = 3, scale = 0.01382448 and C
 = 16.4909.
[1] "Mon Mar 12 16:01:23 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Support Vector Machines with Radial Basis Function Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE      Rsquared   MAE       Selected
  0.01994917   29.48631178  2.312590  0.8002109  1.808693  *       
  0.02453774  716.54763206  2.386945  0.7894291  1.863810          
  0.03809889   10.92654911  2.408507  0.7880675  1.891745          
  0.04538571    0.05087363  4.676967  0.6711951  3.911256          
  0.06185785    0.09000063  4.456101  0.6725796  3.727698          
  0.06765031    0.31422927  3.531541  0.7235650  2.897928          
  0.10196162    1.26571398  2.991808  0.7214745  2.322113          
  0.10211151    1.53566181  2.961613  0.7185144  2.299271          
  0.10258705    1.28271368  2.994850  0.7203887  2.324822          
  0.10344699    0.07993210  4.603434  0.6190995  3.842354          
  0.12463215    0.03201652  4.849369  0.5718854  4.053770          
  0.12888552  804.58931184  3.209657  0.6620404  2.496864          
  0.13295997  685.35889919  3.244841  0.6557351  2.530024          
  0.13741325   81.50471503  3.282913  0.6489535  2.565176          
  0.14141584   39.62560263  3.316349  0.6429943  2.595441          
  0.14285437    0.05708286  4.782700  0.5597307  3.994225          
  0.14772175   14.07163675  3.367886  0.6339519  2.640575          
  0.15880800    9.73915432  3.456427  0.6187999  2.716102          
  0.16895490    0.22191871  4.405869  0.5697139  3.663833          
  0.16987219   27.48151824  3.542447  0.6043747  2.790035          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01994917 and C = 29.48631.
[1] "Mon Mar 12 16:01:38 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  C             RMSE      Rsquared   MAE       Selected
    0.09432586  4.433078  0.6726002  3.707724          
    0.13147802  4.228857  0.6783997  3.527253          
    0.17810813  4.009945  0.6954841  3.331981          
    0.33618531  3.428717  0.7365145  2.795183          
    0.40379059  3.204807  0.7451059  2.594283          
    0.91483506  2.793122  0.7636306  2.200551          
    1.12638478  2.694457  0.7690824  2.073147          
    2.40558150  2.619321  0.7695107  2.005671          
    3.24087819  2.534283  0.7752568  1.945455  *       
    8.01175170  2.700015  0.7483887  2.082674          
   10.33447960  2.585404  0.7626531  2.000359          
   18.00143638  2.650048  0.7567872  2.037208          
   19.24867267  2.670856  0.7507265  2.037952          
   27.84841151  2.613652  0.7583422  2.015017          
   31.89486254  2.735401  0.7420913  2.095040          
   84.35994357  2.586130  0.7634203  2.002009          
  109.41821848  2.658986  0.7552955  2.041491          
  129.45122160  2.681940  0.7507459  2.050101          
  216.76432101  2.643395  0.7560495  2.031593          
  278.48937563  2.676051  0.7526852  2.053915          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was C = 3.240878.
[1] "Mon Mar 12 16:01:53 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Support Vector Machines with Radial Basis Function Kernel 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  sigma       C             RMSE      Rsquared   MAE       Selected
  0.01994917   29.48631178  2.312590  0.8002109  1.808693  *       
  0.02453774  716.54763206  2.386945  0.7894291  1.863810          
  0.03809889   10.92654911  2.408507  0.7880675  1.891745          
  0.04538571    0.05087363  4.676967  0.6711951  3.911256          
  0.06185785    0.09000063  4.456101  0.6725796  3.727698          
  0.06765031    0.31422927  3.531541  0.7235650  2.897928          
  0.10196162    1.26571398  2.991808  0.7214745  2.322113          
  0.10211151    1.53566181  2.961613  0.7185144  2.299271          
  0.10258705    1.28271368  2.994850  0.7203887  2.324822          
  0.10344699    0.07993210  4.603434  0.6190995  3.842354          
  0.12463215    0.03201652  4.849369  0.5718854  4.053770          
  0.12888552  804.58931184  3.209657  0.6620404  2.496864          
  0.13295997  685.35889919  3.244841  0.6557351  2.530024          
  0.13741325   81.50471503  3.282913  0.6489535  2.565176          
  0.14141584   39.62560263  3.316349  0.6429943  2.595441          
  0.14285437    0.05708286  4.782700  0.5597307  3.994225          
  0.14772175   14.07163675  3.367886  0.6339519  2.640575          
  0.15880800    9.73915432  3.456427  0.6187999  2.716102          
  0.16895490    0.22191871  4.405869  0.5697139  3.663833          
  0.16987219   27.48151824  3.542447  0.6043747  2.790035          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were sigma = 0.01994917 and C = 29.48631.
[1] "Mon Mar 12 16:02:08 2018"
Error in if (mean.improvement < 0) { : 
  missing value where TRUE/FALSE needed
In addition: Warning message:
In method$predict(modelFit = modelFit, newdata = newdata, submodels = param) :
  kernlab prediction calculations failed; returning NAs
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
Error : 'x' should be a character matrix with a single column for string kernel methods
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:02:23 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "centernscale"             "HOPPER"                  
 [9] "14th20hp3cv"              "svmSpectrumString"       
Bagged CART 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results:

  RMSE      Rsquared   MAE     
  3.453869  0.5647686  2.834837

[1] "Mon Mar 12 16:02:39 2018"
Partial Least Squares 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  ncomp  RMSE      Rsquared   MAE       Selected
  1      2.873145  0.7062988  2.182776          
  2      2.554676  0.7457723  2.072901          
  3      2.483519  0.7575385  2.027530          
  4      2.455220  0.7624006  2.005845          
  5      2.442977  0.7641861  1.988915          
  6      2.438827  0.7648748  1.987122          
  7      2.438280  0.7649700  1.987337          
  8      2.437872  0.7650387  1.987021  *       

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was ncomp = 8.
[1] "Mon Mar 12 16:03:12 2018"
Error in mvrValstats(object = object, estimate = "train") : 
  could not find function "mvrValstats"
eXtreme Gradient Boosting 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  max_depth  eta         rate_drop   skip_drop  min_child_weight  subsample
   1         0.02336954  0.39684409  0.7924255   4                0.4992785
   1         0.09835572  0.32176263  0.9430486   4                0.5179600
   1         0.12028768  0.37501857  0.5653957  19                0.8881854
   1         0.28950304  0.36417591  0.4227050   2                0.8021057
   2         0.23397383  0.22936036  0.6817553   8                0.9320549
   2         0.57103990  0.36152914  0.1618932  10                0.7977210
   3         0.25309367  0.29160704  0.4496335   1                0.9508319
   3         0.44668392  0.44689362  0.5129835   9                0.8968900
   5         0.40211076  0.48152346  0.6510020  13                0.3658925
   5         0.46975333  0.13419013  0.6894379   0                0.8585882
   6         0.00813858  0.05048838  0.1114423   7                0.7440987
   6         0.36214103  0.43735138  0.5041731   4                0.8781976
   7         0.01656825  0.02352938  0.9047045  18                0.7981664
   7         0.21379531  0.43658604  0.2913077   6                0.3263039
   7         0.43585217  0.18763271  0.7760247  13                0.6907359
   7         0.46814396  0.31914162  0.4130315  14                0.5523367
   9         0.20528514  0.13180149  0.7562208   6                0.9524734
   9         0.22099643  0.13020298  0.3065971   8                0.6733478
  10         0.15003695  0.09619329  0.3983101  20                0.8687620
  10         0.22597885  0.06434046  0.2152386  20                0.8999583
  colsample_bytree  gamma      nrounds  RMSE      Rsquared   MAE       Selected
  0.5253271         1.4459536  612      2.378006  0.7896586  1.882069  *       
  0.4336373         7.3276665  654      4.022623  0.7772155  3.462225          
  0.6026350         0.7648427  168      4.910981  0.6425534  4.205661          
  0.3361312         0.3983850  875      3.843462  0.7728108  3.363261          
  0.5209012         3.2714065  785      3.371278  0.7178224  2.671270          
  0.4423987         0.7663385  618      6.219682  0.6876160  5.526195          
  0.3754161         4.6690254  139      3.974052  0.5977593  3.147570          
  0.5748896         0.4096743  447      3.518873  0.7532912  2.943361          
  0.6862645         8.0283952  418      3.914285  0.4074602  3.227939          
  0.3009323         7.1542115  229      4.299384  0.5183443  3.481823          
  0.4081631         0.7083886  247      8.283695  0.6307464  7.304323          
  0.6845524         0.5106645  760      4.722873  0.6970048  3.948167          
  0.4498363         7.1060242  325      3.186659  0.6362906  2.668323          
  0.4327143         5.2828043  851      6.940169  0.6926636  6.196543          
  0.3839149         8.8015457  534      3.106688  0.6330142  2.434909          
  0.3187483         5.2293829  802      6.179333  0.5906555  5.459693          
  0.5608104         9.9249184  107      2.814221  0.7192168  2.141023          
  0.3887971         9.6428660  667      3.815355  0.6822673  3.275100          
  0.6907229         7.7645733  345      3.588586  0.6071294  2.905939          
  0.3231787         1.9702087  558      3.207287  0.6068007  2.640366          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 612, max_depth = 1, eta
 = 0.5253271, rate_drop = 0.3968441, skip_drop = 0.7924255 and
 min_child_weight = 4.
[1] "Mon Mar 12 16:04:46 2018"
eXtreme Gradient Boosting 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  lambda        alpha         nrounds  eta        RMSE      Rsquared   MAE     
  3.398331e-05  2.289277e-01  35       2.9774755  3.673241  0.4755065  2.900414
  4.908678e-05  1.846482e-04  43       1.4007076  4.689254  0.2844360  3.627195
  6.869763e-05  2.990675e-05  20       0.2294528  4.700477  0.2819339  3.636800
  1.388171e-04  1.819409e-03  79       2.1462634  4.645613  0.2903225  3.594187
  1.700434e-04  5.625255e-03   2       0.2125166  7.851964  0.2553698  6.840086
  4.205922e-04  1.933515e-02   3       2.1318073  6.165953  0.2537049  5.169702
  5.295414e-04  5.618951e-01  25       2.3293720  3.527006  0.5197509  2.749465
  1.226861e-03  2.178511e-03  67       2.4085185  4.611504  0.2925050  3.567988
  1.706586e-03  2.187506e-04  75       0.1229023  4.668712  0.2838336  3.579769
  4.649139e-03  2.533671e-02  73       2.6404637  4.158282  0.3700849  3.273733
  6.163082e-03  7.135852e-01  38       0.5910626  3.417262  0.5448123  2.815128
  1.139410e-02  1.880415e-05   4       0.4337861  5.133159  0.2863097  4.347579
  1.227144e-02  9.307197e-05  96       0.2299016  4.299498  0.3365254  3.353244
  1.847174e-02  2.379605e-05  17       2.1982999  4.045032  0.3893198  3.162451
  2.146598e-02  2.904914e-01  37       2.8928598  3.701369  0.4682694  2.899490
  6.302247e-02  9.168549e-03  61       0.1531994  3.691770  0.4676952  2.875071
  8.405619e-02  6.168390e-05  39       0.9814219  3.640425  0.4849236  2.859721
  1.012562e-01  2.108440e-02  78       1.5688149  3.540181  0.5001109  2.772479
  1.791958e-01  1.306154e-02  36       1.5848413  3.588112  0.4879841  2.904160
  2.364973e-01  2.875549e-05  49       0.1195155  3.534790  0.5200024  2.827948
  Selected
          
          
          
          
          
          
          
          
          
          
  *       
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 38, lambda =
 0.006163082, alpha = 0.7135852 and eta = 0.5910626.
[1] "Mon Mar 12 16:05:14 2018"
eXtreme Gradient Boosting 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  eta         max_depth  gamma      colsample_bytree  min_child_weight
  0.00813858   6         0.7083886  0.5635193          5              
  0.01656825   7         7.1060242  0.5923554          7              
  0.02336954   1         1.4459536  0.4329485         11              
  0.09835572   1         7.3276665  0.4429120          7              
  0.12028768   1         0.7648427  0.6403656         15              
  0.15003695  10         7.7645733  0.6300064         20              
  0.20528514   9         9.9249184  0.6746525         13              
  0.21379531   7         5.2828043  0.3406954          6              
  0.22099643   9         9.6428660  0.5257855          4              
  0.22597885  10         1.9702087  0.6466444          1              
  0.23397383   2         3.2714065  0.6637626         11              
  0.25309367   3         4.6690254  0.6737770          3              
  0.28950304   1         0.3983850  0.5944564          1              
  0.36214103   6         0.5106645  0.6350387         20              
  0.40211076   5         8.0283952  0.3618094         20              
  0.43585217   7         8.8015457  0.5350592          4              
  0.44668392   3         0.4096743  0.6450080         14              
  0.46814396   7         5.2293829  0.4612462          0              
  0.46975333   5         7.1542115  0.6245804          0              
  0.57103990   2         0.7663385  0.5921179          7              
  subsample  nrounds  RMSE      Rsquared   MAE       Selected
  0.3119720  247      4.380746  0.6664239  3.546343          
  0.2707082  325      3.440055  0.6632388  2.861086          
  0.8421083  612      2.559562  0.7482011  2.089076          
  0.7271877  654      2.426714  0.7732564  1.902087          
  0.8087019  168      2.861808  0.6715351  2.335501          
  0.3819285  345      4.987372        NaN  4.193949          
  0.4364308  107      3.249800  0.6409135  2.671228          
  0.9029378  851      2.865944  0.7093537  2.322391          
  0.4339841  667      2.956380  0.6497669  2.260622          
  0.3331742  558      2.921878  0.6815514  2.407183          
  0.5857557  785      2.958586  0.6461548  2.399162          
  0.6810312  139      2.871614  0.6826935  2.370828          
  0.7921060  875      2.244445  0.8083199  1.693682  *       
  0.9041093  760      3.690877  0.5102495  2.904149          
  0.9717196  418      2.964159  0.6464149  2.418708          
  0.5218868  534      2.874897  0.6732084  2.197856          
  0.9187147  447      3.019093  0.6447158  2.377393          
  0.7231759  802      3.666178  0.4894748  2.933353          
  0.4400869  229      3.755994  0.4907699  3.017228          
  0.7880548  618      3.037614  0.6551438  2.407739          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nrounds = 875, max_depth = 1, eta
 = 0.289503, gamma = 0.398385, colsample_bytree = 0.5944564, min_child_weight
 = 1 and subsample = 0.792106.
[1] "Mon Mar 12 16:07:00 2018"
Self-Organizing Maps 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 50, 52 
Resampling results across tuning parameters:

  xdim  ydim  topo         user.weights  RMSE      Rsquared   MAE     
  1      8    hexagonal    0.60277982    4.241898  0.3742470  3.364605
  1     11    rectangular  0.01723262    4.998167  0.1469638  3.777734
  1     15    rectangular  0.59198390    4.021389  0.4377324  3.209105
  2      3    hexagonal    0.15070406    4.004251  0.4201358  3.107695
  2      7    hexagonal    0.17042974    4.758405  0.2430133  3.739220
  2      8    hexagonal    0.35329529    4.325736  0.4503113  3.492910
  2     10    rectangular  0.11697947    4.808502  0.2740099  3.782012
  2     13    rectangular  0.81220422    4.768269  0.2911391  3.703407
  2     14    hexagonal    0.26561058    4.778730  0.2845231  3.902539
  2     18    hexagonal    0.43419692    4.076731  0.4835262  3.218464
  3      5    hexagonal    0.11516437    4.800394  0.2984309  3.962203
  3     13    hexagonal    0.10460698    4.882494  0.2680453  3.914414
  3     16    rectangular  0.29867372    4.755413  0.3262448  3.749582
  4      5    hexagonal    0.67208446    4.894686  0.2182229  3.835599
  4     14    rectangular  0.92750670         NaN        NaN       NaN
  4     16    rectangular  0.73740805         NaN        NaN       NaN
  4     18    hexagonal    0.53249751         NaN        NaN       NaN
  5      5    rectangular  0.68839328    4.617756  0.3252085  3.782679
  7     10    hexagonal    0.49300599         NaN        NaN       NaN
  7     10    hexagonal    0.74932536         NaN        NaN       NaN
  Selected
          
          
          
  *       
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were xdim = 2, ydim = 3, user.weights
 = 0.1507041 and topo = hexagonal.
[1] "Mon Mar 12 16:07:19 2018"
Fitting Repeat 1 

# weights:  193
initial  value 12973.089936 
iter  10 value 12554.416481
final  value 12554.129777 
converged
Fitting Repeat 2 

# weights:  193
initial  value 15183.642558 
iter  10 value 14427.312238
final  value 14425.567424 
converged
Fitting Repeat 3 

# weights:  193
initial  value 14683.720432 
iter  10 value 13978.189371
final  value 13977.011080 
converged
Fitting Repeat 4 

# weights:  193
initial  value 17970.417915 
iter  10 value 17068.910117
final  value 17064.391810 
converged
Fitting Repeat 5 

# weights:  193
initial  value 14323.818641 
iter  10 value 13320.853256
final  value 13317.127207 
converged
Fitting Repeat 1 

# weights:  49
initial  value 17596.167506 
iter  10 value 16937.980833
final  value 16937.150068 
converged
Fitting Repeat 2 

# weights:  49
initial  value 19895.633435 
iter  10 value 18637.757703
final  value 18635.237850 
converged
Fitting Repeat 3 

# weights:  49
initial  value 13766.978255 
iter  10 value 13300.627422
final  value 13300.012634 
converged
Fitting Repeat 4 

# weights:  49
initial  value 16280.228039 
iter  10 value 15791.510056
final  value 15789.979935 
converged
Fitting Repeat 5 

# weights:  49
initial  value 17692.435504 
iter  10 value 16541.442974
final  value 16541.242493 
converged
Fitting Repeat 1 

# weights:  205
initial  value 14506.170854 
iter  10 value 13664.393444
final  value 13664.289878 
converged
Fitting Repeat 2 

# weights:  205
initial  value 14794.838031 
iter  10 value 14472.022050
iter  20 value 14470.722778
final  value 14470.721962 
converged
Fitting Repeat 3 

# weights:  205
initial  value 15241.626318 
iter  10 value 14169.850678
iter  20 value 14168.779846
final  value 14168.778534 
converged
Fitting Repeat 4 

# weights:  205
initial  value 15770.960817 
iter  10 value 15014.000853
iter  20 value 15012.498953
final  value 15012.498111 
converged
Fitting Repeat 5 

# weights:  205
initial  value 16276.484738 
iter  10 value 15428.893465
iter  20 value 15427.165825
final  value 15427.162794 
converged
Fitting Repeat 1 

# weights:  193
initial  value 14635.377487 
iter  10 value 14110.981915
iter  20 value 14110.690470
final  value 14110.683708 
converged
Fitting Repeat 2 

# weights:  193
initial  value 15778.080666 
iter  10 value 14438.002970
iter  20 value 14436.332307
final  value 14436.325420 
converged
Fitting Repeat 3 

# weights:  193
initial  value 17052.126359 
iter  10 value 15771.627226
iter  20 value 15767.631549
iter  30 value 15766.620374
iter  40 value 15766.494009
final  value 15766.489492 
converged
Fitting Repeat 4 

# weights:  193
initial  value 17011.518261 
iter  10 value 15689.362366
final  value 15689.320706 
converged
Fitting Repeat 5 

# weights:  193
initial  value 18614.428182 
iter  10 value 17308.203906
iter  20 value 17306.616153
final  value 17306.557700 
converged
Fitting Repeat 1 

# weights:  241
initial  value 15972.797578 
iter  10 value 14781.875027
iter  20 value 14779.594127
final  value 14779.592840 
converged
Fitting Repeat 2 

# weights:  241
initial  value 20352.082716 
iter  10 value 19370.561494
iter  20 value 19356.090295
final  value 19356.003069 
converged
Fitting Repeat 3 

# weights:  241
initial  value 18000.255503 
iter  10 value 17593.643235
iter  20 value 17590.742119
final  value 17590.741401 
converged
Fitting Repeat 4 

# weights:  241
initial  value 20331.507679 
iter  10 value 19176.728946
iter  20 value 19159.145197
final  value 19159.048781 
converged
Fitting Repeat 5 

# weights:  241
initial  value 12736.619105 
iter  10 value 12357.822315
iter  20 value 12353.482163
iter  20 value 12353.482102
iter  20 value 12353.482102
final  value 12353.482102 
converged
Fitting Repeat 1 

# weights:  205
initial  value 17093.536801 
iter  10 value 15587.995522
iter  20 value 15566.763703
iter  30 value 15566.596408
final  value 15566.591728 
converged
Fitting Repeat 2 

# weights:  205
initial  value 16060.505016 
iter  10 value 15574.460816
iter  20 value 15568.320713
iter  30 value 15566.624194
final  value 15566.595857 
converged
Fitting Repeat 3 

# weights:  205
initial  value 16192.386022 
iter  10 value 15567.476420
iter  20 value 15566.600145
final  value 15566.593913 
converged
Fitting Repeat 4 

# weights:  205
initial  value 15795.130006 
iter  10 value 15568.271069
iter  20 value 15566.620548
final  value 15566.593935 
converged
Fitting Repeat 5 

# weights:  205
initial  value 16312.591649 
iter  10 value 15577.234148
iter  20 value 15568.216945
iter  30 value 15566.955237
iter  40 value 15566.610638
final  value 15566.596875 
converged
Fitting Repeat 1 

# weights:  37
initial  value 16935.361786 
iter  10 value 15851.847768
iter  20 value 15847.279388
iter  20 value 15847.279325
iter  20 value 15847.279302
final  value 15847.279302 
converged
Fitting Repeat 2 

# weights:  37
initial  value 16023.481842 
iter  10 value 15285.194384
iter  20 value 15281.393385
final  value 15281.392842 
converged
Fitting Repeat 3 

# weights:  37
initial  value 15243.313426 
iter  10 value 14320.602280
iter  20 value 14317.403799
iter  20 value 14317.403703
iter  20 value 14317.403703
final  value 14317.403703 
converged
Fitting Repeat 4 

# weights:  37
initial  value 18396.668147 
iter  10 value 17532.503730
final  value 17531.876729 
converged
Fitting Repeat 5 

# weights:  37
initial  value 15733.660503 
iter  10 value 14985.528544
iter  20 value 14983.815995
iter  20 value 14983.815989
iter  20 value 14983.815989
final  value 14983.815989 
converged
Fitting Repeat 1 

# weights:  133
initial  value 17626.928661 
iter  10 value 16938.544767
iter  20 value 16933.110401
iter  30 value 16932.832421
iter  40 value 16932.682262
final  value 16932.675730 
converged
Fitting Repeat 2 

# weights:  133
initial  value 16603.258878 
iter  10 value 15453.354317
iter  20 value 15450.812785
iter  30 value 15450.252840
iter  40 value 15450.061484
final  value 15450.052191 
converged
Fitting Repeat 3 

# weights:  133
initial  value 17435.781941 
iter  10 value 16432.736413
iter  20 value 16432.371575
final  value 16432.360558 
converged
Fitting Repeat 4 

# weights:  133
initial  value 17475.652905 
iter  10 value 16539.106510
iter  20 value 16533.383774
iter  30 value 16532.948811
iter  40 value 16532.722200
final  value 16532.711892 
converged
Fitting Repeat 5 

# weights:  133
initial  value 17122.376841 
iter  10 value 15878.087744
iter  20 value 15875.373746
iter  30 value 15874.832553
iter  40 value 15874.702480
final  value 15874.697120 
converged
Fitting Repeat 1 

# weights:  205
initial  value 16427.080341 
iter  10 value 15567.006892
iter  20 value 15566.503472
final  value 15566.498301 
converged
Fitting Repeat 2 

# weights:  205
initial  value 16815.290442 
iter  10 value 15566.798956
iter  20 value 15566.501074
final  value 15566.498439 
converged
Fitting Repeat 3 

# weights:  205
initial  value 16417.384702 
iter  10 value 15566.947339
iter  20 value 15566.502785
final  value 15566.498402 
converged
Fitting Repeat 4 

# weights:  205
initial  value 17125.400110 
final  value 15567.309132 
converged
Fitting Repeat 5 

# weights:  205
initial  value 16543.100315 
iter  10 value 15567.140501
iter  20 value 15566.505012
final  value 15566.498333 
converged
Fitting Repeat 1 

# weights:  97
initial  value 15894.508922 
iter  10 value 15575.910778
final  value 15575.887999 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16069.789426 
iter  10 value 15576.286444
final  value 15575.887981 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16909.204429 
iter  10 value 15576.130136
final  value 15575.887990 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16622.641834 
iter  10 value 15576.071906
final  value 15575.887977 
converged
Fitting Repeat 5 

# weights:  97
initial  value 16284.243094 
iter  10 value 15577.326222
final  value 15575.887978 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16099.242796 
iter  10 value 15616.875976
iter  20 value 15613.727209
final  value 15613.725599 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16639.906477 
iter  10 value 15618.109053
iter  20 value 15613.737301
final  value 15613.725814 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16742.023657 
iter  10 value 15620.446499
iter  20 value 15613.726782
iter  20 value 15613.726722
iter  20 value 15613.726719
final  value 15613.726719 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16693.613104 
iter  10 value 15617.997628
iter  20 value 15613.743504
final  value 15613.725662 
converged
Fitting Repeat 5 

# weights:  97
initial  value 16333.910220 
iter  10 value 15621.086575
iter  20 value 15613.730207
final  value 15613.725642 
converged
Fitting Repeat 1 

# weights:  97
initial  value 18625.122999 
iter  10 value 18049.623802
iter  20 value 18048.328706
final  value 18048.317496 
converged
Fitting Repeat 2 

# weights:  97
initial  value 17435.988641 
iter  10 value 16618.286502
iter  20 value 16616.776314
final  value 16616.762292 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16656.988741 
iter  10 value 15641.867815
iter  20 value 15640.920153
final  value 15640.915514 
converged
Fitting Repeat 4 

# weights:  97
initial  value 19237.082793 
iter  10 value 18910.350727
iter  20 value 18909.781835
final  value 18909.778804 
converged
Fitting Repeat 5 

# weights:  97
initial  value 18010.093971 
iter  10 value 17012.192621
iter  20 value 17010.856880
final  value 17010.845318 
converged
Fitting Repeat 1 

# weights:  61
initial  value 16349.791821 
iter  10 value 15570.616419
iter  20 value 15566.545087
final  value 15566.519117 
converged
Fitting Repeat 2 

# weights:  61
initial  value 16306.809536 
iter  10 value 15569.422528
iter  20 value 15566.531322
final  value 15566.511153 
converged
Fitting Repeat 3 

# weights:  61
initial  value 16445.624308 
iter  10 value 15569.855574
iter  20 value 15566.536315
final  value 15566.513120 
converged
Fitting Repeat 4 

# weights:  61
initial  value 16607.607628 
iter  10 value 15569.031121
iter  20 value 15566.526810
final  value 15566.517943 
converged
Fitting Repeat 5 

# weights:  61
initial  value 16587.955024 
iter  10 value 15569.438078
iter  20 value 15566.531501
final  value 15566.516869 
converged
Fitting Repeat 1 

# weights:  133
initial  value 16421.134295 
iter  10 value 15579.915108
iter  20 value 15566.652293
final  value 15566.523852 
converged
Fitting Repeat 2 

# weights:  133
initial  value 16563.745411 
iter  10 value 15583.740969
iter  20 value 15566.696403
iter  30 value 15566.522957
final  value 15566.516412 
converged
Fitting Repeat 3 

# weights:  133
initial  value 16472.087734 
iter  10 value 15585.270962
iter  20 value 15566.714042
iter  30 value 15566.524499
final  value 15566.515161 
converged
Fitting Repeat 4 

# weights:  133
initial  value 16028.794695 
iter  10 value 15578.054477
iter  20 value 15566.630842
iter  30 value 15566.520439
final  value 15566.515206 
converged
Fitting Repeat 5 

# weights:  133
initial  value 16607.896040 
iter  10 value 15584.177602
iter  20 value 15566.701437
final  value 15566.527157 
converged
Fitting Repeat 1 

# weights:  25
initial  value 19337.057894 
iter  10 value 18516.985221
iter  20 value 18512.211210
iter  30 value 18512.070250
final  value 18512.061999 
converged
Fitting Repeat 2 

# weights:  25
initial  value 15738.682120 
iter  10 value 15130.220629
iter  20 value 15127.640844
final  value 15127.597030 
converged
Fitting Repeat 3 

# weights:  25
initial  value 14736.911372 
iter  10 value 14156.019035
iter  20 value 14152.784799
iter  30 value 14152.595661
final  value 14152.589537 
converged
Fitting Repeat 4 

# weights:  25
initial  value 13336.065869 
iter  10 value 12654.542520
iter  20 value 12651.115453
iter  30 value 12651.040328
final  value 12651.038517 
converged
Fitting Repeat 5 

# weights:  25
initial  value 14714.360615 
iter  10 value 14287.422966
iter  20 value 14284.938044
iter  30 value 14284.823082
final  value 14284.817132 
converged
Fitting Repeat 1 

# weights:  85
initial  value 16651.861983 
iter  10 value 15567.178759
iter  20 value 15566.860102
iter  30 value 15566.739426
final  value 15566.720511 
converged
Fitting Repeat 2 

# weights:  85
initial  value 16295.678366 
iter  10 value 15567.557995
iter  20 value 15566.817247
iter  30 value 15566.722482
final  value 15566.718438 
converged
Fitting Repeat 3 

# weights:  85
initial  value 16668.373028 
iter  10 value 15567.412488
iter  20 value 15566.796071
iter  30 value 15566.723312
final  value 15566.718322 
converged
Fitting Repeat 4 

# weights:  85
initial  value 16580.184923 
iter  10 value 15576.057577
iter  20 value 15567.354502
iter  30 value 15566.740725
iter  40 value 15566.719107
final  value 15566.718740 
converged
Fitting Repeat 5 

# weights:  85
initial  value 16462.767613 
iter  10 value 15580.797284
iter  20 value 15567.071979
iter  30 value 15566.782309
iter  40 value 15566.725756
final  value 15566.719379 
converged
Fitting Repeat 1 

# weights:  73
initial  value 16811.703838 
iter  10 value 15826.671441
iter  20 value 15824.586913
final  value 15824.577189 
converged
Fitting Repeat 2 

# weights:  73
initial  value 18069.978548 
iter  10 value 17424.854113
iter  20 value 17422.959498
final  value 17422.946304 
converged
Fitting Repeat 3 

# weights:  73
initial  value 14072.227455 
iter  10 value 13494.843349
iter  20 value 13492.948896
final  value 13492.935098 
converged
Fitting Repeat 4 

# weights:  73
initial  value 17575.887303 
iter  10 value 16857.437926
iter  20 value 16854.981946
final  value 16854.963117 
converged
Fitting Repeat 5 

# weights:  73
initial  value 14990.080364 
iter  10 value 13776.743624
final  value 13776.042715 
converged
Fitting Repeat 1 

# weights:  109
initial  value 16997.830301 
iter  10 value 16427.438274
final  value 16427.373572 
converged
Fitting Repeat 2 

# weights:  109
initial  value 16253.773071 
iter  10 value 15668.021730
iter  20 value 15667.515402
final  value 15667.513936 
converged
Fitting Repeat 3 

# weights:  109
initial  value 17947.446219 
iter  10 value 17078.818664
iter  20 value 17078.478654
final  value 17078.477947 
converged
Fitting Repeat 4 

# weights:  109
initial  value 14817.136973 
iter  10 value 14322.060895
iter  20 value 14321.869322
iter  20 value 14321.869277
iter  20 value 14321.869262
final  value 14321.869262 
converged
Fitting Repeat 5 

# weights:  109
initial  value 16401.673090 
iter  10 value 15283.957944
iter  20 value 15281.928836
final  value 15281.927055 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16876.310121 
iter  10 value 16158.941228
iter  20 value 16157.785975
final  value 16157.775703 
converged
Fitting Repeat 2 

# weights:  97
initial  value 13018.987330 
iter  10 value 12402.228324
iter  20 value 12401.586585
final  value 12401.582117 
converged
Fitting Repeat 3 

# weights:  97
initial  value 13881.339333 
iter  10 value 13324.736251
iter  20 value 13324.135408
final  value 13324.131856 
converged
Fitting Repeat 4 

# weights:  97
initial  value 15726.195010 
iter  10 value 14646.313950
iter  20 value 14645.722597
final  value 14645.721700 
converged
Fitting Repeat 5 

# weights:  97
initial  value 15870.975296 
iter  10 value 14829.292364
iter  20 value 14828.608773
final  value 14828.605417 
converged
Fitting Repeat 1 

# weights:  181
initial  value 16007.021184 
iter  10 value 15571.704590
final  value 15569.063540 
converged
Fitting Repeat 2 

# weights:  181
initial  value 16164.987779 
iter  10 value 15569.913641
iter  20 value 15569.097459
final  value 15569.063540 
converged
Fitting Repeat 3 

# weights:  181
initial  value 16093.396361 
iter  10 value 15573.681161
iter  20 value 15569.201891
final  value 15569.063542 
converged
Fitting Repeat 4 

# weights:  181
initial  value 16247.715138 
iter  10 value 15583.285326
iter  20 value 15569.063807
final  value 15569.063536 
converged
Fitting Repeat 5 

# weights:  181
initial  value 16679.627041 
iter  10 value 15569.701014
final  value 15569.063545 
converged
Fitting Repeat 1 

# weights:  193
initial  value 18480.366948 
iter  10 value 17306.212184
iter  20 value 17279.710164
final  value 17279.032008 
converged
Fitting Repeat 2 

# weights:  193
initial  value 17019.066143 
iter  10 value 16248.253457
final  value 16246.959167 
converged
Fitting Repeat 3 

# weights:  193
initial  value 16824.623949 
iter  10 value 16244.781657
final  value 16244.053292 
converged
Fitting Repeat 4 

# weights:  193
initial  value 18365.545477 
iter  10 value 17335.290569
final  value 17330.104444 
converged
Fitting Repeat 5 

# weights:  193
initial  value 17406.810103 
iter  10 value 16305.408951
final  value 16300.172876 
converged
Fitting Repeat 1 

# weights:  49
initial  value 18811.999479 
iter  10 value 17844.892728
final  value 17843.454025 
converged
Fitting Repeat 2 

# weights:  49
initial  value 17086.610720 
iter  10 value 16381.633844
final  value 16380.874139 
converged
Fitting Repeat 3 

# weights:  49
initial  value 18386.856770 
iter  10 value 17730.351037
final  value 17729.024551 
converged
Fitting Repeat 4 

# weights:  49
initial  value 14930.501633 
iter  10 value 14142.322957
final  value 14140.731837 
converged
Fitting Repeat 5 

# weights:  49
initial  value 18692.456186 
iter  10 value 17523.284958
final  value 17521.255257 
converged
Fitting Repeat 1 

# weights:  205
initial  value 18779.572277 
iter  10 value 17984.463087
iter  20 value 17980.116389
iter  20 value 17980.116269
iter  20 value 17980.116261
final  value 17980.116261 
converged
Fitting Repeat 2 

# weights:  205
initial  value 18740.097593 
iter  10 value 17992.722017
iter  20 value 17990.806008
final  value 17990.800042 
converged
Fitting Repeat 3 

# weights:  205
initial  value 18336.118175 
iter  10 value 17243.345823
final  value 17243.127812 
converged
Fitting Repeat 4 

# weights:  205
initial  value 18889.820288 
iter  10 value 17689.735866
iter  20 value 17689.149883
iter  20 value 17689.149765
iter  20 value 17689.149735
final  value 17689.149735 
converged
Fitting Repeat 5 

# weights:  205
initial  value 18409.979255 
iter  10 value 17568.676528
final  value 17568.602016 
converged
Fitting Repeat 1 

# weights:  193
initial  value 18199.103206 
iter  10 value 17731.736712
iter  20 value 17730.443961
final  value 17730.443466 
converged
Fitting Repeat 2 

# weights:  193
initial  value 18055.193505 
iter  10 value 16761.549375
iter  20 value 16761.262424
iter  30 value 16761.172487
final  value 16761.158474 
converged
Fitting Repeat 3 

# weights:  193
initial  value 16945.148880 
iter  10 value 16393.835123
iter  20 value 16392.326590
final  value 16392.301024 
converged
Fitting Repeat 4 

# weights:  193
initial  value 18647.844639 
iter  10 value 17904.331793
iter  20 value 17903.655135
final  value 17903.629090 
converged
Fitting Repeat 5 

# weights:  193
initial  value 17917.829174 
iter  10 value 17515.681275
iter  20 value 17515.411054
final  value 17515.402769 
converged
Fitting Repeat 1 

# weights:  241
initial  value 19273.807442 
iter  10 value 17533.927154
iter  20 value 17531.934497
final  value 17531.929294 
converged
Fitting Repeat 2 

# weights:  241
initial  value 17937.852407 
iter  10 value 16807.622868
iter  20 value 16794.555843
final  value 16794.554718 
converged
Fitting Repeat 3 

# weights:  241
initial  value 19531.518737 
iter  10 value 18894.934044
final  value 18885.904057 
converged
Fitting Repeat 4 

# weights:  241
initial  value 15938.084532 
iter  10 value 14638.512213
iter  20 value 14637.574264
iter  20 value 14637.574157
iter  20 value 14637.574104
final  value 14637.574104 
converged
Fitting Repeat 5 

# weights:  241
initial  value 17089.925946 
iter  10 value 16029.507692
iter  20 value 16011.591111
final  value 16011.588735 
converged
Fitting Repeat 1 

# weights:  205
initial  value 16820.287209 
iter  10 value 16214.182232
iter  20 value 16213.358072
final  value 16213.346753 
converged
Fitting Repeat 2 

# weights:  205
initial  value 17039.282762 
iter  10 value 16224.081581
iter  20 value 16215.186018
iter  30 value 16213.527748
iter  40 value 16213.357281
final  value 16213.345683 
converged
Fitting Repeat 3 

# weights:  205
initial  value 16809.107758 
iter  10 value 16215.294942
iter  20 value 16213.381536
final  value 16213.347759 
converged
Fitting Repeat 4 

# weights:  205
initial  value 16504.281333 
iter  10 value 16214.128448
final  value 16213.366865 
converged
Fitting Repeat 5 

# weights:  205
initial  value 17449.057595 
iter  10 value 16214.119865
iter  20 value 16213.397522
final  value 16213.343884 
converged
Fitting Repeat 1 

# weights:  37
initial  value 18855.001589 
iter  10 value 17756.169298
final  value 17754.534721 
converged
Fitting Repeat 2 

# weights:  37
initial  value 14803.628034 
iter  10 value 14016.363379
final  value 14014.006739 
converged
Fitting Repeat 3 

# weights:  37
initial  value 15706.163252 
iter  10 value 15312.556288
final  value 15311.363660 
converged
Fitting Repeat 4 

# weights:  37
initial  value 17870.836075 
iter  10 value 17354.836283
final  value 17353.641137 
converged
Fitting Repeat 5 

# weights:  37
initial  value 18729.214109 
iter  10 value 17835.484296
final  value 17834.550533 
converged
Fitting Repeat 1 

# weights:  133
initial  value 14658.650053 
iter  10 value 14088.065989
iter  20 value 14088.012861
final  value 14087.971655 
converged
Fitting Repeat 2 

# weights:  133
initial  value 13730.547975 
iter  10 value 12965.755227
iter  20 value 12962.444310
iter  30 value 12961.940088
iter  40 value 12961.804038
final  value 12961.803496 
converged
Fitting Repeat 3 

# weights:  133
initial  value 17643.047751 
iter  10 value 16971.461335
iter  20 value 16967.821332
iter  30 value 16967.024590
final  value 16966.985812 
converged
Fitting Repeat 4 

# weights:  133
initial  value 16854.626590 
iter  10 value 15711.104145
iter  20 value 15708.584147
iter  30 value 15707.510581
final  value 15707.502800 
converged
Fitting Repeat 5 

# weights:  133
initial  value 17984.657432 
iter  10 value 17366.319974
iter  20 value 17365.951856
final  value 17365.934487 
converged
Fitting Repeat 1 

# weights:  205
initial  value 16978.094655 
iter  10 value 16213.716706
iter  20 value 16213.253997
final  value 16213.249170 
converged
Fitting Repeat 2 

# weights:  205
initial  value 16761.823944 
iter  10 value 16213.700686
iter  20 value 16213.253812
final  value 16213.249159 
converged
Fitting Repeat 3 

# weights:  205
initial  value 17617.595055 
iter  10 value 16213.491380
iter  20 value 16213.251399
final  value 16213.249322 
converged
Fitting Repeat 4 

# weights:  205
initial  value 17453.264085 
iter  10 value 16213.656193
iter  20 value 16213.253299
final  value 16213.249327 
converged
Fitting Repeat 5 

# weights:  205
initial  value 16951.006954 
iter  10 value 16213.749219
iter  20 value 16213.254372
final  value 16213.249171 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16741.078935 
iter  10 value 16222.915140
final  value 16222.761512 
converged
Fitting Repeat 2 

# weights:  97
initial  value 17081.980347 
iter  10 value 16229.004474
final  value 16222.761513 
converged
Fitting Repeat 3 

# weights:  97
initial  value 17467.176019 
iter  10 value 16223.983341
final  value 16222.761556 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16962.449493 
iter  10 value 16222.798457
final  value 16222.761510 
converged
Fitting Repeat 5 

# weights:  97
initial  value 17358.179622 
iter  10 value 16226.855469
final  value 16222.761514 
converged
Fitting Repeat 1 

# weights:  97
initial  value 17841.248247 
iter  10 value 16264.716597
iter  20 value 16261.132185
final  value 16261.131571 
converged
Fitting Repeat 2 

# weights:  97
initial  value 17148.977795 
iter  10 value 16267.799347
iter  20 value 16261.145339
final  value 16261.131575 
converged
Fitting Repeat 3 

# weights:  97
initial  value 17338.249493 
iter  10 value 16268.324996
iter  20 value 16261.138453
final  value 16261.131605 
converged
Fitting Repeat 4 

# weights:  97
initial  value 17034.729062 
iter  10 value 16271.900460
iter  20 value 16261.142537
final  value 16261.131639 
converged
Fitting Repeat 5 

# weights:  97
initial  value 17217.780783 
iter  10 value 16271.460353
iter  20 value 16261.237680
final  value 16261.131778 
converged
Fitting Repeat 1 

# weights:  97
initial  value 17807.187653 
iter  10 value 16753.183137
iter  20 value 16751.805172
final  value 16751.793748 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16035.878622 
iter  10 value 14775.363811
iter  20 value 14774.548718
final  value 14774.548205 
converged
Fitting Repeat 3 

# weights:  97
initial  value 17145.988127 
iter  10 value 15889.795600
iter  20 value 15888.778958
final  value 15888.771504 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16869.370537 
iter  10 value 15677.260326
iter  20 value 15676.479112
final  value 15676.474873 
converged
Fitting Repeat 5 

# weights:  97
initial  value 15188.881384 
iter  10 value 14523.997054
iter  20 value 14523.008431
final  value 14522.999936 
converged
Fitting Repeat 1 

# weights:  61
initial  value 16950.262783 
iter  10 value 16216.422214
iter  20 value 16213.285189
final  value 16213.265091 
converged
Fitting Repeat 2 

# weights:  61
initial  value 16982.327106 
iter  10 value 16216.327090
iter  20 value 16213.284093
final  value 16213.267570 
converged
Fitting Repeat 3 

# weights:  61
initial  value 17179.070650 
iter  10 value 16217.040215
iter  20 value 16213.292314
final  value 16213.272347 
converged
Fitting Repeat 4 

# weights:  61
initial  value 17255.269669 
iter  10 value 16216.390395
iter  20 value 16213.288271
iter  30 value 16213.268288
final  value 16213.268028 
converged
Fitting Repeat 5 

# weights:  61
initial  value 17402.546527 
iter  10 value 16216.076691
iter  20 value 16213.311459
final  value 16213.269042 
converged
Fitting Repeat 1 

# weights:  133
initial  value 17178.035609 
iter  10 value 16226.343642
iter  20 value 16213.399576
iter  30 value 16213.270506
final  value 16213.268631 
converged
Fitting Repeat 2 

# weights:  133
initial  value 17069.502490 
iter  10 value 16231.575873
iter  20 value 16213.459899
iter  30 value 16213.274433
final  value 16213.267021 
converged
Fitting Repeat 3 

# weights:  133
initial  value 17356.921123 
iter  10 value 16227.926223
iter  20 value 16213.417821
final  value 16213.279403 
converged
Fitting Repeat 4 

# weights:  133
initial  value 16996.046064 
iter  10 value 16229.735499
iter  20 value 16213.438681
final  value 16213.279582 
converged
Fitting Repeat 5 

# weights:  133
initial  value 16979.335783 
iter  10 value 16229.716171
iter  20 value 16213.438458
iter  30 value 16213.268378
final  value 16213.267293 
converged
Fitting Repeat 1 

# weights:  25
initial  value 16474.196429 
iter  10 value 15535.253977
iter  20 value 15531.542806
iter  30 value 15531.334025
final  value 15531.329315 
converged
Fitting Repeat 2 

# weights:  25
initial  value 16842.336680 
iter  10 value 15711.612926
iter  20 value 15709.186168
final  value 15709.022375 
converged
Fitting Repeat 3 

# weights:  25
initial  value 18039.198751 
iter  10 value 17073.386204
iter  20 value 17069.980623
iter  30 value 17069.768441
final  value 17069.764033 
converged
Fitting Repeat 4 

# weights:  25
initial  value 16245.467212 
iter  10 value 15379.156284
iter  20 value 15375.308862
iter  30 value 15374.925605
final  value 15374.901800 
converged
Fitting Repeat 5 

# weights:  25
initial  value 15839.515117 
iter  10 value 15232.383518
iter  20 value 15229.215500
iter  30 value 15229.078894
final  value 15229.065011 
converged
Fitting Repeat 1 

# weights:  85
initial  value 17078.826253 
iter  10 value 16214.816756
iter  20 value 16213.550042
iter  30 value 16213.473185
final  value 16213.472779 
converged
Fitting Repeat 2 

# weights:  85
initial  value 17162.228017 
iter  10 value 16221.244586
iter  20 value 16214.101613
iter  30 value 16213.509812
iter  40 value 16213.472301
iter  40 value 16213.472249
iter  40 value 16213.472225
final  value 16213.472225 
converged
Fitting Repeat 3 

# weights:  85
initial  value 17095.547621 
iter  10 value 16222.698742
iter  20 value 16214.333900
iter  30 value 16213.515331
final  value 16213.473151 
converged
Fitting Repeat 4 

# weights:  85
initial  value 17335.722913 
iter  10 value 16222.788579
iter  20 value 16214.222105
iter  30 value 16213.495151
final  value 16213.472487 
converged
Fitting Repeat 5 

# weights:  85
initial  value 17277.980091 
iter  10 value 16223.413108
iter  20 value 16214.238017
iter  30 value 16213.555194
iter  40 value 16213.475087
final  value 16213.473797 
converged
Fitting Repeat 1 

# weights:  73
initial  value 16096.363679 
iter  10 value 14963.151871
iter  20 value 14961.847181
final  value 14961.845868 
converged
Fitting Repeat 2 

# weights:  73
initial  value 17020.370297 
iter  10 value 16080.351265
iter  20 value 16077.961770
final  value 16077.946882 
converged
Fitting Repeat 3 

# weights:  73
initial  value 15812.575519 
iter  10 value 15133.578237
iter  20 value 15131.937833
final  value 15131.925273 
converged
Fitting Repeat 4 

# weights:  73
initial  value 15399.435238 
iter  10 value 14443.660308
iter  20 value 14441.864348
final  value 14441.852661 
converged
Fitting Repeat 5 

# weights:  73
initial  value 13077.606190 
iter  10 value 12697.543344
iter  20 value 12696.704781
final  value 12696.701326 
converged
Fitting Repeat 1 

# weights:  109
initial  value 13910.598110 
iter  10 value 13083.477913
iter  20 value 13083.041824
iter  20 value 13083.041747
iter  20 value 13083.041716
final  value 13083.041716 
converged
Fitting Repeat 2 

# weights:  109
initial  value 13447.484953 
iter  10 value 12844.774478
iter  20 value 12844.558217
final  value 12844.557817 
converged
Fitting Repeat 3 

# weights:  109
initial  value 15143.267227 
iter  10 value 14260.534651
iter  20 value 14254.779819
final  value 14254.773123 
converged
Fitting Repeat 4 

# weights:  109
initial  value 16638.296062 
iter  10 value 16229.584397
iter  20 value 16229.210504
iter  20 value 16229.210414
iter  20 value 16229.210411
final  value 16229.210411 
converged
Fitting Repeat 5 

# weights:  109
initial  value 17073.398871 
iter  10 value 16307.762157
iter  20 value 16307.245586
final  value 16307.244593 
converged
Fitting Repeat 1 

# weights:  97
initial  value 17334.029036 
iter  10 value 16473.792244
iter  20 value 16472.777931
final  value 16472.769880 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16552.072153 
iter  10 value 15368.547963
iter  20 value 15367.981377
iter  20 value 15367.981303
iter  20 value 15367.981280
final  value 15367.981280 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16517.317661 
iter  10 value 15498.987615
iter  20 value 15498.250992
final  value 15498.246714 
converged
Fitting Repeat 4 

# weights:  97
initial  value 14550.438566 
iter  10 value 13308.401954
final  value 13308.123161 
converged
Fitting Repeat 5 

# weights:  97
initial  value 18458.422697 
iter  10 value 17410.668170
iter  20 value 17409.879795
final  value 17409.874247 
converged
Fitting Repeat 1 

# weights:  181
initial  value 17232.874072 
iter  10 value 16218.866417
final  value 16215.845314 
converged
Fitting Repeat 2 

# weights:  181
initial  value 17148.868297 
iter  10 value 16235.258487
final  value 16215.845041 
converged
Fitting Repeat 3 

# weights:  181
initial  value 17230.359764 
iter  10 value 16229.880744
iter  20 value 16215.845103
iter  20 value 16215.845043
iter  20 value 16215.845042
final  value 16215.845042 
converged
Fitting Repeat 4 

# weights:  181
initial  value 17400.754861 
iter  10 value 16218.336502
iter  20 value 16216.031791
final  value 16215.845048 
converged
Fitting Repeat 5 

# weights:  181
initial  value 17103.846258 
iter  10 value 16217.492175
final  value 16215.845047 
converged
Fitting Repeat 1 

# weights:  193
initial  value 15304.786470 
iter  10 value 14146.701769
final  value 14137.524963 
converged
Fitting Repeat 2 

# weights:  193
initial  value 14500.158032 
iter  10 value 13991.540009
final  value 13990.212280 
converged
Fitting Repeat 3 

# weights:  193
initial  value 19501.258555 
iter  10 value 18513.547095
final  value 18507.900832 
converged
Fitting Repeat 4 

# weights:  193
initial  value 15926.233289 
iter  10 value 15599.027234
final  value 15598.963955 
converged
Fitting Repeat 5 

# weights:  193
initial  value 18435.908432 
iter  10 value 17580.068718
final  value 17574.488714 
converged
Fitting Repeat 1 

# weights:  49
initial  value 16114.477380 
iter  10 value 15578.058059
final  value 15577.505323 
converged
Fitting Repeat 2 

# weights:  49
initial  value 16391.493626 
iter  10 value 15269.714175
iter  20 value 15225.360474
final  value 15225.283176 
converged
Fitting Repeat 3 

# weights:  49
initial  value 17235.987192 
iter  10 value 16170.522071
final  value 16169.459448 
converged
Fitting Repeat 4 

# weights:  49
initial  value 16567.955659 
iter  10 value 15601.785369
final  value 15599.397322 
converged
Fitting Repeat 5 

# weights:  49
initial  value 16792.919821 
iter  10 value 16174.513150
iter  20 value 16172.045648
iter  20 value 16172.045517
iter  20 value 16172.045513
final  value 16172.045513 
converged
Fitting Repeat 1 

# weights:  205
initial  value 15178.384651 
iter  10 value 13962.385607
final  value 13962.373766 
converged
Fitting Repeat 2 

# weights:  205
initial  value 16951.281325 
iter  10 value 16160.614026
final  value 16157.205958 
converged
Fitting Repeat 3 

# weights:  205
initial  value 17964.129175 
iter  10 value 17029.772793
iter  20 value 17028.868683
iter  20 value 17028.868543
iter  20 value 17028.868524
final  value 17028.868524 
converged
Fitting Repeat 4 

# weights:  205
initial  value 14893.628456 
iter  10 value 14668.983914
final  value 14668.731548 
converged
Fitting Repeat 5 

# weights:  205
initial  value 14405.195194 
iter  10 value 13468.482327
final  value 13464.643120 
converged
Fitting Repeat 1 

# weights:  193
initial  value 17703.927415 
iter  10 value 16803.306913
iter  20 value 16802.439082
iter  20 value 16802.438960
iter  20 value 16802.438831
final  value 16802.438831 
converged
Fitting Repeat 2 

# weights:  193
initial  value 18230.453766 
iter  10 value 17664.427050
iter  20 value 17663.872379
iter  30 value 17663.823862
iter  30 value 17663.823779
iter  30 value 17663.823726
final  value 17663.823726 
converged
Fitting Repeat 3 

# weights:  193
initial  value 18706.423200 
iter  10 value 18154.103800
iter  20 value 18153.742978
final  value 18153.728466 
converged
Fitting Repeat 4 

# weights:  193
initial  value 16831.048223 
iter  10 value 15661.725517
iter  20 value 15660.945867
iter  30 value 15660.902957
final  value 15660.898224 
converged
Fitting Repeat 5 

# weights:  193
initial  value 17088.521364 
iter  10 value 16580.410845
iter  20 value 16574.033313
iter  30 value 16572.718751
iter  40 value 16572.537710
final  value 16572.533291 
converged
Fitting Repeat 1 

# weights:  241
initial  value 16022.435062 
iter  10 value 14653.965396
final  value 14653.637007 
converged
Fitting Repeat 2 

# weights:  241
initial  value 16202.980242 
iter  10 value 15610.569683
iter  20 value 15601.229781
iter  20 value 15601.229700
iter  20 value 15601.229640
final  value 15601.229640 
converged
Fitting Repeat 3 

# weights:  241
initial  value 19004.828002 
iter  10 value 18526.586682
iter  20 value 18519.043493
iter  20 value 18519.043436
iter  20 value 18519.043382
final  value 18519.043382 
converged
Fitting Repeat 4 

# weights:  241
initial  value 17229.521598 
iter  10 value 16081.829839
final  value 16074.488077 
converged
Fitting Repeat 5 

# weights:  241
initial  value 18764.814308 
iter  10 value 17867.538786
final  value 17859.873065 
converged
Fitting Repeat 1 

# weights:  205
initial  value 16502.215296 
iter  10 value 15767.825311
iter  20 value 15767.013238
final  value 15767.010237 
converged
Fitting Repeat 2 

# weights:  205
initial  value 16750.252871 
iter  10 value 15767.364242
iter  20 value 15767.065379
iter  30 value 15767.008140
iter  30 value 15767.008100
iter  30 value 15767.008073
final  value 15767.008073 
converged
Fitting Repeat 3 

# weights:  205
initial  value 16317.928379 
iter  10 value 15767.961335
iter  20 value 15767.147554
iter  30 value 15767.008958
final  value 15767.008320 
converged
Fitting Repeat 4 

# weights:  205
initial  value 17028.384082 
iter  10 value 15767.858261
iter  20 value 15767.013042
final  value 15767.011249 
converged
Fitting Repeat 5 

# weights:  205
initial  value 17105.132298 
iter  10 value 15767.723834
iter  20 value 15767.016972
final  value 15767.008315 
converged
Fitting Repeat 1 

# weights:  37
initial  value 17164.233195 
iter  10 value 16228.203955
final  value 16226.488411 
converged
Fitting Repeat 2 

# weights:  37
initial  value 16487.004056 
iter  10 value 15505.141477
iter  20 value 15501.202219
final  value 15501.201488 
converged
Fitting Repeat 3 

# weights:  37
initial  value 12504.712320 
iter  10 value 11602.132890
final  value 11597.749417 
converged
Fitting Repeat 4 

# weights:  37
initial  value 16970.428174 
iter  10 value 16147.750478
final  value 16146.723030 
converged
Fitting Repeat 5 

# weights:  37
initial  value 15600.344791 
iter  10 value 14952.205016
iter  20 value 14946.508436
final  value 14946.507692 
converged
Fitting Repeat 1 

# weights:  133
initial  value 19392.740569 
iter  10 value 18008.636632
iter  20 value 18007.200292
final  value 18007.185593 
converged
Fitting Repeat 2 

# weights:  133
initial  value 17024.264195 
iter  10 value 16198.310909
iter  20 value 16197.953529
iter  30 value 16197.938517
iter  30 value 16197.938458
iter  30 value 16197.938448
final  value 16197.938448 
converged
Fitting Repeat 3 

# weights:  133
initial  value 19017.834559 
iter  10 value 17621.872796
iter  20 value 17621.455405
iter  30 value 17621.315042
final  value 17621.311568 
converged
Fitting Repeat 4 

# weights:  133
initial  value 17585.020965 
iter  10 value 17188.657607
iter  20 value 17188.318959
iter  30 value 17188.246895
final  value 17188.235659 
converged
Fitting Repeat 5 

# weights:  133
initial  value 15273.628310 
iter  10 value 14283.111943
iter  20 value 14282.696800
iter  30 value 14282.624440
final  value 14282.622977 
converged
Fitting Repeat 1 

# weights:  205
initial  value 17123.482104 
iter  10 value 15767.181873
iter  20 value 15766.915111
final  value 15766.912752 
converged
Fitting Repeat 2 

# weights:  205
initial  value 17038.196605 
iter  10 value 15767.246264
iter  20 value 15766.915854
final  value 15766.912931 
converged
Fitting Repeat 3 

# weights:  205
initial  value 16399.069976 
iter  10 value 15767.368484
iter  20 value 15766.917263
final  value 15766.912572 
converged
Fitting Repeat 4 

# weights:  205
initial  value 17135.017716 
iter  10 value 15767.090360
iter  20 value 15766.914056
final  value 15766.912852 
converged
Fitting Repeat 5 

# weights:  205
initial  value 16786.452930 
iter  10 value 15767.365769
iter  20 value 15766.917232
final  value 15766.912809 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16466.874617 
iter  10 value 15781.693304
final  value 15776.433769 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16462.579356 
iter  10 value 15777.025266
final  value 15776.433998 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16763.783405 
iter  10 value 15778.737437
final  value 15776.433756 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16804.425437 
iter  10 value 15777.586613
final  value 15776.433762 
converged
Fitting Repeat 5 

# weights:  97
initial  value 16604.311059 
iter  10 value 15776.894699
final  value 15776.433780 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16475.360128 
iter  10 value 15827.570305
iter  20 value 15814.745325
final  value 15814.726678 
converged
Fitting Repeat 2 

# weights:  97
initial  value 16929.725291 
iter  10 value 15841.670640
iter  20 value 15814.846801
iter  30 value 15814.728373
final  value 15814.726761 
converged
Fitting Repeat 3 

# weights:  97
initial  value 16349.017132 
iter  10 value 15819.767729
iter  20 value 15814.728341
final  value 15814.726689 
converged
Fitting Repeat 4 

# weights:  97
initial  value 16831.548403 
iter  10 value 15823.783183
iter  20 value 15814.742839
final  value 15814.726784 
converged
Fitting Repeat 5 

# weights:  97
initial  value 16385.597277 
iter  10 value 15825.157075
iter  20 value 15814.733930
final  value 15814.726746 
converged
Fitting Repeat 1 

# weights:  97
initial  value 16481.233628 
iter  10 value 15315.280360
iter  20 value 15314.554664
final  value 15314.552046 
converged
Fitting Repeat 2 

# weights:  97
initial  value 17848.774323 
iter  10 value 16728.540372
iter  20 value 16727.357694
final  value 16727.348622 
converged
Fitting Repeat 3 

# weights:  97
initial  value 15907.500010 
iter  10 value 15033.012464
iter  20 value 15031.586334
final  value 15031.576156 
converged
Fitting Repeat 4 

# weights:  97
initial  value 13185.503948 
iter  10 value 12778.299769
iter  20 value 12777.617755
final  value 12777.613029 
converged
Fitting Repeat 5 

# weights:  97
initial  value 14502.590807 
iter  10 value 13643.947352
iter  20 value 13642.902388
final  value 13642.893950 
converged
Fitting Repeat 1 

# weights:  61
initial  value 16599.489549 
iter  10 value 15770.396374
iter  20 value 15766.952172
final  value 15766.929207 
converged
Fitting Repeat 2 

# weights:  61
initial  value 16744.373636 
iter  10 value 15770.081661
iter  20 value 15766.948544
final  value 15766.927960 
converged
Fitting Repeat 3 

# weights:  61
initial  value 16973.523653 
iter  10 value 15769.436184
iter  20 value 15766.941102
final  value 15766.936263 
converged
Fitting Repeat 4 

# weights:  61
initial  value 16809.259860 
iter  10 value 15769.430121
iter  20 value 15766.942083
final  value 15766.939657 
converged
Fitting Repeat 5 

# weights:  61
initial  value 16465.212644 
iter  10 value 15769.788445
iter  20 value 15766.945163
final  value 15766.927438 
converged
Fitting Repeat 1 

# weights:  133
initial  value 16156.041391 
iter  10 value 15774.226657
iter  20 value 15766.996332
final  value 15766.935725 
converged
Fitting Repeat 2 

# weights:  133
initial  value 16805.815931 
iter  10 value 15788.383478
iter  20 value 15767.159549
final  value 15766.941631 
converged
Fitting Repeat 3 

# weights:  133
initial  value 16623.310330 
iter  10 value 15780.553099
iter  20 value 15767.069271
iter  30 value 15766.932511
final  value 15766.930730 
converged
Fitting Repeat 4 

# weights:  133
initial  value 17072.184945 
iter  10 value 15775.010385
iter  20 value 15767.005368
final  value 15766.931337 
converged
Fitting Repeat 5 

# weights:  133
initial  value 16462.990154 
iter  10 value 15785.111554
iter  20 value 15767.121827
final  value 15766.939623 
converged
Fitting Repeat 1 

# weights:  25
initial  value 15130.018794 
iter  10 value 14126.973726
iter  20 value 14124.039896
iter  30 value 14123.829244
final  value 14123.826445 
converged
Fitting Repeat 2 

# weights:  25
initial  value 17632.529333 
iter  10 value 16750.476690
iter  20 value 16746.444385
iter  30 value 16746.199073
final  value 16746.158694 
converged
Fitting Repeat 3 

# weights:  25
initial  value 14120.104377 
iter  10 value 13153.472957
iter  20 value 13151.691169
iter  30 value 13151.658964
iter  30 value 13151.658875
iter  30 value 13151.658872
final  value 13151.658872 
converged
Fitting Repeat 4 

# weights:  25
initial  value 17407.380725 
iter  10 value 16647.699069
iter  20 value 16643.699828
iter  30 value 16643.505333
iter  30 value 16643.505211
iter  30 value 16643.505185
final  value 16643.505185 
converged
Fitting Repeat 5 

# weights:  25
initial  value 14063.182796 
iter  10 value 13267.808896
iter  20 value 13264.797421
iter  30 value 13264.655873
final  value 13264.644783 
converged
Fitting Repeat 1 

# weights:  85
initial  value 16608.167747 
iter  10 value 15768.330586
iter  20 value 15767.188198
iter  30 value 15767.136729
final  value 15767.135859 
converged
Fitting Repeat 2 

# weights:  85
initial  value 16900.377636 
iter  10 value 15767.839669
iter  20 value 15767.255904
iter  30 value 15767.158698
iter  40 value 15767.137307
final  value 15767.135961 
converged
Fitting Repeat 3 

# weights:  85
initial  value 16101.298636 
iter  10 value 15770.939054
iter  20 value 15767.481861
iter  30 value 15767.143103
final  value 15767.136622 
converged
Fitting Repeat 4 

# weights:  85
initial  value 16645.026428 
iter  10 value 15774.643037
iter  20 value 15768.090791
iter  30 value 15767.148057
final  value 15767.137530 
converged
Fitting Repeat 5 

# weights:  85
initial  value 16539.116107 
iter  10 value 15775.914077
iter  20 value 15767.750725
iter  30 value 15767.156941
final  value 15767.137034 
converged
Fitting Repeat 1 

# weights:  73
initial  value 15812.473388 
iter  10 value 15081.518766
iter  20 value 15079.712172
final  value 15079.698131 
converged
Fitting Repeat 2 

# weights:  73
initial  value 17129.261665 
iter  10 value 16115.752090
iter  20 value 16113.576179
final  value 16113.558840 
converged
Fitting Repeat 3 

# weights:  73
initial  value 15899.823504 
iter  10 value 14829.673271
iter  20 value 14827.771482
final  value 14827.758896 
converged
Fitting Repeat 4 

# weights:  73
initial  value 18103.656202 
iter  10 value 17199.836197
iter  20 value 17198.078600
final  value 17198.069441 
converged
Fitting Repeat 5 

# weights:  73
initial  value 19291.561537 
iter  10 value 18206.816119
iter  20 value 18204.570591
final  value 18204.554595 
converged
Fitting Repeat 1 

# weights:  109
initial  value 19886.146030 
iter  10 value 19300.237181
iter  20 value 19300.035830
iter  20 value 19300.035721
iter  20 value 19300.035712
final  value 19300.035712 
converged
Fitting Repeat 2 

# weights:  109
initial  value 14854.837663 
iter  10 value 14410.862578
iter  20 value 14410.173125
final  value 14410.172768 
converged
Fitting Repeat 3 

# weights:  109
initial  value 17419.605135 
iter  10 value 16562.147379
iter  20 value 16560.451499
final  value 16560.450781 
converged
Fitting Repeat 4 

# weights:  109
initial  value 18322.222843 
iter  10 value 17196.351656
iter  20 value 17195.931662
final  value 17195.931028 
converged
Fitting Repeat 5 

# weights:  109
initial  value 17044.427211 
iter  10 value 16452.212073
iter  20 value 16451.643636
final  value 16451.643272 
converged
Fitting Repeat 1 

# weights:  97
initial  value 14174.491463 
iter  10 value 13473.474694
iter  20 value 13472.610480
final  value 13472.602880 
converged
Fitting Repeat 2 

# weights:  97
initial  value 21265.194477 
iter  10 value 20307.942198
iter  20 value 20306.528094
final  value 20306.515138 
converged
Fitting Repeat 3 

# weights:  97
initial  value 15126.404578 
iter  10 value 14381.762214
iter  20 value 14380.935740
final  value 14380.929290 
converged
Fitting Repeat 4 

# weights:  97
initial  value 19130.609991 
iter  10 value 17765.325674
iter  20 value 17764.660657
final  value 17764.658570 
converged
Fitting Repeat 5 

# weights:  97
initial  value 18997.692294 
iter  10 value 18083.980423
iter  20 value 18082.888140
final  value 18082.878428 
converged
Fitting Repeat 1 

# weights:  181
initial  value 16855.836382 
iter  10 value 15771.449931
iter  20 value 15769.513064
final  value 15769.511526 
converged
Fitting Repeat 2 

# weights:  181
initial  value 17070.328510 
iter  10 value 15774.733098
iter  20 value 15769.512171
final  value 15769.511542 
converged
Fitting Repeat 3 

# weights:  181
initial  value 16843.349362 
iter  10 value 15777.850590
iter  20 value 15769.512512
final  value 15769.511541 
converged
Fitting Repeat 4 

# weights:  181
initial  value 16208.231770 
iter  10 value 15769.789408
final  value 15769.511517 
converged
Fitting Repeat 5 

# weights:  181
initial  value 16496.263056 
iter  10 value 15782.687200
iter  20 value 15769.552836
final  value 15769.511532 
converged
Fitting Repeat 1 

# weights:  205
initial  value 26015.173217 
iter  10 value 23773.566649
iter  20 value 23773.331839
final  value 23773.330150 
converged
Fitting Repeat 2 

# weights:  205
initial  value 24888.887708 
iter  10 value 23774.852779
iter  20 value 23773.346667
final  value 23773.329812 
converged
Fitting Repeat 3 

# weights:  205
initial  value 24242.123978 
iter  10 value 23773.843918
iter  20 value 23773.335035
final  value 23773.329687 
converged
Fitting Repeat 4 

# weights:  205
initial  value 25361.037387 
iter  10 value 23774.427881
iter  20 value 23773.341768
final  value 23773.329902 
converged
Fitting Repeat 5 

# weights:  205
initial  value 25865.108429 
iter  10 value 23773.705591
iter  20 value 23773.333441
final  value 23773.330190 
converged
Model Averaged Neural Network 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  size  decay         bag    RMSE      Rsquared    MAE       Selected
   2    7.603262e-03   TRUE  17.68770  0.28245130  16.31889          
   3    1.340566e+00   TRUE  17.69060  0.29246782  16.32215          
   4    1.169352e-01   TRUE  17.68794  0.29148616  16.31918          
   5    1.750871e-04  FALSE  17.68767  0.29110364  16.31886          
   6    8.994709e-05   TRUE  17.68767  0.28977141  16.31886          
   7    7.559708e-03  FALSE  17.68768  0.29378356  16.31888          
   8    3.729581e-05   TRUE  17.68767  0.19895719  16.31886          
   8    4.930297e-05   TRUE  17.68767  0.26226370  16.31886          
   8    6.965925e-01  FALSE  17.68872  0.30206917  16.32005          
   8    5.606832e+00  FALSE  17.69464  0.30292888  16.32662          
   9    4.598641e-02   TRUE  17.68775  0.29434858  16.31895          
  11    4.981650e-04  FALSE  17.68767  0.29065227  16.31886          
  11    3.099262e-03   TRUE  17.68767  0.29236558  16.31886          
  15    1.984492e-01  FALSE  17.68791  0.30212836  16.31914          
  16    4.269257e-03   TRUE  17.68767  0.29583240  16.31886          
  16    6.696733e-01   TRUE  17.68842  0.30544927  16.31970          
  17    1.131853e-05  FALSE  17.68767  0.06349707  16.31886  *       
  17    4.457117e-03  FALSE  17.68767  0.30204173  16.31886          
  17    6.395376e-02   TRUE  17.68775  0.29186977  16.31895          
  20    2.007697e+00   TRUE  17.68944  0.30796942  16.32083          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were size = 17, decay = 1.131853e-05
 and bag = FALSE.
[1] "Mon Mar 12 16:07:38 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 23 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: Warning messages:
1: model fit failed for Fold1: vars=10 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
2: model fit failed for Fold2: vars=10 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
3: model fit failed for Fold3: vars=10 Error in bag.default(x, y, vars = param$vars, ...) : 
  Please specify 'bagControl' with the appropriate functions
 
4: In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :1     NA's   :1     NA's   :1    
Error : Stopping
In addition: There were 26 warnings (use warnings() to see them)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:07:51 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "bag"                     
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
Loading required package: TeachingDemos
Bagged MARS 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  degree  nprune  RMSE      Rsquared   MAE       Selected
  1        3      4.151586  0.6770414  3.344940          
  1        4      3.287029  0.7963756  2.550992          
  1        5      3.145128  0.8062487  2.373314          
  1        6      2.959155  0.8246351  2.258917          
  1        8      2.666858  0.8636242  2.069360          
  1       12      2.729751  0.8605094  2.150499          
  2        3      4.021957  0.7167372  3.242525          
  2        4      3.503516  0.7698339  2.743861          
  2        6      2.874952  0.8338032  2.256003          
  2        7      2.662105  0.8585165  2.122270          
  2       11      2.437858  0.8831611  1.919816  *       
  2       12      2.572126  0.8697860  2.083433          
  2       14      2.457726  0.8837744  1.957579          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 11 and degree = 2.
[1] "Mon Mar 12 16:08:51 2018"
Bagged MARS using gCV Pruning 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results:

  RMSE      Rsquared   MAE     
  2.703569  0.8526309  2.174781

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 16:09:38 2018"
Loading required package: mgcv
Loading required package: nlme
This is mgcv 1.8-22. For overview type 'help("mgcv-package")'.
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :6     NA's   :6     NA's   :6    
Error : Stopping
In addition: There were 19 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :5     NA's   :5     NA's   :5    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:13:24 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "bam"                     
Error : package arm is required
Error : package arm is required
Error : package arm is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:13:37 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "bayesglm"                
t=100, m=8
t=200, m=4
t=300, m=6
t=400, m=7
t=500, m=6
t=600, m=5
t=700, m=6
t=800, m=7
t=900, m=6
t=100, m=5
t=200, m=4
t=300, m=7
t=400, m=7
t=500, m=7
t=600, m=4
t=700, m=7
t=800, m=6
t=900, m=5
t=100, m=6
t=200, m=7
t=300, m=7
t=400, m=7
t=500, m=5
t=600, m=6
t=700, m=6
t=800, m=5
t=900, m=5
t=100, m=7
t=200, m=6
t=300, m=4
t=400, m=5
t=500, m=5
t=600, m=7
t=700, m=7
t=800, m=6
t=900, m=5
The Bayesian lasso 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  sparsity    RMSE      Rsquared   MAE       Selected
  0.08509428  3.275671  0.7903622  2.468811          
  0.14056540  3.275671  0.7903622  2.468811          
  0.16908356  3.275671  0.7903622  2.468811          
  0.20472461  3.275671  0.7903622  2.468811          
  0.25284510  3.275671  0.7903622  2.468811          
  0.30001649  3.265671  0.7902748  2.465278          
  0.35048447  3.280807  0.7939260  2.469748          
  0.35304203  3.307733  0.7948916  2.482094          
  0.35682166  3.307733  0.7948916  2.482094          
  0.36504880  3.273664  0.7942309  2.475870          
  0.43719640  3.194866  0.7948412  2.423956  *       
  0.50026452  3.264909  0.7963638  2.454133          
  0.50198995  3.264909  0.7963638  2.454133          
  0.71699918  3.264909  0.7963638  2.454133          
  0.77909068  3.264909  0.7963638  2.454133          
  0.79994739  3.264909  0.7963638  2.454133          
  0.82582888  3.264909  0.7963638  2.454133          
  0.82702510  3.264909  0.7963638  2.454133          
  0.84267825  3.264909  0.7963638  2.454133          
  0.96740112  3.264909  0.7963638  2.454133          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was sparsity = 0.4371964.
[1] "Mon Mar 12 16:13:51 2018"
t=100, m=9
t=200, m=8
t=300, m=6
t=400, m=7
t=500, m=5
t=600, m=8
t=700, m=6
t=800, m=6
t=900, m=8
t=100, m=5
t=200, m=5
t=300, m=8
t=400, m=4
t=500, m=4
t=600, m=6
t=700, m=4
t=800, m=6
t=900, m=6
t=100, m=6
t=200, m=5
t=300, m=5
t=400, m=7
t=500, m=8
t=600, m=5
t=700, m=6
t=800, m=6
t=900, m=6
t=100, m=5
t=200, m=5
t=300, m=4
t=400, m=4
t=500, m=5
t=600, m=6
t=700, m=7
t=800, m=6
t=900, m=5
Bayesian Ridge Regression (Model Averaged) 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results:

  RMSE      Rsquared   MAE     
  3.279238  0.7911733  2.474378

[1] "Mon Mar 12 16:14:04 2018"
t=100, m=5
t=200, m=6
t=300, m=4
t=400, m=7
t=500, m=4
t=600, m=5
t=700, m=4
t=800, m=4
t=900, m=7
t=100, m=5
t=200, m=5
t=300, m=6
t=400, m=5
t=500, m=4
t=600, m=7
t=700, m=7
t=800, m=4
t=900, m=4
t=100, m=4
t=200, m=4
t=300, m=6
t=400, m=5
t=500, m=5
t=600, m=7
t=700, m=6
t=800, m=5
t=900, m=4
t=100, m=6
t=200, m=5
t=300, m=5
t=400, m=5
t=500, m=5
t=600, m=5
t=700, m=6
t=800, m=5
t=900, m=5
Bayesian Ridge Regression 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results:

  RMSE      Rsquared  MAE     
  3.237345  0.79386   2.458071

[1] "Mon Mar 12 16:14:17 2018"
Boosted Linear Model 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  nu           mstop  RMSE      Rsquared   MAE       Selected
  0.006370032  413    6.386463  0.3108428  5.509615          
  0.058070731  177    5.836512  0.3723405  4.997591          
  0.070171829  175    5.752989  0.3812770  4.918009          
  0.096239712  127    5.741534  0.3838731  4.905496          
  0.125118199  103    5.706612  0.3892944  4.875652          
  0.170454428  251    5.119951  0.4783842  4.327306          
  0.249710618  250    5.054966  0.5001589  4.234551  *       
  0.263596839  400    5.104084  0.5091020  4.293968          
  0.265463894  413    5.108807  0.5090778  4.298795          
  0.288370751  150    5.097293  0.4846876  4.304996          
  0.288619829   43    5.673807  0.3919226  4.840759          
  0.366652513  219    5.075287  0.5073569  4.259832          
  0.380952300  421    5.151482  0.5100393  4.339966          
  0.407116544   85    5.133390  0.4760809  4.347913          
  0.430048669  358    5.150996  0.5101341  4.339291          
  0.482781987  389    5.160563  0.5100920  4.348073          
  0.484490715  179    5.096416  0.5085960  4.284859          
  0.512874275   71    5.116087  0.4804847  4.322993          
  0.530386031  483    5.163937  0.5101708  4.351091          
  0.574913633  183    5.130431  0.5096750  4.320129          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mstop = 250 and nu = 0.2497106.
[1] "Mon Mar 12 16:15:36 2018"
Conditional Inference Random Forest 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  mtry  RMSE      Rsquared   MAE       Selected
   1    6.362943  0.6077213  5.481696          
   2    5.716366  0.6774803  4.905794          
   3    5.169864  0.6833374  4.367049          
   4    4.742155  0.6803508  3.908034          
   5    4.579573  0.6876266  3.720973          
   6    4.505137  0.6730385  3.643362          
   8    4.473645  0.6398885  3.611945  *       
   9    4.560569  0.6170840  3.663291          
  10    4.554202  0.6073649  3.683800          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mtry = 8.
[1] "Mon Mar 12 16:15:55 2018"
Error in varimp(object, ...) : could not find function "varimp"
Conditional Inference Tree 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  mincriterion  RMSE      Rsquared   MAE       Selected
  0.08509428    6.020311  0.3741655  4.613151          
  0.14056540    6.020311  0.3741655  4.613151          
  0.16908356    6.020311  0.3741655  4.613151          
  0.20472461    6.020311  0.3741655  4.613151          
  0.25284510    6.020311  0.3741655  4.613151          
  0.30001649    6.020311  0.3741655  4.613151          
  0.35048447    6.020311  0.3741655  4.613151          
  0.35304203    6.020311  0.3741655  4.613151          
  0.35682166    6.020311  0.3741655  4.613151          
  0.36504880    6.020311  0.3741655  4.613151          
  0.43719640    6.020311  0.3741655  4.613151          
  0.50026452    6.020311  0.3741655  4.613151          
  0.50198995    6.020311  0.3741655  4.613151          
  0.71699918    6.020311  0.3741655  4.613151          
  0.77909068    6.020311  0.3741655  4.613151          
  0.79994739    6.020311  0.3741655  4.613151          
  0.82582888    6.020311  0.3741655  4.613151          
  0.82702510    6.020311  0.3741655  4.613151          
  0.84267825    6.020311  0.3741655  4.613151  *       
  0.96740112    6.191847  0.3233879  4.895816          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was mincriterion = 0.8426782.
[1] "Mon Mar 12 16:16:10 2018"
Conditional Inference Tree 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  maxdepth  mincriterion  RMSE      Rsquared   MAE       Selected
   2        0.480166660   6.275742  0.3246392  4.883072          
   3        0.677990891   6.017679  0.3737030  4.602792  *       
   3        0.854548038   6.158076  0.3532706  4.739972          
   4        0.158997850   6.020311  0.3741655  4.613151          
   4        0.207209013   6.020311  0.3741655  4.613151          
   5        0.479750837   6.020311  0.3741655  4.613151          
   6        0.095276680   6.020311  0.3741655  4.613151          
   6        0.115478847   6.020311  0.3741655  4.613151          
   6        0.807163131   6.020311  0.3741655  4.613151          
   6        0.958119588   6.191847  0.3233879  4.895816          
   7        0.610438253   6.020311  0.3741655  4.613151          
   8        0.282895539   6.020311  0.3741655  4.613151          
   8        0.415209713   6.020311  0.3741655  4.613151          
  11        0.716274907   6.020311  0.3741655  4.613151          
  12        0.438392052   6.020311  0.3741655  4.613151          
  12        0.804310495   6.020311  0.3741655  4.613151          
  13        0.008964995   6.020311  0.3741655  4.613151          
  13        0.441509005   6.020311  0.3741655  4.613151          
  13        0.634311019   6.020311  0.3741655  4.613151          
  15        0.883783023   6.158076  0.3532706  4.739972          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were maxdepth = 3 and mincriterion
 = 0.6779909.
[1] "Mon Mar 12 16:16:24 2018"
Cubist 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  committees  neighbors  RMSE      Rsquared   MAE       Selected
   1          8          3.139301  0.8006537  2.394354          
  10          3          3.254827  0.7908438  2.558076          
  12          3          3.257344  0.7904639  2.558481          
  16          2          3.424625  0.7682552  2.766244          
  21          2          3.430316  0.7666225  2.764855          
  29          5          3.259052  0.7871203  2.433380          
  42          5          3.244632  0.7893964  2.417742          
  44          7          3.149219  0.7998980  2.366737          
  45          8          3.140361  0.7999095  2.390233          
  48          3          3.269585  0.7882693  2.584099          
  49          0          3.123970  0.7956087  2.412743  *       
  62          4          3.264280  0.7864020  2.493241          
  64          8          3.132111  0.8013643  2.378850          
  68          1          3.689143  0.7361055  2.958240          
  72          7          3.150818  0.7995647  2.369084          
  81          3          3.276783  0.7871267  2.593745          
  81          7          3.156708  0.7985413  2.374775          
  86          1          3.689498  0.7360149  2.958571          
  89          9          3.143762  0.7995614  2.407433          
  96          3          3.271719  0.7878948  2.588279          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were committees = 49 and neighbors = 0.
[1] "Mon Mar 12 16:16:48 2018"
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
begin to train sae ......
training layer 1 autoencoder ...
training layer 2 autoencoder ...
training layer 3 autoencoder ...
sae has been trained.
begin to train deep nn ......
deep nn has been trained.
Stacked AutoEncoder Deep Neural Network 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  layer1  layer2  layer3  hidden_dropout  visible_dropout  RMSE     
   3      11       7      0.36253532      0.6670149         7.867216
   4      18       6      0.03427028      0.2539459         7.731491
   5       5      15      0.49004962      0.1939904        13.213781
   5      14       9      0.07193619      0.6946553         7.146649
   6       5      11      0.29087546      0.1653385        10.737638
   7      11      15      0.38882270      0.4608569        15.488812
   8       3       4      0.09862071      0.1050383         7.191043
   8       4       4      0.17993346      0.3731309         7.565629
   8      17      19      0.32831694      0.2677197        18.427340
   8      20      12      0.65850830      0.3687613         7.694688
  10      13       5      0.01287926      0.6303887         7.271562
  11       7      19      0.25231622      0.1467046        14.068834
  11       9       2      0.30236684      0.3592644         8.547713
  15      15      12      0.07167856      0.3384478         9.285518
  16      17       3      0.29254326      0.5596707         9.221634
  17       2      20      0.31685189      0.3725925        34.901433
  17      10       8      0.54332815      0.5398368        10.784135
  17      10      16      0.45238621      0.6964724        10.120424
  18      14      10      0.07846662      0.6235515        11.653340
  20      18      11      0.24986295      0.3941856         7.400791
  Rsquared    MAE        Selected
  0.02628691   6.502609          
  0.05694761   6.381102          
  0.02142910  11.527858          
  0.05805709   6.120664  *       
  0.26478091   9.180430          
  0.10422512  13.992806          
  0.12726286   6.157973          
  0.17211475   6.413610          
  0.13583196  17.269650          
  0.10690765   6.384942          
  0.24736963   6.157070          
  0.23552487  12.809719          
  0.28462011   7.163720          
  0.23469520   7.437234          
  0.19468632   7.629349          
  0.07412766  34.119718          
  0.18928441   9.137858          
  0.16061319   8.652621          
  0.06491663  10.410870          
  0.05996109   6.303187          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 5, layer2 = 14, layer3 =
 9, hidden_dropout = 0.07193619 and visible_dropout = 0.6946553.
[1] "Mon Mar 12 16:17:02 2018"
Multivariate Adaptive Regression Spline 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  degree  nprune  RMSE      Rsquared   MAE       Selected
  1        3      4.645479  0.5619473  3.777977          
  1        4      4.326965  0.6028977  3.519592          
  1        5      3.310783  0.7706282  2.640208          
  1        6      3.263563  0.7759457  2.695216          
  1        8      3.005579  0.8304779  2.335710          
  1       12      2.754464  0.8579764  2.196722  *       
  2        3      4.638995  0.5658831  3.761911          
  2        4      4.685198  0.5585113  3.838014          
  2        6      3.310146  0.7721975  2.660855          
  2        7      3.067279  0.8028906  2.409374          
  2       11      3.372086  0.7886949  2.661711          
  2       12      3.372086  0.7886949  2.661711          
  2       14      3.372086  0.7886949  2.661711          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nprune = 12 and degree = 1.
[1] "Mon Mar 12 16:17:16 2018"
Loading required package: MASS
Extreme Learning Machine 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  nhid  actfun   RMSE       Rsquared    MAE       Selected
   2    radbas   10.561209  0.06800206  8.959411          
   3    tansig    8.301164  0.27668017  6.814215          
   4    purelin   7.248572  0.13754642  5.963580          
   4    sin       6.233765  0.22174064  5.179856          
   5    sin       7.267205  0.19645341  6.015070          
   6    radbas    7.639611  0.08218033  6.070534          
   7    sin       5.635714  0.31070518  4.809234          
   7    tansig    5.440011  0.40360452  4.663753          
   9    purelin   3.894495  0.72323487  3.097686          
  10    radbas    7.616158  0.14337554  6.028296          
  14    purelin   3.570263  0.75126726  2.743412  *       
  15    tansig    5.186376  0.47667372  4.109262          
  16    radbas    5.831346  0.33133302  4.741645          
  16    sin       4.775490  0.66305994  3.824460          
  17    purelin   3.570263  0.75126726  2.743412          
  19    tansig    4.534728  0.62477756  3.587272          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were nhid = 14 and actfun = purelin.
[1] "Mon Mar 12 16:17:29 2018"
Elasticnet 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  lambda        fraction     RMSE      Rsquared   MAE       Selected
  3.240154e-05  0.480166660  4.039357  0.7672592  3.234636          
  6.972563e-05  0.854548038  3.370868  0.7788656  2.571522          
  1.033954e-04  0.677990891  3.394897  0.7887149  2.584564          
  1.691795e-04  0.207209013  5.519143  0.6206136  4.679252          
  3.289051e-04  0.158997850  5.788818  0.5898038  4.946611          
  6.311011e-04  0.479750837  4.041425  0.7671537  3.236701          
  1.267380e-03  0.115478847  6.053456  0.5419000  5.192142          
  1.312962e-03  0.095276680  6.180306  0.5008115  5.306142          
  1.383343e-03  0.807163131  3.327231  0.7866492  2.542698  *       
  1.549861e-03  0.958119588  3.500146  0.7601847  2.688571          
  4.199314e-03  0.610438253  3.541544  0.7854648  2.753781          
  1.003661e-02  0.415209713  4.356001  0.7495811  3.564041          
  1.027874e-02  0.282895539  5.086040  0.6748235  4.270368          
  2.004449e-01  0.716274907  3.368948  0.7863433  2.576263          
  4.726548e-01  0.804310495  3.356251  0.7796456  2.584678          
  6.304989e-01  0.438392052  4.200637  0.7529056  3.388319          
  9.015156e-01  0.441509005  4.136325  0.7548731  3.319124          
  9.165382e-01  0.008964995  6.751723  0.3020835  5.806803          
  1.137809e+00  0.634311019  3.437850  0.7794543  2.645784          
  6.373922e+00  0.883783023  3.922753  0.7312609  3.130387          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.8071631 and lambda
 = 0.001383343.
[1] "Mon Mar 12 16:17:41 2018"
Error : package evtree is required
In addition: There were 41 warnings (use warnings() to see them)
Error : package evtree is required
Error : package evtree is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:17:53 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "evtree"                  
Error : package extraTrees is required
Error : package extraTrees is required
Error : package extraTrees is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:18:05 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "extraTrees"              
Ridge Regression with Variable Selection 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  lambda        k   RMSE      Rsquared   MAE       Selected
  3.240154e-05   5  3.291079  0.7921881  2.529677          
  6.972563e-05   9  3.543752  0.7553879  2.712374          
  1.033954e-04   7  3.338771  0.7764028  2.594783          
  1.691795e-04   3  4.129995  0.6492870  3.216180          
  3.289051e-04   2  4.542541  0.5662053  3.522982          
  6.311011e-04   5  3.290803  0.7921856  2.529155          
  1.267380e-03   2  4.542467  0.5662099  3.523697          
  1.312962e-03   1  5.933304  0.3020835  5.095340          
  1.383343e-03   9  3.543204  0.7553856  2.711893          
  1.549861e-03  10  3.569573  0.7512668  2.743605          
  4.199314e-03   7  3.336862  0.7764859  2.592892          
  1.003661e-02   5  3.286916  0.7921446  2.520986  *       
  1.027874e-02   3  4.123178  0.6494770  3.203300          
  2.004449e-01   8  3.404415  0.7834147  2.645140          
  4.726548e-01   9  3.653272  0.7961324  2.918725          
  6.304989e-01   5  3.891324  0.7958694  3.150264          
  9.015156e-01   5  4.282539  0.7952785  3.564417          
  9.165382e-01   1  6.109632  0.3020835  5.316799          
  1.137809e+00   7  4.583772  0.7946789  3.870792          
  6.373922e+00   9  7.208931  0.5235135  6.062235          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were k = 5 and lambda = 0.01003661.
[1] "Mon Mar 12 16:18:18 2018"
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 16 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :4     NA's   :4     NA's   :4    
Error : Stopping
In addition: There were 13 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :2     NA's   :2     NA's   :2    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:18:39 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gam"                     
Error : package mboost is required
Error : package mboost is required
Error : package mboost is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:18:51 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gamboost"                
Error : package gam is required
Error : package gam is required
Error : package gam is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:19:03 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gamSpline"               
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:19:19 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprLinear"           
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:19:42 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprPoly"             
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
Error in .local(object, ...) : test vector does not match model !
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:19:58 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gaussprRadial"           
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       44.4573             nan     0.0786    1.6490
     2       40.8735             nan     0.0786    1.9728
     3       38.1908             nan     0.0786    2.5627
     4       36.6020             nan     0.0786    1.0536
     5       34.7222             nan     0.0786   -0.1593
     6       32.9105             nan     0.0786    1.4784
     7       31.1111             nan     0.0786    1.0881
     8       29.7367             nan     0.0786    0.7864
     9       28.5598             nan     0.0786    0.5896
    10       28.1665             nan     0.0786   -1.0466
    20       18.2306             nan     0.0786    0.4973
    40       10.8088             nan     0.0786    0.0673
    60        8.5362             nan     0.0786    0.0290
    80        6.9180             nan     0.0786   -0.0126
   100        6.2072             nan     0.0786   -0.1637
   120        5.5593             nan     0.0786   -0.0213
   140        5.2197             nan     0.0786   -0.0978
   160        4.6884             nan     0.0786   -0.2026
   180        4.3231             nan     0.0786   -0.0888
   200        3.8685             nan     0.0786   -0.0431
   220        3.6937             nan     0.0786   -0.1739
   240        3.3626             nan     0.0786   -0.0390
   260        3.1795             nan     0.0786   -0.0533
   280        2.9799             nan     0.0786   -0.0390
   300        2.7942             nan     0.0786   -0.0474
   320        2.6469             nan     0.0786   -0.0545
   340        2.5228             nan     0.0786   -0.0728
   360        2.3928             nan     0.0786   -0.1004
   380        2.2652             nan     0.0786   -0.0129
   400        2.1118             nan     0.0786   -0.0357
   420        2.0223             nan     0.0786   -0.0594
   440        1.9384             nan     0.0786   -0.0374
   460        1.8364             nan     0.0786   -0.0261
   480        1.7953             nan     0.0786   -0.0236
   500        1.7320             nan     0.0786   -0.0194
   520        1.6422             nan     0.0786   -0.0273
   540        1.5871             nan     0.0786   -0.0306
   560        1.4934             nan     0.0786   -0.0399
   580        1.4274             nan     0.0786   -0.0494
   600        1.3838             nan     0.0786   -0.0248
   620        1.2937             nan     0.0786   -0.0179
   640        1.2348             nan     0.0786   -0.0194
   660        1.2125             nan     0.0786   -0.0323
   680        1.1710             nan     0.0786   -0.0216
   700        1.0936             nan     0.0786   -0.0247
   720        1.0479             nan     0.0786   -0.0103
   740        1.0070             nan     0.0786   -0.0110
   760        0.9651             nan     0.0786   -0.0283
   780        0.9291             nan     0.0786   -0.0127
   800        0.8816             nan     0.0786   -0.0081
   820        0.8497             nan     0.0786   -0.0121
   840        0.8201             nan     0.0786   -0.0066
   860        0.7977             nan     0.0786   -0.0174
   880        0.7709             nan     0.0786   -0.0183
   900        0.7447             nan     0.0786   -0.0029
   920        0.7191             nan     0.0786   -0.0061
   940        0.6915             nan     0.0786   -0.0162
   960        0.6688             nan     0.0786   -0.0064
   980        0.6429             nan     0.0786   -0.0067
  1000        0.6228             nan     0.0786   -0.0115
  1020        0.6065             nan     0.0786   -0.0202
  1040        0.5805             nan     0.0786   -0.0111
  1060        0.5701             nan     0.0786   -0.0039
  1080        0.5509             nan     0.0786   -0.0143
  1100        0.5267             nan     0.0786   -0.0074
  1120        0.5135             nan     0.0786   -0.0111
  1140        0.4968             nan     0.0786   -0.0059
  1160        0.4811             nan     0.0786   -0.0060
  1180        0.4679             nan     0.0786   -0.0071
  1200        0.4521             nan     0.0786   -0.0045
  1220        0.4292             nan     0.0786   -0.0076
  1240        0.4105             nan     0.0786   -0.0155
  1260        0.3919             nan     0.0786   -0.0096
  1280        0.3824             nan     0.0786   -0.0048
  1300        0.3654             nan     0.0786   -0.0072
  1320        0.3511             nan     0.0786   -0.0024
  1340        0.3399             nan     0.0786   -0.0064
  1360        0.3286             nan     0.0786   -0.0019
  1380        0.3156             nan     0.0786   -0.0079
  1400        0.3054             nan     0.0786   -0.0060
  1420        0.2921             nan     0.0786   -0.0037
  1440        0.2839             nan     0.0786   -0.0028
  1460        0.2769             nan     0.0786   -0.0031
  1480        0.2674             nan     0.0786   -0.0044
  1500        0.2587             nan     0.0786   -0.0026
  1520        0.2494             nan     0.0786   -0.0056
  1540        0.2409             nan     0.0786   -0.0066
  1560        0.2342             nan     0.0786   -0.0072
  1580        0.2245             nan     0.0786   -0.0062
  1600        0.2196             nan     0.0786   -0.0056
  1620        0.2153             nan     0.0786   -0.0060
  1640        0.2057             nan     0.0786   -0.0027
  1660        0.1977             nan     0.0786   -0.0063
  1680        0.1909             nan     0.0786   -0.0013
  1700        0.1808             nan     0.0786   -0.0053
  1720        0.1749             nan     0.0786   -0.0034
  1740        0.1691             nan     0.0786   -0.0030
  1753        0.1656             nan     0.0786   -0.0021

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       44.3755             nan     0.0829    1.5031
     2       42.0914             nan     0.0829    1.5371
     3       38.7622             nan     0.0829    2.8773
     4       37.1604             nan     0.0829    0.7604
     5       35.9467             nan     0.0829    0.4813
     6       33.7169             nan     0.0829    2.5905
     7       31.7668             nan     0.0829    0.9400
     8       29.2559             nan     0.0829    1.9297
     9       27.7927             nan     0.0829    0.3120
    10       26.1688             nan     0.0829    1.0703
    20       17.2721             nan     0.0829    0.4388
    40        9.3767             nan     0.0829   -0.0885
    60        6.7008             nan     0.0829   -0.0134
    80        5.1735             nan     0.0829   -0.0746
   100        4.3321             nan     0.0829   -0.0780
   120        3.7950             nan     0.0829   -0.1077
   140        3.3259             nan     0.0829   -0.0872
   160        2.7854             nan     0.0829   -0.0149
   180        2.5413             nan     0.0829   -0.0549
   200        2.3254             nan     0.0829   -0.0042
   220        2.0753             nan     0.0829   -0.0267
   240        1.8031             nan     0.0829   -0.0076
   260        1.6777             nan     0.0829   -0.0426
   280        1.5274             nan     0.0829   -0.0091
   300        1.3623             nan     0.0829   -0.0172
   320        1.2217             nan     0.0829   -0.0160
   340        1.0966             nan     0.0829   -0.0450
   360        1.0196             nan     0.0829   -0.0109
   380        0.9000             nan     0.0829   -0.0270
   400        0.7903             nan     0.0829   -0.0192
   420        0.7320             nan     0.0829   -0.0205
   440        0.6716             nan     0.0829   -0.0093
   460        0.6270             nan     0.0829    0.0011
   480        0.5532             nan     0.0829   -0.0056
   500        0.5131             nan     0.0829   -0.0130
   520        0.4778             nan     0.0829   -0.0138
   540        0.4310             nan     0.0829   -0.0074
   560        0.4081             nan     0.0829   -0.0087
   580        0.3815             nan     0.0829   -0.0032
   600        0.3515             nan     0.0829   -0.0052
   620        0.3314             nan     0.0829   -0.0071
   640        0.3045             nan     0.0829   -0.0041
   660        0.2924             nan     0.0829   -0.0029
   680        0.2743             nan     0.0829   -0.0114
   700        0.2599             nan     0.0829   -0.0087
   720        0.2464             nan     0.0829   -0.0071
   740        0.2341             nan     0.0829   -0.0056
   760        0.2188             nan     0.0829   -0.0031
   780        0.2067             nan     0.0829   -0.0046
   800        0.1906             nan     0.0829   -0.0031
   820        0.1791             nan     0.0829   -0.0028
   840        0.1713             nan     0.0829   -0.0017
   860        0.1585             nan     0.0829   -0.0016
   880        0.1499             nan     0.0829   -0.0029
   900        0.1402             nan     0.0829   -0.0014
   920        0.1333             nan     0.0829   -0.0025
   940        0.1267             nan     0.0829   -0.0020
   960        0.1178             nan     0.0829   -0.0017
   980        0.1125             nan     0.0829   -0.0015
  1000        0.1057             nan     0.0829   -0.0028
  1020        0.0987             nan     0.0829   -0.0018
  1040        0.0917             nan     0.0829   -0.0011
  1060        0.0847             nan     0.0829   -0.0030
  1080        0.0799             nan     0.0829   -0.0010
  1100        0.0774             nan     0.0829   -0.0008
  1120        0.0728             nan     0.0829   -0.0022
  1140        0.0698             nan     0.0829   -0.0015
  1160        0.0657             nan     0.0829   -0.0015
  1180        0.0621             nan     0.0829   -0.0008
  1200        0.0594             nan     0.0829   -0.0010
  1220        0.0558             nan     0.0829   -0.0012
  1240        0.0531             nan     0.0829   -0.0001
  1260        0.0502             nan     0.0829   -0.0008
  1280        0.0473             nan     0.0829   -0.0007
  1300        0.0447             nan     0.0829   -0.0015
  1320        0.0420             nan     0.0829   -0.0015
  1340        0.0401             nan     0.0829   -0.0007
  1360        0.0376             nan     0.0829   -0.0010
  1380        0.0358             nan     0.0829   -0.0010
  1400        0.0332             nan     0.0829   -0.0010
  1420        0.0313             nan     0.0829   -0.0005
  1440        0.0299             nan     0.0829   -0.0007
  1460        0.0288             nan     0.0829   -0.0004
  1480        0.0275             nan     0.0829   -0.0005
  1500        0.0259             nan     0.0829   -0.0002
  1520        0.0247             nan     0.0829   -0.0005
  1540        0.0235             nan     0.0829   -0.0007
  1560        0.0220             nan     0.0829   -0.0007
  1580        0.0208             nan     0.0829   -0.0003
  1600        0.0199             nan     0.0829   -0.0004
  1620        0.0188             nan     0.0829   -0.0002
  1640        0.0182             nan     0.0829   -0.0004
  1660        0.0172             nan     0.0829   -0.0002
  1680        0.0164             nan     0.0829   -0.0003
  1700        0.0157             nan     0.0829   -0.0002
  1720        0.0149             nan     0.0829   -0.0006
  1740        0.0141             nan     0.0829   -0.0003
  1760        0.0136             nan     0.0829   -0.0007
  1765        0.0134             nan     0.0829   -0.0002

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       38.9214             nan     0.1176    7.2866
     2       35.4357             nan     0.1176    1.8651
     3       30.8961             nan     0.1176    1.9913
     4       26.8333             nan     0.1176    2.7049
     5       24.9165             nan     0.1176    1.6527
     6       21.4714             nan     0.1176    2.3973
     7       19.8209             nan     0.1176   -0.4950
     8       18.4916             nan     0.1176    0.4046
     9       16.7964             nan     0.1176    0.6249
    10       14.7601             nan     0.1176    0.9494
    20        6.7602             nan     0.1176    0.1777
    40        2.5125             nan     0.1176   -0.0429
    60        1.2944             nan     0.1176   -0.0287
    80        0.7633             nan     0.1176   -0.0066
   100        0.4385             nan     0.1176   -0.0213
   120        0.2569             nan     0.1176   -0.0157
   140        0.1517             nan     0.1176   -0.0017
   160        0.0998             nan     0.1176   -0.0003
   180        0.0639             nan     0.1176   -0.0001
   200        0.0455             nan     0.1176   -0.0023
   220        0.0324             nan     0.1176   -0.0021
   240        0.0219             nan     0.1176   -0.0001
   260        0.0151             nan     0.1176   -0.0004
   280        0.0106             nan     0.1176   -0.0005
   300        0.0072             nan     0.1176   -0.0000
   320        0.0053             nan     0.1176   -0.0004
   340        0.0043             nan     0.1176   -0.0002
   360        0.0031             nan     0.1176   -0.0002
   380        0.0021             nan     0.1176   -0.0000
   400        0.0015             nan     0.1176   -0.0000
   420        0.0010             nan     0.1176   -0.0001
   440        0.0007             nan     0.1176   -0.0000
   460        0.0005             nan     0.1176   -0.0000
   480        0.0004             nan     0.1176   -0.0000
   500        0.0004             nan     0.1176   -0.0000
   520        0.0003             nan     0.1176   -0.0000
   540        0.0002             nan     0.1176   -0.0000
   560        0.0002             nan     0.1176   -0.0000
   580        0.0002             nan     0.1176   -0.0000
   600        0.0001             nan     0.1176   -0.0000
   620        0.0001             nan     0.1176   -0.0000
   640        0.0001             nan     0.1176   -0.0000
   660        0.0001             nan     0.1176   -0.0000
   680        0.0001             nan     0.1176   -0.0000
   700        0.0000             nan     0.1176   -0.0000
   720        0.0000             nan     0.1176   -0.0000
   740        0.0000             nan     0.1176   -0.0000
   760        0.0000             nan     0.1176   -0.0000
   780        0.0000             nan     0.1176   -0.0000
   800        0.0000             nan     0.1176   -0.0000
   820        0.0000             nan     0.1176   -0.0000
   840        0.0000             nan     0.1176   -0.0000
   860        0.0000             nan     0.1176   -0.0000
   880        0.0000             nan     0.1176   -0.0000
   900        0.0000             nan     0.1176   -0.0000
   920        0.0000             nan     0.1176   -0.0000
   940        0.0000             nan     0.1176   -0.0000
   960        0.0000             nan     0.1176   -0.0000
   980        0.0000             nan     0.1176   -0.0000
  1000        0.0000             nan     0.1176   -0.0000
  1020        0.0000             nan     0.1176   -0.0000
  1040        0.0000             nan     0.1176   -0.0000
  1060        0.0000             nan     0.1176   -0.0000
  1080        0.0000             nan     0.1176   -0.0000
  1100        0.0000             nan     0.1176   -0.0000
  1120        0.0000             nan     0.1176   -0.0000
  1140        0.0000             nan     0.1176   -0.0000
  1160        0.0000             nan     0.1176   -0.0000
  1180        0.0000             nan     0.1176   -0.0000
  1200        0.0000             nan     0.1176   -0.0000
  1220        0.0000             nan     0.1176   -0.0000
  1240        0.0000             nan     0.1176   -0.0000
  1260        0.0000             nan     0.1176   -0.0000
  1280        0.0000             nan     0.1176    0.0000
  1300        0.0000             nan     0.1176   -0.0000
  1320        0.0000             nan     0.1176   -0.0000
  1340        0.0000             nan     0.1176   -0.0000
  1360        0.0000             nan     0.1176   -0.0000
  1380        0.0000             nan     0.1176   -0.0000
  1400        0.0000             nan     0.1176   -0.0000
  1420        0.0000             nan     0.1176   -0.0000
  1440        0.0000             nan     0.1176   -0.0000
  1460        0.0000             nan     0.1176   -0.0000
  1480        0.0000             nan     0.1176   -0.0000
  1500        0.0000             nan     0.1176   -0.0000
  1520        0.0000             nan     0.1176   -0.0000
  1540        0.0000             nan     0.1176   -0.0000
  1560        0.0000             nan     0.1176   -0.0000
  1580        0.0000             nan     0.1176   -0.0000
  1600        0.0000             nan     0.1176   -0.0000
  1620        0.0000             nan     0.1176   -0.0000
  1640        0.0000             nan     0.1176   -0.0000
  1660        0.0000             nan     0.1176   -0.0000
  1680        0.0000             nan     0.1176   -0.0000
  1700        0.0000             nan     0.1176    0.0000
  1720        0.0000             nan     0.1176   -0.0000
  1740        0.0000             nan     0.1176   -0.0000
  1760        0.0000             nan     0.1176   -0.0000
  1780        0.0000             nan     0.1176   -0.0000
  1800        0.0000             nan     0.1176   -0.0000
  1820        0.0000             nan     0.1176    0.0000
  1840        0.0000             nan     0.1176    0.0000
  1860        0.0000             nan     0.1176   -0.0000
  1880        0.0000             nan     0.1176   -0.0000
  1900        0.0000             nan     0.1176   -0.0000
  1920        0.0000             nan     0.1176   -0.0000
  1940        0.0000             nan     0.1176   -0.0000
  1960        0.0000             nan     0.1176   -0.0000
  1980        0.0000             nan     0.1176   -0.0000
  2000        0.0000             nan     0.1176   -0.0000
  2020        0.0000             nan     0.1176    0.0000
  2040        0.0000             nan     0.1176   -0.0000
  2060        0.0000             nan     0.1176   -0.0000
  2080        0.0000             nan     0.1176   -0.0000
  2100        0.0000             nan     0.1176   -0.0000
  2120        0.0000             nan     0.1176   -0.0000
  2140        0.0000             nan     0.1176   -0.0000
  2160        0.0000             nan     0.1176   -0.0000
  2180        0.0000             nan     0.1176   -0.0000
  2186        0.0000             nan     0.1176   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       41.6646             nan     0.1352    3.3348
     2       34.1166             nan     0.1352    4.9700
     3       28.1626             nan     0.1352    4.5434
     4       24.9976             nan     0.1352    2.6693
     5       22.6457             nan     0.1352    1.2543
     6       20.0092             nan     0.1352    1.8221
     7       18.7443             nan     0.1352    0.6746
     8       17.2275             nan     0.1352    0.9182
     9       15.7590             nan     0.1352   -0.0929
    10       14.3299             nan     0.1352   -0.0364
    20        7.3418             nan     0.1352   -0.1045
    40        3.4791             nan     0.1352    0.0818
    60        2.0517             nan     0.1352    0.0223
    80        1.3376             nan     0.1352   -0.0779
   100        0.7801             nan     0.1352   -0.0154
   120        0.4987             nan     0.1352   -0.0331
   140        0.3360             nan     0.1352   -0.0312
   160        0.2293             nan     0.1352   -0.0089
   180        0.1500             nan     0.1352   -0.0022
   200        0.1183             nan     0.1352   -0.0062
   220        0.0768             nan     0.1352   -0.0031
   240        0.0500             nan     0.1352   -0.0018
   260        0.0344             nan     0.1352   -0.0027
   280        0.0220             nan     0.1352   -0.0010
   300        0.0147             nan     0.1352   -0.0010
   320        0.0101             nan     0.1352   -0.0006
   340        0.0071             nan     0.1352   -0.0004
   360        0.0050             nan     0.1352   -0.0003
   380        0.0033             nan     0.1352   -0.0003
   400        0.0023             nan     0.1352   -0.0002
   420        0.0017             nan     0.1352   -0.0000
   440        0.0012             nan     0.1352   -0.0000
   460        0.0009             nan     0.1352   -0.0001
   480        0.0006             nan     0.1352   -0.0000
   500        0.0004             nan     0.1352   -0.0000
   520        0.0003             nan     0.1352   -0.0000
   540        0.0002             nan     0.1352   -0.0000
   560        0.0002             nan     0.1352   -0.0000
   580        0.0001             nan     0.1352   -0.0000
   600        0.0001             nan     0.1352   -0.0000
   620        0.0001             nan     0.1352   -0.0000
   640        0.0001             nan     0.1352   -0.0000
   660        0.0000             nan     0.1352   -0.0000
   680        0.0000             nan     0.1352   -0.0000
   700        0.0000             nan     0.1352   -0.0000
   703        0.0000             nan     0.1352   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       39.0906             nan     0.2413    8.3784
     2       33.0455             nan     0.2413    4.3897
     3       27.0415             nan     0.2413    5.0149
     4       23.4412             nan     0.2413    1.6812
     5       20.9267             nan     0.2413    2.0637
     6       18.9058             nan     0.2413   -0.6945
     7       16.0841             nan     0.2413    1.5095
     8       12.6327             nan     0.2413    1.4336
     9       12.4422             nan     0.2413   -1.4020
    10       10.7883             nan     0.2413    0.7405
    20        6.0536             nan     0.2413   -0.8528
    40        3.3321             nan     0.2413   -0.0353
    60        2.0949             nan     0.2413   -0.3270
    80        1.0667             nan     0.2413   -0.1447
   100        0.6858             nan     0.2413   -0.0959
   120        0.4168             nan     0.2413    0.0167
   140        0.2569             nan     0.2413   -0.0501
   160        0.1747             nan     0.2413   -0.0085
   180        0.1203             nan     0.2413   -0.0076
   200        0.0875             nan     0.2413   -0.0029
   220        0.0531             nan     0.2413   -0.0018
   240        0.0418             nan     0.2413   -0.0055
   260        0.0251             nan     0.2413   -0.0011
   280        0.0181             nan     0.2413   -0.0021
   300        0.0129             nan     0.2413   -0.0009
   320        0.0099             nan     0.2413   -0.0020
   340        0.0071             nan     0.2413   -0.0006
   360        0.0057             nan     0.2413   -0.0004
   380        0.0043             nan     0.2413   -0.0002
   400        0.0029             nan     0.2413   -0.0001
   420        0.0018             nan     0.2413   -0.0001
   440        0.0014             nan     0.2413   -0.0001
   460        0.0010             nan     0.2413   -0.0001
   480        0.0007             nan     0.2413   -0.0000
   500        0.0006             nan     0.2413   -0.0000
   520        0.0004             nan     0.2413   -0.0000
   540        0.0003             nan     0.2413   -0.0000
   560        0.0002             nan     0.2413   -0.0000
   580        0.0001             nan     0.2413   -0.0000
   600        0.0001             nan     0.2413   -0.0000
   620        0.0001             nan     0.2413   -0.0000
   640        0.0001             nan     0.2413   -0.0000
   660        0.0000             nan     0.2413   -0.0000
   680        0.0000             nan     0.2413   -0.0000
   700        0.0000             nan     0.2413   -0.0000
   720        0.0000             nan     0.2413   -0.0000
   740        0.0000             nan     0.2413   -0.0000
   760        0.0000             nan     0.2413   -0.0000
   780        0.0000             nan     0.2413   -0.0000
   800        0.0000             nan     0.2413   -0.0000
   820        0.0000             nan     0.2413   -0.0000
   840        0.0000             nan     0.2413   -0.0000
   846        0.0000             nan     0.2413   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       36.9815             nan     0.2603    7.8707
     2       31.0946             nan     0.2603    5.7914
     3       23.8113             nan     0.2603    4.7502
     4       20.9940             nan     0.2603    2.6072
     5       18.4236             nan     0.2603    1.5652
     6       16.3973             nan     0.2603    1.2521
     7       13.4453             nan     0.2603    2.0859
     8       12.3215             nan     0.2603   -0.4893
     9       11.5977             nan     0.2603   -0.6030
    10        9.8944             nan     0.2603   -0.7660
    20        5.0529             nan     0.2603   -0.6307
    40        2.1405             nan     0.2603   -0.2314
    60        1.0422             nan     0.2603   -0.0177
    80        0.6098             nan     0.2603   -0.0422
   100        0.3367             nan     0.2603   -0.0233
   120        0.1857             nan     0.2603   -0.0125
   140        0.1253             nan     0.2603   -0.0084
   160        0.0813             nan     0.2603   -0.0053
   180        0.0362             nan     0.2603   -0.0035
   200        0.0200             nan     0.2603   -0.0021
   220        0.0113             nan     0.2603   -0.0007
   240        0.0064             nan     0.2603   -0.0000
   260        0.0034             nan     0.2603   -0.0004
   280        0.0021             nan     0.2603   -0.0002
   300        0.0013             nan     0.2603   -0.0002
   320        0.0006             nan     0.2603   -0.0001
   340        0.0003             nan     0.2603   -0.0000
   360        0.0002             nan     0.2603   -0.0000
   380        0.0001             nan     0.2603   -0.0000
   400        0.0001             nan     0.2603   -0.0000
   420        0.0001             nan     0.2603   -0.0000
   440        0.0000             nan     0.2603   -0.0000
   460        0.0000             nan     0.2603   -0.0000
   480        0.0000             nan     0.2603   -0.0000
   500        0.0000             nan     0.2603   -0.0000
   520        0.0000             nan     0.2603   -0.0000
   540        0.0000             nan     0.2603   -0.0000
   560        0.0000             nan     0.2603   -0.0000
   580        0.0000             nan     0.2603   -0.0000
   600        0.0000             nan     0.2603   -0.0000
   620        0.0000             nan     0.2603   -0.0000
   640        0.0000             nan     0.2603   -0.0000
   660        0.0000             nan     0.2603   -0.0000
   680        0.0000             nan     0.2603   -0.0000
   700        0.0000             nan     0.2603   -0.0000
   720        0.0000             nan     0.2603   -0.0000
   740        0.0000             nan     0.2603   -0.0000
   760        0.0000             nan     0.2603   -0.0000
   780        0.0000             nan     0.2603   -0.0000
   800        0.0000             nan     0.2603   -0.0000
   820        0.0000             nan     0.2603   -0.0000
   840        0.0000             nan     0.2603   -0.0000
   860        0.0000             nan     0.2603   -0.0000
   880        0.0000             nan     0.2603   -0.0000
   900        0.0000             nan     0.2603   -0.0000
   920        0.0000             nan     0.2603   -0.0000
   940        0.0000             nan     0.2603   -0.0000
   960        0.0000             nan     0.2603   -0.0000
   980        0.0000             nan     0.2603   -0.0000
  1000        0.0000             nan     0.2603   -0.0000
  1020        0.0000             nan     0.2603   -0.0000
  1040        0.0000             nan     0.2603   -0.0000
  1060        0.0000             nan     0.2603   -0.0000
  1080        0.0000             nan     0.2603   -0.0000
  1100        0.0000             nan     0.2603   -0.0000
  1120        0.0000             nan     0.2603   -0.0000
  1140        0.0000             nan     0.2603   -0.0000
  1160        0.0000             nan     0.2603   -0.0000
  1180        0.0000             nan     0.2603   -0.0000
  1200        0.0000             nan     0.2603   -0.0000
  1220        0.0000             nan     0.2603   -0.0000
  1240        0.0000             nan     0.2603   -0.0000
  1260        0.0000             nan     0.2603   -0.0000
  1280        0.0000             nan     0.2603   -0.0000
  1300        0.0000             nan     0.2603   -0.0000
  1320        0.0000             nan     0.2603   -0.0000
  1340        0.0000             nan     0.2603   -0.0000
  1360        0.0000             nan     0.2603   -0.0000
  1380        0.0000             nan     0.2603   -0.0000
  1400        0.0000             nan     0.2603   -0.0000
  1420        0.0000             nan     0.2603   -0.0000
  1440        0.0000             nan     0.2603   -0.0000
  1460        0.0000             nan     0.2603   -0.0000
  1480        0.0000             nan     0.2603   -0.0000
  1500        0.0000             nan     0.2603   -0.0000
  1520        0.0000             nan     0.2603   -0.0000
  1540        0.0000             nan     0.2603   -0.0000
  1560        0.0000             nan     0.2603   -0.0000
  1580        0.0000             nan     0.2603   -0.0000
  1600        0.0000             nan     0.2603   -0.0000
  1620        0.0000             nan     0.2603   -0.0000
  1640        0.0000             nan     0.2603   -0.0000
  1660        0.0000             nan     0.2603   -0.0000
  1680        0.0000             nan     0.2603   -0.0000
  1700        0.0000             nan     0.2603   -0.0000
  1720        0.0000             nan     0.2603   -0.0000
  1740        0.0000             nan     0.2603   -0.0000
  1760        0.0000             nan     0.2603   -0.0000
  1780        0.0000             nan     0.2603   -0.0000
  1800        0.0000             nan     0.2603   -0.0000
  1820        0.0000             nan     0.2603   -0.0000
  1840        0.0000             nan     0.2603   -0.0000
  1860        0.0000             nan     0.2603   -0.0000
  1880        0.0000             nan     0.2603   -0.0000
  1900        0.0000             nan     0.2603    0.0000
  1920        0.0000             nan     0.2603   -0.0000
  1940        0.0000             nan     0.2603   -0.0000
  1960        0.0000             nan     0.2603   -0.0000
  1980        0.0000             nan     0.2603   -0.0000
  2000        0.0000             nan     0.2603   -0.0000
  2020        0.0000             nan     0.2603   -0.0000
  2040        0.0000             nan     0.2603   -0.0000
  2060        0.0000             nan     0.2603   -0.0000
  2080        0.0000             nan     0.2603   -0.0000
  2100        0.0000             nan     0.2603   -0.0000
  2120        0.0000             nan     0.2603   -0.0000
  2140        0.0000             nan     0.2603   -0.0000
  2160        0.0000             nan     0.2603   -0.0000
  2180        0.0000             nan     0.2603   -0.0000
  2200        0.0000             nan     0.2603   -0.0000
  2220        0.0000             nan     0.2603   -0.0000
  2240        0.0000             nan     0.2603   -0.0000
  2260        0.0000             nan     0.2603   -0.0000
  2280        0.0000             nan     0.2603   -0.0000
  2300        0.0000             nan     0.2603   -0.0000
  2320        0.0000             nan     0.2603   -0.0000
  2340        0.0000             nan     0.2603   -0.0000
  2360        0.0000             nan     0.2603   -0.0000
  2380        0.0000             nan     0.2603   -0.0000
  2400        0.0000             nan     0.2603   -0.0000
  2420        0.0000             nan     0.2603   -0.0000
  2440        0.0000             nan     0.2603   -0.0000
  2460        0.0000             nan     0.2603   -0.0000
  2480        0.0000             nan     0.2603   -0.0000
  2500        0.0000             nan     0.2603   -0.0000
  2520        0.0000             nan     0.2603   -0.0000
  2540        0.0000             nan     0.2603   -0.0000
  2560        0.0000             nan     0.2603   -0.0000
  2580        0.0000             nan     0.2603   -0.0000
  2600        0.0000             nan     0.2603   -0.0000
  2620        0.0000             nan     0.2603   -0.0000
  2640        0.0000             nan     0.2603   -0.0000
  2660        0.0000             nan     0.2603   -0.0000
  2680        0.0000             nan     0.2603   -0.0000
  2700        0.0000             nan     0.2603   -0.0000
  2720        0.0000             nan     0.2603   -0.0000
  2740        0.0000             nan     0.2603   -0.0000
  2760        0.0000             nan     0.2603   -0.0000
  2780        0.0000             nan     0.2603   -0.0000
  2800        0.0000             nan     0.2603   -0.0000
  2820        0.0000             nan     0.2603   -0.0000
  2840        0.0000             nan     0.2603   -0.0000
  2860        0.0000             nan     0.2603   -0.0000
  2880        0.0000             nan     0.2603   -0.0000
  2900        0.0000             nan     0.2603   -0.0000
  2920        0.0000             nan     0.2603   -0.0000
  2940        0.0000             nan     0.2603   -0.0000
  2960        0.0000             nan     0.2603   -0.0000
  2980        0.0000             nan     0.2603   -0.0000
  3000        0.0000             nan     0.2603   -0.0000
  3020        0.0000             nan     0.2603   -0.0000
  3040        0.0000             nan     0.2603   -0.0000
  3060        0.0000             nan     0.2603   -0.0000
  3080        0.0000             nan     0.2603   -0.0000
  3100        0.0000             nan     0.2603   -0.0000
  3120        0.0000             nan     0.2603   -0.0000
  3140        0.0000             nan     0.2603   -0.0000
  3160        0.0000             nan     0.2603   -0.0000
  3180        0.0000             nan     0.2603   -0.0000
  3200        0.0000             nan     0.2603   -0.0000
  3220        0.0000             nan     0.2603   -0.0000
  3240        0.0000             nan     0.2603   -0.0000
  3260        0.0000             nan     0.2603   -0.0000
  3280        0.0000             nan     0.2603   -0.0000
  3300        0.0000             nan     0.2603   -0.0000
  3320        0.0000             nan     0.2603   -0.0000
  3340        0.0000             nan     0.2603   -0.0000
  3360        0.0000             nan     0.2603   -0.0000
  3380        0.0000             nan     0.2603   -0.0000
  3400        0.0000             nan     0.2603   -0.0000
  3420        0.0000             nan     0.2603   -0.0000
  3440        0.0000             nan     0.2603   -0.0000
  3460        0.0000             nan     0.2603   -0.0000
  3480        0.0000             nan     0.2603   -0.0000
  3500        0.0000             nan     0.2603   -0.0000
  3520        0.0000             nan     0.2603   -0.0000
  3540        0.0000             nan     0.2603   -0.0000
  3560        0.0000             nan     0.2603   -0.0000
  3580        0.0000             nan     0.2603   -0.0000
  3600        0.0000             nan     0.2603   -0.0000
  3620        0.0000             nan     0.2603   -0.0000
  3640        0.0000             nan     0.2603   -0.0000
  3660        0.0000             nan     0.2603   -0.0000
  3680        0.0000             nan     0.2603   -0.0000
  3700        0.0000             nan     0.2603   -0.0000
  3720        0.0000             nan     0.2603   -0.0000
  3740        0.0000             nan     0.2603   -0.0000
  3760        0.0000             nan     0.2603    0.0000
  3780        0.0000             nan     0.2603   -0.0000
  3800        0.0000             nan     0.2603   -0.0000
  3820        0.0000             nan     0.2603   -0.0000
  3840        0.0000             nan     0.2603   -0.0000
  3860        0.0000             nan     0.2603   -0.0000
  3880        0.0000             nan     0.2603    0.0000
  3900        0.0000             nan     0.2603   -0.0000
  3920        0.0000             nan     0.2603   -0.0000
  3940        0.0000             nan     0.2603   -0.0000
  3960        0.0000             nan     0.2603   -0.0000
  3980        0.0000             nan     0.2603   -0.0000
  4000        0.0000             nan     0.2603   -0.0000
  4020        0.0000             nan     0.2603   -0.0000
  4040        0.0000             nan     0.2603   -0.0000
  4060        0.0000             nan     0.2603   -0.0000
  4080        0.0000             nan     0.2603   -0.0000
  4100        0.0000             nan     0.2603   -0.0000
  4120        0.0000             nan     0.2603   -0.0000
  4140        0.0000             nan     0.2603   -0.0000
  4160        0.0000             nan     0.2603   -0.0000
  4180        0.0000             nan     0.2603   -0.0000
  4200        0.0000             nan     0.2603   -0.0000
  4213        0.0000             nan     0.2603   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       34.9272             nan     0.3353    4.9885
     2       26.4820             nan     0.3353    6.6181
     3       23.6527             nan     0.3353   -8.6586
     4       21.4867             nan     0.3353   -1.0340
     5       21.0410             nan     0.3353   -0.7522
     6       18.5280             nan     0.3353    1.8933
     7       15.2282             nan     0.3353    1.9825
     8       12.5918             nan     0.3353   -1.2173
     9       11.7354             nan     0.3353   -1.0262
    10       11.1375             nan     0.3353   -1.4980
    20        5.6020             nan     0.3353   -0.5321
    40        1.8488             nan     0.3353   -0.1214
    60        0.8015             nan     0.3353   -0.2100
    80        0.3374             nan     0.3353   -0.0417
   100        0.1302             nan     0.3353   -0.0047
   120        0.0690             nan     0.3353   -0.0078
   140        0.0341             nan     0.3353   -0.0045
   160        0.0181             nan     0.3353   -0.0031
   180        0.0094             nan     0.3353   -0.0003
   200        0.0052             nan     0.3353   -0.0013
   220        0.0024             nan     0.3353   -0.0005
   240        0.0013             nan     0.3353   -0.0001
   260        0.0006             nan     0.3353   -0.0000
   280        0.0003             nan     0.3353   -0.0000
   300        0.0002             nan     0.3353   -0.0000
   320        0.0001             nan     0.3353   -0.0000
   340        0.0000             nan     0.3353   -0.0000
   360        0.0000             nan     0.3353   -0.0000
   380        0.0000             nan     0.3353    0.0000
   400        0.0000             nan     0.3353   -0.0000
   420        0.0000             nan     0.3353   -0.0000
   440        0.0000             nan     0.3353   -0.0000
   460        0.0000             nan     0.3353   -0.0000
   480        0.0000             nan     0.3353   -0.0000
   500        0.0000             nan     0.3353   -0.0000
   520        0.0000             nan     0.3353   -0.0000
   540        0.0000             nan     0.3353   -0.0000
   560        0.0000             nan     0.3353   -0.0000
   580        0.0000             nan     0.3353   -0.0000
   600        0.0000             nan     0.3353   -0.0000
   620        0.0000             nan     0.3353   -0.0000
   640        0.0000             nan     0.3353   -0.0000
   660        0.0000             nan     0.3353   -0.0000
   680        0.0000             nan     0.3353   -0.0000
   700        0.0000             nan     0.3353   -0.0000
   720        0.0000             nan     0.3353   -0.0000
   740        0.0000             nan     0.3353   -0.0000
   760        0.0000             nan     0.3353   -0.0000
   780        0.0000             nan     0.3353   -0.0000
   800        0.0000             nan     0.3353   -0.0000
   820        0.0000             nan     0.3353   -0.0000
   840        0.0000             nan     0.3353   -0.0000
   860        0.0000             nan     0.3353   -0.0000
   880        0.0000             nan     0.3353   -0.0000
   900        0.0000             nan     0.3353   -0.0000
   920        0.0000             nan     0.3353   -0.0000
   940        0.0000             nan     0.3353   -0.0000
   960        0.0000             nan     0.3353   -0.0000
   980        0.0000             nan     0.3353   -0.0000
  1000        0.0000             nan     0.3353   -0.0000
  1020        0.0000             nan     0.3353   -0.0000
  1040        0.0000             nan     0.3353   -0.0000
  1060        0.0000             nan     0.3353   -0.0000
  1080        0.0000             nan     0.3353   -0.0000
  1100        0.0000             nan     0.3353   -0.0000
  1120        0.0000             nan     0.3353   -0.0000
  1140        0.0000             nan     0.3353   -0.0000
  1160        0.0000             nan     0.3353   -0.0000
  1180        0.0000             nan     0.3353    0.0000
  1200        0.0000             nan     0.3353   -0.0000
  1220        0.0000             nan     0.3353   -0.0000
  1240        0.0000             nan     0.3353   -0.0000
  1260        0.0000             nan     0.3353   -0.0000
  1280        0.0000             nan     0.3353   -0.0000
  1300        0.0000             nan     0.3353   -0.0000
  1320        0.0000             nan     0.3353   -0.0000
  1340        0.0000             nan     0.3353   -0.0000
  1360        0.0000             nan     0.3353   -0.0000
  1380        0.0000             nan     0.3353   -0.0000
  1400        0.0000             nan     0.3353   -0.0000
  1420        0.0000             nan     0.3353   -0.0000
  1440        0.0000             nan     0.3353   -0.0000
  1460        0.0000             nan     0.3353   -0.0000
  1480        0.0000             nan     0.3353   -0.0000
  1500        0.0000             nan     0.3353   -0.0000
  1520        0.0000             nan     0.3353   -0.0000
  1540        0.0000             nan     0.3353   -0.0000
  1560        0.0000             nan     0.3353   -0.0000
  1580        0.0000             nan     0.3353   -0.0000
  1600        0.0000             nan     0.3353   -0.0000
  1620        0.0000             nan     0.3353   -0.0000
  1640        0.0000             nan     0.3353   -0.0000
  1660        0.0000             nan     0.3353   -0.0000
  1680        0.0000             nan     0.3353   -0.0000
  1700        0.0000             nan     0.3353   -0.0000
  1720        0.0000             nan     0.3353   -0.0000
  1740        0.0000             nan     0.3353   -0.0000
  1760        0.0000             nan     0.3353   -0.0000
  1780        0.0000             nan     0.3353   -0.0000
  1800        0.0000             nan     0.3353   -0.0000
  1820        0.0000             nan     0.3353   -0.0000
  1840        0.0000             nan     0.3353   -0.0000
  1860        0.0000             nan     0.3353   -0.0000
  1880        0.0000             nan     0.3353   -0.0000
  1900        0.0000             nan     0.3353   -0.0000
  1920        0.0000             nan     0.3353   -0.0000
  1940        0.0000             nan     0.3353   -0.0000
  1960        0.0000             nan     0.3353   -0.0000
  1980        0.0000             nan     0.3353   -0.0000
  2000        0.0000             nan     0.3353   -0.0000
  2020        0.0000             nan     0.3353   -0.0000
  2040        0.0000             nan     0.3353   -0.0000
  2060        0.0000             nan     0.3353   -0.0000
  2080        0.0000             nan     0.3353   -0.0000
  2100        0.0000             nan     0.3353   -0.0000
  2120        0.0000             nan     0.3353   -0.0000
  2140        0.0000             nan     0.3353   -0.0000
  2160        0.0000             nan     0.3353   -0.0000
  2180        0.0000             nan     0.3353    0.0000
  2200        0.0000             nan     0.3353   -0.0000
  2220        0.0000             nan     0.3353   -0.0000
  2240        0.0000             nan     0.3353   -0.0000
  2260        0.0000             nan     0.3353   -0.0000
  2280        0.0000             nan     0.3353   -0.0000
  2300        0.0000             nan     0.3353   -0.0000
  2320        0.0000             nan     0.3353    0.0000
  2340        0.0000             nan     0.3353    0.0000
  2360        0.0000             nan     0.3353   -0.0000
  2380        0.0000             nan     0.3353   -0.0000
  2400        0.0000             nan     0.3353   -0.0000
  2420        0.0000             nan     0.3353   -0.0000
  2440        0.0000             nan     0.3353   -0.0000
  2460        0.0000             nan     0.3353   -0.0000
  2480        0.0000             nan     0.3353   -0.0000
  2500        0.0000             nan     0.3353   -0.0000
  2520        0.0000             nan     0.3353   -0.0000
  2540        0.0000             nan     0.3353   -0.0000
  2560        0.0000             nan     0.3353   -0.0000
  2580        0.0000             nan     0.3353   -0.0000
  2600        0.0000             nan     0.3353   -0.0000
  2620        0.0000             nan     0.3353   -0.0000
  2640        0.0000             nan     0.3353   -0.0000
  2660        0.0000             nan     0.3353   -0.0000
  2680        0.0000             nan     0.3353   -0.0000
  2700        0.0000             nan     0.3353   -0.0000
  2720        0.0000             nan     0.3353   -0.0000
  2740        0.0000             nan     0.3353   -0.0000
  2760        0.0000             nan     0.3353   -0.0000
  2780        0.0000             nan     0.3353   -0.0000
  2800        0.0000             nan     0.3353   -0.0000
  2820        0.0000             nan     0.3353   -0.0000
  2840        0.0000             nan     0.3353   -0.0000
  2860        0.0000             nan     0.3353   -0.0000
  2880        0.0000             nan     0.3353   -0.0000
  2900        0.0000             nan     0.3353   -0.0000
  2920        0.0000             nan     0.3353   -0.0000
  2940        0.0000             nan     0.3353   -0.0000
  2960        0.0000             nan     0.3353   -0.0000
  2980        0.0000             nan     0.3353   -0.0000
  3000        0.0000             nan     0.3353   -0.0000
  3020        0.0000             nan     0.3353   -0.0000
  3040        0.0000             nan     0.3353   -0.0000
  3060        0.0000             nan     0.3353   -0.0000
  3080        0.0000             nan     0.3353   -0.0000
  3100        0.0000             nan     0.3353   -0.0000
  3120        0.0000             nan     0.3353   -0.0000
  3140        0.0000             nan     0.3353   -0.0000
  3160        0.0000             nan     0.3353   -0.0000
  3180        0.0000             nan     0.3353   -0.0000
  3200        0.0000             nan     0.3353   -0.0000
  3220        0.0000             nan     0.3353   -0.0000
  3240        0.0000             nan     0.3353   -0.0000
  3260        0.0000             nan     0.3353   -0.0000
  3280        0.0000             nan     0.3353   -0.0000
  3300        0.0000             nan     0.3353   -0.0000
  3320        0.0000             nan     0.3353   -0.0000
  3340        0.0000             nan     0.3353   -0.0000
  3360        0.0000             nan     0.3353   -0.0000
  3380        0.0000             nan     0.3353   -0.0000
  3400        0.0000             nan     0.3353   -0.0000
  3420        0.0000             nan     0.3353   -0.0000
  3440        0.0000             nan     0.3353   -0.0000
  3460        0.0000             nan     0.3353   -0.0000
  3480        0.0000             nan     0.3353   -0.0000
  3500        0.0000             nan     0.3353   -0.0000
  3520        0.0000             nan     0.3353   -0.0000
  3540        0.0000             nan     0.3353   -0.0000
  3560        0.0000             nan     0.3353   -0.0000
  3580        0.0000             nan     0.3353   -0.0000
  3585        0.0000             nan     0.3353   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       47.0400             nan     0.0786    2.5925
     2       43.6538             nan     0.0786    2.5821
     3       40.6054             nan     0.0786    1.6432
     4       39.0657             nan     0.0786    1.0307
     5       37.1049             nan     0.0786    1.6045
     6       35.3416             nan     0.0786    0.5016
     7       33.1024             nan     0.0786    1.9175
     8       31.3262             nan     0.0786    1.6359
     9       30.2913             nan     0.0786    0.8220
    10       28.6964             nan     0.0786    1.5189
    20       18.6792             nan     0.0786    0.2839
    40       11.3086             nan     0.0786   -0.3315
    60        8.5595             nan     0.0786    0.0140
    80        7.0882             nan     0.0786   -0.1111
   100        6.3465             nan     0.0786   -0.1911
   120        5.7428             nan     0.0786   -0.0388
   140        5.2499             nan     0.0786   -0.0347
   160        4.7062             nan     0.0786   -0.0694
   180        4.3961             nan     0.0786   -0.0548
   200        4.0789             nan     0.0786   -0.0158
   220        3.8001             nan     0.0786   -0.0288
   240        3.5506             nan     0.0786   -0.0416
   260        3.2858             nan     0.0786   -0.0927
   280        3.1048             nan     0.0786   -0.0443
   300        2.9104             nan     0.0786   -0.0901
   320        2.7667             nan     0.0786   -0.0247
   340        2.6171             nan     0.0786   -0.0536
   360        2.4717             nan     0.0786   -0.0004
   380        2.3716             nan     0.0786   -0.0403
   400        2.2874             nan     0.0786   -0.0510
   420        2.1123             nan     0.0786   -0.0181
   440        1.9973             nan     0.0786   -0.0490
   460        1.9127             nan     0.0786   -0.0261
   480        1.8182             nan     0.0786   -0.0181
   500        1.7290             nan     0.0786   -0.0032
   520        1.6115             nan     0.0786   -0.0115
   540        1.5314             nan     0.0786   -0.0251
   560        1.4855             nan     0.0786   -0.0277
   580        1.3817             nan     0.0786   -0.0084
   600        1.3216             nan     0.0786   -0.0122
   620        1.2701             nan     0.0786   -0.0279
   640        1.2282             nan     0.0786   -0.0114
   660        1.1721             nan     0.0786   -0.0096
   680        1.1024             nan     0.0786   -0.0209
   700        1.0751             nan     0.0786   -0.0097
   720        1.0373             nan     0.0786   -0.0394
   740        1.0104             nan     0.0786   -0.0141
   760        0.9680             nan     0.0786   -0.0128
   780        0.9277             nan     0.0786   -0.0205
   800        0.8832             nan     0.0786   -0.0100
   820        0.8609             nan     0.0786   -0.0141
   840        0.8274             nan     0.0786   -0.0088
   860        0.8036             nan     0.0786   -0.0217
   880        0.7754             nan     0.0786   -0.0194
   900        0.7439             nan     0.0786   -0.0175
   920        0.7147             nan     0.0786   -0.0124
   940        0.6683             nan     0.0786   -0.0038
   960        0.6461             nan     0.0786   -0.0127
   980        0.6332             nan     0.0786   -0.0139
  1000        0.6060             nan     0.0786   -0.0144
  1020        0.5854             nan     0.0786   -0.0056
  1040        0.5722             nan     0.0786   -0.0185
  1060        0.5494             nan     0.0786   -0.0003
  1080        0.5247             nan     0.0786   -0.0146
  1100        0.5027             nan     0.0786   -0.0058
  1120        0.4900             nan     0.0786   -0.0116
  1140        0.4840             nan     0.0786   -0.0073
  1160        0.4678             nan     0.0786   -0.0026
  1180        0.4487             nan     0.0786   -0.0108
  1200        0.4392             nan     0.0786   -0.0026
  1220        0.4278             nan     0.0786   -0.0114
  1240        0.4129             nan     0.0786   -0.0137
  1260        0.4023             nan     0.0786   -0.0061
  1280        0.3885             nan     0.0786   -0.0049
  1300        0.3755             nan     0.0786   -0.0024
  1320        0.3616             nan     0.0786   -0.0038
  1340        0.3540             nan     0.0786   -0.0052
  1360        0.3447             nan     0.0786   -0.0046
  1380        0.3324             nan     0.0786   -0.0084
  1400        0.3272             nan     0.0786   -0.0075
  1420        0.3262             nan     0.0786   -0.0066
  1440        0.3164             nan     0.0786   -0.0015
  1460        0.3037             nan     0.0786   -0.0028
  1480        0.2982             nan     0.0786   -0.0064
  1500        0.2895             nan     0.0786   -0.0041
  1520        0.2818             nan     0.0786   -0.0044
  1540        0.2740             nan     0.0786   -0.0017
  1560        0.2720             nan     0.0786   -0.0054
  1580        0.2663             nan     0.0786   -0.0065
  1600        0.2600             nan     0.0786   -0.0055
  1620        0.2523             nan     0.0786   -0.0024
  1640        0.2483             nan     0.0786   -0.0018
  1660        0.2465             nan     0.0786   -0.0031
  1680        0.2387             nan     0.0786   -0.0031
  1700        0.2304             nan     0.0786   -0.0032
  1720        0.2263             nan     0.0786   -0.0058
  1740        0.2209             nan     0.0786   -0.0038
  1753        0.2172             nan     0.0786   -0.0032

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       47.7386             nan     0.0829    3.3037
     2       45.1636             nan     0.0829    1.9200
     3       42.0051             nan     0.0829    2.3848
     4       39.6810             nan     0.0829    1.7969
     5       36.6351             nan     0.0829    1.9270
     6       34.3583             nan     0.0829    1.8962
     7       31.8147             nan     0.0829    1.2659
     8       30.2966             nan     0.0829    1.1384
     9       28.6608             nan     0.0829    0.3148
    10       27.6310             nan     0.0829    0.2186
    20       15.8949             nan     0.0829    0.1125
    40        8.9335             nan     0.0829    0.0845
    60        6.4535             nan     0.0829   -0.2429
    80        5.2303             nan     0.0829    0.0125
   100        4.5012             nan     0.0829   -0.0157
   120        4.0624             nan     0.0829   -0.0874
   140        3.6703             nan     0.0829   -0.0424
   160        3.1663             nan     0.0829   -0.0271
   180        2.8838             nan     0.0829   -0.0796
   200        2.6801             nan     0.0829   -0.0555
   220        2.4635             nan     0.0829   -0.0651
   240        2.2991             nan     0.0829   -0.0468
   260        2.1429             nan     0.0829   -0.0588
   280        1.9832             nan     0.0829   -0.0168
   300        1.8029             nan     0.0829   -0.0199
   320        1.6636             nan     0.0829   -0.0408
   340        1.5412             nan     0.0829   -0.0713
   360        1.4310             nan     0.0829   -0.0313
   380        1.3426             nan     0.0829   -0.0212
   400        1.2609             nan     0.0829   -0.0629
   420        1.1923             nan     0.0829   -0.0120
   440        1.1095             nan     0.0829   -0.0241
   460        1.0381             nan     0.0829   -0.0196
   480        0.9732             nan     0.0829   -0.0114
   500        0.9048             nan     0.0829   -0.0117
   520        0.8481             nan     0.0829   -0.0159
   540        0.7830             nan     0.0829   -0.0240
   560        0.7412             nan     0.0829   -0.0143
   580        0.6970             nan     0.0829   -0.0187
   600        0.6763             nan     0.0829   -0.0144
   620        0.6201             nan     0.0829   -0.0068
   640        0.5733             nan     0.0829   -0.0173
   660        0.5289             nan     0.0829   -0.0115
   680        0.4958             nan     0.0829   -0.0121
   700        0.4585             nan     0.0829   -0.0115
   720        0.4323             nan     0.0829   -0.0133
   740        0.4130             nan     0.0829   -0.0103
   760        0.3916             nan     0.0829   -0.0156
   780        0.3722             nan     0.0829   -0.0016
   800        0.3521             nan     0.0829   -0.0064
   820        0.3291             nan     0.0829   -0.0047
   840        0.3095             nan     0.0829   -0.0022
   860        0.2935             nan     0.0829   -0.0021
   880        0.2791             nan     0.0829   -0.0027
   900        0.2625             nan     0.0829   -0.0062
   920        0.2498             nan     0.0829   -0.0057
   940        0.2408             nan     0.0829   -0.0029
   960        0.2264             nan     0.0829   -0.0052
   980        0.2147             nan     0.0829   -0.0040
  1000        0.2022             nan     0.0829   -0.0041
  1020        0.1876             nan     0.0829   -0.0024
  1040        0.1783             nan     0.0829   -0.0069
  1060        0.1656             nan     0.0829   -0.0047
  1080        0.1565             nan     0.0829   -0.0069
  1100        0.1491             nan     0.0829   -0.0016
  1120        0.1413             nan     0.0829   -0.0052
  1140        0.1347             nan     0.0829   -0.0049
  1160        0.1264             nan     0.0829   -0.0011
  1180        0.1221             nan     0.0829   -0.0043
  1200        0.1158             nan     0.0829   -0.0044
  1220        0.1093             nan     0.0829   -0.0017
  1240        0.1026             nan     0.0829   -0.0019
  1260        0.0984             nan     0.0829   -0.0029
  1280        0.0931             nan     0.0829   -0.0023
  1300        0.0884             nan     0.0829   -0.0017
  1320        0.0837             nan     0.0829   -0.0025
  1340        0.0796             nan     0.0829   -0.0009
  1360        0.0763             nan     0.0829   -0.0018
  1380        0.0704             nan     0.0829   -0.0017
  1400        0.0673             nan     0.0829   -0.0009
  1420        0.0644             nan     0.0829   -0.0009
  1440        0.0614             nan     0.0829   -0.0006
  1460        0.0585             nan     0.0829   -0.0001
  1480        0.0559             nan     0.0829   -0.0001
  1500        0.0541             nan     0.0829   -0.0007
  1520        0.0516             nan     0.0829   -0.0017
  1540        0.0493             nan     0.0829   -0.0004
  1560        0.0466             nan     0.0829   -0.0009
  1580        0.0451             nan     0.0829   -0.0012
  1600        0.0438             nan     0.0829   -0.0006
  1620        0.0427             nan     0.0829   -0.0005
  1640        0.0407             nan     0.0829   -0.0006
  1660        0.0388             nan     0.0829   -0.0016
  1680        0.0368             nan     0.0829   -0.0004
  1700        0.0350             nan     0.0829   -0.0002
  1720        0.0331             nan     0.0829   -0.0009
  1740        0.0312             nan     0.0829   -0.0008
  1760        0.0290             nan     0.0829   -0.0005
  1765        0.0288             nan     0.0829   -0.0009

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       45.7835             nan     0.1176    3.6521
     2       39.8557             nan     0.1176    5.8329
     3       35.2247             nan     0.1176    4.2453
     4       30.0286             nan     0.1176    2.7899
     5       26.0896             nan     0.1176    3.1815
     6       22.9420             nan     0.1176    2.6651
     7       21.5303             nan     0.1176    0.1416
     8       19.9715             nan     0.1176    1.0345
     9       16.9434             nan     0.1176    2.0123
    10       15.3835             nan     0.1176    0.9507
    20        6.9203             nan     0.1176    0.2210
    40        2.3782             nan     0.1176   -0.0957
    60        1.3496             nan     0.1176   -0.0570
    80        0.8433             nan     0.1176   -0.0305
   100        0.4288             nan     0.1176   -0.0439
   120        0.2161             nan     0.1176   -0.0034
   140        0.1353             nan     0.1176   -0.0035
   160        0.0961             nan     0.1176   -0.0039
   180        0.0667             nan     0.1176   -0.0027
   200        0.0488             nan     0.1176   -0.0005
   220        0.0328             nan     0.1176   -0.0018
   240        0.0225             nan     0.1176   -0.0014
   260        0.0151             nan     0.1176   -0.0003
   280        0.0116             nan     0.1176   -0.0004
   300        0.0084             nan     0.1176   -0.0005
   320        0.0056             nan     0.1176   -0.0001
   340        0.0045             nan     0.1176   -0.0000
   360        0.0035             nan     0.1176   -0.0002
   380        0.0028             nan     0.1176   -0.0001
   400        0.0019             nan     0.1176   -0.0001
   420        0.0014             nan     0.1176   -0.0000
   440        0.0009             nan     0.1176   -0.0000
   460        0.0007             nan     0.1176   -0.0000
   480        0.0005             nan     0.1176   -0.0000
   500        0.0004             nan     0.1176   -0.0000
   520        0.0003             nan     0.1176   -0.0000
   540        0.0002             nan     0.1176   -0.0000
   560        0.0002             nan     0.1176   -0.0000
   580        0.0002             nan     0.1176   -0.0000
   600        0.0001             nan     0.1176   -0.0000
   620        0.0001             nan     0.1176   -0.0000
   640        0.0001             nan     0.1176   -0.0000
   660        0.0001             nan     0.1176   -0.0000
   680        0.0000             nan     0.1176   -0.0000
   700        0.0000             nan     0.1176   -0.0000
   720        0.0000             nan     0.1176   -0.0000
   740        0.0000             nan     0.1176   -0.0000
   760        0.0000             nan     0.1176   -0.0000
   780        0.0000             nan     0.1176   -0.0000
   800        0.0000             nan     0.1176   -0.0000
   820        0.0000             nan     0.1176   -0.0000
   840        0.0000             nan     0.1176   -0.0000
   860        0.0000             nan     0.1176   -0.0000
   880        0.0000             nan     0.1176   -0.0000
   900        0.0000             nan     0.1176   -0.0000
   920        0.0000             nan     0.1176   -0.0000
   940        0.0000             nan     0.1176   -0.0000
   960        0.0000             nan     0.1176   -0.0000
   980        0.0000             nan     0.1176   -0.0000
  1000        0.0000             nan     0.1176   -0.0000
  1020        0.0000             nan     0.1176   -0.0000
  1040        0.0000             nan     0.1176   -0.0000
  1060        0.0000             nan     0.1176   -0.0000
  1080        0.0000             nan     0.1176   -0.0000
  1100        0.0000             nan     0.1176   -0.0000
  1120        0.0000             nan     0.1176   -0.0000
  1140        0.0000             nan     0.1176   -0.0000
  1160        0.0000             nan     0.1176   -0.0000
  1180        0.0000             nan     0.1176   -0.0000
  1200        0.0000             nan     0.1176    0.0000
  1220        0.0000             nan     0.1176   -0.0000
  1240        0.0000             nan     0.1176   -0.0000
  1260        0.0000             nan     0.1176   -0.0000
  1280        0.0000             nan     0.1176   -0.0000
  1300        0.0000             nan     0.1176   -0.0000
  1320        0.0000             nan     0.1176   -0.0000
  1340        0.0000             nan     0.1176   -0.0000
  1360        0.0000             nan     0.1176   -0.0000
  1380        0.0000             nan     0.1176   -0.0000
  1400        0.0000             nan     0.1176   -0.0000
  1420        0.0000             nan     0.1176   -0.0000
  1440        0.0000             nan     0.1176   -0.0000
  1460        0.0000             nan     0.1176    0.0000
  1480        0.0000             nan     0.1176   -0.0000
  1500        0.0000             nan     0.1176   -0.0000
  1520        0.0000             nan     0.1176   -0.0000
  1540        0.0000             nan     0.1176   -0.0000
  1560        0.0000             nan     0.1176   -0.0000
  1580        0.0000             nan     0.1176   -0.0000
  1600        0.0000             nan     0.1176   -0.0000
  1620        0.0000             nan     0.1176   -0.0000
  1640        0.0000             nan     0.1176   -0.0000
  1660        0.0000             nan     0.1176   -0.0000
  1680        0.0000             nan     0.1176   -0.0000
  1700        0.0000             nan     0.1176   -0.0000
  1720        0.0000             nan     0.1176   -0.0000
  1740        0.0000             nan     0.1176   -0.0000
  1760        0.0000             nan     0.1176   -0.0000
  1780        0.0000             nan     0.1176   -0.0000
  1800        0.0000             nan     0.1176   -0.0000
  1820        0.0000             nan     0.1176   -0.0000
  1840        0.0000             nan     0.1176   -0.0000
  1860        0.0000             nan     0.1176   -0.0000
  1880        0.0000             nan     0.1176   -0.0000
  1900        0.0000             nan     0.1176   -0.0000
  1920        0.0000             nan     0.1176   -0.0000
  1940        0.0000             nan     0.1176   -0.0000
  1960        0.0000             nan     0.1176   -0.0000
  1980        0.0000             nan     0.1176   -0.0000
  2000        0.0000             nan     0.1176   -0.0000
  2020        0.0000             nan     0.1176   -0.0000
  2040        0.0000             nan     0.1176   -0.0000
  2060        0.0000             nan     0.1176   -0.0000
  2080        0.0000             nan     0.1176   -0.0000
  2100        0.0000             nan     0.1176   -0.0000
  2120        0.0000             nan     0.1176   -0.0000
  2140        0.0000             nan     0.1176   -0.0000
  2160        0.0000             nan     0.1176   -0.0000
  2180        0.0000             nan     0.1176   -0.0000
  2186        0.0000             nan     0.1176   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       43.1365             nan     0.1352    7.0011
     2       36.6963             nan     0.1352    5.7437
     3       33.2184             nan     0.1352    0.7278
     4       29.7288             nan     0.1352    1.8917
     5       26.5535             nan     0.1352    2.8252
     6       22.9248             nan     0.1352    1.3340
     7       19.0694             nan     0.1352    3.3328
     8       17.6655             nan     0.1352   -0.4046
     9       15.5075             nan     0.1352    2.0166
    10       13.3910             nan     0.1352    0.6959
    20        6.9901             nan     0.1352    0.1303
    40        2.8497             nan     0.1352   -0.0366
    60        1.7601             nan     0.1352   -0.0574
    80        1.1056             nan     0.1352   -0.0278
   100        0.7458             nan     0.1352   -0.0316
   120        0.5529             nan     0.1352   -0.0226
   140        0.3894             nan     0.1352   -0.0146
   160        0.2431             nan     0.1352   -0.0208
   180        0.1855             nan     0.1352   -0.0028
   200        0.1362             nan     0.1352   -0.0051
   220        0.0992             nan     0.1352   -0.0030
   240        0.0747             nan     0.1352   -0.0047
   260        0.0590             nan     0.1352   -0.0037
   280        0.0458             nan     0.1352   -0.0019
   300        0.0356             nan     0.1352   -0.0025
   320        0.0249             nan     0.1352   -0.0006
   340        0.0159             nan     0.1352   -0.0009
   360        0.0137             nan     0.1352   -0.0002
   380        0.0100             nan     0.1352   -0.0005
   400        0.0079             nan     0.1352   -0.0003
   420        0.0063             nan     0.1352   -0.0001
   440        0.0048             nan     0.1352   -0.0003
   460        0.0036             nan     0.1352   -0.0002
   480        0.0027             nan     0.1352   -0.0001
   500        0.0020             nan     0.1352   -0.0001
   520        0.0017             nan     0.1352   -0.0001
   540        0.0015             nan     0.1352   -0.0001
   560        0.0014             nan     0.1352   -0.0000
   580        0.0012             nan     0.1352   -0.0000
   600        0.0009             nan     0.1352   -0.0000
   620        0.0009             nan     0.1352   -0.0000
   640        0.0007             nan     0.1352   -0.0000
   660        0.0006             nan     0.1352   -0.0000
   680        0.0005             nan     0.1352   -0.0000
   700        0.0004             nan     0.1352   -0.0000
   703        0.0004             nan     0.1352   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       40.6726             nan     0.2413    3.8501
     2       31.0496             nan     0.2413    5.6039
     3       26.0945             nan     0.2413    2.6644
     4       22.5522             nan     0.2413    1.9856
     5       17.2103             nan     0.2413    4.2432
     6       14.2919             nan     0.2413    1.4434
     7       13.1623             nan     0.2413    0.2249
     8       11.4341             nan     0.2413    0.3203
     9       10.3767             nan     0.2413   -0.2377
    10        9.5724             nan     0.2413    0.0553
    20        5.8741             nan     0.2413   -0.3013
    40        2.9749             nan     0.2413   -0.2299
    60        1.7276             nan     0.2413   -0.1763
    80        1.0063             nan     0.2413   -0.0565
   100        0.5378             nan     0.2413   -0.0227
   120        0.2977             nan     0.2413   -0.0332
   140        0.1930             nan     0.2413   -0.0174
   160        0.1158             nan     0.2413   -0.0139
   180        0.0770             nan     0.2413   -0.0026
   200        0.0535             nan     0.2413   -0.0038
   220        0.0382             nan     0.2413   -0.0019
   240        0.0252             nan     0.2413   -0.0032
   260        0.0159             nan     0.2413   -0.0016
   280        0.0099             nan     0.2413   -0.0003
   300        0.0065             nan     0.2413   -0.0005
   320        0.0042             nan     0.2413   -0.0001
   340        0.0029             nan     0.2413    0.0000
   360        0.0018             nan     0.2413   -0.0001
   380        0.0012             nan     0.2413   -0.0002
   400        0.0008             nan     0.2413   -0.0001
   420        0.0006             nan     0.2413   -0.0000
   440        0.0005             nan     0.2413   -0.0001
   460        0.0003             nan     0.2413   -0.0001
   480        0.0002             nan     0.2413   -0.0000
   500        0.0002             nan     0.2413   -0.0000
   520        0.0001             nan     0.2413   -0.0000
   540        0.0001             nan     0.2413   -0.0000
   560        0.0000             nan     0.2413   -0.0000
   580        0.0000             nan     0.2413   -0.0000
   600        0.0000             nan     0.2413   -0.0000
   620        0.0000             nan     0.2413   -0.0000
   640        0.0000             nan     0.2413   -0.0000
   660        0.0000             nan     0.2413   -0.0000
   680        0.0000             nan     0.2413   -0.0000
   700        0.0000             nan     0.2413   -0.0000
   720        0.0000             nan     0.2413   -0.0000
   740        0.0000             nan     0.2413   -0.0000
   760        0.0000             nan     0.2413   -0.0000
   780        0.0000             nan     0.2413   -0.0000
   800        0.0000             nan     0.2413   -0.0000
   820        0.0000             nan     0.2413   -0.0000
   840        0.0000             nan     0.2413   -0.0000
   846        0.0000             nan     0.2413   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       38.7765             nan     0.2603    7.0146
     2       31.0805             nan     0.2603    5.5088
     3       23.6893             nan     0.2603    5.9742
     4       19.8558             nan     0.2603    0.6752
     5       18.8126             nan     0.2603   -1.8759
     6       15.5687             nan     0.2603    2.1707
     7       12.0916             nan     0.2603    1.8557
     8       10.7920             nan     0.2603    0.2345
     9        9.9804             nan     0.2603   -0.1195
    10        9.2926             nan     0.2603   -0.7579
    20        5.3301             nan     0.2603   -0.3628
    40        3.1718             nan     0.2603   -0.2377
    60        2.0130             nan     0.2603   -0.2184
    80        1.2194             nan     0.2603   -0.1685
   100        0.8627             nan     0.2603   -0.0042
   120        0.5916             nan     0.2603   -0.0471
   140        0.2768             nan     0.2603   -0.0084
   160        0.2066             nan     0.2603   -0.0256
   180        0.1374             nan     0.2603   -0.0121
   200        0.1085             nan     0.2603   -0.0108
   220        0.0815             nan     0.2603   -0.0087
   240        0.0590             nan     0.2603   -0.0046
   260        0.0433             nan     0.2603   -0.0054
   280        0.0333             nan     0.2603   -0.0014
   300        0.0236             nan     0.2603   -0.0015
   320        0.0168             nan     0.2603   -0.0021
   340        0.0121             nan     0.2603   -0.0004
   360        0.0090             nan     0.2603   -0.0010
   380        0.0074             nan     0.2603   -0.0005
   400        0.0053             nan     0.2603   -0.0004
   420        0.0034             nan     0.2603   -0.0002
   440        0.0026             nan     0.2603   -0.0002
   460        0.0016             nan     0.2603   -0.0001
   480        0.0011             nan     0.2603   -0.0000
   500        0.0010             nan     0.2603   -0.0000
   520        0.0006             nan     0.2603   -0.0000
   540        0.0005             nan     0.2603   -0.0000
   560        0.0004             nan     0.2603   -0.0000
   580        0.0003             nan     0.2603   -0.0000
   600        0.0002             nan     0.2603   -0.0000
   620        0.0002             nan     0.2603   -0.0000
   640        0.0001             nan     0.2603   -0.0000
   660        0.0001             nan     0.2603    0.0000
   680        0.0001             nan     0.2603   -0.0000
   700        0.0001             nan     0.2603   -0.0000
   720        0.0000             nan     0.2603   -0.0000
   740        0.0000             nan     0.2603   -0.0000
   760        0.0000             nan     0.2603   -0.0000
   780        0.0000             nan     0.2603   -0.0000
   800        0.0000             nan     0.2603   -0.0000
   820        0.0000             nan     0.2603   -0.0000
   840        0.0000             nan     0.2603   -0.0000
   860        0.0000             nan     0.2603   -0.0000
   880        0.0000             nan     0.2603   -0.0000
   900        0.0000             nan     0.2603   -0.0000
   920        0.0000             nan     0.2603   -0.0000
   940        0.0000             nan     0.2603   -0.0000
   960        0.0000             nan     0.2603   -0.0000
   980        0.0000             nan     0.2603   -0.0000
  1000        0.0000             nan     0.2603   -0.0000
  1020        0.0000             nan     0.2603   -0.0000
  1040        0.0000             nan     0.2603   -0.0000
  1060        0.0000             nan     0.2603   -0.0000
  1080        0.0000             nan     0.2603   -0.0000
  1100        0.0000             nan     0.2603   -0.0000
  1120        0.0000             nan     0.2603   -0.0000
  1140        0.0000             nan     0.2603   -0.0000
  1160        0.0000             nan     0.2603   -0.0000
  1180        0.0000             nan     0.2603   -0.0000
  1200        0.0000             nan     0.2603   -0.0000
  1220        0.0000             nan     0.2603   -0.0000
  1240        0.0000             nan     0.2603   -0.0000
  1260        0.0000             nan     0.2603   -0.0000
  1280        0.0000             nan     0.2603   -0.0000
  1300        0.0000             nan     0.2603   -0.0000
  1320        0.0000             nan     0.2603   -0.0000
  1340        0.0000             nan     0.2603    0.0000
  1360        0.0000             nan     0.2603   -0.0000
  1380        0.0000             nan     0.2603   -0.0000
  1400        0.0000             nan     0.2603   -0.0000
  1420        0.0000             nan     0.2603   -0.0000
  1440        0.0000             nan     0.2603   -0.0000
  1460        0.0000             nan     0.2603   -0.0000
  1480        0.0000             nan     0.2603   -0.0000
  1500        0.0000             nan     0.2603   -0.0000
  1520        0.0000             nan     0.2603   -0.0000
  1540        0.0000             nan     0.2603   -0.0000
  1560        0.0000             nan     0.2603   -0.0000
  1580        0.0000             nan     0.2603   -0.0000
  1600        0.0000             nan     0.2603   -0.0000
  1620        0.0000             nan     0.2603   -0.0000
  1640        0.0000             nan     0.2603   -0.0000
  1660        0.0000             nan     0.2603   -0.0000
  1680        0.0000             nan     0.2603   -0.0000
  1700        0.0000             nan     0.2603   -0.0000
  1720        0.0000             nan     0.2603   -0.0000
  1740        0.0000             nan     0.2603   -0.0000
  1760        0.0000             nan     0.2603   -0.0000
  1780        0.0000             nan     0.2603   -0.0000
  1800        0.0000             nan     0.2603   -0.0000
  1820        0.0000             nan     0.2603   -0.0000
  1840        0.0000             nan     0.2603   -0.0000
  1860        0.0000             nan     0.2603   -0.0000
  1880        0.0000             nan     0.2603   -0.0000
  1900        0.0000             nan     0.2603   -0.0000
  1920        0.0000             nan     0.2603   -0.0000
  1940        0.0000             nan     0.2603   -0.0000
  1960        0.0000             nan     0.2603   -0.0000
  1980        0.0000             nan     0.2603   -0.0000
  2000        0.0000             nan     0.2603   -0.0000
  2020        0.0000             nan     0.2603   -0.0000
  2040        0.0000             nan     0.2603   -0.0000
  2060        0.0000             nan     0.2603   -0.0000
  2080        0.0000             nan     0.2603   -0.0000
  2100        0.0000             nan     0.2603   -0.0000
  2120        0.0000             nan     0.2603   -0.0000
  2140        0.0000             nan     0.2603   -0.0000
  2160        0.0000             nan     0.2603   -0.0000
  2180        0.0000             nan     0.2603   -0.0000
  2200        0.0000             nan     0.2603   -0.0000
  2220        0.0000             nan     0.2603   -0.0000
  2240        0.0000             nan     0.2603   -0.0000
  2260        0.0000             nan     0.2603   -0.0000
  2280        0.0000             nan     0.2603   -0.0000
  2300        0.0000             nan     0.2603   -0.0000
  2320        0.0000             nan     0.2603   -0.0000
  2340        0.0000             nan     0.2603   -0.0000
  2360        0.0000             nan     0.2603   -0.0000
  2380        0.0000             nan     0.2603   -0.0000
  2400        0.0000             nan     0.2603   -0.0000
  2420        0.0000             nan     0.2603   -0.0000
  2440        0.0000             nan     0.2603   -0.0000
  2460        0.0000             nan     0.2603   -0.0000
  2480        0.0000             nan     0.2603   -0.0000
  2500        0.0000             nan     0.2603   -0.0000
  2520        0.0000             nan     0.2603   -0.0000
  2540        0.0000             nan     0.2603   -0.0000
  2560        0.0000             nan     0.2603   -0.0000
  2580        0.0000             nan     0.2603   -0.0000
  2600        0.0000             nan     0.2603   -0.0000
  2620        0.0000             nan     0.2603   -0.0000
  2640        0.0000             nan     0.2603   -0.0000
  2660        0.0000             nan     0.2603   -0.0000
  2680        0.0000             nan     0.2603   -0.0000
  2700        0.0000             nan     0.2603   -0.0000
  2720        0.0000             nan     0.2603   -0.0000
  2740        0.0000             nan     0.2603    0.0000
  2760        0.0000             nan     0.2603   -0.0000
  2780        0.0000             nan     0.2603   -0.0000
  2800        0.0000             nan     0.2603   -0.0000
  2820        0.0000             nan     0.2603   -0.0000
  2840        0.0000             nan     0.2603   -0.0000
  2860        0.0000             nan     0.2603   -0.0000
  2880        0.0000             nan     0.2603   -0.0000
  2900        0.0000             nan     0.2603   -0.0000
  2920        0.0000             nan     0.2603   -0.0000
  2940        0.0000             nan     0.2603    0.0000
  2960        0.0000             nan     0.2603   -0.0000
  2980        0.0000             nan     0.2603   -0.0000
  3000        0.0000             nan     0.2603   -0.0000
  3020        0.0000             nan     0.2603   -0.0000
  3040        0.0000             nan     0.2603   -0.0000
  3060        0.0000             nan     0.2603   -0.0000
  3080        0.0000             nan     0.2603   -0.0000
  3100        0.0000             nan     0.2603   -0.0000
  3120        0.0000             nan     0.2603   -0.0000
  3140        0.0000             nan     0.2603   -0.0000
  3160        0.0000             nan     0.2603   -0.0000
  3180        0.0000             nan     0.2603   -0.0000
  3200        0.0000             nan     0.2603   -0.0000
  3220        0.0000             nan     0.2603   -0.0000
  3240        0.0000             nan     0.2603   -0.0000
  3260        0.0000             nan     0.2603   -0.0000
  3280        0.0000             nan     0.2603   -0.0000
  3300        0.0000             nan     0.2603   -0.0000
  3320        0.0000             nan     0.2603   -0.0000
  3340        0.0000             nan     0.2603   -0.0000
  3360        0.0000             nan     0.2603   -0.0000
  3380        0.0000             nan     0.2603   -0.0000
  3400        0.0000             nan     0.2603   -0.0000
  3420        0.0000             nan     0.2603   -0.0000
  3440        0.0000             nan     0.2603   -0.0000
  3460        0.0000             nan     0.2603   -0.0000
  3480        0.0000             nan     0.2603   -0.0000
  3500        0.0000             nan     0.2603   -0.0000
  3520        0.0000             nan     0.2603   -0.0000
  3540        0.0000             nan     0.2603   -0.0000
  3560        0.0000             nan     0.2603   -0.0000
  3580        0.0000             nan     0.2603   -0.0000
  3600        0.0000             nan     0.2603   -0.0000
  3620        0.0000             nan     0.2603   -0.0000
  3640        0.0000             nan     0.2603   -0.0000
  3660        0.0000             nan     0.2603   -0.0000
  3680        0.0000             nan     0.2603   -0.0000
  3700        0.0000             nan     0.2603    0.0000
  3720        0.0000             nan     0.2603   -0.0000
  3740        0.0000             nan     0.2603   -0.0000
  3760        0.0000             nan     0.2603   -0.0000
  3780        0.0000             nan     0.2603   -0.0000
  3800        0.0000             nan     0.2603   -0.0000
  3820        0.0000             nan     0.2603   -0.0000
  3840        0.0000             nan     0.2603   -0.0000
  3860        0.0000             nan     0.2603   -0.0000
  3880        0.0000             nan     0.2603   -0.0000
  3900        0.0000             nan     0.2603    0.0000
  3920        0.0000             nan     0.2603   -0.0000
  3940        0.0000             nan     0.2603   -0.0000
  3960        0.0000             nan     0.2603   -0.0000
  3980        0.0000             nan     0.2603   -0.0000
  4000        0.0000             nan     0.2603   -0.0000
  4020        0.0000             nan     0.2603   -0.0000
  4040        0.0000             nan     0.2603   -0.0000
  4060        0.0000             nan     0.2603   -0.0000
  4080        0.0000             nan     0.2603   -0.0000
  4100        0.0000             nan     0.2603   -0.0000
  4120        0.0000             nan     0.2603   -0.0000
  4140        0.0000             nan     0.2603   -0.0000
  4160        0.0000             nan     0.2603   -0.0000
  4180        0.0000             nan     0.2603   -0.0000
  4200        0.0000             nan     0.2603   -0.0000
  4213        0.0000             nan     0.2603   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       42.8068             nan     0.2885    5.3170
     2       33.1006             nan     0.2885    7.8204
     3       27.8749             nan     0.2885    3.3560
     4       23.4454             nan     0.2885    2.5378
     5       20.7306             nan     0.2885    1.1303
     6       19.5383             nan     0.2885    1.6448
     7       17.6031             nan     0.2885    1.2860
     8       15.9877             nan     0.2885    0.9123
     9       16.1180             nan     0.2885   -2.3116
    10       16.4107             nan     0.2885   -1.5421
    20       11.2106             nan     0.2885   -0.1763
    40        7.7118             nan     0.2885   -0.2110
    60        6.0744             nan     0.2885   -0.2922
    80        5.1944             nan     0.2885   -0.2142
   100        4.4175             nan     0.2885   -0.2172
   120        3.5706             nan     0.2885   -0.2074
   140        3.1510             nan     0.2885   -0.2362
   160        2.6131             nan     0.2885   -0.1339
   180        2.3502             nan     0.2885   -0.0581
   200        2.2094             nan     0.2885   -0.1323
   220        2.0545             nan     0.2885   -0.1557
   240        1.8515             nan     0.2885   -0.0429
   260        1.7165             nan     0.2885   -0.0882
   280        1.5367             nan     0.2885   -0.0124
   300        1.4246             nan     0.2885   -0.0639
   320        1.3424             nan     0.2885   -0.0941
   340        1.3821             nan     0.2885   -0.1056
   360        1.2622             nan     0.2885   -0.0311
   380        1.1639             nan     0.2885   -0.0152
   400        1.0673             nan     0.2885   -0.0077
   420        0.9897             nan     0.2885   -0.0285
   440        0.9201             nan     0.2885   -0.0350
   460        0.8638             nan     0.2885    0.0030
   480        0.8100             nan     0.2885   -0.1071
   500        0.7882             nan     0.2885   -0.1063
   520        0.6976             nan     0.2885    0.0059
   540        0.6612             nan     0.2885   -0.0247
   560        0.6167             nan     0.2885   -0.0473
   580        0.6363             nan     0.2885   -0.0504
   600        0.5573             nan     0.2885   -0.0296
   620        0.5332             nan     0.2885   -0.0375
   640        0.4773             nan     0.2885   -0.0445
   660        0.4182             nan     0.2885   -0.0163
   680        0.3955             nan     0.2885   -0.0338
   700        0.3842             nan     0.2885   -0.0082
   720        0.3606             nan     0.2885   -0.0192
   740        0.3504             nan     0.2885   -0.0255
   760        0.3320             nan     0.2885   -0.0095
   780        0.3215             nan     0.2885   -0.0106
   800        0.3145             nan     0.2885   -0.0086
   820        0.2901             nan     0.2885   -0.0191
   840        0.2816             nan     0.2885   -0.0031
   860        0.2639             nan     0.2885   -0.0047
   880        0.2497             nan     0.2885   -0.0109
   900        0.2370             nan     0.2885   -0.0138
   920        0.2247             nan     0.2885   -0.0057
   940        0.2270             nan     0.2885   -0.0084
   960        0.2132             nan     0.2885   -0.0110
   980        0.2027             nan     0.2885   -0.0076
  1000        0.1970             nan     0.2885   -0.0105
  1020        0.1763             nan     0.2885   -0.0123
  1040        0.1645             nan     0.2885   -0.0008
  1060        0.1658             nan     0.2885   -0.0158
  1080        0.1550             nan     0.2885   -0.0034
  1100        0.1554             nan     0.2885   -0.0123
  1120        0.1500             nan     0.2885   -0.0114
  1140        0.1415             nan     0.2885   -0.0032
  1160        0.1354             nan     0.2885   -0.0034
  1180        0.1346             nan     0.2885   -0.0073
  1200        0.1288             nan     0.2885   -0.0087
  1220        0.1202             nan     0.2885   -0.0071
  1240        0.1135             nan     0.2885   -0.0082
  1260        0.1119             nan     0.2885   -0.0076
  1280        0.1088             nan     0.2885   -0.0051
  1300        0.1019             nan     0.2885   -0.0032
  1320        0.0973             nan     0.2885   -0.0049
  1340        0.0918             nan     0.2885   -0.0034
  1360        0.0866             nan     0.2885   -0.0030
  1380        0.0859             nan     0.2885   -0.0038
  1400        0.0850             nan     0.2885   -0.0046
  1420        0.0823             nan     0.2885   -0.0080
  1440        0.0778             nan     0.2885   -0.0025
  1460        0.0758             nan     0.2885   -0.0050
  1480        0.0741             nan     0.2885   -0.0052
  1500        0.0741             nan     0.2885   -0.0022
  1520        0.0691             nan     0.2885   -0.0019
  1540        0.0716             nan     0.2885   -0.0036
  1560        0.0666             nan     0.2885   -0.0037
  1580        0.0685             nan     0.2885   -0.0010
  1600        0.0647             nan     0.2885   -0.0015
  1620        0.0670             nan     0.2885   -0.0011
  1640        0.0618             nan     0.2885   -0.0038
  1660        0.0596             nan     0.2885   -0.0023
  1680        0.0571             nan     0.2885   -0.0035
  1700        0.0577             nan     0.2885   -0.0015
  1720        0.0545             nan     0.2885   -0.0016
  1740        0.0521             nan     0.2885   -0.0004
  1760        0.0495             nan     0.2885   -0.0012
  1780        0.0481             nan     0.2885   -0.0012
  1800        0.0472             nan     0.2885   -0.0011
  1820        0.0444             nan     0.2885   -0.0023
  1840        0.0430             nan     0.2885   -0.0018
  1860        0.0413             nan     0.2885   -0.0013
  1880        0.0400             nan     0.2885   -0.0013
  1900        0.0385             nan     0.2885   -0.0020
  1920        0.0371             nan     0.2885   -0.0017
  1940        0.0361             nan     0.2885   -0.0019
  1960        0.0344             nan     0.2885   -0.0012
  1980        0.0350             nan     0.2885   -0.0022
  2000        0.0342             nan     0.2885   -0.0010
  2020        0.0328             nan     0.2885   -0.0006
  2040        0.0323             nan     0.2885   -0.0020
  2060        0.0307             nan     0.2885   -0.0005
  2080        0.0302             nan     0.2885   -0.0020
  2100        0.0297             nan     0.2885   -0.0011
  2120        0.0289             nan     0.2885   -0.0008
  2140        0.0277             nan     0.2885   -0.0012
  2160        0.0270             nan     0.2885   -0.0005
  2180        0.0258             nan     0.2885   -0.0007
  2200        0.0258             nan     0.2885   -0.0014
  2220        0.0242             nan     0.2885   -0.0008
  2240        0.0231             nan     0.2885   -0.0012
  2260        0.0227             nan     0.2885   -0.0014
  2280        0.0226             nan     0.2885   -0.0004
  2300        0.0212             nan     0.2885   -0.0007
  2320        0.0206             nan     0.2885   -0.0012
  2340        0.0200             nan     0.2885   -0.0007
  2360        0.0196             nan     0.2885   -0.0013
  2380        0.0189             nan     0.2885   -0.0010
  2400        0.0186             nan     0.2885   -0.0009
  2420        0.0188             nan     0.2885   -0.0007
  2440        0.0182             nan     0.2885   -0.0008
  2460        0.0184             nan     0.2885   -0.0008
  2480        0.0187             nan     0.2885   -0.0007
  2500        0.0185             nan     0.2885   -0.0006
  2520        0.0176             nan     0.2885   -0.0010
  2540        0.0160             nan     0.2885   -0.0006
  2560        0.0154             nan     0.2885   -0.0012
  2580        0.0158             nan     0.2885   -0.0007
  2600        0.0156             nan     0.2885   -0.0007
  2620        0.0148             nan     0.2885   -0.0007
  2640        0.0144             nan     0.2885   -0.0007
  2660        0.0141             nan     0.2885   -0.0008
  2680        0.0139             nan     0.2885   -0.0005
  2700        0.0132             nan     0.2885   -0.0017
  2720        0.0124             nan     0.2885   -0.0004
  2740        0.0119             nan     0.2885   -0.0008
  2760        0.0115             nan     0.2885   -0.0005
  2780        0.0113             nan     0.2885   -0.0004
  2800        0.0108             nan     0.2885   -0.0001
  2820        0.0106             nan     0.2885   -0.0003
  2840        0.0103             nan     0.2885   -0.0006
  2860        0.0101             nan     0.2885   -0.0004
  2880        0.0097             nan     0.2885   -0.0002
  2900        0.0094             nan     0.2885   -0.0005
  2920        0.0091             nan     0.2885   -0.0006
  2940        0.0087             nan     0.2885   -0.0002
  2960        0.0088             nan     0.2885   -0.0003
  2980        0.0086             nan     0.2885   -0.0004
  3000        0.0083             nan     0.2885   -0.0004
  3020        0.0086             nan     0.2885   -0.0005
  3040        0.0080             nan     0.2885   -0.0002
  3060        0.0078             nan     0.2885   -0.0003
  3080        0.0076             nan     0.2885   -0.0001
  3100        0.0074             nan     0.2885   -0.0003
  3120        0.0072             nan     0.2885   -0.0003
  3140        0.0071             nan     0.2885   -0.0005
  3160        0.0070             nan     0.2885   -0.0004
  3180        0.0069             nan     0.2885   -0.0003
  3200        0.0066             nan     0.2885   -0.0004
  3220        0.0065             nan     0.2885   -0.0003
  3240        0.0062             nan     0.2885   -0.0004
  3260        0.0057             nan     0.2885   -0.0002
  3280        0.0056             nan     0.2885   -0.0002
  3300        0.0056             nan     0.2885   -0.0003
  3320        0.0053             nan     0.2885   -0.0003
  3340        0.0053             nan     0.2885   -0.0001
  3360        0.0053             nan     0.2885   -0.0002
  3380        0.0051             nan     0.2885   -0.0004
  3400        0.0049             nan     0.2885   -0.0003
  3420        0.0050             nan     0.2885   -0.0003
  3440        0.0048             nan     0.2885   -0.0002
  3460        0.0046             nan     0.2885    0.0000
  3480        0.0047             nan     0.2885   -0.0004
  3500        0.0044             nan     0.2885   -0.0003
  3520        0.0042             nan     0.2885   -0.0003
  3540        0.0042             nan     0.2885   -0.0002
  3560        0.0037             nan     0.2885   -0.0002
  3580        0.0036             nan     0.2885   -0.0002
  3600        0.0036             nan     0.2885   -0.0001
  3620        0.0036             nan     0.2885   -0.0000
  3640        0.0034             nan     0.2885   -0.0002
  3660        0.0034             nan     0.2885   -0.0002
  3680        0.0032             nan     0.2885   -0.0001
  3700        0.0032             nan     0.2885   -0.0002
  3720        0.0031             nan     0.2885   -0.0002
  3740        0.0029             nan     0.2885   -0.0000
  3760        0.0028             nan     0.2885   -0.0001
  3780        0.0028             nan     0.2885   -0.0001
  3800        0.0028             nan     0.2885   -0.0002
  3820        0.0027             nan     0.2885   -0.0002
  3840        0.0026             nan     0.2885   -0.0001
  3860        0.0026             nan     0.2885   -0.0001
  3880        0.0025             nan     0.2885   -0.0002
  3900        0.0024             nan     0.2885   -0.0000
  3920        0.0024             nan     0.2885   -0.0001
  3940        0.0023             nan     0.2885   -0.0000
  3960        0.0022             nan     0.2885   -0.0001
  3980        0.0022             nan     0.2885   -0.0001
  4000        0.0021             nan     0.2885   -0.0000
  4020        0.0020             nan     0.2885   -0.0001
  4040        0.0020             nan     0.2885   -0.0002
  4060        0.0019             nan     0.2885   -0.0002
  4080        0.0018             nan     0.2885   -0.0001
  4100        0.0018             nan     0.2885   -0.0001
  4120        0.0017             nan     0.2885   -0.0000
  4140        0.0017             nan     0.2885   -0.0002
  4160        0.0016             nan     0.2885   -0.0001
  4180        0.0015             nan     0.2885   -0.0001
  4200        0.0015             nan     0.2885   -0.0001
  4220        0.0015             nan     0.2885   -0.0000
  4240        0.0014             nan     0.2885   -0.0000
  4260        0.0013             nan     0.2885   -0.0001
  4280        0.0013             nan     0.2885   -0.0001
  4300        0.0012             nan     0.2885   -0.0000
  4320        0.0012             nan     0.2885   -0.0000
  4340        0.0012             nan     0.2885   -0.0001
  4360        0.0012             nan     0.2885   -0.0001
  4380        0.0012             nan     0.2885   -0.0000
  4400        0.0011             nan     0.2885   -0.0001
  4420        0.0011             nan     0.2885   -0.0000
  4440        0.0011             nan     0.2885   -0.0000
  4460        0.0010             nan     0.2885   -0.0000
  4480        0.0010             nan     0.2885   -0.0000
  4500        0.0010             nan     0.2885   -0.0001
  4520        0.0010             nan     0.2885   -0.0001
  4540        0.0010             nan     0.2885   -0.0001
  4560        0.0010             nan     0.2885   -0.0000
  4580        0.0009             nan     0.2885   -0.0001
  4600        0.0010             nan     0.2885   -0.0000
  4620        0.0009             nan     0.2885   -0.0001
  4640        0.0009             nan     0.2885   -0.0000
  4660        0.0009             nan     0.2885   -0.0000
  4680        0.0008             nan     0.2885   -0.0000
  4700        0.0008             nan     0.2885   -0.0000
  4720        0.0008             nan     0.2885   -0.0000
  4740        0.0007             nan     0.2885   -0.0000
  4760        0.0007             nan     0.2885   -0.0000
  4780        0.0007             nan     0.2885   -0.0000
  4800        0.0006             nan     0.2885   -0.0000
  4820        0.0006             nan     0.2885   -0.0000
  4837        0.0006             nan     0.2885   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       37.3652             nan     0.3353    8.7486
     2       26.2088             nan     0.3353    8.2297
     3       19.5089             nan     0.3353    3.0210
     4       16.5630             nan     0.3353    1.2339
     5       15.4067             nan     0.3353    0.1753
     6       12.7677             nan     0.3353    1.5296
     7       11.5829             nan     0.3353    1.2232
     8       11.5439             nan     0.3353   -1.1051
     9       10.6389             nan     0.3353   -0.0205
    10        9.4251             nan     0.3353    0.2976
    20        4.3147             nan     0.3353    0.1411
    40        1.5595             nan     0.3353   -0.0112
    60        0.6870             nan     0.3353   -0.1547
    80        0.3751             nan     0.3353   -0.0255
   100        0.2037             nan     0.3353   -0.0180
   120        0.0989             nan     0.3353   -0.0154
   140        0.0495             nan     0.3353   -0.0026
   160        0.0311             nan     0.3353   -0.0023
   180        0.0165             nan     0.3353   -0.0020
   200        0.0102             nan     0.3353   -0.0001
   220        0.0064             nan     0.3353   -0.0006
   240        0.0051             nan     0.3353   -0.0006
   260        0.0039             nan     0.3353   -0.0003
   280        0.0023             nan     0.3353   -0.0004
   300        0.0017             nan     0.3353   -0.0002
   320        0.0011             nan     0.3353   -0.0000
   340        0.0008             nan     0.3353   -0.0000
   360        0.0005             nan     0.3353   -0.0000
   380        0.0003             nan     0.3353   -0.0000
   400        0.0002             nan     0.3353   -0.0000
   420        0.0001             nan     0.3353   -0.0000
   440        0.0001             nan     0.3353   -0.0000
   460        0.0000             nan     0.3353   -0.0000
   480        0.0000             nan     0.3353   -0.0000
   500        0.0000             nan     0.3353   -0.0000
   520        0.0000             nan     0.3353   -0.0000
   540        0.0000             nan     0.3353   -0.0000
   560        0.0000             nan     0.3353   -0.0000
   580        0.0000             nan     0.3353   -0.0000
   600        0.0000             nan     0.3353   -0.0000
   620        0.0000             nan     0.3353   -0.0000
   640        0.0000             nan     0.3353   -0.0000
   660        0.0000             nan     0.3353   -0.0000
   680        0.0000             nan     0.3353   -0.0000
   700        0.0000             nan     0.3353   -0.0000
   720        0.0000             nan     0.3353   -0.0000
   740        0.0000             nan     0.3353   -0.0000
   760        0.0000             nan     0.3353   -0.0000
   780        0.0000             nan     0.3353   -0.0000
   800        0.0000             nan     0.3353   -0.0000
   820        0.0000             nan     0.3353   -0.0000
   840        0.0000             nan     0.3353   -0.0000
   860        0.0000             nan     0.3353   -0.0000
   880        0.0000             nan     0.3353   -0.0000
   900        0.0000             nan     0.3353   -0.0000
   920        0.0000             nan     0.3353   -0.0000
   940        0.0000             nan     0.3353   -0.0000
   960        0.0000             nan     0.3353   -0.0000
   980        0.0000             nan     0.3353   -0.0000
  1000        0.0000             nan     0.3353   -0.0000
  1020        0.0000             nan     0.3353   -0.0000
  1040        0.0000             nan     0.3353   -0.0000
  1060        0.0000             nan     0.3353   -0.0000
  1080        0.0000             nan     0.3353   -0.0000
  1100        0.0000             nan     0.3353   -0.0000
  1120        0.0000             nan     0.3353   -0.0000
  1140        0.0000             nan     0.3353   -0.0000
  1160        0.0000             nan     0.3353   -0.0000
  1180        0.0000             nan     0.3353   -0.0000
  1200        0.0000             nan     0.3353   -0.0000
  1220        0.0000             nan     0.3353   -0.0000
  1240        0.0000             nan     0.3353   -0.0000
  1260        0.0000             nan     0.3353   -0.0000
  1280        0.0000             nan     0.3353   -0.0000
  1300        0.0000             nan     0.3353   -0.0000
  1320        0.0000             nan     0.3353   -0.0000
  1340        0.0000             nan     0.3353   -0.0000
  1360        0.0000             nan     0.3353   -0.0000
  1380        0.0000             nan     0.3353   -0.0000
  1400        0.0000             nan     0.3353   -0.0000
  1420        0.0000             nan     0.3353   -0.0000
  1440        0.0000             nan     0.3353   -0.0000
  1460        0.0000             nan     0.3353   -0.0000
  1480        0.0000             nan     0.3353   -0.0000
  1500        0.0000             nan     0.3353   -0.0000
  1520        0.0000             nan     0.3353   -0.0000
  1540        0.0000             nan     0.3353   -0.0000
  1560        0.0000             nan     0.3353   -0.0000
  1580        0.0000             nan     0.3353   -0.0000
  1600        0.0000             nan     0.3353   -0.0000
  1620        0.0000             nan     0.3353   -0.0000
  1640        0.0000             nan     0.3353   -0.0000
  1660        0.0000             nan     0.3353   -0.0000
  1680        0.0000             nan     0.3353   -0.0000
  1700        0.0000             nan     0.3353   -0.0000
  1720        0.0000             nan     0.3353   -0.0000
  1740        0.0000             nan     0.3353   -0.0000
  1760        0.0000             nan     0.3353   -0.0000
  1780        0.0000             nan     0.3353    0.0000
  1800        0.0000             nan     0.3353   -0.0000
  1820        0.0000             nan     0.3353   -0.0000
  1840        0.0000             nan     0.3353    0.0000
  1860        0.0000             nan     0.3353   -0.0000
  1880        0.0000             nan     0.3353   -0.0000
  1900        0.0000             nan     0.3353   -0.0000
  1920        0.0000             nan     0.3353   -0.0000
  1940        0.0000             nan     0.3353   -0.0000
  1960        0.0000             nan     0.3353   -0.0000
  1980        0.0000             nan     0.3353   -0.0000
  2000        0.0000             nan     0.3353   -0.0000
  2020        0.0000             nan     0.3353   -0.0000
  2040        0.0000             nan     0.3353   -0.0000
  2060        0.0000             nan     0.3353   -0.0000
  2080        0.0000             nan     0.3353   -0.0000
  2100        0.0000             nan     0.3353   -0.0000
  2120        0.0000             nan     0.3353   -0.0000
  2140        0.0000             nan     0.3353   -0.0000
  2160        0.0000             nan     0.3353    0.0000
  2180        0.0000             nan     0.3353   -0.0000
  2200        0.0000             nan     0.3353   -0.0000
  2220        0.0000             nan     0.3353   -0.0000
  2240        0.0000             nan     0.3353   -0.0000
  2260        0.0000             nan     0.3353   -0.0000
  2280        0.0000             nan     0.3353   -0.0000
  2300        0.0000             nan     0.3353   -0.0000
  2320        0.0000             nan     0.3353   -0.0000
  2340        0.0000             nan     0.3353   -0.0000
  2360        0.0000             nan     0.3353   -0.0000
  2380        0.0000             nan     0.3353   -0.0000
  2400        0.0000             nan     0.3353   -0.0000
  2420        0.0000             nan     0.3353   -0.0000
  2440        0.0000             nan     0.3353   -0.0000
  2460        0.0000             nan     0.3353   -0.0000
  2480        0.0000             nan     0.3353   -0.0000
  2500        0.0000             nan     0.3353   -0.0000
  2520        0.0000             nan     0.3353   -0.0000
  2540        0.0000             nan     0.3353   -0.0000
  2560        0.0000             nan     0.3353   -0.0000
  2580        0.0000             nan     0.3353   -0.0000
  2600        0.0000             nan     0.3353   -0.0000
  2620        0.0000             nan     0.3353   -0.0000
  2640        0.0000             nan     0.3353   -0.0000
  2660        0.0000             nan     0.3353   -0.0000
  2680        0.0000             nan     0.3353   -0.0000
  2700        0.0000             nan     0.3353   -0.0000
  2720        0.0000             nan     0.3353   -0.0000
  2740        0.0000             nan     0.3353   -0.0000
  2760        0.0000             nan     0.3353   -0.0000
  2780        0.0000             nan     0.3353   -0.0000
  2800        0.0000             nan     0.3353   -0.0000
  2820        0.0000             nan     0.3353   -0.0000
  2840        0.0000             nan     0.3353   -0.0000
  2860        0.0000             nan     0.3353   -0.0000
  2880        0.0000             nan     0.3353   -0.0000
  2900        0.0000             nan     0.3353   -0.0000
  2920        0.0000             nan     0.3353   -0.0000
  2940        0.0000             nan     0.3353   -0.0000
  2960        0.0000             nan     0.3353   -0.0000
  2980        0.0000             nan     0.3353   -0.0000
  3000        0.0000             nan     0.3353   -0.0000
  3020        0.0000             nan     0.3353   -0.0000
  3040        0.0000             nan     0.3353   -0.0000
  3060        0.0000             nan     0.3353   -0.0000
  3080        0.0000             nan     0.3353   -0.0000
  3100        0.0000             nan     0.3353   -0.0000
  3120        0.0000             nan     0.3353   -0.0000
  3140        0.0000             nan     0.3353   -0.0000
  3160        0.0000             nan     0.3353   -0.0000
  3180        0.0000             nan     0.3353   -0.0000
  3200        0.0000             nan     0.3353   -0.0000
  3220        0.0000             nan     0.3353   -0.0000
  3240        0.0000             nan     0.3353   -0.0000
  3260        0.0000             nan     0.3353   -0.0000
  3280        0.0000             nan     0.3353   -0.0000
  3300        0.0000             nan     0.3353   -0.0000
  3320        0.0000             nan     0.3353   -0.0000
  3340        0.0000             nan     0.3353   -0.0000
  3360        0.0000             nan     0.3353   -0.0000
  3380        0.0000             nan     0.3353   -0.0000
  3400        0.0000             nan     0.3353   -0.0000
  3420        0.0000             nan     0.3353   -0.0000
  3440        0.0000             nan     0.3353   -0.0000
  3460        0.0000             nan     0.3353   -0.0000
  3480        0.0000             nan     0.3353   -0.0000
  3500        0.0000             nan     0.3353   -0.0000
  3520        0.0000             nan     0.3353   -0.0000
  3540        0.0000             nan     0.3353   -0.0000
  3560        0.0000             nan     0.3353   -0.0000
  3580        0.0000             nan     0.3353   -0.0000
  3585        0.0000             nan     0.3353   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       36.2474             nan     0.5600    9.1602
     2       24.3335             nan     0.5600    7.8674
     3       18.8804             nan     0.5600    0.2219
     4       15.0192             nan     0.5600   -2.0830
     5       13.4340             nan     0.5600    1.0145
     6       13.7601             nan     0.5600   -3.1573
     7       13.3659             nan     0.5600   -0.0871
     8       12.4588             nan     0.5600   -2.2189
     9       10.9936             nan     0.5600    1.2611
    10       10.8828             nan     0.5600   -0.9268
    20        9.8478             nan     0.5600   -1.1238
    40        6.7644             nan     0.5600    0.0512
    60        5.8982             nan     0.5600   -2.3344
    80        4.1961             nan     0.5600   -0.5311
   100        3.3881             nan     0.5600   -0.1548
   120        2.5506             nan     0.5600   -0.3912
   140        2.3677             nan     0.5600   -0.0627
   160        2.0762             nan     0.5600   -0.2775
   180        1.7697             nan     0.5600   -0.1634
   200        1.4918             nan     0.5600   -0.0215
   220        1.3119             nan     0.5600   -0.0497
   240        1.3219             nan     0.5600   -0.0882
   260        1.2090             nan     0.5600   -0.0449
   280        1.0236             nan     0.5600   -0.0189
   300        0.9673             nan     0.5600   -0.1019
   320        0.9464             nan     0.5600   -0.1446
   340        0.8912             nan     0.5600   -0.1957
   360        0.7878             nan     0.5600   -0.0818
   380        0.8283             nan     0.5600   -0.0065
   400        0.6856             nan     0.5600   -0.0542
   420        0.5986             nan     0.5600   -0.0354
   440        0.5788             nan     0.5600   -0.1381
   460        0.4743             nan     0.5600   -0.0401
   480        0.4520             nan     0.5600   -0.0443
   500        0.3980             nan     0.5600   -0.0490
   520        0.4006             nan     0.5600   -0.1084
   540        0.3762             nan     0.5600   -0.0315
   560        0.3435             nan     0.5600   -0.0508
   580        0.3189             nan     0.5600   -0.0240
   600        0.2729             nan     0.5600   -0.0160
   620        0.2748             nan     0.5600   -0.0193
   640        0.2383             nan     0.5600   -0.0091
   660        0.2207             nan     0.5600   -0.0352
   680        0.2165             nan     0.5600   -0.0296
   700        0.2131             nan     0.5600   -0.0501
   720        0.1720             nan     0.5600    0.0013
   740        0.1647             nan     0.5600   -0.0095
   760        0.1435             nan     0.5600   -0.0123
   780        0.1413             nan     0.5600   -0.0203
   800        0.1410             nan     0.5600   -0.0198
   820        0.1287             nan     0.5600   -0.0095
   840        0.1145             nan     0.5600   -0.0059
   860        0.1141             nan     0.5600   -0.0135
   880        0.1033             nan     0.5600   -0.0157
   900        0.1070             nan     0.5600   -0.0100
   920        0.0848             nan     0.5600   -0.0055
   940        0.0767             nan     0.5600   -0.0098
   960        0.0749             nan     0.5600   -0.0101
   980        0.0711             nan     0.5600   -0.0087
  1000        0.0679             nan     0.5600   -0.0069
  1020        0.0701             nan     0.5600   -0.0083
  1040        0.0614             nan     0.5600   -0.0006
  1060        0.0584             nan     0.5600   -0.0036
  1080        0.0553             nan     0.5600   -0.0088
  1100        0.0547             nan     0.5600   -0.0074
  1120        0.0511             nan     0.5600   -0.0075
  1140        0.0477             nan     0.5600   -0.0030
  1160        0.0443             nan     0.5600   -0.0057
  1180        0.0402             nan     0.5600   -0.0066
  1200        0.0388             nan     0.5600   -0.0036
  1220        0.0380             nan     0.5600   -0.0068
  1240        0.0344             nan     0.5600   -0.0007
  1260        0.0350             nan     0.5600   -0.0038
  1280        0.0363             nan     0.5600   -0.0043
  1300        0.0320             nan     0.5600   -0.0019
  1320        0.0296             nan     0.5600   -0.0063
  1340        0.0264             nan     0.5600   -0.0025
  1360        0.0268             nan     0.5600   -0.0033
  1380        0.0249             nan     0.5600   -0.0024
  1400        0.0260             nan     0.5600   -0.0031
  1420        0.0225             nan     0.5600   -0.0029
  1440        0.0208             nan     0.5600   -0.0015
  1460        0.0192             nan     0.5600   -0.0024
  1480        0.0174             nan     0.5600   -0.0008
  1500        0.0163             nan     0.5600   -0.0010
  1520        0.0156             nan     0.5600   -0.0024
  1540        0.0168             nan     0.5600   -0.0010
  1560        0.0155             nan     0.5600   -0.0009
  1580        0.0146             nan     0.5600   -0.0003
  1600        0.0126             nan     0.5600   -0.0023
  1620        0.0108             nan     0.5600   -0.0009
  1640        0.0115             nan     0.5600   -0.0018
  1660        0.0113             nan     0.5600   -0.0022
  1680        0.0102             nan     0.5600   -0.0009
  1700        0.0091             nan     0.5600   -0.0014
  1720        0.0091             nan     0.5600   -0.0016
  1740        0.0081             nan     0.5600   -0.0011
  1760        0.0078             nan     0.5600   -0.0008
  1780        0.0072             nan     0.5600   -0.0005
  1800        0.0062             nan     0.5600   -0.0005
  1820        0.0056             nan     0.5600   -0.0006
  1840        0.0050             nan     0.5600   -0.0002
  1860        0.0048             nan     0.5600    0.0001
  1880        0.0048             nan     0.5600   -0.0003
  1900        0.0044             nan     0.5600   -0.0002
  1920        0.0042             nan     0.5600   -0.0003
  1940        0.0041             nan     0.5600   -0.0003
  1960        0.0044             nan     0.5600   -0.0002
  1980        0.0042             nan     0.5600   -0.0011
  2000        0.0041             nan     0.5600   -0.0010
  2020        0.0040             nan     0.5600   -0.0007
  2040        0.0034             nan     0.5600   -0.0003
  2060        0.0030             nan     0.5600   -0.0002
  2080        0.0029             nan     0.5600   -0.0001
  2100        0.0029             nan     0.5600    0.0001
  2120        0.0028             nan     0.5600   -0.0008
  2140        0.0030             nan     0.5600   -0.0003
  2160        0.0027             nan     0.5600   -0.0001
  2180        0.0027             nan     0.5600   -0.0005
  2200        0.0024             nan     0.5600   -0.0002
  2220        0.0025             nan     0.5600   -0.0003
  2240        0.0022             nan     0.5600   -0.0002
  2260        0.0023             nan     0.5600   -0.0002
  2280        0.0021             nan     0.5600   -0.0003
  2300        0.0022             nan     0.5600   -0.0001
  2320        0.0019             nan     0.5600   -0.0002
  2340        0.0018             nan     0.5600   -0.0002
  2360        0.0018             nan     0.5600   -0.0004
  2380        0.0017             nan     0.5600   -0.0000
  2400        0.0016             nan     0.5600   -0.0000
  2420        0.0015             nan     0.5600   -0.0002
  2440        0.0014             nan     0.5600   -0.0001
  2460        0.0014             nan     0.5600   -0.0002
  2480        0.0014             nan     0.5600   -0.0001
  2500        0.0014             nan     0.5600   -0.0001
  2510        0.0013             nan     0.5600   -0.0001

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       39.4070             nan     0.0786    2.3737
     2       37.4508             nan     0.0786    1.4444
     3       35.0478             nan     0.0786    1.8058
     4       33.1716             nan     0.0786    1.5646
     5       31.5909             nan     0.0786    1.4058
     6       30.9827             nan     0.0786    0.0573
     7       28.5658             nan     0.0786    1.4120
     8       28.0252             nan     0.0786   -0.6519
     9       26.6085             nan     0.0786    1.3178
    10       25.0266             nan     0.0786    0.4533
    20       16.9266             nan     0.0786    0.3568
    40       10.1658             nan     0.0786    0.1622
    60        7.6512             nan     0.0786    0.0219
    80        6.6567             nan     0.0786   -0.1430
   100        5.7760             nan     0.0786   -0.1075
   120        5.2049             nan     0.0786   -0.0943
   140        4.8647             nan     0.0786   -0.2062
   160        4.4781             nan     0.0786   -0.0556
   180        4.1244             nan     0.0786   -0.0181
   200        3.8913             nan     0.0786   -0.0285
   220        3.6742             nan     0.0786   -0.0548
   240        3.4743             nan     0.0786   -0.0506
   260        3.1191             nan     0.0786   -0.0771
   280        2.9189             nan     0.0786   -0.0180
   300        2.6993             nan     0.0786   -0.0417
   320        2.5922             nan     0.0786   -0.0619
   340        2.4737             nan     0.0786   -0.0168
   360        2.3449             nan     0.0786   -0.0753
   380        2.2199             nan     0.0786   -0.0506
   400        2.1499             nan     0.0786   -0.0211
   420        1.9911             nan     0.0786   -0.0136
   440        1.9026             nan     0.0786   -0.0463
   460        1.8273             nan     0.0786   -0.0265
   480        1.7119             nan     0.0786   -0.0252
   500        1.6419             nan     0.0786   -0.0181
   520        1.5724             nan     0.0786   -0.0215
   540        1.4990             nan     0.0786   -0.0092
   560        1.4433             nan     0.0786   -0.0288
   580        1.3720             nan     0.0786   -0.0319
   600        1.3267             nan     0.0786   -0.0098
   620        1.2731             nan     0.0786   -0.0180
   640        1.2172             nan     0.0786   -0.0291
   660        1.1775             nan     0.0786   -0.0083
   680        1.1475             nan     0.0786   -0.0355
   700        1.1061             nan     0.0786   -0.0204
   720        1.0697             nan     0.0786   -0.0155
   740        1.0405             nan     0.0786   -0.0166
   760        0.9793             nan     0.0786   -0.0217
   780        0.9462             nan     0.0786   -0.0202
   800        0.9026             nan     0.0786   -0.0254
   820        0.8775             nan     0.0786   -0.0161
   840        0.8550             nan     0.0786   -0.0172
   860        0.8357             nan     0.0786   -0.0117
   880        0.8054             nan     0.0786   -0.0180
   900        0.7913             nan     0.0786   -0.0097
   920        0.7596             nan     0.0786   -0.0106
   940        0.7301             nan     0.0786   -0.0171
   960        0.7117             nan     0.0786   -0.0105
   980        0.6974             nan     0.0786   -0.0120
  1000        0.6768             nan     0.0786   -0.0117
  1020        0.6604             nan     0.0786   -0.0060
  1040        0.6358             nan     0.0786   -0.0098
  1060        0.6122             nan     0.0786   -0.0120
  1080        0.5851             nan     0.0786   -0.0088
  1100        0.5721             nan     0.0786   -0.0074
  1120        0.5599             nan     0.0786   -0.0128
  1140        0.5482             nan     0.0786   -0.0025
  1160        0.5318             nan     0.0786   -0.0050
  1180        0.5192             nan     0.0786   -0.0091
  1200        0.5051             nan     0.0786   -0.0073
  1220        0.4822             nan     0.0786   -0.0065
  1240        0.4693             nan     0.0786   -0.0043
  1260        0.4567             nan     0.0786   -0.0110
  1280        0.4440             nan     0.0786   -0.0099
  1300        0.4258             nan     0.0786   -0.0116
  1320        0.4095             nan     0.0786   -0.0034
  1340        0.4033             nan     0.0786   -0.0092
  1360        0.3871             nan     0.0786   -0.0067
  1380        0.3758             nan     0.0786   -0.0037
  1400        0.3555             nan     0.0786   -0.0067
  1420        0.3465             nan     0.0786   -0.0075
  1440        0.3369             nan     0.0786   -0.0085
  1460        0.3242             nan     0.0786   -0.0050
  1480        0.3153             nan     0.0786   -0.0032
  1500        0.3048             nan     0.0786   -0.0049
  1520        0.2962             nan     0.0786   -0.0029
  1540        0.2848             nan     0.0786   -0.0033
  1560        0.2794             nan     0.0786   -0.0038
  1580        0.2725             nan     0.0786   -0.0018
  1600        0.2667             nan     0.0786   -0.0043
  1620        0.2599             nan     0.0786   -0.0024
  1640        0.2540             nan     0.0786   -0.0059
  1660        0.2443             nan     0.0786   -0.0052
  1680        0.2370             nan     0.0786   -0.0067
  1700        0.2355             nan     0.0786   -0.0044
  1720        0.2317             nan     0.0786   -0.0035
  1740        0.2266             nan     0.0786   -0.0046
  1753        0.2232             nan     0.0786   -0.0030

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       39.9720             nan     0.0829    1.9246
     2       37.0349             nan     0.0829    1.8005
     3       35.1316             nan     0.0829    1.1865
     4       32.6723             nan     0.0829    2.3095
     5       30.7554             nan     0.0829    1.8358
     6       29.7122             nan     0.0829   -0.2437
     7       27.9650             nan     0.0829    0.5287
     8       26.8594             nan     0.0829    0.8246
     9       25.5779             nan     0.0829    0.4457
    10       24.5786             nan     0.0829    1.0150
    20       15.5176             nan     0.0829    0.0060
    40        9.2574             nan     0.0829    0.0194
    60        6.5824             nan     0.0829   -0.0073
    80        5.3116             nan     0.0829   -0.1026
   100        4.3063             nan     0.0829   -0.0319
   120        3.7788             nan     0.0829   -0.0632
   140        3.2599             nan     0.0829   -0.0397
   160        2.9502             nan     0.0829   -0.0171
   180        2.6809             nan     0.0829   -0.0433
   200        2.3782             nan     0.0829   -0.0461
   220        2.1891             nan     0.0829   -0.0199
   240        2.0076             nan     0.0829   -0.0123
   260        1.8258             nan     0.0829   -0.0239
   280        1.6746             nan     0.0829   -0.0459
   300        1.5214             nan     0.0829   -0.0663
   320        1.3794             nan     0.0829    0.0006
   340        1.2717             nan     0.0829   -0.0154
   360        1.2093             nan     0.0829   -0.0190
   380        1.1204             nan     0.0829   -0.0213
   400        1.0352             nan     0.0829   -0.0217
   420        0.9725             nan     0.0829   -0.0283
   440        0.9129             nan     0.0829   -0.0304
   460        0.8524             nan     0.0829   -0.0085
   480        0.8119             nan     0.0829   -0.0174
   500        0.7407             nan     0.0829   -0.0170
   520        0.6712             nan     0.0829   -0.0051
   540        0.6312             nan     0.0829   -0.0188
   560        0.6114             nan     0.0829   -0.0187
   580        0.5675             nan     0.0829   -0.0114
   600        0.5305             nan     0.0829   -0.0083
   620        0.4933             nan     0.0829   -0.0015
   640        0.4672             nan     0.0829   -0.0076
   660        0.4454             nan     0.0829   -0.0134
   680        0.4247             nan     0.0829   -0.0041
   700        0.4026             nan     0.0829   -0.0086
   720        0.3767             nan     0.0829   -0.0044
   740        0.3549             nan     0.0829   -0.0006
   760        0.3396             nan     0.0829   -0.0111
   780        0.3223             nan     0.0829   -0.0113
   800        0.3044             nan     0.0829   -0.0038
   820        0.2864             nan     0.0829   -0.0047
   840        0.2761             nan     0.0829   -0.0071
   860        0.2615             nan     0.0829   -0.0033
   880        0.2494             nan     0.0829   -0.0088
   900        0.2357             nan     0.0829   -0.0039
   920        0.2215             nan     0.0829   -0.0040
   940        0.2064             nan     0.0829   -0.0084
   960        0.1934             nan     0.0829   -0.0029
   980        0.1844             nan     0.0829   -0.0022
  1000        0.1740             nan     0.0829   -0.0029
  1020        0.1664             nan     0.0829   -0.0038
  1040        0.1559             nan     0.0829   -0.0022
  1060        0.1449             nan     0.0829   -0.0048
  1080        0.1373             nan     0.0829   -0.0027
  1100        0.1303             nan     0.0829   -0.0029
  1120        0.1205             nan     0.0829   -0.0031
  1140        0.1145             nan     0.0829   -0.0019
  1160        0.1081             nan     0.0829   -0.0040
  1180        0.1041             nan     0.0829   -0.0035
  1200        0.0980             nan     0.0829   -0.0009
  1220        0.0930             nan     0.0829   -0.0021
  1240        0.0896             nan     0.0829   -0.0034
  1260        0.0860             nan     0.0829   -0.0022
  1280        0.0803             nan     0.0829   -0.0007
  1300        0.0772             nan     0.0829   -0.0028
  1320        0.0732             nan     0.0829   -0.0021
  1340        0.0704             nan     0.0829   -0.0029
  1360        0.0668             nan     0.0829   -0.0016
  1380        0.0617             nan     0.0829   -0.0006
  1400        0.0586             nan     0.0829   -0.0016
  1420        0.0546             nan     0.0829   -0.0011
  1440        0.0521             nan     0.0829   -0.0020
  1460        0.0487             nan     0.0829   -0.0006
  1480        0.0460             nan     0.0829   -0.0013
  1500        0.0435             nan     0.0829   -0.0010
  1520        0.0402             nan     0.0829   -0.0012
  1540        0.0380             nan     0.0829   -0.0012
  1560        0.0358             nan     0.0829   -0.0002
  1580        0.0348             nan     0.0829   -0.0012
  1600        0.0330             nan     0.0829   -0.0006
  1620        0.0310             nan     0.0829   -0.0008
  1640        0.0295             nan     0.0829   -0.0006
  1660        0.0285             nan     0.0829   -0.0005
  1680        0.0263             nan     0.0829   -0.0006
  1700        0.0247             nan     0.0829   -0.0006
  1720        0.0243             nan     0.0829   -0.0005
  1740        0.0236             nan     0.0829   -0.0002
  1760        0.0228             nan     0.0829   -0.0008
  1765        0.0227             nan     0.0829   -0.0007

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       38.3223             nan     0.1176    1.1122
     2       34.1663             nan     0.1176    2.4437
     3       31.6114             nan     0.1176    0.3110
     4       27.7328             nan     0.1176    3.1299
     5       24.8593             nan     0.1176    1.9494
     6       21.6453             nan     0.1176    1.1793
     7       19.4888             nan     0.1176    0.7573
     8       17.6478             nan     0.1176    0.9092
     9       16.2722             nan     0.1176    0.5341
    10       14.8117             nan     0.1176    1.6995
    20        6.8052             nan     0.1176   -0.0135
    40        2.5314             nan     0.1176    0.0119
    60        1.1980             nan     0.1176   -0.0527
    80        0.7655             nan     0.1176   -0.0306
   100        0.4323             nan     0.1176   -0.0150
   120        0.2541             nan     0.1176   -0.0130
   140        0.1599             nan     0.1176   -0.0085
   160        0.1021             nan     0.1176   -0.0033
   180        0.0628             nan     0.1176   -0.0022
   200        0.0445             nan     0.1176   -0.0019
   220        0.0284             nan     0.1176   -0.0012
   240        0.0185             nan     0.1176    0.0000
   260        0.0120             nan     0.1176   -0.0004
   280        0.0089             nan     0.1176    0.0001
   300        0.0062             nan     0.1176   -0.0003
   320        0.0045             nan     0.1176   -0.0003
   340        0.0032             nan     0.1176   -0.0001
   360        0.0023             nan     0.1176   -0.0000
   380        0.0018             nan     0.1176   -0.0001
   400        0.0012             nan     0.1176   -0.0001
   420        0.0009             nan     0.1176   -0.0000
   440        0.0005             nan     0.1176   -0.0000
   460        0.0004             nan     0.1176   -0.0000
   480        0.0002             nan     0.1176   -0.0000
   500        0.0002             nan     0.1176   -0.0000
   520        0.0001             nan     0.1176   -0.0000
   540        0.0001             nan     0.1176   -0.0000
   560        0.0001             nan     0.1176   -0.0000
   580        0.0000             nan     0.1176   -0.0000
   600        0.0000             nan     0.1176   -0.0000
   620        0.0000             nan     0.1176   -0.0000
   640        0.0000             nan     0.1176   -0.0000
   660        0.0000             nan     0.1176   -0.0000
   680        0.0000             nan     0.1176   -0.0000
   700        0.0000             nan     0.1176   -0.0000
   720        0.0000             nan     0.1176   -0.0000
   740        0.0000             nan     0.1176   -0.0000
   760        0.0000             nan     0.1176   -0.0000
   780        0.0000             nan     0.1176   -0.0000
   800        0.0000             nan     0.1176   -0.0000
   820        0.0000             nan     0.1176   -0.0000
   840        0.0000             nan     0.1176   -0.0000
   860        0.0000             nan     0.1176   -0.0000
   880        0.0000             nan     0.1176   -0.0000
   900        0.0000             nan     0.1176   -0.0000
   920        0.0000             nan     0.1176   -0.0000
   940        0.0000             nan     0.1176   -0.0000
   960        0.0000             nan     0.1176   -0.0000
   980        0.0000             nan     0.1176   -0.0000
  1000        0.0000             nan     0.1176   -0.0000
  1020        0.0000             nan     0.1176   -0.0000
  1040        0.0000             nan     0.1176   -0.0000
  1060        0.0000             nan     0.1176   -0.0000
  1080        0.0000             nan     0.1176   -0.0000
  1100        0.0000             nan     0.1176   -0.0000
  1120        0.0000             nan     0.1176   -0.0000
  1140        0.0000             nan     0.1176   -0.0000
  1160        0.0000             nan     0.1176    0.0000
  1180        0.0000             nan     0.1176   -0.0000
  1200        0.0000             nan     0.1176   -0.0000
  1220        0.0000             nan     0.1176   -0.0000
  1240        0.0000             nan     0.1176   -0.0000
  1260        0.0000             nan     0.1176   -0.0000
  1280        0.0000             nan     0.1176   -0.0000
  1300        0.0000             nan     0.1176   -0.0000
  1320        0.0000             nan     0.1176   -0.0000
  1340        0.0000             nan     0.1176   -0.0000
  1360        0.0000             nan     0.1176   -0.0000
  1380        0.0000             nan     0.1176   -0.0000
  1400        0.0000             nan     0.1176   -0.0000
  1420        0.0000             nan     0.1176   -0.0000
  1440        0.0000             nan     0.1176   -0.0000
  1460        0.0000             nan     0.1176   -0.0000
  1480        0.0000             nan     0.1176   -0.0000
  1500        0.0000             nan     0.1176   -0.0000
  1520        0.0000             nan     0.1176   -0.0000
  1540        0.0000             nan     0.1176   -0.0000
  1560        0.0000             nan     0.1176   -0.0000
  1580        0.0000             nan     0.1176   -0.0000
  1600        0.0000             nan     0.1176   -0.0000
  1620        0.0000             nan     0.1176   -0.0000
  1640        0.0000             nan     0.1176   -0.0000
  1660        0.0000             nan     0.1176   -0.0000
  1680        0.0000             nan     0.1176   -0.0000
  1700        0.0000             nan     0.1176   -0.0000
  1720        0.0000             nan     0.1176   -0.0000
  1740        0.0000             nan     0.1176   -0.0000
  1760        0.0000             nan     0.1176   -0.0000
  1780        0.0000             nan     0.1176   -0.0000
  1800        0.0000             nan     0.1176   -0.0000
  1820        0.0000             nan     0.1176   -0.0000
  1840        0.0000             nan     0.1176   -0.0000
  1860        0.0000             nan     0.1176   -0.0000
  1880        0.0000             nan     0.1176   -0.0000
  1900        0.0000             nan     0.1176   -0.0000
  1920        0.0000             nan     0.1176   -0.0000
  1940        0.0000             nan     0.1176   -0.0000
  1960        0.0000             nan     0.1176   -0.0000
  1980        0.0000             nan     0.1176   -0.0000
  2000        0.0000             nan     0.1176   -0.0000
  2020        0.0000             nan     0.1176   -0.0000
  2040        0.0000             nan     0.1176   -0.0000
  2060        0.0000             nan     0.1176   -0.0000
  2080        0.0000             nan     0.1176   -0.0000
  2100        0.0000             nan     0.1176   -0.0000
  2120        0.0000             nan     0.1176   -0.0000
  2140        0.0000             nan     0.1176   -0.0000
  2160        0.0000             nan     0.1176   -0.0000
  2180        0.0000             nan     0.1176   -0.0000
  2186        0.0000             nan     0.1176   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       34.5926             nan     0.1352    6.7770
     2       31.1847             nan     0.1352    2.0035
     3       26.8765             nan     0.1352    1.6141
     4       23.3498             nan     0.1352    1.8325
     5       21.1315             nan     0.1352    1.5090
     6       18.8359             nan     0.1352    0.7628
     7       17.3292             nan     0.1352    0.2366
     8       15.4686             nan     0.1352    0.7719
     9       13.9832             nan     0.1352    0.5417
    10       13.1203             nan     0.1352    0.1018
    20        7.1785             nan     0.1352   -0.0279
    40        3.2656             nan     0.1352   -0.2055
    60        1.9155             nan     0.1352   -0.0358
    80        1.2851             nan     0.1352   -0.0651
   100        0.7670             nan     0.1352   -0.0299
   120        0.5113             nan     0.1352   -0.0126
   140        0.3619             nan     0.1352   -0.0257
   160        0.2479             nan     0.1352   -0.0048
   180        0.1582             nan     0.1352   -0.0050
   200        0.1095             nan     0.1352   -0.0039
   220        0.0739             nan     0.1352   -0.0058
   240        0.0541             nan     0.1352   -0.0023
   260        0.0412             nan     0.1352   -0.0007
   280        0.0282             nan     0.1352   -0.0005
   300        0.0233             nan     0.1352   -0.0009
   320        0.0162             nan     0.1352   -0.0010
   340        0.0130             nan     0.1352   -0.0003
   360        0.0092             nan     0.1352   -0.0003
   380        0.0071             nan     0.1352   -0.0003
   400        0.0060             nan     0.1352   -0.0002
   420        0.0053             nan     0.1352   -0.0002
   440        0.0038             nan     0.1352   -0.0002
   460        0.0031             nan     0.1352   -0.0001
   480        0.0024             nan     0.1352   -0.0001
   500        0.0018             nan     0.1352   -0.0001
   520        0.0014             nan     0.1352   -0.0000
   540        0.0010             nan     0.1352   -0.0000
   560        0.0007             nan     0.1352   -0.0000
   580        0.0005             nan     0.1352   -0.0000
   600        0.0004             nan     0.1352   -0.0000
   620        0.0003             nan     0.1352   -0.0000
   640        0.0002             nan     0.1352   -0.0000
   660        0.0002             nan     0.1352   -0.0000
   680        0.0002             nan     0.1352   -0.0000
   700        0.0001             nan     0.1352   -0.0000
   703        0.0001             nan     0.1352   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       32.9903             nan     0.2413    5.2546
     2       25.4330             nan     0.2413    4.1845
     3       21.6652             nan     0.2413    1.6282
     4       19.1043             nan     0.2413    0.7881
     5       16.4033             nan     0.2413    1.3577
     6       14.1566             nan     0.2413   -0.1110
     7       11.9684             nan     0.2413    0.5396
     8       10.6891             nan     0.2413    0.0654
     9        8.9908             nan     0.2413   -0.0457
    10        8.2674             nan     0.2413    0.0979
    20        4.6762             nan     0.2413   -0.3270
    40        2.3587             nan     0.2413   -0.2192
    60        1.1270             nan     0.2413   -0.1308
    80        0.7280             nan     0.2413   -0.0270
   100        0.4414             nan     0.2413   -0.0195
   120        0.2832             nan     0.2413   -0.0162
   140        0.1578             nan     0.2413   -0.0090
   160        0.1186             nan     0.2413   -0.0102
   180        0.0862             nan     0.2413   -0.0064
   200        0.0535             nan     0.2413   -0.0020
   220        0.0324             nan     0.2413   -0.0032
   240        0.0201             nan     0.2413   -0.0022
   260        0.0155             nan     0.2413   -0.0027
   280        0.0089             nan     0.2413   -0.0004
   300        0.0056             nan     0.2413   -0.0002
   320        0.0038             nan     0.2413   -0.0001
   340        0.0026             nan     0.2413   -0.0003
   360        0.0016             nan     0.2413   -0.0000
   380        0.0011             nan     0.2413   -0.0000
   400        0.0009             nan     0.2413    0.0000
   420        0.0005             nan     0.2413   -0.0000
   440        0.0003             nan     0.2413   -0.0000
   460        0.0002             nan     0.2413   -0.0000
   480        0.0001             nan     0.2413   -0.0000
   500        0.0001             nan     0.2413   -0.0000
   520        0.0001             nan     0.2413    0.0000
   540        0.0000             nan     0.2413   -0.0000
   560        0.0000             nan     0.2413   -0.0000
   580        0.0000             nan     0.2413   -0.0000
   600        0.0000             nan     0.2413   -0.0000
   620        0.0000             nan     0.2413   -0.0000
   640        0.0000             nan     0.2413   -0.0000
   660        0.0000             nan     0.2413   -0.0000
   680        0.0000             nan     0.2413   -0.0000
   700        0.0000             nan     0.2413   -0.0000
   720        0.0000             nan     0.2413   -0.0000
   740        0.0000             nan     0.2413   -0.0000
   760        0.0000             nan     0.2413   -0.0000
   780        0.0000             nan     0.2413   -0.0000
   800        0.0000             nan     0.2413   -0.0000
   820        0.0000             nan     0.2413   -0.0000
   840        0.0000             nan     0.2413   -0.0000
   846        0.0000             nan     0.2413   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       33.9452             nan     0.2603    7.6508
     2       26.0873             nan     0.2603    7.2417
     3       20.2380             nan     0.2603    3.6592
     4       16.0742             nan     0.2603    2.1230
     5       14.1144             nan     0.2603    0.7092
     6       12.2911             nan     0.2603    0.9654
     7       11.0322             nan     0.2603    0.3191
     8        9.8614             nan     0.2603   -0.0496
     9        9.2638             nan     0.2603   -0.4948
    10        8.3559             nan     0.2603   -0.1168
    20        4.0321             nan     0.2603   -0.8752
    40        1.8122             nan     0.2603   -0.0624
    60        0.9655             nan     0.2603   -0.0271
    80        0.5642             nan     0.2603   -0.0630
   100        0.3364             nan     0.2603   -0.0276
   120        0.2001             nan     0.2603   -0.0112
   140        0.1298             nan     0.2603   -0.0073
   160        0.0746             nan     0.2603   -0.0078
   180        0.0460             nan     0.2603   -0.0065
   200        0.0297             nan     0.2603   -0.0017
   220        0.0204             nan     0.2603   -0.0023
   240        0.0122             nan     0.2603   -0.0011
   260        0.0088             nan     0.2603   -0.0004
   280        0.0052             nan     0.2603   -0.0000
   300        0.0033             nan     0.2603   -0.0005
   320        0.0025             nan     0.2603   -0.0002
   340        0.0015             nan     0.2603   -0.0001
   360        0.0009             nan     0.2603   -0.0001
   380        0.0006             nan     0.2603   -0.0000
   400        0.0004             nan     0.2603   -0.0000
   420        0.0002             nan     0.2603   -0.0000
   440        0.0002             nan     0.2603   -0.0000
   460        0.0001             nan     0.2603   -0.0000
   480        0.0001             nan     0.2603   -0.0000
   500        0.0001             nan     0.2603   -0.0000
   520        0.0000             nan     0.2603   -0.0000
   540        0.0000             nan     0.2603   -0.0000
   560        0.0000             nan     0.2603   -0.0000
   580        0.0000             nan     0.2603   -0.0000
   600        0.0000             nan     0.2603   -0.0000
   620        0.0000             nan     0.2603   -0.0000
   640        0.0000             nan     0.2603   -0.0000
   660        0.0000             nan     0.2603   -0.0000
   680        0.0000             nan     0.2603   -0.0000
   700        0.0000             nan     0.2603   -0.0000
   720        0.0000             nan     0.2603   -0.0000
   740        0.0000             nan     0.2603   -0.0000
   760        0.0000             nan     0.2603   -0.0000
   780        0.0000             nan     0.2603   -0.0000
   800        0.0000             nan     0.2603   -0.0000
   820        0.0000             nan     0.2603   -0.0000
   840        0.0000             nan     0.2603   -0.0000
   860        0.0000             nan     0.2603   -0.0000
   880        0.0000             nan     0.2603   -0.0000
   900        0.0000             nan     0.2603   -0.0000
   920        0.0000             nan     0.2603   -0.0000
   940        0.0000             nan     0.2603   -0.0000
   960        0.0000             nan     0.2603   -0.0000
   980        0.0000             nan     0.2603   -0.0000
  1000        0.0000             nan     0.2603   -0.0000
  1020        0.0000             nan     0.2603   -0.0000
  1040        0.0000             nan     0.2603   -0.0000
  1060        0.0000             nan     0.2603   -0.0000
  1080        0.0000             nan     0.2603   -0.0000
  1100        0.0000             nan     0.2603   -0.0000
  1120        0.0000             nan     0.2603   -0.0000
  1140        0.0000             nan     0.2603   -0.0000
  1160        0.0000             nan     0.2603   -0.0000
  1180        0.0000             nan     0.2603   -0.0000
  1200        0.0000             nan     0.2603   -0.0000
  1220        0.0000             nan     0.2603   -0.0000
  1240        0.0000             nan     0.2603   -0.0000
  1260        0.0000             nan     0.2603   -0.0000
  1280        0.0000             nan     0.2603   -0.0000
  1300        0.0000             nan     0.2603   -0.0000
  1320        0.0000             nan     0.2603   -0.0000
  1340        0.0000             nan     0.2603   -0.0000
  1360        0.0000             nan     0.2603    0.0000
  1380        0.0000             nan     0.2603   -0.0000
  1400        0.0000             nan     0.2603   -0.0000
  1420        0.0000             nan     0.2603   -0.0000
  1440        0.0000             nan     0.2603   -0.0000
  1460        0.0000             nan     0.2603   -0.0000
  1480        0.0000             nan     0.2603   -0.0000
  1500        0.0000             nan     0.2603   -0.0000
  1520        0.0000             nan     0.2603   -0.0000
  1540        0.0000             nan     0.2603   -0.0000
  1560        0.0000             nan     0.2603   -0.0000
  1580        0.0000             nan     0.2603   -0.0000
  1600        0.0000             nan     0.2603   -0.0000
  1620        0.0000             nan     0.2603   -0.0000
  1640        0.0000             nan     0.2603   -0.0000
  1660        0.0000             nan     0.2603    0.0000
  1680        0.0000             nan     0.2603   -0.0000
  1700        0.0000             nan     0.2603   -0.0000
  1720        0.0000             nan     0.2603   -0.0000
  1740        0.0000             nan     0.2603   -0.0000
  1760        0.0000             nan     0.2603   -0.0000
  1780        0.0000             nan     0.2603   -0.0000
  1800        0.0000             nan     0.2603   -0.0000
  1820        0.0000             nan     0.2603   -0.0000
  1840        0.0000             nan     0.2603   -0.0000
  1860        0.0000             nan     0.2603   -0.0000
  1880        0.0000             nan     0.2603   -0.0000
  1900        0.0000             nan     0.2603   -0.0000
  1920        0.0000             nan     0.2603   -0.0000
  1940        0.0000             nan     0.2603   -0.0000
  1960        0.0000             nan     0.2603   -0.0000
  1980        0.0000             nan     0.2603   -0.0000
  2000        0.0000             nan     0.2603   -0.0000
  2020        0.0000             nan     0.2603   -0.0000
  2040        0.0000             nan     0.2603   -0.0000
  2060        0.0000             nan     0.2603   -0.0000
  2080        0.0000             nan     0.2603   -0.0000
  2100        0.0000             nan     0.2603   -0.0000
  2120        0.0000             nan     0.2603   -0.0000
  2140        0.0000             nan     0.2603   -0.0000
  2160        0.0000             nan     0.2603   -0.0000
  2180        0.0000             nan     0.2603   -0.0000
  2200        0.0000             nan     0.2603   -0.0000
  2220        0.0000             nan     0.2603   -0.0000
  2240        0.0000             nan     0.2603   -0.0000
  2260        0.0000             nan     0.2603   -0.0000
  2280        0.0000             nan     0.2603   -0.0000
  2300        0.0000             nan     0.2603   -0.0000
  2320        0.0000             nan     0.2603   -0.0000
  2340        0.0000             nan     0.2603   -0.0000
  2360        0.0000             nan     0.2603   -0.0000
  2380        0.0000             nan     0.2603   -0.0000
  2400        0.0000             nan     0.2603   -0.0000
  2420        0.0000             nan     0.2603   -0.0000
  2440        0.0000             nan     0.2603   -0.0000
  2460        0.0000             nan     0.2603   -0.0000
  2480        0.0000             nan     0.2603   -0.0000
  2500        0.0000             nan     0.2603   -0.0000
  2520        0.0000             nan     0.2603   -0.0000
  2540        0.0000             nan     0.2603   -0.0000
  2560        0.0000             nan     0.2603   -0.0000
  2580        0.0000             nan     0.2603   -0.0000
  2600        0.0000             nan     0.2603   -0.0000
  2620        0.0000             nan     0.2603   -0.0000
  2640        0.0000             nan     0.2603   -0.0000
  2660        0.0000             nan     0.2603   -0.0000
  2680        0.0000             nan     0.2603   -0.0000
  2700        0.0000             nan     0.2603   -0.0000
  2720        0.0000             nan     0.2603   -0.0000
  2740        0.0000             nan     0.2603   -0.0000
  2760        0.0000             nan     0.2603   -0.0000
  2780        0.0000             nan     0.2603   -0.0000
  2800        0.0000             nan     0.2603   -0.0000
  2820        0.0000             nan     0.2603   -0.0000
  2840        0.0000             nan     0.2603   -0.0000
  2860        0.0000             nan     0.2603   -0.0000
  2880        0.0000             nan     0.2603   -0.0000
  2900        0.0000             nan     0.2603   -0.0000
  2920        0.0000             nan     0.2603   -0.0000
  2940        0.0000             nan     0.2603   -0.0000
  2960        0.0000             nan     0.2603   -0.0000
  2980        0.0000             nan     0.2603   -0.0000
  3000        0.0000             nan     0.2603   -0.0000
  3020        0.0000             nan     0.2603   -0.0000
  3040        0.0000             nan     0.2603   -0.0000
  3060        0.0000             nan     0.2603   -0.0000
  3080        0.0000             nan     0.2603   -0.0000
  3100        0.0000             nan     0.2603   -0.0000
  3120        0.0000             nan     0.2603   -0.0000
  3140        0.0000             nan     0.2603   -0.0000
  3160        0.0000             nan     0.2603   -0.0000
  3180        0.0000             nan     0.2603   -0.0000
  3200        0.0000             nan     0.2603   -0.0000
  3220        0.0000             nan     0.2603   -0.0000
  3240        0.0000             nan     0.2603   -0.0000
  3260        0.0000             nan     0.2603   -0.0000
  3280        0.0000             nan     0.2603   -0.0000
  3300        0.0000             nan     0.2603   -0.0000
  3320        0.0000             nan     0.2603   -0.0000
  3340        0.0000             nan     0.2603   -0.0000
  3360        0.0000             nan     0.2603   -0.0000
  3380        0.0000             nan     0.2603   -0.0000
  3400        0.0000             nan     0.2603   -0.0000
  3420        0.0000             nan     0.2603   -0.0000
  3440        0.0000             nan     0.2603   -0.0000
  3460        0.0000             nan     0.2603   -0.0000
  3480        0.0000             nan     0.2603   -0.0000
  3500        0.0000             nan     0.2603   -0.0000
  3520        0.0000             nan     0.2603   -0.0000
  3540        0.0000             nan     0.2603   -0.0000
  3560        0.0000             nan     0.2603   -0.0000
  3580        0.0000             nan     0.2603   -0.0000
  3600        0.0000             nan     0.2603   -0.0000
  3620        0.0000             nan     0.2603   -0.0000
  3640        0.0000             nan     0.2603   -0.0000
  3660        0.0000             nan     0.2603    0.0000
  3680        0.0000             nan     0.2603   -0.0000
  3700        0.0000             nan     0.2603   -0.0000
  3720        0.0000             nan     0.2603   -0.0000
  3740        0.0000             nan     0.2603   -0.0000
  3760        0.0000             nan     0.2603   -0.0000
  3780        0.0000             nan     0.2603   -0.0000
  3800        0.0000             nan     0.2603   -0.0000
  3820        0.0000             nan     0.2603   -0.0000
  3840        0.0000             nan     0.2603   -0.0000
  3860        0.0000             nan     0.2603   -0.0000
  3880        0.0000             nan     0.2603   -0.0000
  3900        0.0000             nan     0.2603   -0.0000
  3920        0.0000             nan     0.2603   -0.0000
  3940        0.0000             nan     0.2603   -0.0000
  3960        0.0000             nan     0.2603   -0.0000
  3980        0.0000             nan     0.2603   -0.0000
  4000        0.0000             nan     0.2603   -0.0000
  4020        0.0000             nan     0.2603   -0.0000
  4040        0.0000             nan     0.2603   -0.0000
  4060        0.0000             nan     0.2603   -0.0000
  4080        0.0000             nan     0.2603   -0.0000
  4100        0.0000             nan     0.2603   -0.0000
  4120        0.0000             nan     0.2603   -0.0000
  4140        0.0000             nan     0.2603   -0.0000
  4160        0.0000             nan     0.2603   -0.0000
  4180        0.0000             nan     0.2603   -0.0000
  4200        0.0000             nan     0.2603   -0.0000
  4213        0.0000             nan     0.2603   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       31.5797             nan     0.3353    8.5964
     2       27.1834             nan     0.3353    0.5048
     3       20.0580             nan     0.3353   -1.6287
     4       14.4662             nan     0.3353    3.9249
     5       11.4879             nan     0.3353    1.4950
     6        9.5351             nan     0.3353    0.9556
     7        9.1378             nan     0.3353   -0.6591
     8        7.8309             nan     0.3353    0.3105
     9        7.0418             nan     0.3353   -0.4856
    10        6.5799             nan     0.3353   -0.3191
    20        2.8142             nan     0.3353   -0.1677
    40        1.1283             nan     0.3353   -0.3583
    60        0.5760             nan     0.3353   -0.0743
    80        0.2698             nan     0.3353   -0.0221
   100        0.1513             nan     0.3353   -0.0253
   120        0.0692             nan     0.3353   -0.0079
   140        0.0474             nan     0.3353   -0.0049
   160        0.0202             nan     0.3353   -0.0021
   180        0.0144             nan     0.3353   -0.0028
   200        0.0088             nan     0.3353   -0.0008
   220        0.0046             nan     0.3353   -0.0007
   240        0.0026             nan     0.3353   -0.0003
   260        0.0014             nan     0.3353   -0.0002
   280        0.0010             nan     0.3353   -0.0002
   300        0.0006             nan     0.3353   -0.0000
   320        0.0004             nan     0.3353   -0.0000
   340        0.0002             nan     0.3353   -0.0000
   360        0.0001             nan     0.3353   -0.0000
   380        0.0001             nan     0.3353   -0.0000
   400        0.0000             nan     0.3353   -0.0000
   420        0.0000             nan     0.3353   -0.0000
   440        0.0000             nan     0.3353   -0.0000
   460        0.0000             nan     0.3353   -0.0000
   480        0.0000             nan     0.3353   -0.0000
   500        0.0000             nan     0.3353   -0.0000
   520        0.0000             nan     0.3353   -0.0000
   540        0.0000             nan     0.3353   -0.0000
   560        0.0000             nan     0.3353   -0.0000
   580        0.0000             nan     0.3353   -0.0000
   600        0.0000             nan     0.3353   -0.0000
   620        0.0000             nan     0.3353   -0.0000
   640        0.0000             nan     0.3353   -0.0000
   660        0.0000             nan     0.3353   -0.0000
   680        0.0000             nan     0.3353   -0.0000
   700        0.0000             nan     0.3353   -0.0000
   720        0.0000             nan     0.3353   -0.0000
   740        0.0000             nan     0.3353   -0.0000
   760        0.0000             nan     0.3353   -0.0000
   780        0.0000             nan     0.3353   -0.0000
   800        0.0000             nan     0.3353   -0.0000
   820        0.0000             nan     0.3353   -0.0000
   840        0.0000             nan     0.3353   -0.0000
   860        0.0000             nan     0.3353   -0.0000
   880        0.0000             nan     0.3353   -0.0000
   900        0.0000             nan     0.3353   -0.0000
   920        0.0000             nan     0.3353   -0.0000
   940        0.0000             nan     0.3353   -0.0000
   960        0.0000             nan     0.3353   -0.0000
   980        0.0000             nan     0.3353   -0.0000
  1000        0.0000             nan     0.3353   -0.0000
  1020        0.0000             nan     0.3353   -0.0000
  1040        0.0000             nan     0.3353   -0.0000
  1060        0.0000             nan     0.3353   -0.0000
  1080        0.0000             nan     0.3353   -0.0000
  1100        0.0000             nan     0.3353   -0.0000
  1120        0.0000             nan     0.3353   -0.0000
  1140        0.0000             nan     0.3353   -0.0000
  1160        0.0000             nan     0.3353   -0.0000
  1180        0.0000             nan     0.3353   -0.0000
  1200        0.0000             nan     0.3353   -0.0000
  1220        0.0000             nan     0.3353   -0.0000
  1240        0.0000             nan     0.3353   -0.0000
  1260        0.0000             nan     0.3353   -0.0000
  1280        0.0000             nan     0.3353   -0.0000
  1300        0.0000             nan     0.3353   -0.0000
  1320        0.0000             nan     0.3353    0.0000
  1340        0.0000             nan     0.3353   -0.0000
  1360        0.0000             nan     0.3353   -0.0000
  1380        0.0000             nan     0.3353   -0.0000
  1400        0.0000             nan     0.3353   -0.0000
  1420        0.0000             nan     0.3353   -0.0000
  1440        0.0000             nan     0.3353   -0.0000
  1460        0.0000             nan     0.3353   -0.0000
  1480        0.0000             nan     0.3353   -0.0000
  1500        0.0000             nan     0.3353   -0.0000
  1520        0.0000             nan     0.3353   -0.0000
  1540        0.0000             nan     0.3353   -0.0000
  1560        0.0000             nan     0.3353   -0.0000
  1580        0.0000             nan     0.3353   -0.0000
  1600        0.0000             nan     0.3353   -0.0000
  1620        0.0000             nan     0.3353   -0.0000
  1640        0.0000             nan     0.3353   -0.0000
  1660        0.0000             nan     0.3353   -0.0000
  1680        0.0000             nan     0.3353   -0.0000
  1700        0.0000             nan     0.3353   -0.0000
  1720        0.0000             nan     0.3353   -0.0000
  1740        0.0000             nan     0.3353   -0.0000
  1760        0.0000             nan     0.3353   -0.0000
  1780        0.0000             nan     0.3353   -0.0000
  1800        0.0000             nan     0.3353   -0.0000
  1820        0.0000             nan     0.3353   -0.0000
  1840        0.0000             nan     0.3353   -0.0000
  1860        0.0000             nan     0.3353   -0.0000
  1880        0.0000             nan     0.3353   -0.0000
  1900        0.0000             nan     0.3353   -0.0000
  1920        0.0000             nan     0.3353   -0.0000
  1940        0.0000             nan     0.3353   -0.0000
  1960        0.0000             nan     0.3353   -0.0000
  1980        0.0000             nan     0.3353   -0.0000
  2000        0.0000             nan     0.3353   -0.0000
  2020        0.0000             nan     0.3353   -0.0000
  2040        0.0000             nan     0.3353   -0.0000
  2060        0.0000             nan     0.3353   -0.0000
  2080        0.0000             nan     0.3353   -0.0000
  2100        0.0000             nan     0.3353   -0.0000
  2120        0.0000             nan     0.3353   -0.0000
  2140        0.0000             nan     0.3353   -0.0000
  2160        0.0000             nan     0.3353   -0.0000
  2180        0.0000             nan     0.3353    0.0000
  2200        0.0000             nan     0.3353   -0.0000
  2220        0.0000             nan     0.3353   -0.0000
  2240        0.0000             nan     0.3353   -0.0000
  2260        0.0000             nan     0.3353   -0.0000
  2280        0.0000             nan     0.3353   -0.0000
  2300        0.0000             nan     0.3353   -0.0000
  2320        0.0000             nan     0.3353   -0.0000
  2340        0.0000             nan     0.3353   -0.0000
  2360        0.0000             nan     0.3353   -0.0000
  2380        0.0000             nan     0.3353   -0.0000
  2400        0.0000             nan     0.3353   -0.0000
  2420        0.0000             nan     0.3353   -0.0000
  2440        0.0000             nan     0.3353   -0.0000
  2460        0.0000             nan     0.3353   -0.0000
  2480        0.0000             nan     0.3353   -0.0000
  2500        0.0000             nan     0.3353   -0.0000
  2520        0.0000             nan     0.3353   -0.0000
  2540        0.0000             nan     0.3353   -0.0000
  2560        0.0000             nan     0.3353   -0.0000
  2580        0.0000             nan     0.3353   -0.0000
  2600        0.0000             nan     0.3353   -0.0000
  2620        0.0000             nan     0.3353   -0.0000
  2640        0.0000             nan     0.3353   -0.0000
  2660        0.0000             nan     0.3353   -0.0000
  2680        0.0000             nan     0.3353   -0.0000
  2700        0.0000             nan     0.3353   -0.0000
  2720        0.0000             nan     0.3353   -0.0000
  2740        0.0000             nan     0.3353   -0.0000
  2760        0.0000             nan     0.3353   -0.0000
  2780        0.0000             nan     0.3353   -0.0000
  2800        0.0000             nan     0.3353   -0.0000
  2820        0.0000             nan     0.3353    0.0000
  2840        0.0000             nan     0.3353   -0.0000
  2860        0.0000             nan     0.3353   -0.0000
  2880        0.0000             nan     0.3353   -0.0000
  2900        0.0000             nan     0.3353   -0.0000
  2920        0.0000             nan     0.3353   -0.0000
  2940        0.0000             nan     0.3353   -0.0000
  2960        0.0000             nan     0.3353   -0.0000
  2980        0.0000             nan     0.3353   -0.0000
  3000        0.0000             nan     0.3353   -0.0000
  3020        0.0000             nan     0.3353   -0.0000
  3040        0.0000             nan     0.3353   -0.0000
  3060        0.0000             nan     0.3353    0.0000
  3080        0.0000             nan     0.3353   -0.0000
  3100        0.0000             nan     0.3353   -0.0000
  3120        0.0000             nan     0.3353   -0.0000
  3140        0.0000             nan     0.3353   -0.0000
  3160        0.0000             nan     0.3353   -0.0000
  3180        0.0000             nan     0.3353   -0.0000
  3200        0.0000             nan     0.3353   -0.0000
  3220        0.0000             nan     0.3353   -0.0000
  3240        0.0000             nan     0.3353   -0.0000
  3260        0.0000             nan     0.3353   -0.0000
  3280        0.0000             nan     0.3353   -0.0000
  3300        0.0000             nan     0.3353   -0.0000
  3320        0.0000             nan     0.3353   -0.0000
  3340        0.0000             nan     0.3353   -0.0000
  3360        0.0000             nan     0.3353   -0.0000
  3380        0.0000             nan     0.3353   -0.0000
  3400        0.0000             nan     0.3353   -0.0000
  3420        0.0000             nan     0.3353   -0.0000
  3440        0.0000             nan     0.3353   -0.0000
  3460        0.0000             nan     0.3353   -0.0000
  3480        0.0000             nan     0.3353   -0.0000
  3500        0.0000             nan     0.3353   -0.0000
  3520        0.0000             nan     0.3353   -0.0000
  3540        0.0000             nan     0.3353   -0.0000
  3560        0.0000             nan     0.3353   -0.0000
  3580        0.0000             nan     0.3353   -0.0000
  3585        0.0000             nan     0.3353   -0.0000

Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1       34.5368             nan     0.3353    6.0714
     2       22.4009             nan     0.3353   12.0297
     3       17.1055             nan     0.3353    2.4583
     4       14.0315             nan     0.3353   -0.5432
     5       11.3128             nan     0.3353    1.7306
     6       10.5455             nan     0.3353   -0.9447
     7        8.3653             nan     0.3353    1.6993
     8        7.8013             nan     0.3353   -1.3501
     9        7.2422             nan     0.3353   -0.1868
    10        6.7054             nan     0.3353    0.1161
    20        3.0145             nan     0.3353   -0.2072
    40        0.9279             nan     0.3353   -0.0776
    60        0.4355             nan     0.3353   -0.0830
    80        0.1616             nan     0.3353   -0.0180
   100        0.0741             nan     0.3353   -0.0138
   120        0.0302             nan     0.3353   -0.0033
   140        0.0159             nan     0.3353   -0.0019
   160        0.0094             nan     0.3353   -0.0012
   180        0.0050             nan     0.3353   -0.0004
   200        0.0031             nan     0.3353   -0.0001
   220        0.0018             nan     0.3353   -0.0002
   240        0.0011             nan     0.3353   -0.0000
   260        0.0005             nan     0.3353   -0.0000
   280        0.0002             nan     0.3353   -0.0000
   300        0.0002             nan     0.3353   -0.0000
   320        0.0001             nan     0.3353   -0.0000
   340        0.0000             nan     0.3353   -0.0000
   360        0.0000             nan     0.3353   -0.0000
   380        0.0000             nan     0.3353   -0.0000
   400        0.0000             nan     0.3353   -0.0000
   420        0.0000             nan     0.3353   -0.0000
   440        0.0000             nan     0.3353   -0.0000
   460        0.0000             nan     0.3353   -0.0000
   480        0.0000             nan     0.3353   -0.0000
   500        0.0000             nan     0.3353   -0.0000
   520        0.0000             nan     0.3353   -0.0000
   540        0.0000             nan     0.3353   -0.0000
   560        0.0000             nan     0.3353   -0.0000
   580        0.0000             nan     0.3353   -0.0000
   600        0.0000             nan     0.3353   -0.0000
   620        0.0000             nan     0.3353   -0.0000
   640        0.0000             nan     0.3353   -0.0000
   660        0.0000             nan     0.3353   -0.0000
   680        0.0000             nan     0.3353   -0.0000
   700        0.0000             nan     0.3353   -0.0000
   720        0.0000             nan     0.3353   -0.0000
   740        0.0000             nan     0.3353   -0.0000
   760        0.0000             nan     0.3353   -0.0000
   780        0.0000             nan     0.3353   -0.0000
   800        0.0000             nan     0.3353   -0.0000
   820        0.0000             nan     0.3353   -0.0000
   840        0.0000             nan     0.3353   -0.0000
   860        0.0000             nan     0.3353   -0.0000
   880        0.0000             nan     0.3353   -0.0000
   900        0.0000             nan     0.3353   -0.0000
   920        0.0000             nan     0.3353   -0.0000
   940        0.0000             nan     0.3353   -0.0000
   960        0.0000             nan     0.3353   -0.0000
   980        0.0000             nan     0.3353   -0.0000
  1000        0.0000             nan     0.3353   -0.0000
  1020        0.0000             nan     0.3353   -0.0000
  1040        0.0000             nan     0.3353   -0.0000
  1060        0.0000             nan     0.3353   -0.0000
  1080        0.0000             nan     0.3353   -0.0000
  1100        0.0000             nan     0.3353   -0.0000
  1120        0.0000             nan     0.3353   -0.0000
  1140        0.0000             nan     0.3353   -0.0000
  1160        0.0000             nan     0.3353   -0.0000
  1180        0.0000             nan     0.3353   -0.0000
  1200        0.0000             nan     0.3353   -0.0000
  1220        0.0000             nan     0.3353   -0.0000
  1240        0.0000             nan     0.3353   -0.0000
  1260        0.0000             nan     0.3353    0.0000
  1280        0.0000             nan     0.3353    0.0000
  1300        0.0000             nan     0.3353   -0.0000
  1320        0.0000             nan     0.3353   -0.0000
  1340        0.0000             nan     0.3353   -0.0000
  1360        0.0000             nan     0.3353   -0.0000
  1380        0.0000             nan     0.3353   -0.0000
  1400        0.0000             nan     0.3353   -0.0000
  1420        0.0000             nan     0.3353   -0.0000
  1440        0.0000             nan     0.3353   -0.0000
  1460        0.0000             nan     0.3353    0.0000
  1480        0.0000             nan     0.3353   -0.0000
  1500        0.0000             nan     0.3353   -0.0000
  1520        0.0000             nan     0.3353   -0.0000
  1540        0.0000             nan     0.3353   -0.0000
  1560        0.0000             nan     0.3353   -0.0000
  1580        0.0000             nan     0.3353   -0.0000
  1600        0.0000             nan     0.3353   -0.0000
  1620        0.0000             nan     0.3353   -0.0000
  1640        0.0000             nan     0.3353   -0.0000
  1660        0.0000             nan     0.3353   -0.0000
  1680        0.0000             nan     0.3353   -0.0000
  1700        0.0000             nan     0.3353   -0.0000
  1720        0.0000             nan     0.3353   -0.0000
  1740        0.0000             nan     0.3353   -0.0000
  1760        0.0000             nan     0.3353    0.0000
  1780        0.0000             nan     0.3353   -0.0000
  1800        0.0000             nan     0.3353   -0.0000
  1820        0.0000             nan     0.3353   -0.0000
  1840        0.0000             nan     0.3353   -0.0000
  1860        0.0000             nan     0.3353   -0.0000
  1880        0.0000             nan     0.3353   -0.0000
  1900        0.0000             nan     0.3353   -0.0000
  1920        0.0000             nan     0.3353   -0.0000
  1940        0.0000             nan     0.3353   -0.0000
  1960        0.0000             nan     0.3353   -0.0000
  1980        0.0000             nan     0.3353   -0.0000
  2000        0.0000             nan     0.3353   -0.0000
  2020        0.0000             nan     0.3353   -0.0000
  2040        0.0000             nan     0.3353   -0.0000
  2060        0.0000             nan     0.3353   -0.0000
  2080        0.0000             nan     0.3353   -0.0000
  2100        0.0000             nan     0.3353   -0.0000
  2120        0.0000             nan     0.3353   -0.0000
  2140        0.0000             nan     0.3353    0.0000
  2160        0.0000             nan     0.3353   -0.0000
  2180        0.0000             nan     0.3353   -0.0000
  2200        0.0000             nan     0.3353   -0.0000
  2220        0.0000             nan     0.3353   -0.0000
  2240        0.0000             nan     0.3353    0.0000
  2260        0.0000             nan     0.3353   -0.0000
  2280        0.0000             nan     0.3353   -0.0000
  2300        0.0000             nan     0.3353   -0.0000
  2320        0.0000             nan     0.3353   -0.0000
  2340        0.0000             nan     0.3353   -0.0000
  2360        0.0000             nan     0.3353   -0.0000
  2380        0.0000             nan     0.3353   -0.0000
  2400        0.0000             nan     0.3353   -0.0000
  2420        0.0000             nan     0.3353   -0.0000
  2440        0.0000             nan     0.3353   -0.0000
  2460        0.0000             nan     0.3353    0.0000
  2480        0.0000             nan     0.3353   -0.0000
  2500        0.0000             nan     0.3353   -0.0000
  2520        0.0000             nan     0.3353   -0.0000
  2540        0.0000             nan     0.3353   -0.0000
  2560        0.0000             nan     0.3353   -0.0000
  2580        0.0000             nan     0.3353   -0.0000
  2600        0.0000             nan     0.3353   -0.0000
  2620        0.0000             nan     0.3353   -0.0000
  2640        0.0000             nan     0.3353   -0.0000
  2660        0.0000             nan     0.3353   -0.0000
  2680        0.0000             nan     0.3353    0.0000
  2700        0.0000             nan     0.3353   -0.0000
  2720        0.0000             nan     0.3353   -0.0000
  2740        0.0000             nan     0.3353   -0.0000
  2760        0.0000             nan     0.3353   -0.0000
  2780        0.0000             nan     0.3353   -0.0000
  2800        0.0000             nan     0.3353   -0.0000
  2820        0.0000             nan     0.3353   -0.0000
  2840        0.0000             nan     0.3353   -0.0000
  2860        0.0000             nan     0.3353   -0.0000
  2880        0.0000             nan     0.3353   -0.0000
  2900        0.0000             nan     0.3353   -0.0000
  2920        0.0000             nan     0.3353   -0.0000
  2940        0.0000             nan     0.3353   -0.0000
  2960        0.0000             nan     0.3353   -0.0000
  2980        0.0000             nan     0.3353   -0.0000
  3000        0.0000             nan     0.3353   -0.0000
  3020        0.0000             nan     0.3353   -0.0000
  3040        0.0000             nan     0.3353   -0.0000
  3060        0.0000             nan     0.3353   -0.0000
  3080        0.0000             nan     0.3353   -0.0000
  3100        0.0000             nan     0.3353   -0.0000
  3120        0.0000             nan     0.3353   -0.0000
  3140        0.0000             nan     0.3353   -0.0000
  3160        0.0000             nan     0.3353   -0.0000
  3180        0.0000             nan     0.3353   -0.0000
  3200        0.0000             nan     0.3353   -0.0000
  3220        0.0000             nan     0.3353   -0.0000
  3240        0.0000             nan     0.3353   -0.0000
  3260        0.0000             nan     0.3353   -0.0000
  3280        0.0000             nan     0.3353   -0.0000
  3300        0.0000             nan     0.3353   -0.0000
  3320        0.0000             nan     0.3353   -0.0000
  3340        0.0000             nan     0.3353   -0.0000
  3360        0.0000             nan     0.3353   -0.0000
  3380        0.0000             nan     0.3353   -0.0000
  3400        0.0000             nan     0.3353   -0.0000
  3420        0.0000             nan     0.3353   -0.0000
  3440        0.0000             nan     0.3353   -0.0000
  3460        0.0000             nan     0.3353   -0.0000
  3480        0.0000             nan     0.3353   -0.0000
  3500        0.0000             nan     0.3353   -0.0000
  3520        0.0000             nan     0.3353   -0.0000
  3540        0.0000             nan     0.3353   -0.0000
  3560        0.0000             nan     0.3353   -0.0000
  3580        0.0000             nan     0.3353   -0.0000
  3585        0.0000             nan     0.3353   -0.0000

Stochastic Gradient Boosting 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  shrinkage   interaction.depth  n.minobsinnode  n.trees  RMSE      Rsquared 
  0.01825578   5                 14              2501          NaN        NaN
  0.06262002   9                 13              3895          NaN        NaN
  0.07859767   2                 10              1753     4.048944  0.7064180
  0.08285497   1                  7              1765     3.562547  0.7585508
  0.11761534   7                  5              2186     3.530974  0.7727091
  0.13520014   9                  6               703     3.616576  0.7532775
  0.17300592   5                 15               426          NaN        NaN
  0.20453318   5                 21              3999          NaN        NaN
  0.24126209   7                  7               846     4.057740  0.6963624
  0.26030753   7                  7              4213     3.685559  0.7386005
  0.28848962   9                 12              4837     4.767981  0.6048520
  0.28876497   2                 13              1264          NaN        NaN
  0.33082635  10                 24              1825          NaN        NaN
  0.33527563   8                  7              3585     3.464818  0.7689094
  0.41357597   3                 19              1024          NaN        NaN
  0.42402126   5                 16              1500          NaN        NaN
  0.47378616   5                 18              4129          NaN        NaN
  0.53909033   9                 14              1784          NaN        NaN
  0.56004139   3                 12              2510     4.822646  0.5558168
  0.57580713   1                 14              4135          NaN        NaN
  MAE       Selected
       NaN          
       NaN          
  3.326842          
  2.783761          
  2.800035          
  2.839293          
       NaN          
       NaN          
  3.387589          
  2.879762          
  3.813527          
       NaN          
       NaN          
  2.811011  *       
       NaN          
       NaN          
       NaN          
       NaN          
  4.124855          
       NaN          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were n.trees = 3585, interaction.depth =
 8, shrinkage = 0.3352756 and n.minobsinnode = 7.
[1] "Mon Mar 12 16:20:16 2018"
Error in relative.influence(object, n.trees = numTrees) : 
  could not find function "relative.influence"
In addition: There were 39 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :20    NA's   :20    NA's   :20   
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :8     NA's   :8     NA's   :8    
Error : Stopping
In addition: There were 25 warnings (use warnings() to see them)
Something is wrong; all the RMSE metric values are missing:
      RMSE        Rsquared        MAE     
 Min.   : NA   Min.   : NA   Min.   : NA  
 1st Qu.: NA   1st Qu.: NA   1st Qu.: NA  
 Median : NA   Median : NA   Median : NA  
 Mean   :NaN   Mean   :NaN   Mean   :NaN  
 3rd Qu.: NA   3rd Qu.: NA   3rd Qu.: NA  
 Max.   : NA   Max.   : NA   Max.   : NA  
 NA's   :9     NA's   :9     NA's   :9    
Error : Stopping
In addition: There were 50 or more warnings (use warnings() to see the first 50)
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:23:23 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "gbm_h2o"                 
Multivariate Adaptive Regression Splines 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results:

  RMSE      Rsquared   MAE     
  2.754464  0.8579764  2.196722

Tuning parameter 'degree' was held constant at a value of 1
[1] "Mon Mar 12 16:23:35 2018"
Generalized Linear Model 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results:

  RMSE      Rsquared   MAE     
  3.570263  0.7512673  2.743412

[1] "Mon Mar 12 16:23:47 2018"
Negative Binomial Generalized Linear Model 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  link      RMSE      Rsquared   MAE       Selected
  identity  3.522208  0.7517320  2.717397  *       
  log       4.179235  0.7011479  3.107484          
  sqrt      3.680252  0.7422895  2.736029          

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was link = identity.
[1] "Mon Mar 12 16:23:59 2018"
Error : package mboost is required
In addition: There were 50 or more warnings (use warnings() to see the first 50)
Error : package mboost is required
Error : package mboost is required
 [1] "failed"                   "failed"                  
 [3] "Mon Mar 12 16:24:12 2018" "Friedman's 1st"          
 [5] "ignore"                   "none"                    
 [7] "expoTrans"                "HOPPER"                  
 [9] "14th20hp3cv"              "glmboost"                
glmnet 

76 samples
10 predictors

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 50, 52, 50 
Resampling results across tuning parameters:

  alpha       lambda       RMSE      Rsquared   MAE       Selected
  0.08509428  0.073922973  3.557555  0.7523871  2.739065          
  0.14056540  2.157147010  3.639384  0.7806998  2.878805          
  0.16908356  0.439483707  3.476267  0.7640011  2.686164          
  0.20472461  0.006318147  3.558096  0.7525457  2.736826          
  0.25284510  0.004091859  3.557324  0.7526708  2.735956          
  0.30001649  0.073646506  3.534739  0.7553436  2.720989          
  0.35048447  0.002764482  3.555818  0.7529159  2.734206          
  0.35304203  0.002304384  3.555827  0.7529161  2.734198          
  0.35682166  1.407486127  3.508804  0.7888629  2.744972          
  0.36504880  5.485239613  5.363955  0.7266880  4.565486          
  0.43719640  0.239101864  3.432096  0.7696042  2.637912          
  0.50026452  0.041169686  3.538098  0.7550818  2.721460          
  0.50198995  0.012496276  3.554321  0.7531255  2.732856          
  0.71699918  0.620530396  3.349744  0.7899456  2.561975          
  0.77909068  1.371767938  3.795172  0.7813351  3.017036          
  0.79994739  0.050733998  3.508678  0.7588838  2.697807          
  0.82582888  0.052179143  3.505106  0.7593539  2.694838          
  0.82702510  0.001058726  3.552154  0.7534470  2.730611          
  0.84267825  0.296487512  3.319921  0.7874333  2.545097  *       
  0.96740112  2.807292313  5.803574  0.5879299  4.938335          

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were alpha = 0.8426782 and lambda
 = 0.2964875.
[1] "Mon Mar 12 16:24:26 2018"

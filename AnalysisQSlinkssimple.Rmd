---
title: "Results analysis"
---
load packages read files comboine files
[1] "pq9TargRank"    "gainin30"       "spearman2"      "pearson2"      
  [5] "pMAE"           "pRMSE"          "ocvRMSE"        "RMSEutrans"    
  [9] "MAEutrans"      "date"           "algomodel"      "trgCol"        
 [13] "transTarg"      "task"           "missing"        "append"        
 [17] "transform"      "pc"             "expirament"     "fold"          
 [21] "maxfold"        "seed"           "seedit"         "foldseed"      
 [25] "RMSEmean"       "RMSEmeantrain"  "hpGen"          "time"          
 [29] "validmethod"    "tuneLength"     "cvcount"        "ignrepeats"    
 [33] "adaptivemin"    "bestTuneparams" "btp1"           "btp2"          
 [37] "btp3"           "btp4"           "btp5"           "btp6"          
 [41] "btp7"           "btp8"           "btp9"           "btp10"         
 [45] "btp11"
```{r}
col_typed <- "dddddddddccddccccccdddddddcdcddc??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"

try({setwd(mainDir)})
mainDir<-getwd()
subDir<-"QSlinks_simple"
#dir.create(file.path(mainDir, subDir))
  require(readr)
require(netCoin)
require(data.table)
require(naniar) 
require(VIM)
require(ggplot2)

resultDir<-"ACEREBOUT"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3ACE <- read_csv("outQSlink3ACEREBOUTwindowsx64.csv",col_types = col_typed)
#summary(QSlink3ACE[,1:30])
 spec(QSlink3ACE)
 problems(QSlink3ACE)
 setwd(file.path(mainDir))


 
 resultDir<-"HOPPER"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3HOP <- read_csv("outQSlink3HOPPERwindowsx64.csv",col_types = col_typed)
#summary(QSlink3ACE[,1:30])
 spec(QSlink3HOP)
 problems(QSlink3HOP)
 setwd(file.path(mainDir))
 
 names(QSlink3HOP)
 
  resultDir<-"LAPTOPBBQ"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3BBQ <- read_csv("outQSlink3LAPTOPBBQwindowsx64.csv",col_types = col_typed)
#summary(QSlink3ACE[,1:30])
 spec(QSlink3BBQ)
 problems(QSlink3BBQ)
 setwd(file.path(mainDir))

  
require(data.table)
DF<-as.data.table(QSlink3ACE)
DF1<-as.data.table(QSlink3HOP)
DF2<-as.data.table(QSlink3BBQ)

DF<-rbindlist(list(DF,DF1,DF2),fill = T)
rm(DF1,DF2,QSlink3BBQ,QSlink3HOP)
```
remove useless columns, convert time, check for unused columns, seed changes?
```{r}

DF$date<-as.numeric(as.POSIXct(DF$date,format = "%a %b %d %H:%M:%S %Y"))
#str(DF)
summary(DF$pq9TargRank)

#what is seedit? and seed bcause foldseed is always the same
summary(DF[,.(seed,seedit,foldseed)])#need foldseed now
unique(DF$seed);unique(DF$seedit);unique(DF$foldseed)
unique(DF$trgCol);unique(DF$missing);unique(DF$append)
unique(DF$hpGen);unique(DF$tuneLength);unique(DF$ignrepeats)
usless<-c("trgCol","missing","append")
names(DF<-DF[,1:45,with=F])
names(DF<-DF[,-(usless),with=F])

```

Which models were somehow missed? Just checking just incase errors or 
machines did not run fast enough
Also if some hype hype parameters were way overblown
```{r}
DF[,keyMiss:=paste0(task,transform,transTarg,expirament,fold)]
luMiss<-length(unique(DF$keyMiss))
DF[,unmissed:=length(unique(keyMiss)),by=algomodel]
DF[unmissed<luMiss][order(unmissed)]
max(DF$unmissed)-luMiss
plot(DF$unmissed[order(DF$unmissed)])
```



for netcoinnetwork grap write out features
these will be used later on to separate data and for more detailed views
outliers too
```{r}
getwd()
DF$imperfect <- DF$algomodel=="perfect"
DF$totFail <- is.na(DF$hpGen) & is.na(DF$RMSEutrans) & is.na(DF$spearman2) & is.na(DF$pq9TargRank)
DF$totFail <- DF$totFail |  DF$algomodel=="ignore"
DF[,noPear:=is.na(spearman2) & !totFail]
DF[,noUtrans:=is.na(RMSEutrans) & !totFail]


if(F){
DF$btp4[is.na(DF$btp4)]<-"NA"
DF$btp4[DF$btp4=="NA"]<-"4.4"
DF$btp5[is.na(DF$btp5)]<-"NA"
DF$btp3[is.na(DF$btp3)]<-"NA"
}
DF$btp3<-(DF$btp3=="3")
summary(DF$btp3)

```


plot the netcoin graphs
first focusing onperfection then droping it
"seed" btw is just a function of R version change
```{r}
require(netCoin)
#,"seedit"
set <- c("transTarg","task","transform","fold","imperfect","seed","btp3")

essCoin <- surCoin(data = DF, variables = set, lwidth ="Haberman", lweight="Haberman", color="variable", lcolor="Haberman", ltext="Haberman", linkFilter="Haberman", repulsion=90)
essCoin
plot(essCoin)

set <- c("algomodel", "transTarg", "task", "transform", "fold", "pc", "totFail", "noPear", "noUtrans")


```
perfect/ideal results at section start
check for oddness, remove
```{r}

names(DF)
DF[,keyRedun:=paste0(transTarg,task,transform,pc,expirament,fold)]
Perf<-DF[(imperfect)]
DF<-DF[!(imperfect)]
summary(Perf[,1:20])

if(F){ #transtarg1 causes utrans to get numbers other than 0
halfver<-median(unique(Perf$RMSEutrans))
P1<-Perf[RMSEutrans>=halfver,]
P2<-Perf[RMSEutrans<=halfver,]
summary(P1[,1:30])
summary(P2[,1:30])
}

Perf[,.N,by=fold]
if(F){ #fold changes max possible but rarely!?
for(i in unique(Perf$fold)){
print(summary(Perf[Perf$fold==i,c(1:30),with=FALSE]))
} }

#check for redundancy against possible combinatorial explosion
Explosion<-1
for(i in which(names(Perf) %in% c("transTarg","task","transform","pc","expirament","fold"))){
  Explosion<-Explosion * dim(unique(Perf[,(i),with=F]))[1]
  print(Explosion)
}
length(unique(Perf$keyRedun))
length(Perf$keyRedun)/length(unique(Perf$keyRedun))
Perf[,.N,by=keyRedun]
```
infinte checked and converted to NA
```{r}
DF[,infin:=0]
for(i in 1:45){
DF$infin <- is.infinite(DF[,i,with=F][[1]]) + DF$infin
}

(infinDF<-DF[(infin>0)])

for(i in names(DF)[1:45]){#i<-2 
  nafound<-as.vector(is.infinite(DF[,i,with=F][[1]]))
  summary(nafound)
  na.replace<-NA
  DF[(nafound),(i):=na.replace]
  
  #DF[(nafound)]
}

DF[(infin>0)]#not getting everything?
infinDF
```


Percent success
```{r}
#install.packages("VIM")
require(VIM)
mice_plot <- aggr(DF[,1:20], col=c('navyblue','yellow'),
numbers=TRUE, sortVars=TRUE,
labels=names(DF[,1:20]), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
mice_plot <- aggr(DF[,21:40], col=c('navyblue','yellow'),
numbers=TRUE, sortVars=TRUE,
labels=names(DF[,21:40]), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
```
```{r}
require(naniar)
gg_miss_upset(DF[,1:25],nsets = 15,nintersects=20)
```


Full fails checked and removed
mlr fail state not updated algo "ignore" also a fail
```{r}
sum(DF$totFail)
dim(DF)[1]
DF[(totFail),][1:30,]

TF<-DF[(totFail),]
DF<-DF[!(totFail)]

```
```{r}
require(naniar)
gg_miss_upset(DF[,1:35],nsets = 15,nintersects=15)
require(VIM)
mice_plot <- aggr(DF[,1:35], col=c('navyblue','yellow'),
numbers=TRUE, sortVars=TRUE,
labels=names(DF[,1:35]), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))

```


Which models were often failed or missed again

```{r}
DF[,keyMiss:=paste0(task,transform,transTarg,expirament,fold)]
luMiss<-length(unique(DF$keyMiss))
DF[,unmissed:=length(unique(keyMiss)),by=algomodel]
DF[unmissed<luMiss][order(unmissed)]
max(DF$unmissed)-luMiss
plot(DF$unmissed[order(DF$unmissed)])
```

Checks of variation inside redundant runs
If summary 0s and 1s then no NA variance
redonePQ9TF does range of 90th percentile rank by more than .02?

```{r}
DF[,keyOnerun:=paste0(expirament,task,pc,fold,transTarg,transform,algomodel,seed)]
DF[,redonee:=.N,by=keyOnerun]
DF[,redoneNA:=(sum(noPear) + sum(noUtrans))/redonee,by=keyOnerun]
summary(DF$redoneNA)
DF[,redonePQ9:=max(pq9TargRank,na.rm = T)-min(pq9TargRank,na.rm = T),by=keyOnerun]
DF[,redonePQ9TF:=redonePQ9>.02]
(specimen<-DF[(redonePQ9TF)][order(keyOnerun)][1:100])
(exems<-unique(specimen$keyOnerun))
DF[keyOnerun=="QSlink3QSlinks03_95ACEREBOUT21centernscaleSL.bartMachine403"]
```

outliers moved to some more reasonable state
may be column specific
lots of NAS not dealt with; let NA algo deal with them
```{r}

for(i in names(DF)[1:9]){#i<-2
  bxs <- boxplot.stats(DF[,i,with=F][[1]],coef=3)
  print(bxs$stats)
  nafound <- as.vector(DF[,i,with=F]<bxs$stats[[1]])
  print(summary(nafound))
  #DF[(is.na(nafound))];
  #print(DF[(nafound)])
  DF[(nafound),(i):=bxs$stats[[1]]]
  nafound <- as.vector(DF[,i,with=F]>bxs$stats[[5]])
  print(summary(nafound))
  DF[(nafound),(i):=bxs$stats[[5]]]
}
 
```

net coicidence with everything to be removed, removed
```{r}
#DF$noPear
#DF$noUtrans
# 1 2345 67 8 910
# #get rid of this part when redoing
binnerr<-function(x){
  if(x==1) return(1)
  if(x>=2 & x<=5) return(3)
  if(x>=6 & x<=10) return(8)
}
DF[,rowNum:=seq_len(.N)]
length(unique(DF$rowNum))==dim(DF)[1]
DF[,redoneeb:=binnerr(redonee),by=rowNum]
summary(DF$redoneeb)

set <- c("algomodel", "transTarg", "task", "transform", "fold", "pc",  "noPear", "noUtrans","redonePQ9TF","redoneeb")
essCoin <- surCoin(data = DF, variables = set, lwidth ="Haberman", lweight="Haberman", color="variable", lcolor="Haberman", ltext="Haberman", linkFilter="Haberman", repulsion=90)
essCoin
plot(essCoin)
```
fold now depends on seed so fold becomes paste of fold and seed
```{r}
DF[,foldOrg:=fold]
DF[,fold:=paste0(fold,".",foldseed)]
```

remove completely redundant rows
```{r}
DF[,keyOnerun:=paste0(expirament,task,pc,fold,transTarg,transform,algomodel,seed)]
dim(DF)
uDF<-unique(DF,by = c("pq9TargRank","pearson2","pRMSE","keyOnerun"))
dim(uDF)
uDF[order(-spearman2)][order(-pRMSE)][order(-pq9TargRank)][order(-gainin30)]
```
missing values in metric will be made 80%tile
only the better score really matter so 
to prevent crashing later score its set to 80%
```{r}
for(i in names(uDF)[1:9]){#i<-2
  print(na.replace<-quantile(uDF[,i,with=F],.8,na.rm=T))
  nafound<-as.vector(is.na(uDF[,i,with=F]))
  summary(nafound)
  uDF[(nafound),(i):=na.replace]
  #uDF[(nafound)]
}
summary(uDF[,1:9,with=F]) 
```
Getting into selecting metrics here; lets see their correlations and distributions
```{r}
uDF[,topmetrics:=(gainin30>quantile(gainin30,.801))+(pq9TargRank>quantile(pq9TargRank,.801))+(spearman2>quantile(spearman2,.801))+(pearson2>quantile(pearson2,.801))+(pMAE>quantile(pMAE,.801))+(pRMSE>quantile(pRMSE,.801))]
iDF<-uDF[(topmetrics>0)][order(-topmetrics)]
summary(iDF$topmetrics)
for(i in names(iDF)[1:9]){#i<-2
  yy<-as.vector(iDF[,i,with=F][[1]])
  print(summary(yy))
  yy<-as.integer(yy*10000)
  print(summary(yy))
  #gg<-ecdf(yy);summary(gg)
  #unique(yy);dim(iDF[,i,with=F]);unique(iDF[,i,with=F]);iDF[,i,with=F]
  plot.ecdf(yy, verticals = TRUE, do.points = FALSE)
  title(i, adj = 1)
}

```
```{r}
require(ggplot2)
for(i in names(iDF)[1:9]){
print(ggplot(iDF, aes(x=get(i))) + geom_histogram(aes(y=..density..), stat="bin", position="stack", alpha=0.5, bins=16) + geom_path(aes(y=..density..), stat="bin", position="identity", linetype="dashed", bins=16, pad=TRUE) + geom_density(aes(y=..density..), stat="density", position="identity", alpha=0.5) + theme_grey() + theme(text=element_text(family="sans", face="plain", color="#000000", size=15, hjust=0.5, vjust=0.5)) + xlab(i) + ylab("density"))
}



```

```{r}

uDF[,redonePQ9:=max(pq9TargRank,na.rm = T)-min(pq9TargRank,na.rm = T),by=keyOnerun]
uDF[,redonePQ9TF:=redonePQ9>.02]
(specimen<-uDF[(redonePQ9TF)][order(keyOnerun)][1:100])
(exems<-unique(specimen$keyOnerun))

uDF[order(-spearman2)][order(-pRMSE)][order(-pq9TargRank)][order(-gainin30)]
```
two medianings because fold is very special compared to just hyperparameer jiggling. 
```{r}

uDF[order(-spearman2)][order(-pRMSE)][order(-pq9TargRank)][order(-gainin30)]
uDF[,keyOneRecipe:=paste0(expirament,task,transTarg,transform,algomodel,fold)]

 
uDF[,gainin30:=median(gainin30,na.rm = T),by=keyOneRecipe]
uDF[,pq9TargRank:=median(pq9TargRank,na.rm = T),by=keyOneRecipe]
uDF[,spearman2:=median(spearman2,na.rm = T),by=keyOneRecipe]
uDF[,pearson2:=median(pearson2,na.rm = T),by=keyOneRecipe]
uDF[,pMAE:=median(pMAE,na.rm = T),by=keyOneRecipe]
uDF[,pRMSE:=median(pRMSE,na.rm = T),by=keyOneRecipe]
uDF[,ocvRMSE:=median(ocvRMSE,na.rm = T),by=keyOneRecipe]
uDF[,RMSEutrans:=median(RMSEutrans,na.rm = T),by=keyOneRecipe]
uDF[,MAEutrans:=median(MAEutrans,na.rm = T),by=keyOneRecipe] 

uDF[,hyperUnqs:=.N,by=keyOneRecipe]
uDF <- unique(uDF,by = c("pq9TargRank", "pearson2", "pRMSE", "keyOneRecipe"))
dim(uDF)

```
prep for duplication of recipes removal
#wierd effect of lowered variance drasticaly dropping interval 
using variance of all recipes
here quick view of these variances
```{r}
uDF[,keyOneRecipe:=paste0(expirament,task,transTarg,transform,algomodel)]
uDF[,foldCount:=.N,by=keyOneRecipe]

if(F){
uDF[,staDev:=sd(pearson2),by=keyOneRecipe]
uDF[,meanie:=mean(pearson2),by=keyOneRecipe]
summary(uDF$staDev)
summary(uDF$meanie)
summary(uDF$foldCount)
ggplot(data = uDF,mapping = aes(x=as.factor(foldCount),staDev)) + geom_boxplot() + geom_smooth(method="lm")

ggplot(data = uDF,mapping = aes(meanie,staDev)) + geom_point() + geom_smooth()
}
```
actualy applying manual ttest 
and then collapsing all unique folds

```{r}

cl <- 0.7
ttOK <- function(x){
  vax<-var(x)
  if(is.na(vax)) return(F)
  if(vax==0) return(F)
  return(T)
}

tt1 <- function(x,o=1,coi=.95){
  x <- c(x,o)
  return(t.test(x,conf.level = coi))
}

#manual mean interval ttest with outside SD if vector small
ttm <- function(x,psd,cl=.95,u_l="u",dsda=10){
  if(cl>=1 | cl<=0) warning("confidence level out of bounds")
  if(!is.vector(x)) warning("x not a vector")
  n <- length(x)
  if( n < 1 ) warning("x has length 0")
  if(n>1){
    nsd <- sd(x) * min( n / dsda , 1) + psd * max((1 - n / dsda) , 0)
  } else {
    nsd <- psd
  }
  ttt <- qt((1-(1-cl)/2),df = max((n-1),1))
  entrvl <- ttt * nsd / sqrt(n)
  if(u_l=="u"){  return( mean(x) + entrvl) }
  if(u_l=="l"){  return( mean(x) - entrvl) }
}

dsda<-10
lastCol<-dim(uDF)[2]
for(i in names(uDF)[1:9]){#i<-"spearman2"
  uDF[(foldCount>3), intSD := sd(get(i),na.rm=T), by=keyOneRecipe]
  #summary(uDF$intSD)
  print(osd <- quantile(uDF[,.SD[1], by=keyOneRecipe]$intSD, na.rm=T,.8))

  uDF[(foldCount<=dsda),paste0(i,"u") := ttm(get(i), osd, cl = cl, dsda = dsda), by=keyOneRecipe]
  uDF[(foldCount<=dsda),paste0(i,"l") := ttm(get(i), osd, u_l="l", cl = cl, dsda = dsda),  by=keyOneRecipe] 
  uDF[(foldCount>dsda),paste0(i,"u") := t.test(get(i),conf.level = cl)$co[2], by=keyOneRecipe]
  uDF[(foldCount>dsda),paste0(i,"l") := t.test(get(i),conf.level = cl)$co[1], by=keyOneRecipe] 

#print(summary(uDF[,(c(i,paste0(i,"u"),paste0(i,"l"))),with=F]))
  uDF[,(i):=mean(get(i),na.rm = T),by=keyOneRecipe] 
  print(summary(uDF[,(c(i,paste0(i,"u"),paste0(i,"l"))),with=F]))
}

if(F){ #us and ls NAs to means
for(i in names(uDF)[lastCol:dim(uDF)[2]]){#i<-"spearman2l"
uDF[(is.na(get(i))),(i):=mean(get(i),na.rm = T)] 
print(summary(uDF[,(c(i,paste0(i,"u"),paste0(i,"l"))),with=F]))
}
}
  
uDF <- unique(uDF,by = c("pq9TargRank", "pearson2", "pRMSE", "keyOneRecipe"))
dim(uDF)
```


best algo is just first 3 scores in 95% and number of oflds too
else just use median

maybe all this is too much work and I should just take tops?
by tops I mean lower bound confidence intervals of each of the four merics
```{r}
#summary(uDF[,.(gainin30l,pq9TargRankl,spearman2l,pearson2l,pMAEl,pRMSEl)])
uDF[,topmetrics:=(gainin30l>quantile(gainin30l,.95, na.rm=T))+(pq9TargRankl>quantile(pq9TargRankl,.96, na.rm=T))+(spearman2l>quantile(spearman2l,.96, na.rm=T))+(pearson2l>quantile(pearson2l,.95, na.rm=T))+(pMAEl>quantile(pMAEl,.99, na.rm=T))+(pRMSEl>quantile(pRMSEl,.99, na.rm=T))]


summary(uDF[(topmetrics>1)][(foldCount>2)][,.(gainin30l,pq9TargRankl,spearman2l,pearson2l,pMAEl,pRMSEl)])
uDF[(topmetrics>1)][(foldCount>2)][order(-gainin30l)]
uDF[(topmetrics>1)][(foldCount>2)][order(-pq9TargRankl)]
uDF[(topmetrics>1)][(foldCount>2)][order(-spearman2l)]
uDF[(topmetrics>1)][(foldCount>2)][order(-pearson2l)]
uDF[(topmetrics>1)][(foldCount>2)][order(-pq9TargRankl)][order(-topmetrics)]

```


Here user must make a combined metric by 
multipying metrics distance from optimal by user set score then summing result. knowing distribution of metrics here is kindof important

```{r}
uDF[,sqrdcomb:=((max(gainin30l, na.rm=T)-gainin30l)^2) *.5 + ((max(pq9TargRankl, na.rm=T)-pq9TargRankl)^2)*1.5 + ((max(spearman2l, na.rm=T)-spearman2l)^2) + ((max(pearson2l, na.rm=T)-pearson2l)^2) ]
uDF[,l1comb:=((max(gainin30l, na.rm=T)-gainin30l)) *.5 + ((max(pq9TargRankl, na.rm=T)-pq9TargRankl))*1.5 + ((max(spearman2l, na.rm=T)-spearman2l)) + ((max(pearson2l, na.rm=T)-pearson2l)) ]

uDF[(foldCount>2)][order(sqrdcomb)]
uDF[(foldCount>2)][order(l1comb)]
```

QSlink3QSlinks14_55Int2range01SL.ranger	#### winner best mean on most####

which models need to be run more
in other words which models have best chance to be really good
```{r}
uDF[,sqrdcombu:=((max(gainin30u, na.rm=T)-gainin30u)^2) *.5 + ((max(pq9TargRanku, na.rm=T)-pq9TargRanku)^2)*1.5 + ((max(spearman2u, na.rm=T)-spearman2u)^2) + ((max(pearson2u, na.rm=T)-pearson2u)^2) ]
uDF[,l1combu:=((max(gainin30u, na.rm=T)-gainin30u)) *.5 + ((max(pq9TargRanku, na.rm=T)-pq9TargRanku))*1.5 + ((max(spearman2u, na.rm=T)-spearman2u)) + ((max(pearson2u, na.rm=T)-pearson2u)) ]

uDF[order(sqrdcombu)]
uDF[order(l1combu)]
h<-which(names(uDF)=="pq9TargRanku")
h<-names(uDF)[h:dim(uDF)[2]]
h<-c(c("expirament","task","transTarg","transform","algomodel","hpGen","time","validmethod","tuneLength","cvcount","unmissed","keyMiss","foldCount"),h)
uDF[order(l1combu)][,h,with=F]
```
 
maybe all this is too much work and I should just take tops?
```{r}
summary(uDF[,.(gainin30l,pq9TargRankl,spearman2l,pearson2l,pMAEl,pRMSEl)])
uDF[,topmetrics:= (gainin30l>quantile(gainin30l,.92, na.rm=T)) + (pq9TargRankl>quantile(pq9TargRankl,.99, na.rm=T)) + (spearman2l>quantile(spearman2l,.99, na.rm=T)) + (pearson2l>quantile(pearson2l,.99,  na.rm=T)) + (pMAEl>quantile(pMAEl,.99, na.rm=T)) + (pRMSEl>quantile(pRMSEl,.99, na.rm=T))]
uDF[(topmetrics>5)]
```


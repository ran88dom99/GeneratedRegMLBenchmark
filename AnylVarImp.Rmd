---
title: "Variable importance"
---
load all varimp files and check problems and remove some problems
```{r}
col_typed <- "cccddccccccddddddcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcdcd?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"
#71 per row so 142+ more than 20?
#167
try({setwd(mainDir)})
mainDir<-getwd()
subDir<-"QSlinks_simple"
#dir.create(file.path(mainDir, subDir))
require(readr)
require(netCoin)
require(data.table)
require(naniar) 
require(VIM)
require(ggplot2)
require(stringr)

resultDir<-"ACEREBOUT"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3ACE <- read_csv("importanceQSlink3ACEREBOUTwindowsx64.csv",col_types = col_typed)
resetACE <- read_csv("test after which reset.csv")
names(resetACE)<-c("date","model","task","fold","cv","arch","arch2","pc")
#QSlinkImp3ACE <- read_csv("importanceQSlink3ACEREBOUTwindowsx64.csv",col_types = col_typed)
#summary(QSlink3ACE[,1:30])
 #spec(QSlink3ACE)
 prACE<-problems(QSlink3ACE)
 unique(prACE$actual)
 #prACE[!str_detect(prACE$actual,"columns"),]
 rowrem<-prACE[!str_detect(prACE$actual,"columns"),]$row
 QSlink3ACE<-QSlink3ACE[-rowrem,]
 setwd(file.path(mainDir))

 
 resultDir<-"HOPPER"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3HOP <- read_csv("importanceQSlink3HOPPERwindowsx64.csv",col_types = col_typed)
resetHOP <- read_csv("test after which reset.csv")
names(resetHOP)<-c("date","model","task","fold","cv","arch","arch2","pc")
#summary(QSlink3ACE[,1:30])
 #spec(QSlink3HOP)
 prHOP<-problems(QSlink3HOP)
 unique(prHOP$actual)
 #prHOP[!str_detect(prHOP$actual,"columns"),]
  rowrem<-prHOP[!str_detect(prHOP$actual,"columns"),]$row
 QSlink3HOP<-QSlink3HOP[-rowrem,]
 setwd(file.path(mainDir))
 #names(QSlink3HOP)
 
  resultDir<-"LAPTOPBBQ"
#setwd(resultDir)
setwd(file.path(mainDir, resultDir))
QSlink3BBQ <- read_csv("importanceQSlink3LAPTOPBBQwindowsx64.csv",col_types = col_typed)
resetBBQ <- read_csv("test after which reset.csv")
names(resetBBQ)<-c("date","model","task","fold","cv","arch","arch2","pc")
#summary(QSlink3ACE[,1:30])
 #spec(QSlink3BBQ)
 prBBQ<-problems(QSlink3BBQ)
 unique(prBBQ$actual)
 #prBBQ[!str_detect(prBBQ$actual,"columns"),] 
 rowrem<-prBBQ[!str_detect(prBBQ$actual,"columns"),]$row
 QSlink3BBQ<-QSlink3BBQ[-rowrem,]
 setwd(file.path(mainDir))

```
tibble into data.table
```{r}

require(data.table)
DF<-as.data.table(QSlink3ACE)
DF1<-as.data.table(QSlink3HOP)
DF2<-as.data.table(QSlink3BBQ)

DF<-rbindlist(list(DF,DF1,DF2),fill = T)

rm(DF1,DF2,QSlink3BBQ,QSlink3HOP,QSlink3ACE)

reset<-as.data.table(resetACE)
reset1<-as.data.table(resetHOP)
reset2<-as.data.table(resetBBQ)

reset<-rbindlist(list(reset,reset1,reset2),fill = T,idcol=T)
rm(reset1,reset2,resetACE,resetHOP,resetBBQ)
gc()
```
check rename remove nas
```{r}
print("any fails in 4th and fifith coulums? success and transtarg")
sum(str_detect(DF$X4,"FAIL"),na.rm = T)
sum(str_detect(DF$X5,"FAIL"),na.rm = T)
#str(DF)
head(DF)
unique(DF$X13)

DFnam<-c("modelset","model","datetime","success","transTarg","task","missing","append","transform","pc","expirament","fold","maxfold","seed","seedit","foldseed","RMSEmean")
colNum<-T
firCol<-length(DFnam)+1
for (i in (firCol):dim(DF)[2]) {
  if(colNum){
    DFnam<-c(DFnam,paste0("Col",i))
    colNum<-F
  } else {
    DFnam<-c(DFnam,paste0("Val",i))
    colNum<-T
  }
}
names(DF)<-DFnam
summary((DF$Val27))
DF[(is.na(Val29) & is.na(Val27)),]
DF[(is.infinite(Val29)),]
DF<-DF[!(is.na(Val29) & is.na(Val27)),]
DF<-DF[!(is.infinite(Val29)),]
print("summary of first and 4th and 5th valuse")
summary((DF$Val19));summary((DF$Val29));summary((DF$Val27))
```
poor organization; separated and duplicated results per each recipe into tiddy table
```{r}
cols<-which(str_detect(names(DF),fixed("Col")))
vals<-which(str_detect(names(DF),fixed("Val")))
(fir<-min(cols))
(endd<-max(cols))
(lst<-max(cols))
if(lst==dim(DF)[2]) {
  DF[,(lst):=NULL]
  cols<-cols[-length(cols)]
  lst<-lst-2
}#dim(DF)
 strt<-fir#names(DF)[strt]
nnd<-lst+1
 DF[,keyVarImp:=paste0(modelset, model, transTarg, task, transform, expirament)]

prd<-data.table(allColNm=unique(as.vector(DF[,(cols),with=F][[1]])))
i<-unique(DF$keyVarImp)[1]

for(i in unique(DF$keyVarImp)){
  wrk<-DF[(keyVarImp==i)][,(strt:nnd),with=F]
  wrk<-unique(wrk)
  indx<-(unlist(wrk[,seq(by=2,from=1,to=dim(wrk)[2]),with=F]))
  val<-(unlist(wrk[,seq(by=2,from=2,to=dim(wrk)[2]),with=F]))
  unique(indx)
  unique(val)
  wrk<-data.table(indx,val)
  wrk$val<-as.numeric(wrk$val)
  dim(wrk)
  wrk<-wrk[(order(indx))]
  wrk[,sdw:=sd(val),by=indx]
  wrk[,Nw:=.N,by=indx]
  wrk[,val:=mean(val),by=indx]
  wrk<-unique(wrk)
  if(dim(wrk)[1]!=length(unique(wrk$indx))) stop("more work, multiple valuse for same index")
  names(wrk)<-c("indx",paste0(i,"val"),paste0(i,"sd"),paste0(i,"N"))
  merge(prd, wrk,  by.x = "allColNm", by.y = "indx", all = T)
}
prd
length(names(prd))#removed transformed target so not 601
names(prd)[c(1,2,3,6,7,8,15,16,17)]

```
ordered by highest multi column into melted long one "colname" one value
```{r}
if(F){
cols<-which(str_detect(names(DF),fixed("Col")))
vals<-which(str_detect(names(DF),fixed("Val")))
(fir<-min(cols))
(endd<-max(cols))
(lst<-max(cols))
if(lst==dim(DF)[2]) {
  DF[,(lst):=NULL]
  cols<-cols[-length(cols)]
  lst<-lst-2
}

if(F){
allcols<-unique(as.vector(DF[,(cols),with=F][[1]]))
allcols[which(str_detect(allcols,"V")==F)]
for(i in 1:length(cols)){
print(paste(i,sum(str_detect(DF[,(cols[i]),with=F][[1]],fixed("QSlinks115_60centernscale2QSlink3regr.randomForestval")))))
}
DF[(str_detect(DF[,(cols[2]),with=F][[1]],fixed("QSlinks115_60centernscale2QSlink3regr.randomForestval")))]
}

#create key and remove all in key then 
#make 3 column long table of key col val 
#require 8 gb of ram. NVM 
#cast the very long table to make it wide
 
 DF$keyVarImp[1:30]
 
 rmcols<-names(DF)[1:(fir-1)]
for(i in rmcols) DF[,(i):=NULL] #i<-allcols[3]
 
cols<-which(str_detect(names(DF),fixed("Col")))
vals<-which(str_detect(names(DF),fixed("Val")))
(fir<-min(cols))
(endd<-max(cols))
(lst<-max(cols))
if(lst==dim(DF)[2]) {
  DF[,(lst):=NULL]
  cols<-cols[-length(cols)]
  lst<-lst-2
}

mint<-data.table()
whereVarimp<-dim(DF)[2]
for(i in cols){#i<-cols[3]
  hold<-DF[,c(i,(i+1),whereVarimp),with=F]
  hold<-hold[!(is.na(hold[,(1),with=F][[1]]) | is.na(hold[,(2),with=F][[1]]))]
  mint<-rbindlist(list(mint,hold),use.names = F)
}
rm(DF);gc()
}
```
time to cast melted data
```{r}
if(F){
require(reshape2)
names(mint);str(mint)
#
mint[(is.na(as.numeric(Val19)))]
mint$Val19<-as.numeric(mint$Val19)
dcast(mint,Col18 ~ Val19, mean)
?dcast



names(airquality) <- tolower(names(airquality))
aqm <- melt(airquality, id=c("month", "day"), na.rm=TRUE)

acast(aqm, day ~ month ~ variable)
acast(aqm, month ~ variable, mean)
acast(aqm, month ~ variable, mean, margins = TRUE)
dcast(aqm, month ~ variable, mean, margins = c("month", "variable"))
}
```

by task get all remotley useful variables and graph best ones
```{r}

```
select best from pessimist file ! not finished just copied from outside
```{r}
load(file="routuDFpessimistUnique.RData")
(bestest<-Unq[(1)])
nnd<-which(names(Unq)=="datePOSIX")
strt<-which(names(Unq)=="btp8")
Unqn<-Unq[(is.na(btp11))]
Unqf<-Unq[(!is.na(btp11))]
Unqf[,dum1:=0]
Unqf[,dum2:=0]
Unqf[,dum3:=0]
Unqn[,dum1:=0]
Unqn[,dum2:=0]
Unqn[,dum3:=0]
colord<-names(Unqf)
ol<-length(colord)
(colord<-c(colord[1:strt],colord[(ol-2):ol],colord[(strt+1):(nnd-4)],colord[(nnd):(ol-3)]))
setcolorder(Unqf,colord)

```
by task, get all remotley useful variables and graph best ones
```{r}

```
look for highly uncorrelated recipies
by task select single best model and add to possiblyUniq
then those models that most strongly discorrelate are also added
```{r}

```





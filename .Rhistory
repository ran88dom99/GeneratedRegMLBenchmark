for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[!is.nan(means),]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=0.95,vjust=0.2))+scale_x_discrete(position = "top")+
xlab(paste("fail learners in",exp.name)) + ylab("diff")
ttdd<-data.frame((failruncomp[r,,]))
ttdd[ttdd>1.1]<-NA
maxttdd<-data.frame((maxruncomp[r,,]))
#maxttdd[maxttdd>1]<-NA
maxttdd[maxttdd<.3]<-.3
ttdd<-ttdd/maxttdd
rownames(ttdd)<-u.learns
#p <- ggplot(melt(ttdd), aes(x=melt(maxttdd)[,2],y=melt(ttdd)[,2],colour = melt(ttdd)[,1]))
#p+geom_point(), x = reorder(, I.power.df[,8])
means<-vector()
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[!is.nan(means),]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=0.95,vjust=0.2))+scale_x_discrete(position = "top")+
xlab(paste("fail learners in",exp.name)) + ylab("diff")
summary(failruncomp[1,1,])
summary(failruncomp[1,,1])
summary(failruncomp[1,40,])
summary(failruncomp[1,420,])
summary(failruncomp[1,20,])
summary(failruncomp[1,10,])
summary(failruncomp[1,,2])
summary(failruncomp[1,,3])
summary(failruncomp[1,,4])
summary(failruncomp[1,,5])
summary(failruncomp[1,,6])
(failruncomp[1,,6])
(failruncomp[1,,50])
(failruncomp[1,,20])
(failruncomp[1,50,])
(failruncomp[1,10,])
(failruncomp[1,20,])
(failruncomp[1,30,])
(failruncomp[1,1,])
(failruncomp[1,2,])
(failruncomp[1,3,])
(failruncomp[1,4,])
u.learns
ttdd<-data.frame((failruncomp[r,,]))
ttdd[ttdd==0]<-NA
maxttdd<-data.frame((maxruncomp[r,,]))
maxttdd[maxttdd>1]<-NA
maxttdd[maxttdd<.3]<-.3
ttdd<-ttdd/maxttdd
rownames(ttdd)<-u.learns
#p <- ggplot(melt(ttdd), aes(x=melt(maxttdd)[,2],y=melt(ttdd)[,2],colour = melt(ttdd)[,1]))
#p+geom_point(), x = reorder(, I.power.df[,8])
means<-vector()
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[!is.nan(means),]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=0.95,vjust=0.2))+scale_x_discrete(position = "top")+
xlab(paste("fail learners in",exp.name)) + ylab("diff")
ggsave(paste(exp.name," and boxes per learnr fail.png", sep = ""),plot = last_plot(),scale = 3)
}
for(r in 1:length(runsTcompare[,1])){
ttdd<-data.frame((failruncomp[r,,]))
ttdd[ttdd==2]<-NA
maxttdd<-data.frame((maxruncomp[r,,]))
maxttdd[maxttdd>1]<-NA
maxttdd[maxttdd<.3]<-.3
ttdd<-ttdd/maxttdd
colnames(ttdd)<-u.gens
ttdd<-t(ttdd)
means<-vector()
for(m in 1:length(u.gens)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.gens)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[,!is.nan(means)]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=1,vjust=0.3))+scale_x_discrete(position = "top")+
xlab(paste("fail gens in",exp.name)) + ylab("diff")
source('C:/Users/Dm/Desktop/generated data test/run comparison.R')
?
>
source('C:/Users/Dm/Desktop/generated data test/run comparison.R')
ttdd<-data.frame((failruncomp[r,,]))
ttdd[ttdd==2]<-NA
maxttdd<-data.frame((maxruncomp[r,,]))
maxttdd[maxttdd>1]<-NA
maxttdd[maxttdd<.3]<-.3
ttdd<-ttdd/maxttdd
rownames(ttdd)<-u.learns
#p <- ggplot(melt(ttdd), aes(x=melt(maxttdd)[,2],y=melt(ttdd)[,2],colour = melt(ttdd)[,1]))
#p+geom_point(), x = reorder(, I.power.df[,8])
means<-vector()
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[!is.nan(means),]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=0.95,vjust=0.2))+scale_x_discrete(position = "top")+
xlab(paste("fail learners in",exp.name)) + ylab("diff")
ttdd<-data.frame((failruncomp[r,,]))
#ttdd[ttdd==2]<-NA
maxttdd<-data.frame((maxruncomp[r,,]))
maxttdd[maxttdd>1]<-NA
maxttdd[maxttdd<.3]<-.3
ttdd<-ttdd/maxttdd
rownames(ttdd)<-u.learns
#p <- ggplot(melt(ttdd), aes(x=melt(maxttdd)[,2],y=melt(ttdd)[,2],colour = melt(ttdd)[,1]))
#p+geom_point(), x = reorder(, I.power.df[,8])
means<-vector()
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ordos<- order(means,ttdd[,1])
ttdd<-rbind(ttdd)[ordos,]
for(m in 1:length(u.learns)){
means[m]<-mean(as.numeric(ttdd[m,]),na.rm = T)}
ttdd<-ttdd[!is.nan(means),]
ttdd<-t(ttdd)
exp.name<-runsTcompare[r,3]
p <- ggplot(melt(ttdd), aes(x=(melt(ttdd)[,2]),y=melt((ttdd))[,3]))
p  + geom_boxplot(width=0.4)+ stat_summary(fun.y = "mean", colour = "red", size = 1, geom = "point")+
theme(axis.text.x=element_text(size=7, angle=270,hjust=0.95,vjust=0.2))+scale_x_discrete(position = "top")+
xlab(paste("fail learners in",exp.name)) + ylab("diff")
source('C:/Users/Dm/Desktop/generated data test/run comparison.R')
setwd("C:/Users/Dm/Desktop/generated data test")
source('C:/Users/Dm/Desktop/generated data test/run comparison.R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
setwd("C:/Users/Dm/Desktop/generated data test")
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
library(mlbench)
data("Boston")
data("BostonHousing")
View(BostonHousing)
data("BostonHousing2")
View(BostonHousing2)
View(BostonHousing)
summary("BostonHousing")
is.data.frame(BostonHousing)
BostonHow<-data.frame(BostonHousing[,mdev])
BostonHow<-data.frame(BostonHousing[,"mdev"])
BostonHow<-data.frame(BostonHousing[,14])
BostonHow
BostonHow<-data.frame(BostonHousing[,14],BostonHousing[,1:13])
BostonHow
F1<-mlbench.friedman1(n, sd=1)
F1<-mlbench.friedman1(100, sd=1)
summary("BostonHousing")
F1
FF1<-data.frame(F1[,11],F1[,1:10])
FF1<-data.frame(F1[[y]],F1[,1:10])
F1[["y"]]
FF1<-data.frame(F1[["y"]],F1[,1:10])
F1[,1:10]
F1[,1]
F1[[1]]
F1[[1:10]]
F1[[10]]
F1[[9]]
FF1<-data.frame(F1[["y"]],F1[[1]])
FF1
F1<-mlbench.xor(100, d=2)
FF1<-data.frame(F1[["y"]],F1[[1]])
F1
F1<-mlbench.xor(100, d=4)
F1
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
data(BostonHousing, package = "mlbench")
regr.task = makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
regr.task
library(mlr)
data(BostonHousing, package = "mlbench")
regr.task = makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
regr.task
data(BostonHousing2, package = "mlbench")
regr.task = makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
regr.task
str(getTaskData(regr.task))
library(mlbench)
data("BostonHousing2")
summary("BostonHousing2")
str("BostonHousing2")
data(BostonHousing2, package = "mlbench")
regr.task = makeRegrTask(id = "bh", data = BostonHousing, target = "medv")
regr.task
str(getTaskData(regr.task))
data("BostonHousing2")
look.into<-data("BostonHousing2")
is.data.frame(BostonHousing2)
str(getTaskData(regr.task))
regr.lrn = makeLearner("regr.gbm",fix.factors.prediction = T, par.vals = list(n.trees = 500, interaction.depth = 3))
regr.lrn$par.vals
regr.lrn$par.set
regr.lrn$predict.type
regr.lrn$par.set
regr.lrn$par.set
getHyperPars(regr.lrn)
getParamSet(regr.lrn)
regr.lrn$par.set
getLearnerPackages(cluster.lrn)
getLearnerPackages(regr.lrn)
regr.lrn
lrns = listLearners()
lrns
head(lrns[c("class", "package")])
lrns[c("class", "package")]
lrns = listLearners(iris.task, properties = "prob")
head(lrns[c("class", "package")])
lrns[c("class", "package")]
lrns = listLearners("classif", properties = "prob")
head(lrns[c("class", "package")])
lrns = listLearners(iris.task, properties = "prob")
head(lrns[c("class", "package")])
regr.lrn
head(listLearners("cluster", create = TRUE), 3)
source('C:/Users/Dm/Desktop/generated data test/MLR notes.R')
head(listLearners("cluster", create = TRUE), 2)
listLearners("cluster", create = TRUE)
listLear<-listLearners("cluster", create = TRUE)
listLear
mod = train(regr.lrn, regr.task)
mod
names(mod)
mod$learner
mod$features
mod$time
getLearnerModel(mod)
n = getTaskSize(bh.task)
train.set = sample(n, size = n/3)
train.set
mod = train("regr.lm", bh.task, subset = train.set)
mod
n = getTaskSize(bh.task)
train.set = seq(1, n, by = 2)
test.set = seq(2, n, by = 2)
lrn = makeLearner("regr.gbm", n.trees = 100)
mod = train(lrn, bh.task, subset = train.set)
task.pred = predict(mod, task = bh.task, subset = test.set)
task.pred
plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
regr.lrn
plotLearnerPrediction(regr.lrn, features = c("lstat", "rm"), task = bh.task)
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
n = getTaskSize(bh.task)
lrn = makeLearner("regr.gbm", n.trees = 1000)
mod = train(lrn, task = bh.task, subset = seq(1, n, 2))
pred = predict(mod, task = bh.task, subset = seq(2, n, 2))
performance(pred)
performance(pred, measures = medse)
performance(pred, measures = list(mse, medse, mae))
performance(pred, measures = list(mse, medse, mae))
performance(pred, measures = c(mse, medse, mae))
performance(pred, measures = list( medse, mae))
performance(pred, measures = ( medse, mae))
performance(pred, measures = (  mae))
performance(pred, measures =   mae)
performance(pred, measures =   "mae")
performance(pred, measures =  list( "mae"))
performance(pred, measures = list(mse, medse, mlr::mae))
performance(pred, measures = timeboth, model = mod)
ps = makeModelMultiplexerParamSet(lrn,
makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
makeIntegerParam("ntree", lower = 1L, upper = 500L)
)
base.learners = list(
makeLearner("classif.ksvm"),
makeLearner("classif.randomForest")
)
lrn = makeModelMultiplexer(base.learners)
ps = makeModelMultiplexerParamSet(lrn,
makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
makeIntegerParam("ntree", lower = 1L, upper = 500L)
)
print(ps)
rdesc = makeResampleDesc("CV", iters = 2L)
ctrl = makeTuneControlIrace(maxExperiments = 200L)
res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
print(head(as.data.frame(res$opt.path)))
rdesc
ctrl = makeTuneControlIrace(maxExperiments = 2000L)
res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
ps = makeModelMultiplexerParamSet(lrn#,
#makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
#makeIntegerParam("ntree", lower = 1L, upper = 500L)
)
ctrl = makeTuneControlIrace(maxExperiments = 200L)
res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
print(ps)
print(head(as.data.frame(res$opt.path)))
training
lrns = listLearners()
lrns
head(lrns[c("class", "package")])
lrns[c("class", "package")]
listLearners("regr")
mlrallmodels<-listLearners("regr")
mlrallmodels
mlrallmodels[[installed]]
mlrallmodels[["installed"]]
mlrallmodels<-listLearners("regr")
str(mlrallmodels)
mlrallmodels[[1]]
View(mlrallmodels)
getLearnerPackages(regr.lrn)
getLearnerPackages(regr.svm)
getLearnerPackages("regr.svm")
almode<-regr.svm
almode<-"regr.svm"
getLearnerPackages(almode)
configureMlr(on.learner.error = "warn")
regr.task = makeRegrTask(id = "recc", data = training, target = "V1")
allmodel<-"regr.xyf"
mod<-train(allmodel, regr.task)
allmodel<-"regr.RRF"
mod<-train(allmodel, regr.task)
predicted.outcomes<-predict(mod, newdata=(testing))
p <- data.frame(predicted.outcomes,testing)
p
predicted.outcomes
p <- data.frame(predicted.outcomes[,2],testing)
p <- data.frame(predicted.outcomes[[2]],testing)
p
predicted.outcomes[[2]]
predicted.outcomes[2]
testing
predicted.outcomes[2]
predicted.outcomes[,2]
str(predicted.outcomes)
predicted.outcomes$data[,2]
p <- data.frame(predicted.outcomes$data[,2],testing[,1])
p
library(mlr)
library(mlbench)
configureMlr(on.learner.error = "warn")
regr.task = makeRegrTask(id = "recc", data = training, target = "V1")
mlrallmodels<-listLearners("regr")
for(allmodel in mlrallmodels[[1]]){#just before all models define d.f and reduce it
write.table(allmodel,file = "last algorithm tried.csv",  quote = F, row.names = F,col.names = F)
bad.models=c("DENFIS","neuralnet","partDSA","blackboost","bstSm","bstTree","penalized","brnn","gamLoess","ANFIS","FIR.DM","FS.HGD","nodeHarvest","mlpWeightDecayML","monmlp","mlp","mlpWeightDecay","mlpSGD","rbf","rbfDDA","rfRules","GFS.FR.MOGUL","mlpML","HYFIS","GFS.THRIFT" ,"GFS.LT.RS")
#too slow neuralnet# dnfis useless and just stops on huge datasets
if(allmodel %in% bad.models) {next()} #gamLoess crashes. the capitals are slow and terrible
library(caret) #mlp...s creat some bizzare problem that breaks caret::train ##nodeHarvest is SLOW ##"rbf"crash R "rbfDDA" crash train and really bad #rfRules is REALLY slow.##"pythonKnnReg",pythonKnnReg can not install
#penalized slow then fails
slow.models=c("leapSeq","glmStepAIC","ppr","qrnn")#leapSeq
if(allmodel %in% slow.models && datasource=="needles in haystack"){next()}#too slow for many columns
if(allmodel %in% slow.models && datasource=="needles hay noise"){next()}#too slow for many columns
slow.models=c("qrnn")
if(allmodel %in% slow.models){next()}#too slow for much cv
noNA.models=c("kknn")#leapSeq
if(allmodel %in% noNA.models && datasource=="sparsity NA"){next()}#too slow for many columns
seed.var=seed.var+1
list.of.packages <-getLearnerPackages(allmodel)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dep = TRUE)
if(length(list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])])){
write.table(paste("Fail","Fail","Fail","Fail","PackageFail",date(),allmodel,column.to.predict,trans.y,datasource,missingdata,withextra,norming,round(proc.time()[3]-when[3]),  sep = ","),
file = "gen test out.csv", append =TRUE, quote = F, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = F,
col.names = F, qmethod = "double")
next()}
when<-proc.time()
if(length(df.previous.calcs[,1])>0){
if(check.redundant(df=df.previous.calcs,norming=norming,trans.y=trans.y,withextra=withextra,missingdata=missingdata,datasource=datasource ,column.to.predict=column.to.predict,allmodel=allmodel)){next}}
not.failed=0
set.seed(seed.var)
try({mod<-train(allmodel, regr.task)
predicted.outcomes<-predict(mod, newdata=(testing))
p <- data.frame(predicted.outcomes$data[,2],testing[,1])
#Rsqd =(1-sum((p[,2]-p[,1])^2, na.rm = T)/sum((p[,2]-mean(p[,2]))^2, na.rm = T))
Rsqd=1-RMSE(p[,1],p[,2])/RMSE(p[,2],mean(p[,2], na.rm = T))
#mean.improvement=1-mean(abs(p[,2]-p[,1]), na.rm = T)/mean(abs(p[,2]-median(p[,2])), na.rm = T)
mean.improvement=1-MAE(p[,1],p[,2])/MAE(p[,2],mean(p[,2], na.rm = T))
p<- data.frame(predict(loess.model,predicted.outcomes),y.untransformed[-inTrain])
#RMSE=(sqrt(mean((p[,1]-p[,2])^2, na.rm = T)))
RMSE=RMSE(p[,1],p[,2])
#RMSE.mean=(sqrt(mean((p[,2]-mean(p[,2]))^2, na.rm = T)))
RMSE.mean=RMSE(p[,2],mean(p[,2], na.rm = T))
#MMAAEE=mean(abs(p[,2]-p[,1]), na.rm = T)
MMAAEE=MAE(p[,1],p[,2])
overRMSE=-1
if(replace.overRMSE==1){overRMSE=-1}
if(length(overRMSE)<1){overRMSE=-1}
#print(c(Rsqd,RMSE,overRMSE,date(),allmodel,column.to.predict,datasource,missingdata,withextra,norming,adaptControl$search,seed.const,adaptControl$method,tuneLength,adaptControl$number,adaptControl$repeats,adaptControl$adaptive$min,trainedmodel$bestTune))
write.table(c(round(mean.improvement,digits = 3),round(Rsqd,digits = 3),
round(overRMSE,digits = 3),round(RMSE,digits = 3),round(MMAAEE,digits = 3),
date(),allmodel,column.to.predict,trans.y,datasource,missingdata,
withextra,norming,RMSE.mean,adaptControl$search,seed.var,round(proc.time()[3]-when[3]),
adaptControl$method,tuneLength,adaptControl$number,adaptControl$repeats,
adaptControl$adaptive$min,trainedmodel$bestTune),
file = "gen test out.csv", append =TRUE, quote = F, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = F,
col.names = F, qmethod = "double")
print(date())
not.failed=1
})
if(not.failed==0) {
print(c("failed","failed",date(),datasource,missingdata,withextra,norming,allmodel))
write.table(paste("Fail","Fail","Fail","Fail","Fail",date(),allmodel,column.to.predict,trans.y,datasource,missingdata,withextra,norming,round(proc.time()[3]-when[3]),  sep = ","),
file = "gen test out.csv", append =TRUE, quote = F, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = F,
col.names = F, qmethod = "double")
write.table(paste("Fail",date(),allmodel,  sep = ", "),
file = "backup.csv", append =TRUE, quote = F, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = F,
col.names = F, qmethod = "double")
}
if(not.failed==1) {
write.table(paste("Succ",date(),allmodel,  sep = ", "),
file = "backup.csv", append =TRUE, quote = F, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = F,
col.names = F, qmethod = "double")}
}
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
getLearnerPackages(allmodel)
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
debugSource('C:/Users/Dm/Desktop/generated data test/MLR part.R')
predicted.outcomes<-predict(mod, newdata=(testing))
p <- data.frame(predicted.outcomes$data[,2],testing[,1])
try({mod<-train(allmodel, regr.task)
>
mod<-train(allmodel, regr.task)
predicted.outcomes<-predict(mod, newdata=(testing))
p <- data.frame(predicted.outcomes$data[,2],testing[,1])
Rsqd=1-RMSE(p[,1],p[,2])/RMSE(p[,2],mean(p[,2], na.rm = T))
mean.improvement=1-MAE(p[,1],p[,2])/MAE(p[,2],mean(p[,2], na.rm = T))
p<- data.frame(predict(loess.model,predicted.outcomes),y.untransformed[-inTrain])
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
library(mlrHyperopt)
devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
R.utils::gcDLLs()
install.packages("R.utils")
R.utils::gcDLLs()
devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
setwd("C:/Users/Dm/Desktop/generated data test")
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
install.packages("swirl")
source('C:/Users/Dm/Desktop/generated data test/MLR part.R')
library(mlrHyperopt)
R.utils::gcDLLs()
devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
install.packages("ParamHelpers")
devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
list.of.packages <- c("ParamHelpers","devtools")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, dep = TRUE)
devtools::install_github("jakob-r/mlrHyperopt", dependencies = TRUE)
devtools::install_github("berndbischl/ParamHelpers") # version >= 1.11 needed.
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
library(mlr)
library(mlbench)
configureMlr(on.learner.error = "warn")
regr.task = makeRegrTask(id = "recc", data = training, target = "V1")
library(mlrHyperopt)
res = hyperopt(regr.task, learner = "regr.svm")
hyper.control<-makeHyperControl(mlr.control = makeTuneControlRandom(maxit = 20),
resampling = cv3,
measures = mse)
res = hyperopt(regr.task, learner = "regr.svm")
res = hyperopt(regr.task, learner = "regr.svm", hyper.control =hyper.control)
res
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
source('C:/Users/Dm/Desktop/generated data test/Model Tester Quick .R')
